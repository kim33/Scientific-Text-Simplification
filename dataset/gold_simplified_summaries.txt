Efficient Parsing Of Highly Ambiguous Context-Free Grammars With Bit Vectors An efficient bit-vector-based CKY-style parser for context-free parsing is presented. Researchers presented a new deep learning model, a combination of convolutional and recurrent neural networks, that understands texts better. The new model, performing well with noisy data and long-range text dependencies, shows a 15% improved accuracy compared to traditional methods. The authors plan to continue on improving the model's efficiency and scalability, and highlight potential uses in machine translation, imformation extraction, and sentiment analysis (detecting opinions). Bit-vector operations, a type of computer operation, are used by parsers to do multiple parsing tasks simultaneously. This parser is particularly useful when every possible analysis is needed. Authors use the Biterbi algorithm  which is good at handling situations with many possible grammar interpretations.

A Monolingual Tree-based Translation Model for Sentence Simplification. In this paper, we look at sentence simplification as a type of translation where the complicated sentence is translated into a simple sentence. We suggest a Tree-based Simplification Model (TSM), which we believe, is the first statistical model that combines splitting sentences, removing unnecessary parts, changing the order of words, and replacing words all together. We also explain an effective way to train our model using a large set of matching data from Wikipedia and Simple Wikipedia. The testing shows that our model makes sentences easier to read compared to other systems. We use a tree-based simplification model which applies methods from statistical machine translation (SMT) with this data. Our data consists of paired articles from English Wikipedia and Simple Wikipedia. We suggest that simplifying sentences can be seen as a task of translating within the same language, aiming for an output that is easier to read but has the same meaning as the input.

Robust Sentiment Detection on Twitter from Biased and Noisy Data. In this paper, we propose a way to automatically identify moods in Twitter messages (tweets) by looking at how tweets are written and additional information about the words in these messages. We also use imperfect data from some websites that identify moods on Twitter as our training data. In our tests, we demonstrate that because our method captures a more general representation of tweets, it works better than previous methods and handles imperfect and biased data better. We introduce a two-step method to identify these moods in tweets using support vector machine (SVM) classifiers (a type of machine learning model) with general features.

Enhanced Sentiment Learning Using Twitter Hashtags and Smileys. Automated identification of different types of moods can be helpful for many natural language processing (NLP) systems like those that summarize reviews or analyze public media. In some of these systems, there is an option to give a mood value to a single sentence or a very short text. In this paper, we suggest a supervised way to classify moods using data from Twitter, a widely used microblogging platform. By using 50 Twitter tags and 15 smileys (faces showing emotions) as labels for moods, this method avoids the need for time-consuming manual labeling, allowing it to identify and categorize different mood types in short texts. We check how different features help in categorizing feelings and show that our method can successfully identify moods in sentences that aren't tagged. The accuracy of categorizing moods was confirmed by human judges. We also look at the connections and similarities between different mood types shown by smileys and Twitter hashtags. We used 50 hashtags and 15 emoticons (simple pictures expressing emotions) as rough labels to create a dataset for categorizing moods on Twitter.

D-PATR: A Development Environment For Unification-Based Grammars. We explain systems where Feature Structures (FSs) can be changed by default statements so that this characteristic does not always apply automatically.

Categorial Unification Grammars. Categorial unification grammars (CUGs) combine key features of both unification and categorial grammar systems. They offer an effective and consistent way to express language knowledge using familiar and widely accepted methods, making them useful for computer applications and language research. This paper will introduce the basic ideas of CUGs and give simple examples of how they are used. It will suggest that the methods and possibilities of CUGs make them worth further study in the larger field of research on unification grammars. The paper will also talk about how CUGs can handle certain language topics like long-distance dependencies (connections between parts of a sentence that are far apart), adjuncts (extra information in a sentence), word order (the arrangement of words in a sentence), and extraposition (moving parts of a sentence to a different place).

Statistical Approach To Language Translation. An approach to automatic translation is explained that uses statistical methods to gather information from large databases. The method relies on having large pairs of texts that are translations of each other. In our case, the texts are in English and French. Key to the technique is a detailed list (glossary) of matching phrases. The steps in the translation process are: (1) Break down the original text into set phrases. (2) Use the glossary and surrounding information to choose the matching set phrases to form the final sentence. (3) Arrange the words of the chosen phrases to form the final sentence. We have created statistical methods to help both making the glossary automatically and carry out these three translation steps, using aligned matching sentences in the two texts. Although we cannot yet show examples of French/English translation, we present promising initial results about creating the glossary and arranging word sequences. We develop the so-called IBM models, using basic actions like moving, copying, and translating applied to individual words in the original sentence. Generally, a statistical machine translation system has three parts: a language model, a translation model, and a decoder. The language model shows how likely a sentence is in the original language, the translation model shows how likely a target sentence is a translation of the original, and the decoder is what actually translates the original sentence.

Synchronous Tree-Adjoining Grammars Tree Adjoining Grammars (TAGs) have traditionally been used for modeling sentence structure, but their specialized design makes them less suitable for tasks like semantic interpretation or machine translation. To address this, we introduce Synchronous Tree Adjoining Grammars (Synchronous TAGs), a variant that captures structural correspondences between languages. This framework allows for aligning words and syntactic structures in one language with their logical meanings or translations in another, extending the application of TAGs beyond syntax alone. We illustrate the use of synchronous TAGs through practical examples and briefly discuss the computational challenges involved in interpreting them. While initially proposed for semantic analysis, synchronous TAGs also provide a foundation for bilingual translation by generating parallel syntactic structures—highlighting structural similarities across languages.

Parsing Strategies With 'Lexicalized' Grammars: Application To Tree Adjoining Grammars. In this paper, we introduce a general method for breaking down sentences (parsing) that came from creating a parsing algorithm similar to Earley's for Tree Adjoining Grammars (TAGs) and from recent studies on TAGs. In our method, basic sentence structures are linked with their main words (lexical heads). These structures cover larger areas than traditional grammars, allowing us to set rules about them. These rules either apply to the sentence structure itself or explain how it can connect with other structures. We explain when grammars based on simple rules can be turned into 'lexicalized' (broken down) ones without changing the original sentence structures. We claim that even if you expand simple grammars to include whole trees, just using substitution doesn't let you choose the main word of each structure freely. We demonstrate how using 'adjunction' (adding extra elements) allows us to easily create 'lexicalized' grammars from simple ones. We then explain how 'lexicalized' grammars come naturally from the larger areas covered by TAGs and discuss some language benefits of our method. We talk about a new general method for parsing 'lexicalized' grammars. First, the parser creates structures corresponding to the sentence, then it analyzes the sentence based on these structures. This method works with any language theory or grammar system, but we focus on TAGs. Since the number of trees needed for parsing a sentence is limited, any search method can be used. Particularly, a top-down method can be used as it avoids problems with repeating patterns. The parser can also use information from outside the immediate area to help with the search. We then describe how the Earley-type parser for TAGs can be changed to use this new method. Lexicalized grammars provide major benefits because the number of steps needed to break down a sentence is clearly limited by the sentence's length.

A Uniform Architecture For Parsing And Generation. The idea of using one set of language rules (grammar) for both analyzing (parsing) and creating (generation) sentences is attractive and has been considered by many researchers. In this paper, we explore a more daring idea: not only can one grammar be used by different processes working in different ways, but the same system for analyzing and creating language can be used to handle the grammar in different modes. Specifically, analyzing and creating language can be seen as two tasks performed by a single flexible system that figures out the logical meaning of the structure. We explain our current version of this system, which is designed to work for either task with language rules written in a specific format called PATR. Additionally, this system can be adjusted to match different methods of processing, including models for analyzing language that are designed to imitate human language understanding. These adjustments allow the system to work as efficiently as previous systems focused only on analyzing, but with much more ability to handle other tasks. We mention that to ensure all possibilities are covered when using a pre-prepared entry in the system's record, the entry must cover the formula being created from the top down.

Feature Structures Based Tree Adjoining Grammars We have introduced Feature Structure-Based TAG (FTAG) that integrates TAG, a linguistic framework known for handling complex syntactic dependencies, with feature structures, which allow for the unification of linguistic information. We show that FTAG provides greater expressive power and flexibility. Additionally, we investigate simplified variants of the system and propose potential grammatical constraints that can be incorporated. We outline a representation method for FTAG structures, building on the logical foundation of feature structures proposed by Rounds and Kasper. A Feature-based TAG is made up of basic tree structures (either auxiliary or initial) and involves two main ways to combine these trees: substitution, which means replacing one part with another, and adjunction, which means adding extra parts.

Word Sense Disambiguation With Very Large Neural Networks Extracted From Machine Readable Dictionaries In this paper, we present a method for automatically constructing very large neural networks (VLNNs) using definition texts from machine-readable dictionaries. We demonstrate how these networks can be applied to the task of word sense disambiguation (WSD): the use of machine-readable dictionaries and spreading and activation models. Our approach integrates two previously distinct methods in WSD research. By building these networks at scale, we are able to conduct experiments with realistically large neural architectures that model aspects of human language understanding. We employ traditional spreading activation techniques within these networks to infer the appropriate sense of a word based on its context.

Constraint Grammar as a Framework for Parsing Natural Language Many existing parsers are based on grammatical theories, such as Government and Binding, Generalized Phrase Structure Grammar, or Lexical-Functional Grammar, that were not originally designed for real-world sentence parsing. In contrast, the Constraint Grammar (CG) framework provides a practical, language-based approach to parsing that aligns more closely with actual language use. Rather than defining strict syntactic structures, CG uses constraints, rules grounded in linguistic observations, to guide parsing. These constraints help resolve ambiguity (when a sentence can be interpreted in multiple ways) by eliminating unlikely interpretations. While the system does not explicitly use probabilities, it incorporates probabilistic thinking implicitly by favoring more likely structures. A core principle of CG is to maximize the use of morphological information (such as word forms and grammatical features) in parsing. Words are first morphologically analyzed, and the parser—called the Constraint Grammar Parser (CGP), implemented in Lisp—then applies constraints to discard incorrect readings. This bottom-up process infers sentence structure from individual word tokens by systematically removing incompatible alternatives. The CG approach treats morphological disambiguation and syntactic labeling using the same mechanism: constraint-based elimination. This allows for a unified, flexible parsing strategy that can adapt to any natural language, supports real-world texts, remains separate from implementation code, and is efficient and relatively easy to configure—ideally through automated means. For these reasons, we advocate Constraint Grammar as a robust and scalable framework for natural language parsing.

Toward Memory-Based Translation A key challenge in example-based machine translation is effectively utilizing multiple translation examples to translate a single source sentence. This paper presents a method to address this issue through a mechanism called a matching expression, which represents how segments from various translation examples can be combined. The proposed translation process involves three stages: (1) generating a source matching expression from the input sentence, (2) converting this into a corresponding target matching expression, and (3) constructing the final target sentence based on the target expression. The system produces several possible translations, and to select the most appropriate one, it assigns a score to each option. This scoring is based on a composite distance metric that integrates structural similarity and lexical differences between examples and the input.

Typed Unification Grammars We present TFS, a computational system within the domain of logic-based frameworks, designed to support a robust method for categorizing linguistic information. At its core are typed feature structures—structured templates that encode grammatical information. The system employs a type hierarchy that follows an object-oriented paradigm, allowing properties to be inherited across types (multiple inheritance) and enabling the definition of relationships between different linguistic levels, conceptualized as classes. Beginning with a simple Definite Clause Grammar (DCG), we demonstrate how the type system can enforce general constraints and modularize grammar descriptions. We also show how extending these principles leads to a more expressive grammar formalism, resembling Head-driven Phrase Structure Grammar (HPSG). While this approach increases structural complexity, it offers greater descriptive power and modularity.

Automatic Processing Of Large Corpora For The Resolution Of Anaphora References Manually acquiring semantic constraints across broad domains is resource-intensive. This paper proposes an automated approach for identifying frequently co-occurring patterns within a large text corpus. These patterns often reflect underlying semantic constraints and can be leveraged to disambiguate unclear references and sentence structures. The approach builds on outputs from existing language processing tools. To evaluate its effectiveness, an experiment was conducted to resolve references of the pronoun “it” in randomly selected sentences. The results demonstrate that frequent co-occurrence patterns generally align with semantic constraints and can thus serve as a foundation for tools that enhance textual clarity. Specifically, we use the distributional context of pronouns to identify likely antecedents. This work presents one of the earliest methods to utilize word co-occurrence frequencies for pronoun resolution.

Word Identification For Mandarin Chinese Sentences Chinese sentences are composed of continuous sequences of characters without spaces to mark word boundaries. However, for meaningful linguistic analysis, words—rather than individual characters—serve as the fundamental unit. Therefore, the first step in processing Chinese text is accurate word segmentation. This task presents several challenges: (1) identifying complex word structures, such as combinations involving measure words, reduplications, or compound words; (2) correctly recognizing proper nouns; and (3) resolving the ambiguous segmentations, ambiguities in word boundaries. In this paper, we propose solutions to address these issues, focusing particularly on handling ambiguous segmentations. We apply a matching-based approach incorporating six practical segmentation rules, achieving a success rate of 99.77%. Among the methods evaluated, the forward maximum matching algorithm proved to be the most effective, and we recommend its use in practical applications.

Two-Level Morphology With Composition We recognize that the two-level model can be implemented by creating a cascade of composed Finite-State Transducers (FSTs), which are computational tools that map between a word's underlying lexical structure and its final surface spelling. We notice that the various sets of rule transducers can be merged with the lexicon transducer in a highly efficient way through this composition, and that the final transducer ended up being roughly similar in complexity and size to the original lexicon transducer by itself.

A Stochastic Japanese Morphological Analyzer Using A Forward-DP Backward-A* N-Best Search Algorithm This paper introduces a new method for analyzing Japanese sentences by segmenting them into words and assigning parts of speech. The approach uses a statistical language model and an efficient two-pass N-best search algorithm. This method is particularly suitable for written Japanese as it does not require delimiters between words. When trained and test on the ATR Corpus, the Japanese morphological analyzer achieved a 95.1% recall and 94.6% precision for open text. The authors also propose a novel way to search for the N best sets of word segmentations.

PRINCIPAR - An Efficient Broad-Coverage Principle-Based Parser This paper introduces PRINCIPAR, an efficient and broad-coverage principle-based parser for the English language. Implemented in C++ for SUN Sparcstations with X-windows, it features a lexicon of over 90,000 entries automatically built from machine-readable dictionaries using extraction and conversion rules. The authors also provide Minipar, a fast and robust parser designed for analyzing grammatical dependency relations.

A TAG-Based Noisy-Channel Model Of Speech Repairs This paper describes a noisy-channel model designed to identify and correct speech repairs in written transcripts. It utilizes a syntactic parser, which analyzes sentence structure, as its source model, and a new type of Tree Adjoining Grammar (TAG)-based transducer as its channel model. The use of TAG is based on the idea that the “reparandum,” or the original error, is a “rough copy” of the “repair,” or correction. The model was trained and tested using the Switchboard disfluency-annotated corpus, a collection of spoken language with marked errors. While noisy channel models generally perform well at disfluency detection, a log-linear re-ranker can further enhance performance. The proposed TAG system achieves a high EDIT-F score due to its explicit tracking of overlapping words between the reparanda and alterations (the corrected parts).

Dependency Tree Kernels For Relation Extraction This paper extends prior research on tree kernels to assess the similarity between dependency trees of sentences. Using this kernel within a Support Vector Machine, the authors detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles. The study examines the usefulness of various features, such as Wordnet hypernyms, parts of speech, and entity types, finding that the dependency tree kernel improves F1 score by 20% over a bag-of-words kernel (which simply counts word occurrences). To compare relations in two sentences, the researchers propose comparing the subtrees formed by the relation arguments by computing the node kernel between their lowest common ancestors (LCA) in the dependency tree. This dependency tree kernel is also applied to identify Named Entity classes in natural language texts.

Comlex Syntax: Building A Computational Lexicon This paper details the design of Comlex Syntax, a computational lexicon offering extensive syntactic information for approximately 38,000 English headwords. The authors discuss the types of errors that can occur during lexicon creation and methods for their measurement and control. This COMLEX syntax dictionary provides verb subcategorization information and syntactic paraphrases, but because these are indexed by individual words, they are not directly suitable for generating new sentences.

Recognizing Text Genres With Simple Metrics Using Discriminant Analysis This paper demonstrates a straightforward method for categorizing texts into predefined text genres using discriminant analysis, a standard statistical technique, with an application to the Brown corpus. Discriminant analysis allows for the use of numerous parameters – which can be specific to a particular text collection or information stream – to be combined into a few functions. In this process, parameters are weighted based on their effectiveness in distinguishing text genres. The authors also discuss its application in information retrieval, and they use word length as an indicator of formality for tasks like genre classification.

Discovering Relations Among Named Entities From Large Corpora This paper presents an unsupervised method for discovering relations among named entities from large corpora, which is highly beneficial for information retrieval, question answering, and summarization. Unlike prior methods that required expensive, large annotated corpora, this new approach clusters pairs of named entities based on the similarity of the context words between them. Experiments using a year of newspaper articles demonstrated high recall and precision in detecting these relations, along with the automatic provision of appropriate labels. The authors introduce a fully unsupervised Open Information Extraction (Open IE) system, which forms clusters of entity pairs sharing the same relations by analyzing their contexts and utilizes large corpora and an Extended Named Entity tagger to find novel relations and their participants.

Collective Information Extraction With Relational Markov Network Most information extraction (IE) systems independently process potential extractions, but considering the influences between them can enhance accuracy. This paper introduces a novel IE method that uses Relational Markov Networks, a generalization of Conditional Random Fields (CRFs), to represent arbitrary dependencies between extractions. This enables “collective information extraction,” which leverages the mutual influence among possible extractions. Experiments involving the extraction of protein names for biomedical text demonstrates the benefits of this approach. The authors also present Almed, a corpus specifically for evaluating protein-protein interaction (PPI) extraction systems.

A High-Performance Semi-Supervised Learning Method For Text Chunking This paper is about machine learning for text chunking, a process in which a system learns to break down long texts into smaller chunks in a way that is suitable for downstream tasks. Specifically, the paper investigates whether semi-supervised learning can help to improve downstream classification performance. In semi-supervised learning, unlabeled data is added to the training data in addition to the labeled data used in typical supervised learning. The authors introduce a new semi-supervised method using a learning paradigm they call structural learning. In structural learning, the unlabeled data is used to automatically generate labeled data for other tasks, for which a text chunking strategy is learned (these tasks are called auxiliary tasks in the paper). The hypothesis is that when a wide enough selection of auxiliary tasks and data are provided, the text chunking system will learn a common strategy or structure for how to divide texts into chunks for many different tasks. The authors further employ a technique called alternating structural optimization (ASO) and compute a low-dimensional linear approximation to the pivot predictor space for computational and statistical reasons. An evaluation on CoNLL’00 syntactic chunking and CoNLL’03 named entity chunking (English and German) shows that this approach leads to better results than previous approaches.

A Hierarchical Phrase-Based Model For Statistical Machine Translation This paper is about machine translation. Specifically, it introduces a statistical phrase-based translation model that uses hierarchical phrases (phrases that contain subphrases) and is an evolution from the traditional word (Brown et al, 1993) and phrase based (Koehn et al, 2003a) models. The model is technically a synchronous context-free grammar (which means that it captures grammatical rules that apply to both the source and the target language at the same time), but it is learned from pairs of aligned texts (bitext) without using any grammar rules. The model is built in such a way that the learned rules are weighted (in a way that is called a log-linear model) and that for each input multiple options are returned for a translation (using a method called the k-best parsing algorithm). The weights are chosen in such a way that they maximize BLEU scores, where BLEU is a common machine translation metric. This means it shifts to using formal grammar rules like syntax-based translation systems but without relying on predefined language rules. Another important detail is that the translations are constructed in parts, meaning that when the parts are combined to form the final translation it is possible that the transition is not smooth. To combat this, a syntactic match count feature is used to bias toward syntax-respecting translations, but not strictly enforce them. In an evaluation the model shows a 7.5% improvement compared to a state-of-the-art phrase-based system called Pharaoh.

Reading Level Assessment Using Support Vector Machines And Statistical Language Models This paper is about a method for automatically assessing reading proficiency, which is a fundamental component of language competency. Since finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers, this paper aims to automate that process. Previous work and pilot studies by the authors showed that statistical language models are a promising tool to accomplish this. The proposed method therefore uses support vector machines (SVM - a type of learning algorithm) to combine different features from traditional reading level methods, statistical language models, and other language tools to create a better way to evaluate reading levels. The method combines a text classifier based on trigram language models (which looks at three-word combinations for each difficulty level), some features like average tree height from parsing (breaking down sentences into parts), and variables usually used to determine readability. Sentence structure features, such as the height of parse trees (a visual breakdown of sentence structure) or the number of passive sentences, are used to estimate reading grade levels.

Probabilistic CFG With Latent Annotations This paper introduces a method called "Probabilistic Context-Free Grammar with Latent Annotations", or PCFG-LA, a method for creating parse trees, which are structures used in understanding sentences. This method improves upon a basic version known as PCFG by adding latent variables to the non-terminal symbols used in the trees. Fine-grained rules for constructing these trees are automatically learned from a collection of example sentences by training the PCFG-LA model using an expectation-maximization algorithm. Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared. In experiments using the Penn WSJ corpus, the automatically trained model gave a performance of 86.6% F1 for sentences up to 40 words long, which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection. The authors use a markovized grammar to get a better results during decoding, but do not do this during training. They further right-binarize the tree bank data to construct grammars with only unary and binary productions.

Dependency Treelet Translation: Syntactically Informed Phrasal SMT This paper is about a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation. This method requires a source language dependency parser, target language word segmentation and an unsupervised word alignment component. The authors align sentences from a bilingual dataset, then project the sentence structure (from the source language) onto the target sentence. They extract translation pairs based on small connected parts of these structures, called treelets, and use them to train a model that learns how to order words correctly in the target language. An efficient decoder is introduced to use these treelet-based models, which are combined with conventional statistical machine translation models to improve performance. The system is trained on a large set of about 4.6 million sentence pairs from sources like bilingual books, dictionaries, and web texts. The results show that using these grammar fragments (treelets) helps improve the quality of phrasal translations.

Semantic Role Labeling Using Different Syntactic Views This paper is about a way to understand the roles of words in a sentence that show their meaning and role, a task called Semantic Role Labeling. The authors use a Support Vector Machine (SVM) which is a type of machine learning model, to do this labeling. They make their system better by doing three main things: 1) Adding extra information, like how words in the sentence are connected (called dependency parses). 2) Carefully picking and adjusting which information to use. 3) Combining results from different ways of analyzing the sentence’s structure. They found that many mistakes were caused by errors in understanding the structure of the sentence. So, they used multiple structure analyzers — including Minipar, a simplified method, and one based on Charniak’s approach — to reduce those mistakes. They mixed different kinds of sentence analysis like: Phrase-structure: breaks the sentence into chunks or phrases. Dependency parsing: shows how each word relates to others. Shallow parsing: identifies only simple parts like noun or verb phrases. They use details about sentence parts and their functions to teach the machine how to label word roles better. By combining the results from different analysis tools, they give the system a fuller understanding of sentence structure and improve how it assigns meaning to words.

Earlier research used collections of similar texts in the same language to find and create paraphrases which are different ways of saying the same thing. In this work, we explain a method to measure how likely a phrase is to be a paraphrase. This helps us sort and organize paraphrases found in bilingual text collections, based on how likely they are to be translations of the same phrase. We also show how this method can be improved by taking into account the words that come before and after the phrase (context). By using techniques from computer programs that translate languages, we show how paraphrases in one language can be found by using a phrase in another language as a middle step. This can be done using collections of texts in two different languages, which are more widely available than monolingual paraphrase data. We test our methods for finding and organizing paraphrases using a set of carefully matched word pairs, and we compare the quality of these paraphrases with ones found using automatic matching methods. Finally, we define how likely two phrases are paraphrases based on the chances of them being translated into the same phrase through all possible middle translations.

Using Emoticons To Reduce Dependency In Machine Learning Techniques For Sentiment Classification  Sentiment Classification is the task of figuring out whether a piece of text shows a positive or negative feeling from the writer about a topic. Traditional machine learning methods have been used for this and usually give good results, but they work best when the training data (the examples used to teach the computer) and the test data (new examples used to check what it learned) are about similar topics. This paper shows that it's also important for the data to be similar in subject and time. It presents early experiments using training data that includes emoticons (like smiley faces or frowns). These emoticons might help because they are not tied to a specific topic or time period. In sentiment analysis research, we use emoticons found in online posts to collect examples that are useful for training programs that detect emotions. We notice that when people include emoticons in their messages, they are actually giving clear emotional signals through their text.

Minimum Cut Model For Spoken Lecture Segmentation  We are working on a way to divide spoken lectures into sections without needing any prior training data. We treat this task like solving a puzzle, where we cut a graph (a structure that shows how sentences connect) into parts in the best way possible. Our method looks at the overall structure of the lecture instead of just focusing on small parts. It considers how well everything holds together over longer parts of the talk, rather than just nearby sentences. This broader view helps us segment lectures more accurately, even when there are errors in the speech-to-text conversion. We focus on finding the best places to divide the lecture by looking at how similar the sentences are to each other. We use a technique similar to cosine similarity, which is a way to measure how much two pieces of text are alike. Our goal is to find where new topics begin in the written version of the lecture. To test our method, we collected a set of lectures that were manually divided into sections by four different people, each using their own careful method.

Joint Learning Improves Semantic Role Labeling  Even though there has been a lot of progress in semantic role labeling (figuring out the roles words play in a sentence), most earlier methods used separate tools that worked on their own. Sometimes, these tools were combined with other models that sequence the labels using a method called Viterbi decoding.  However, this approach doesn’t match the linguistic idea that the main parts of a sentence (called the core argument frame) are connected and dependent on one another.  In this work, we show how to build a joint model that captures these connections. We use new features that describe how sentence parts interact, and apply them in advanced log-linear models (a type of mathematical model often used in machine learning).  Our joint system reduces errors by 22% for all arguments and 32% for core arguments, when compared to the best separate classifier—as long as perfect sentence structure information from PropBank (a linguistic resource) is used.  We present a combined approach to semantic role labeling and show that looking at the whole sentence structure—instead of each part separately—can significantly lower the error rate.  To make the training process faster and more efficient, we use a technique called decomposition. This means we train the model using only a subset of the data—specifically, examples that include a case marker (a word that shows the grammatical role of a noun).

Randomized Algorithms and NLP: Using Locality Sensitive Hash Functions for High-Speed Noun Clustering  In this paper, we explore how randomized algorithms can help solve the problem of handling very large amounts of data. We apply these algorithms to create lists of similar nouns using data from 70 million web pages.  We manage to reduce the running time from quadratic time (which grows very fast as the amount of data increases) to practically linear time (which grows much more slowly as the data increases), based on the number of elements we need to process.  We show that by using Locality Sensitive Hashing (LSH) which is a technique for quickly finding similar items, we can compute nearest neighbors (the most similar items) in O(nd) time. This is a way to describe the speed of the algorithm based on the number of data points (n) and their size or dimensions (d).  Our method achieves over 70% accuracy in finding synonyms, or words with similar meanings.

Multi-Engine Machine Translation Guided by Explicit Word Matching  In this paper, we introduce a new method for combining the results of different Machine Translation (MT) systems that are translating the same text. The goal is to create a final translation that is better than any of the original ones.  Our method treats each MT system as an independent source, meaning they don’t have to work together or be connected. We use a decoding process that looks at specific word matches, takes into account how confident each system is in its translations, and uses a three-word language model (which looks at how natural groups of three words sound together) to evaluate and rank different combinations of sentences made from the outputs of the original systems.  The best-ranked sentence is selected as the final translation.  We tested this method using several Arabic-to-English MT systems that are similar in quality, and we found that it significantly improved the translation quality.  We also propose a rule-based approach that allows for flexible word matching between different sentence versions during the combination process.

Bootstrapping Path-Based Pronoun Resolution  We present a method for figuring out what pronouns refer to by using paths through sentence structures. Through a simple learning process, we calculate how likely it is that a pronoun refers to a certain noun, based on the path between them in a sentence diagram (which shows how the sentence is structured).  These path patterns help us solve difficult cases of pronoun reference, and they also work well with traditional grammar rules used for pronouns. When these paths are highly connected, they also help us collect useful details about a noun’s gender (like male or female) and number (singular or plural).  We combine this statistical path information with well-known features in a machine learning system that resolves pronouns. This leads to major improvements in accuracy across several different datasets.  Using a large collection of automatically processed text, we take from each sentence a path showing the relationship between a pronoun and a possible noun it might refer to. These paths are built from a sequence of labeled points (such as words and grammatical links), and we gather statistical data from these paths to estimate how likely the pronoun and noun are linked.  We also find that learning gender information is the most important part of our system. Our method reaches top-level accuracy in identifying the gender of nouns, and we share the noun gender database we built online.  In our model, we use paths that include the base form of the middle words, but we replace the two ends of each path with general labels: “noun,” “pronoun,” or a special type for reflexive pronouns, instead of using exact words. This helps the system focus on the pattern, not the specific nouns or pronouns.

Discriminative Word Alignment With Conditional Random Fields In this paper we present a novel approach for inducing word alignments from sentence aligned data. We use a Conditional Random Field (CRF), a discriminative model, which is estimated on a small supervised training set. The CRF is conditioned on both the source and target texts, and thus allows for the use of arbitrary and overlapping features over these data. Moreover, the CRF has efficient training and decoding processes which both find globally best solutions.::: We show how a large number of highly predictive features can be easily incorporated into the CRF, and show that even with only a few hundred word-aligned training sentences, our model improves over the current best model with [alignment] error rates of 5.29 and 25.8 for the two tasks respectively. We apply this alignment model to both French-English and Romanian-English language pairs. We show how a large number of highly predictive features can be easily incorporated into the CRF, and demonstrate that even with only a few hundred word-aligned training sentences, our model improves over the current state-of-the-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively.

Extracting Parallel Sub-Sentential Fragments From Non-Parallel Corpora We present a novel method for extracting parallel sub-sentential fragments from comparable, non-parallel bilingual corpora. By analyzing potentially similar sentence pairs using a signal processing-inspired approach, we detect which segments of the source sentence are translated into segments in the target sentence, and which are not. This method enables us to extract useful machine translation training data even from very non-parallel corpora, which contain no parallel sentence pairs. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. This method enables us to extract useful machine translation training data even from very [non-parallel corpora], which contain no parallel sentence pairs.::: We first use the GI ZA++ with grow-diag-final-and heuristic to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words.::: We first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon.::: We perform phrase extraction by combining clean [alignment lexica] for initial signals with heuristics to smooth alignments for final fragment extraction. We first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. We use standard information retrieval together with simple word-based translation for cross-lingual information retrieval (CLIR), and extract phrases from the retrieval results using a clean bilingual lexicon and an averaging filter. We perform phrase extraction by combining clean alignment lexica for initial signals with heuristics to smooth alignments for final fragment extraction.

Reranking And Self-Training For Parser Adaptation. Statistical parsers trained and tested on the Penn Wall Street Journal (WSJ) treebank have shown vast improvements over the last 10 years. Much of this improvement, however, is based upon an steadily growing number of features to be trained on (typically) the WSJ treebank data. This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres. Such worries have merit. The standard "Charniak parser" checks in at a labeled precision-recall f-measure of 89.7% on the Penn WSJ test set, but only 82.9% on the test set from the Brown treebank corpus. This paper should allay these fears. In particular, we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%. Furthermore, use of the self-training techniques described in (McClosky et al., 2006) raise this to 87.8% (an error reduction of 28%) again without any use of labeled Brown data. Much of this improvement, however, is based upon an ever-increasing number of features to be trained on (typically) the WSJ treebank data.::: This paper should allay these fears.::: We successfully applied self-training to parsing by making use of available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation.

Espresso: Leveraging Generic Patterns For Automatically Harvesting Semantic Relations. In this paper, we present Espresso, a weakly-supervised, general-purpose, and accurate algorithm for harvesting semantic relations. The main contributions are: i) a method for exploiting generic patterns by filtering incorrect instances using the Web; and ii) a principled measure of pattern and instance reliability enabling the filtering algorithm. We present an empirical comparison of Espresso with various state of the art systems, on different size and types of text collections, on extracting various general and specific relations.::: Experimental results show that our use of generic patterns substantially increases system recall with small effect on overall accuracy.::: In the pattern creation step, our system computes a trust score for each candidate pattern based on the weighted pointwise mutual information, PMI, of the pattern with all instances extracted so far. Experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision. In the pattern induction step, our system computes a reliability score for each candidate pattern based on the weighted pointwise mutual information, PMI, of the pattern with all instances extracted so far. We induce specific reliable patterns in a bootstrapping manner for entity relation extraction. Our minimally-supervised Espresso algorithm is initialized with a single set that mixes seeds of heterogeneous types, such as leader-panel and oxygen-water, which respectively correspond to the member-of and sub-quantity-of relations in the taxonomy of Keet and Artale (2008).

Efficient Unsupervised Discovery Of Word Categories Using Symmetric Patterns And High Frequency Words. We present a novel approach for discovering word categories, sets of words sharing a significant aspect of their meaning. We utilize templates of high-frequency words and content words in order to discover pattern candidates.::: Symmetric patterns are then identified using graph-based measures, and word categories are created based on groups of closely connected words. Our method is the first pattern-based method that requires no corpus annotation or manually provided seed patterns or words. We evaluate our algorithm on very large corpora in two languages, using both human judgments and WordNet-based evaluation. Our fully unsupervised results are superior to previous work that used a POS tagged corpus, and computation time for huge corpora are orders of magnitude faster than previously reported. We show that pairs of words that often appear together in symmetric patterns tend to belong to the same class (that is, they share some notable aspect of their semantics).

Named Entity Transliteration With Comparable Corpora. In this paper we investigate Chinese-English name transliteration using comparable corpora, corpora where texts in the two languages deal in some of the same topics — and therefore share references to named entities — but are not translations of each other. We present two different methods for transliteration, one approach using sound-based transliteration, and the second using the temporal distribution of candidate pairs.::: We then propose a new score-sharing method that utilizes the co-occurrence of transliteration pairs within document pairs.::: This spreading method achieves further improvement over the best results from the previous step. Each of these approaches works quite well, but by combining the approaches one can achieve even better results. We then propose a novel score propagation method that utilizes the co-occurrence of transliteration pairs within document pairs. This propagation method achieves further improvement over the best results from the previous step. We compare names from comparable and contemporaneous English and Chinese texts, scoring matches by training a learning algorithm to compare the phonemic representations of the names in the pair, in addition to taking into account the frequency distribution of the pair over time.

Meaningful Clustering Of Senses Helps Boost Word Sense Disambiguation Performance. Fine-grained sense distinctions are one of the major obstacles to successful Word Sense Disambiguation. In this paper, we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense levels, namely the Oxford Dictionary of English.::: In our simplified task, the sense inventory was first clustered semi-automatically with each cluster representing an equivalence class over senses. We assess the quality of the mapping and the induced clustering, and evaluate the performance of coarse WSD systems in the Senseval-3 English all-words task. In our coarse-grained task, the sense inventory was first clustered semi-automatically with each cluster representing an equivalence class over senses. We present an automatic approach for mapping between sense inventories; here similarities in gloss definition and structured relations between the two sense inventories are exploited in order to map between WordNet senses and distinctions made within the coarser-grained Oxford English Dictionary. We argue that automatically creating new alignments is difficult because of word ambiguities, different granularities of senses, or language specific conceptualizations.

Correcting ESL Errors Using Phrasal SMT Techniques. This paper presents a pilot study of the use of phrasal Statistical Machine Translation (SMT) techniques to identify and correct writing errors made by learners of English as a Second Language (ESL). Using examples of mass noun errors found in the Chinese Learner Error Corpus (CLEC) to guide creation of an engineered training set, we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers. Our system was able to correct 61.81% of mistakes in a set of naturally-occurring examples of mass noun errors found on the World Wide Web, suggesting that efforts to collect matching sets of texts of pre- and post-editing ESL writing samples can enable the development of SMT-based writing assistance tools capable of repairing many of the difficult grammar and vocabulary issues found in the writing of ESL learners.::: We utilize phrasal Statistical Machine Translation (SMT) techniques to correct ESL writing errors and demonstrate that this training-heavy SMT method is very promising, but we also point out SMT approach relies on the availability of large amount of training data. We utilize phrasal Statistical Machine Translation (SMT) techniques to correct ESL writing errors and demonstrate that this data-intensive SMT approach is very promising, but we also point out SMT approach relies on the availability of large amount of training data.

