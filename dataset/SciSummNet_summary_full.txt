TnT - A Statistical Part-Of-Speech Tagger Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Unlike what others have said in scientific writings, we believe that a tagger using Markov models works just as well as other methods like the Maximum Entropy approach. A recent comparison even showed that TnT works much better for the tested text collections. We explain the basic model of TnT, the methods used for smoothing (making data less rough) and for dealing with unknown words. Additionally, we show results from tests on two text collections. We successfully automate the tagging of grammatical function tags (marking parts of speech) based on sentence structure, including phrase chunks (groups of words) and syntactic roles (word functions), trained with guidance from a German language database.

Sentence Reduction For Automatic Text Summarization We introduce a new system that automatically cuts out unnecessary parts from sentences taken from a document to make summaries. The system uses different types of information to figure out which parts of a sentence can be removed, like sentence structure knowledge, surrounding context, and data from a collection of examples written by experts. Cutting down sentences can greatly improve how clear and brief summaries are. We explore a new way to remove extra parts from sentences by using various types of information to decide which parts can be taken out. In our method, the choices about what to keep or remove in sentence summaries are not based on how often words appear, but instead on models that predict which parts of a sentence can be deleted, based on examples of sentences and their summaries.

Advances In Domain Independent Linear Text Segmentation This paper describes a method for breaking down text into parts, which is twice as accurate and over seven times faster than the latest technology (Reynar, 1998). Instead of comparing sentences directly, it uses their position within the nearby text. Boundaries, or breaks in the text, are found using a method that splits the text into groups. We create a fake dataset by joining together short pieces of text taken from the Brown corpus, which is a collection of texts used for research.

A Simple Approach To Building Ensembles Of Naive Bayesian Classifiers For Word Sense Disambiguation This paper talks about a method to figure out the meaning of words that uses a group (ensemble) of simple mathematical models called Naive Bayesian classifiers. These models look at words that appear around the target word in different-sized sections of text to help understand its meaning. Even though this method is simple, tests on common words like "line" and "interest" show that it can be just as accurate as the best methods out there. We use a group of eighty-one Naive Bayesian classifiers that examine the words surrounding the target word to help figure out its meaning.

A Maximum-Entropy-Inspired Parser We present a new parser, a tool for analyzing sentences, that breaks down text into a structure similar to the Penn Treebank, a well-known database. It achieves 90.1% accuracy for sentences with up to 40 words, and 89.5% accuracy for sentences with up to 100 words when tested on standard parts of the Wall Street Journal database. This is a 13% improvement in reducing errors compared to the best previous single-parser results on this data. The main technical advancement is using a "maximum-entropy-inspired" model, a method that helps in making predictions and handling data smoothly, allowing us to test and combine many different variables. We also show some partial results that demonstrate how different types of information affect the parser, including a surprising 2% improvement from guessing the word type before the main word in a sentence. Instead of using fixed rules, we suggest automatically identifying specific tags in the Penn database. Our parser works in two stages: the first stage is a refined Markov grammar (a model using up to three previous sentence parts as context), and the second stage is a detailed Markov grammar with extra details about the sentence structure.

An Unsupervised Method For Detecting Grammatical Errors We introduce a method that doesn't need supervision for finding grammar mistakes by figuring out what is wrong from edited collections of texts. The system was created and tested using essay-length answers to questions on the Test of English as a Foreign Language (TOEFL). The error-detecting system, ALEK, works with about 80% accuracy and 20% ability to find errors. We try to find errors based on the context—specifically a 2-word range around the word we are interested in, looking at small words that show relationships and part-of-speech tags, which tell what role words play in a sentence. We use a measure of mutual information (a way to find how much knowing one thing tells us about another) along with the simple count of word combinations. The grammar feature includes errors like incomplete sentences, wrong verb forms, and pronoun mistakes. We use mutual information and a statistical method called chi-square to find common contexts for a small set of targeted words from a large collection of correctly written texts.

Cut And Paste Based Text Summarization We present a cut and paste based text summarizer, which uses methods learned from studying human-written summaries. The summarizer edits chosen sentences by shortening them to remove unnecessary parts and combining them to form clear sentences. Our work includes a computer program that uses statistics to break down sentences and find out where parts of a summary come from in the original document, creating a matched set of summaries and articles that we used to build the summarizer. We first pick sentences, then take out repeated parts, and use (manual) rules to put them together into clear output. We manually study 30 human-written summaries and find that 19% of the sentences cannot be explained by just cutting and pasting from the original text.

Trainable Methods For Surface Natural Language Generation We introduce three systems for creating natural language sentences from data that can be trained using labeled text samples. The first two systems, NLG1 and NLG2, need text with only specific topic-related meanings, while the third system, NLG3, needs text with both meanings and information on sentence structure. All systems aim to create a correct language sentence from a given topic-specific meaning. NLG1 is a basic system that generates an entire sentence in one go using common phrase patterns, while NLG2 and NLG3 use complex statistical models to generate each word separately. NLG2 and NLG3 learn how to choose words and arrange them in order. We conduct tests to generate sentences about flights in the air travel topic. We use statistical models to guide the sentence creation process, considering meaning features not yet expressed. We use many templates to help form sentences. We present statistical models to learn how to order attributes and choose words for sentence creation from a structured list of feature-value pairs, focused on the air travel topic.

A Novel Use Of Statistical Parsing To Extract Information From Text Since 1995, a few statistical parsing methods have shown a big improvement in understanding sentence structure, as compared to the UPenn TREEBANK, which is a top standard for checking accuracy. In this paper, we talk about modifying a type of sentence structure analysis called a lexicalized, probabilistic context-free parser to pull out information and test this new method on the MUC-7 template parts and their connections. Our rule-based methods use several language rules to identify patterns in relationships. We believe that finding relationships is just a type of probability-based sentence analysis where sentence diagrams are enhanced to find all connections. We combine different tasks like identifying parts of speech (like nouns and verbs), recognizing names, extracting template parts, and finding relationships, into one model. We merge recognizing names, sentence analysis, and finding relationships into a single, jointly-trained statistical model that performs better on all these small tasks. Part of what we are adding with this work is to show that working on tasks together can be effective even when training them together isn’t possible because there is no data labeled for both.

Assigning Function Tags To Parsed Text It is generally understood that the usual non-terminal labels for parts of a sentence structure (like NP for noun phrase, VP for verb phrase, etc.) don't cover all the details about the structure and meaning one might want. For example, the Penn Treebank adds extra labels called 'function tags' to show roles and other information that aren't covered by the basic labels. We introduce a method using statistics to add these function tags, which works on text that already has basic labels. It gets an accuracy score of 87%, which goes up to 99% if we count choosing 'no tag' as correct. Instead of using fixed rules, we suggest automatically finding the Penn functional tags.

Using Semantic Preferences To Identify Verbal Participation In Role Switching Alternations We suggest a way to spot changes in how verbs are used when a certain type of noun (like a subject or object) appears in different roles in sentences. This method looks at how often certain words are linked together, using information from WordNet, which is like a big dictionary that shows how words are related. We compare these word linkages by seeing how similar they are. We tested this method on specific verb changes, like when a verb can mean causing something or doing something without a direct object, but it can work for other verb changes too and doesn’t need special knowledge beforehand. We use a special method called skew divergence, which is a type of comparison suggested by Lee in 1999, to see if a verb uses different sentence structures by comparing one noun position in a sentence to another, like comparing the subject of a sentence where there’s no direct object to the object position in a sentence with a direct object.

A Stochastic Parts Program And Noun Phrase Parser For Unrestricted Text Our part of speech tagger can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency. Our part-of-speech tagger performs not only part-of-speech analysis, but it also identifies the most basic types of noun phrases, which are mostly sequences of words like articles, adjectives, and main nouns, by placing brackets around them.

Applied Text Generation We break down the task of creating text into three steps: First, the text planner works with information about the purpose of the communication, the context of the conversation, and the meaning, and it creates a basic outline of the text's structure and content. Next, the sentence planner picks basic language tools. It sends a rough outline of words and sentence structure to the Realizer, which then adjusts word forms, adds necessary small words, and organizes the text into a final readable form.

A Practical Part-Of-Speech Tagger We present a way to create a part-of-speech tagger using a hidden Markov model, which is a mathematical model that helps predict sequences. This method allows for strong and accurate tagging without needing many resources. You only need a word list (lexicon) and some text that hasn't been labeled for training. The accuracy is over 96%. We explain how we set it up and made it run fast. There are three uses for tagging: recognizing phrases, figuring out the meaning of words, and assigning grammatical roles. Our model, which uses both labeled (annotated) and unlabeled texts, learns from both types. We use a statistical approach with the expectation-maximization algorithm, which is a method to find hidden patterns in data. We achieved very high accuracy (96% on the Brown corpus, which is a collection of text) for tagging without supervision by using Hidden Markov Models (HMMs) along with carefully crafted tag dictionaries and grouping similar items (equivalence classes).

A Simple Rule-Based Part Of Speech Tagger Automatic part of speech tagging is a field in computer language processing where using data analysis methods has been more successful than using fixed rules. In this paper, we introduce a simple rule-based part of speech tagger that automatically creates its own rules and tags with accuracy similar to data-based (stochastic) taggers. The rule-based tagger has several advantages over these taggers, including needing much less stored information, having a clear and small set of useful rules, being easy to improve, and being more flexible to use with different sets of tags, types of texts, or languages. Perhaps the most important contribution of this work is showing that using data analysis is not the only successful way for part of speech tagging. The fact that a simple rule-based tagger that learns its rules on its own can work so well should encourage researchers to explore rule-based tagging further, looking for better and more detailed sets of rule patterns and other variations on the simple but effective approach described below. Our rule-based POS tagging methods create rules from a set of example sentences and use these rules to tag new sentences. We also show that assigning the most common part of speech for each word gives a basic accuracy of 90%.

Termight: Identifying And Translating Technical Terminology We propose a semi-automatic tool, termight, that helps professional translators and terminologists find technical terms and their translations. The tool uses part-of-speech tagging (a way to identify words as nouns, verbs, etc.) and word-alignment programs to pick out possible terms and their translations. Although these programs aren't perfect, it's not very difficult for the user to separate the valuable information from the irrelevant. The algorithms used for finding terms focus on being thorough. Other methods might miss important but rare terms/translations. To make it easier for the user during the filtering process, the terms are shown in a helpful order, along with useful examples, in a user-friendly interface that requires minimal typing. Termight is currently being used by translators at AT&T Business Translation Services (formerly AT&T Language Line Services).

Does Baum-Welch Re-Estimation Help Taggers? In part of speech tagging using a Hidden Markov Model (a statistical method), the goal is to label words with their grammatical roles. Early models were trained using a dataset tagged by humans. Recently, Cutting et al. (1992) proposed that training can be done with a small dictionary and limited initial probability information, by using Baum-Welch re-estimation. This method updates the model automatically. This paper discusses two experiments to find out how much human-provided training data is necessary. The first experiment shows that starting with either word or transition probabilities is crucial for good results. The second experiment finds that Baum-Welch re-estimation can follow three different patterns. In two of these, re-estimation actually lowers accuracy instead of improving it. The applicable pattern depends on the quality of the initial model and how similar the tagged training data is to the data being tagged. Guidelines for effective re-estimation use are provided. The findings align with Merialdo (1994), but offer more detail on different model components' roles. We report accuracy rates of 75.49%, 80.87%, and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, using a set of 134 tags.

Three Heads Are Better Than One Machine translation (MT) systems currently can't deliver the best quality translations for free text, no matter which method they use. We believe MT quality will improve if different MT systems work together on the same text. In the latest Pan-gloss MT project, we gather translations from three engines, usually parts of sentences, into a chart. Since each MT system works separately, their results might be missing parts, disagree with each other, or repeat the same things. We use simple rules to judge the quality of each part and pick the best sequence of parts (the "best cover"). This paper explains how we combine these parts, showing the method and examples of its progress on real translations. It uses a method called dynamic programming to quickly compare the average scores of neighboring translation parts. The current system mainly works with help from a human. The translation delivery system and an editing helper are briefly explained, along with an initial look at how useful this method is. Individual MT engines will be discussed in other reports, so they are not explained here. We create the first multi-engine MT (MEMT) system by combining outputs from three different MT engines, using knowledge of how the engines work. We develop a system that builds a chart with translation parts from each input system and uses a method called a chart walk algorithm to find the best translation of the original sentence.

A Maximum Entropy Approach To Identifying Sentence Boundaries We introduce a model that can learn to find the end of sentences in plain text. Using a collection of text where the sentence ends are already marked, our model learns to decide if a dot (.), question mark (?), or slash (/) is really the end of a sentence or not. This learning process does not need any special rules, dictionaries, grammar tags, or specific subject area info. This means the model can be easily taught on any kind of English text and likely on any other language that uses the Roman alphabet. Its results are as good as or better than similar systems, and it is easy to adjust for different types of text. Our system, called mxTerminator, uses simple word details from before and after the potential sentence end.

A Non-Projective Dependency Parser We explain a useful tool for analyzing sentence structure with no limitations. This tool connects words and labels the connections based on their roles in the sentence. We start by discussing the older Constraint Grammar tool, which inspired many of our ideas. Then, we explain the main concepts of our new tool. Lastly, we test how well the tool works.

An Annotation Scheme For Free Word Order Languages We explain a method and a tool designed to create linguistically marked collections of texts for languages with flexible word order. Because the needs for this method are different from those for languages with fixed word order, we added several features that affect the design of the method. The final method shows a layered idea of language and makes only basic assumptions about how these layers relate to each other. We share the NEGRA collection, a manually analyzed set of German newspaper texts with around 20,000 sentences.

Nymble: A High-Performance Learning Name-Finder This paper introduces a method based on statistics to find names and other specific items in text, using a version of the hidden Markov model, which is a mathematical model used for prediction. We explain why this problem is important, our method, the model details, and the successful results. We created Nymble, a system that tags names in English and Spanish using a hidden Markov model, which learns from data to identify names in text.

Disambiguation Of Proper Names In Text Identifying when proper names appear in text and what they refer to can be tricky because one name can refer to many things, and one thing can have many names. We look into the types of confusion -- related to structure and meaning -- that make it hard to find proper names in text, and we explain the methods used to clear up these confusions in Nominator, a complete tool for recognizing proper names created at the IBM T.J. Watson Research Center. We use specially created rules and collections of knowledge to sort proper names into general groups.

A Fast And Portable Realizer For Text Generation Systems We introduce RealPro, a tool that helps generate text, and it's designed to be easily added to other systems. Our RealPro tool creates sentences or phrases in a language.

Automatic Extraction Of Subcategorization From Corpora We describe a new method and built system to create a subcategorization dictionary from text collections. Each entry in the dictionary shows how often different subcategorization types appear in English. An early test with 14 verbs that can be used in different ways shows that this method is as accurate as older methods, which only cover a small range of subcategorization types. We also show that using a subcategorization dictionary made with this system makes a parser more accurate by a noticeable amount. We use a set of grammar rules and an advanced tool to separate main sentence parts from additional information.

Exploiting A Probabilistic Hierarchical Model For Generation Previous random-based methods for creating language do not use a tree structure to represent sentence structure. While this might work well for some uses, others benefit from using as much information about sentence structure as possible, leaving only the parts that grammar doesn't decide to a random-based model. We show early results indicating that using a tree model from a collection of texts with sentence structure markers is better than using one without them, and that a random-based tree model with a custom-made grammar performs better than both. Our system, FERGUS, uses dependency structures (relationship between words) as inputs and creates XTAG derivations (detailed sentence structures) using a random-based tree model that is automatically learned from a marked-up text collection. The Fergus system uses a statistical tree model to choose likely trees and a word sequence model to rank the possible sentences made from the best trees.

Effects Of Adjective Orientation And Gradability On Sentence Subjectivity Subjectivity is a practical feature at the sentence level that is important for handling text in areas like pulling out information and finding information. We look at how changeable adjectives, adjectives with specific meanings, and adjectives that can vary in degree affect a simple tool that decides how subjective a sentence is, and we find that these adjectives are strong indicators of subjectivity. A new method that can be taught to statistically combine two signs of how much an adjective can vary is introduced and tested, enhancing current automatic methods for giving orientation labels. Unlike nouns, many adjectives are naturally subjective, and the number of adjectives in texts matches up with how people judge their subjectivity. We share a statistical link between the number of adjectives in a text and how people judge its subjectivity. We demonstrate that adjectives that can change in intensity are helpful for determining how subjective a text is.

The Automated Acquisition Of Topic Signatures For Text Summarization In order to produce a good summary, one has to find the most important parts of a given text. We explain a method in this paper for automatically creating topic signatures, which are groups of related words with importance levels, centered around main topics, and we show an example with a signature made from 6,194 texts from the TREC collection on 4 chosen topics. We discuss how topic signatures can be combined with ontologies (big systems of knowledge) and tested in an automated text summarization system. We first introduced topic signatures, which are terms related to a topic used for making summaries.

Automatic Acquisition Of Domain Knowledge For Information Extraction In creating a system that pulls out specific information (IE) for new types of events or relationships, a major job is figuring out all the different ways these can be written in text. Usually, this involves going through and sometimes labeling large amounts of text where these events are mentioned. This paper suggests a different method using a computer program called EXDISCO, which automatically finds important documents and event patterns from text that hasn't been labeled, starting with a few example patterns. We test EXDISCO by seeing how well the patterns it finds work compared to systems made by people for real extraction tasks. We suggest a method for learning how to find patterns from a few examples, which makes it much easier for developers and speeds up the process of gathering knowledge. We assume that documents with many already known patterns related to a specific IE situation will likely have more useful patterns. ExDisco uses a process called bootstrapping to find new patterns by using texts that haven't been labeled and some starting patterns as initial input.

More Accurate Tests For The Statistical Significance Of Result Differences Statistical significance testing of differences in values of metrics like recall, precision, and balanced F-score is a necessary part of empirical natural language processing. Unfortunately, we find in a set of experiments that many commonly used tests often underestimate the significance and so are less likely to detect differences that exist between different techniques. This underestimation comes from an independence assumption that is often violated. We point out some useful tests that do not make this assumption, including computationally-intensive randomization tests. Standard deviations for F scores are estimated with bootstrap resampling.

Statistical significance testing means checking if the differences in numbers like recall, precision, and balanced F-score are meaningful or just by chance. These numbers are used in natural language processing, which is a field that deals with how computers understand human language. However, we found that many common tests don't show how important these differences actually are, meaning they might miss real differences between methods. This mistake happens because these tests assume things work independently when they often don't. We suggest some better tests that don't make this mistake, including randomization tests, which are very detailed and take a lot of computing power. To figure out how much F-scores can vary, we use a method called bootstrap resampling.

A Comparison Of Alignment Models For Statistical Machine Translation In this paper, we present and compare different methods for matching words or phrases in statistical machine translation. We suggest evaluating how good a matching method is by comparing its results to a manually created match and explain a better way to make suitable reference matches. We also look at how different matching methods affect the quality of translations in a statistical machine translation system. To improve word sequence models in the Hidden Markov Model (HMM) based matching, we make these models depend on word categories.

Base Noun Phrase Translation Using Web Data And The EM Algorithm We consider here the problem of translating base noun phrases, which are simple noun groups like "big dog" or "red apple". We suggest a new way to do this. First, we search for possible translations of these phrases on the web. Then, we choose the best translation from these options using one of two methods we created. In one method, we use a group of simple decision-making tools called Naive Bayesian Classifiers, which are set up with a technique called the EM Algorithm (a way to find patterns in data). In the other method, we use TF-IDF vectors, which help find important words in documents, also set up with the EM Algorithm. Our tests show that our method is much better in terms of finding and correctly translating phrases than older methods that use current technology. In our approach, we create possible translations by putting together the translations of the smaller parts of the phrase and then rank them by how well they match the meaning in the original language.

Efficient Support Vector Classifiers For Named Entity Recognition Named Entity (NE) recognition is a task where names and numbers are pulled from documents and sorted into groups like person, organization, and date. It's important for pulling information from text and answering questions. We first show that using Support Vector Machines (SVMs) for NE recognition works better than older methods. But, standard SVM tools are too slow for this job. So, we introduce a way to make the system much quicker. This method can also be used for similar tasks like breaking text into parts and identifying word types. We also introduce a way to pick important features using SVMs and a faster way to train the system. We suggest a method called Kernel Expansion to change a complex classifier into a simpler, faster one. We introduce XQK (eXpand the Quadratic Kernel) to make the NE recognizer much faster.

A Graph Model For Unsupervised Lexical Acquisition This paper introduces a method that doesn't require labeled data for gathering word meaning from a text that is marked with parts of speech, using network methods. The network model is created by connecting pairs of words that have certain grammatical connections. We concentrate on the equal relationship between pairs of nouns that appear together in lists. A step-by-step grouping method using this part of the network achieves 82% success in a task of learning new words, compared to WordNet categories. The model naturally identifies different meanings in specific topics and text as separate parts in the network around a word with multiple meanings. We aim to find parts of the network that are more connected inside themselves than with the outside.

Identifying Anaphoric And Non-Anaphoric Noun Phrases To Improve Coreference Resolution We introduce a method using supervised learning (a technique where a computer learns from examples) to identify anaphoric noun phrases (words that refer back to something previously mentioned) and non-anaphoric noun phrases (words that don't refer back). This information is used to enhance a system that figures out which words in a text refer to the same thing. Our improved system performs better than the best existing coreference resolution systems from MUC-6 and MUC-7 (specific tests for measuring these systems) on their respective datasets, achieving F-measures (a combined measure of accuracy and completeness) of 66.2 and 64.0. We create a separate tool to identify anaphoricity (whether a word refers back or not) alongside the coreference model. We question the benefit of adding such tools, as we find they do not improve results and can even make them worse.

Concept Discovery From Text Broad-coverage lexical resources like WordNet are very helpful, but they often include many uncommon meanings while missing meanings specific to certain areas. We introduce a grouping method called CBC (Clustering By Committee) that automatically finds concepts from text. It starts by finding a set of closely related groups called committees that are well spread out in the space of similarity. The average of the committee members is used as the main feature of the group. We then assign items to the group they are most similar to. Checking the quality of these groups has always been tricky. We introduce a new way to evaluate them based on the editing distance, which is a way to measure how different two things are, between the groups we create and the categories taken from WordNet (the answer key). Our tests show that CBC works better than several well-known grouping methods in terms of quality. Mutual information (MI) is a measure from information theory, which is a study of how information is measured and communicated, and we used it in our method for grouping words.

Building A Large-Scale Annotated Chinese Corpus In this paper we discuss problems related to creating a large collection of Chinese language data. We try to answer four questions: (i) how to make the annotation process faster, (ii) how to keep the annotation quality high, (iii) what the corpus can be used for, and finally (iv) what future work we expect to do.

Learning Question Classifiers In order to correctly answer a factual question from a large collection of texts, one needs to understand the question well enough to identify some limits or rules it places on the possible answer. These limits might include figuring out the type or category of answer needed and suggesting different ways to find and confirm a potential answer. This paper introduces a machine learning method for classifying questions. We create a step-by-step classifier that uses a structured guide of answer categories, eventually sorting questions into detailed groups. We show precise results using a large set of free-form questions from TREC 10. We categorize a question into one of fifty types based on certain features in the question. We have developed a machine learning method using the SNoW learning system.

The LinGO Redwoods Treebank: Motivation And Preliminary Applications The LinGO Redwoods project is a starting activity for creating a new kind of treebank, which is a database of sentences analyzed for linguistic information. While there are already several medium to large treebanks for English and other major languages, the available resources have some limitations: (i) they only show one layer of information, either how phrases are structured or how words depend on each other, (ii) they don't go very deep into the language details, (iii) they have a fixed way of representing language that limits how information can be retrieved, and (iv) they don't change over time and tend to become outdated as language studies progress. LinGO Redwoods wants to create a new method for treebanks that is both rich in detail and flexible, allowing different levels of information to be accessed and regularly updated as language studies advance. Since October 2001, the project has been working on building this new type of treebank, developing basic tools to make and maintain it, and creating an initial set of 10,000 analyzed sentences to share publicly with the tools. We are developing the HPSG LinGo Redwoods Treebank. The Redwoods treebank was made to provide training material that helps combine statistical models to solve ambiguities with the precise meanings given by the ERG. The Redwoods treebank introduces new ways of treebanking that are dynamic and based on choosing between different possibilities in English.

Deterministic Dependency Parsing Of English Text This paper introduces a method for analyzing English sentences called a deterministic dependency parser, which uses memory-based learning and works quickly. When tested on a specific set of English sentences from the Wall Street Journal, the parser correctly connected words in sentences 87.1% of the time. Unlike older systems, this parser creates labeled diagrams that show how words depend on each other, using a mix of labels that show sentence structure and grammatical roles from a popular linguistic database. The parser's best accuracy for finding both the main word and its label was 86.0% using basic grammar labels (7 labels) and 84.4% using all possible labels (50 labels). We suggest a simpler version of a previous model by Yamada and Matsumoto that makes the process faster, from a slow quadratic pace to a faster linear pace. Our approach, which uses a quick decision-making process called shift/reduce classifier-based dependency parsing, is highly accurate and efficient.

Efficient Parsing Of Highly Ambiguous Context-Free Grammars With Bit Vectors An efficient bit-vector-based CKY-style parser for context-free parsing is presented. The parser creates a condensed representation (called a parse forest) of all the possible ways a sentence can be analyzed using large sets of grammatical rules and long sentences. The parser uses bit-vector operations, which are a type of computer operation, to do multiple parsing tasks at the same time. This parser is especially useful when every possible analysis is needed, not just the most likely one. We use the Viterbi algorithm, which is good at handling situations with many possible grammar interpretations.

The Importance Of Supertagging For Wide-Coverage CCG Parsing This paper explains the role of supertagging, which is like a label assigning process, in a CCG parser that uses a mathematical method to choose the best analysis. The supertagger makes the process more efficient by reducing the number of possible options the model needs to evaluate, which also makes training faster. It also significantly speeds up the parser. We demonstrate that you can achieve much faster speeds by closely linking the supertagger with the CCG grammar and parser. This is the first known successful attempt to combine a supertagger with a complete parser that uses a grammar created by a machine. We also cut down on options by using rules that limit how categories can be combined. The result is a very fast and accurate CCG parser compared to other systems that use similar complex rules. Our results show how the accuracy of supertagging is related to the overall accuracy of understanding sentence structure. We describe two mathematical models for CCG parsing: one for how sentences are built and another for understanding relationships between words. CCG parsing has two steps: first, the supertagger assigns likely categories to each word, and then a few rules, along with type-changing and punctuation rules, help build a structured representation using a method called the CKY algorithm. We suggest a way to combine the supertagger with the parser: at first, only a few categories are given to each word, and more are added if the parser can't find a complete analysis.

Confidence Estimation For Machine Translation We present a detailed study of confidence estimation for machine translation. Various methods for determining whether MT (Machine Translation) output is correct are investigated, for both whole sentences and words. Since the idea of correctness is not easy to understand in this context, different ways of defining it are proposed. We present results on data from the NIST 2003 Chinese-to-English MT evaluation. We introduce a sentence level QE (Quality Estimation) system where a chosen limit is used to classify the MT output as good or bad. We study sentence and word level features for translation error prediction.

Unsupervised Construction Of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources We look into methods that don't need human guidance for finding paraphrases, which are sentences with the same meaning but different words, from a collection of news articles grouped by time and topic from many online news sources. We use two methods: (1) simple string edit distance, which measures how different two sentences are by counting changes needed to make them the same, and (2) a heuristic strategy, which is a rule of thumb that pairs starting sentences, likely summaries, from different news stories in the same group. We assess both data sets using a tool that aligns words and a scoring method from translating languages. Results show that the edit distance data is cleaner and easier to match with an alignment error rate (AER) of 11.58% on a similar test set. For data from the heuristic strategy, both training sets perform similarly, with AERs of 13.2% and 14.7%. Looking at 100 sentence pairs from each set shows that the edit distance data misses some of the complex word and sentence changes typical in paraphrases. The summary sentences, though harder to match, keep more of the interesting changes useful for learning paraphrases. We introduce the Microsoft Research Paraphrase Corpus (MSRPC), which uses combined news stories from the web to learn both sentence and word alignments.

Language Model Adaptation For Statistical Machine Translation Via Structured Query Models We explore methods to adjust language models for machine translation without human guidance. The results from the translation process are turned into search requests at different levels of detail and used to find similar sentences from a huge collection of text in one language. Specific language models are then created from this gathered data and combined with a general model. Tests show big improvements when translating with these adjusted language models. We use a slightly different method by first creating a list of possible translations, then finding similar sentences in a text collection of the target language. We make specific language models by using machine translation results as search requests to find similar sentences in large text collections. We change initial machine translation guesses into search requests and find similar sentences from a large collection of text in one language.

:ORANGE: A Method For Evaluating Automatic Evaluation Metrics For Machine Translation Comparisons of automatic evaluation measures for machine translation are usually done on a large set of texts using statistical methods like Pearson’s correlation coefficient or Spearman’s rank correlation coefficient. These methods compare the scores given by humans and the automatic scores. However, these comparisons depend on human opinions about how good the translation is, looking at things like how accurate and smooth it is. Unfortunately, these human opinions can be unreliable and costly to get. In this paper, we introduce a new method called ORANGE, which allows us to evaluate automatic translation evaluation tools without needing extra human input, except for using a set of reference translations. We also present the results of comparing some existing automatic tools and three new ones using ORANGE. BLEU, a common evaluation tool, is adjusted (Lin and Och, 2004b) and it only looks at two-word matches because this aligns better with human opinions than using longer word sequences. Adjusted per-sentence BLEU was used as a way to measure similarity.

Improving A Statistical MT System With Automatically Learned Rewrite Patterns Current group-based statistical machine translation (MT) systems have two problems with word order: First, they don't have a way to handle changes in word order that cover different language phrases. Second, the order of words in the output doesn't follow natural language phrase breaks. To fix these problems, we suggest using automatically learned rewrite patterns to rearrange the words in the original sentences so they match the order of the target language. Our system combines different methods. The main model is statistical, but we also use rule-based parsers (tools that analyze sentence structure) in two ways - one during training to learn rewrite patterns, and the other while running to rearrange the original sentences. Our tests show a 10% improvement in the Bleu measure (a way to check translation quality). We explain a method for translating from French to English, where reordering rules are learned automatically. Our reordering rules are learned automatically by matching sentence structures in both the original and target sentences.

Part-Of-Speech Tagging In Context We introduce a new HMM tagger, a tool that labels words in a sentence, using the words around each word to make decisions. We test it in two different ways: without help (unsupervised) and with help (supervised). We also provide the first detailed comparison of unsupervised methods for labeling parts of speech, pointing out that previous results have not been consistent across different sets of texts or dictionaries. We find that the dictionary's quality greatly affects how accurate the labeling is, so we offer a way to train HMMs that makes the labeling more accurate when the dictionary is unstable. Lastly, we demonstrate that this new tagger achieves top results using a simpler method that doesn't require extensive training. While repeating previous tests, we find that success relies on cleaning up the tag dictionaries with helpful data from the text. We show that the expectation maximization algorithm, a method for improving accuracy in tagging, works well with HMMs when we have a dictionary and certain favorable conditions. We notice that past high results in unsupervised HMM-EM tests were due to using improved dictionaries, where only common word analyses were included.

Chinese Segmentation And New Word Detection Using Conditional Random Fields Chinese word segmentation is a challenging and important problem that involves breaking down sentences into words. This paper shows how a method called linear-chain conditional random fields (CRFs) can effectively handle Chinese word segmentation by providing a structured approach that easily incorporates specialized knowledge using various lists of characters and words. We also introduce a method for finding new words, which helps improve accuracy. Our system is tested on four sets of data from a recent Chinese word segmentation competition, achieving top-level results. The effectiveness of CRFs for processing Chinese information is also shown in word segmentation. CRF is a statistical tool for analyzing sequences, introduced by Lafferty et al (2001), and we use it for Chinese word segmentation by treating it as a task where you make yes-or-no decisions. We apply this method by labeling each character as either the start of a new word or a continuation of the current word. We define the word segmentation problem by deciding if each character is the start of a new word or continues from the previous character.

Question Answering Based On Semantic Structures The ability to answer hard questions written in everyday language depends on (1) how detailed the meaning (semantic) information is and (2) the reasoning methods that this information supports. In this paper, we describe a question-answering (QA) system where questions are examined and possible answers are generated by 1) identifying the main parts of sentences and the meaning behind them from the input, and 2) using these extracted relationships to make informed guesses within a specific topic or situation. A new feature of our system is a flexible and clear way of showing actions and events using Coordinated Probabilistic Relational Models (CPRM), which is a method to predict outcomes based on relationships. In this paper, we share how well the system can make different types of informed guesses based on probability and time to find answers to difficult questions. The results show better accuracy compared to the best current QA systems. We look into how understanding the meaning behind words helps in answering questions. We show that answering questions can benefit from wide-ranging understanding of meanings. Our QA system uses PropBank/FrameNet labels as input, which help identify what actions are being described and with what details, and then it gives an answer by using models that predict actions based on chance and reasoning.

Towards Terascale Semantic Acquisition Although there is a lot of text data available for free, many natural language processing (NLP) methods use only a small part of it. In this paper, we look at the difficulties of working with very large amounts of data, called terascale. We introduce a method designed for handling terascale data that identifies "is-a" relationships (like "a dog is a mammal") and performs similarly to a very advanced language-based method. We examine how accurate these two systems are, based on how long they take to process and the size of the data set. We suggest a similar, very expandable method using an edit-distance technique (a way to measure how different two pieces of text are) to learn patterns of words and parts of speech (POS), demonstrating both good results and efficiency. We improve the process of finding "is-a" relationships on a large scale and automatically find broader category patterns using minimal text differences. We suggest a pattern-based method for finding "is-a" relationships and compare it to a basic method that uses how often words appear together in sentences (referred to as syntactic co-occurrence in their paper).

Characterising Measures Of Lexical Distributional Similarity This study looks at how the closest related words to a given word change depending on how we measure similarity. We find that one type of change is how often these related words appear compared to the main word. We then show a three-way link between how often similar words appear, a broad idea of word usage, and a specific word meaning relationship called hyponymy (where one word is a more specific version of another). Finally, we examine how this affects a method used to judge how words work together in phrases. By looking beyond specific examples, we try to find patterns and language features that affect how well similarity measures work. We also discovered that how often words appear was important in deciding which word was more general, with the more general word often appearing more. We studied how the closest related words to a word change with different ways of measuring similarity. We aimed to improve the goal of similarity to predict if one word is a broader or narrower version of another.

Wide-Coverage Semantic Representations From A CCG Parser This paper explains how to create meaning-based structures (semantic representations) from the results made by a comprehensive CCG parser. Unlike the basic connection structures (dependency structures) the parser gives, these can be used directly to understand meaning (semantic interpretation). We show that well-made semantic representations can be created for more than 97% of the sentences in new sections of The Wall Street Journal (WSJ) text. We think this is a big step towards understanding meaning in a wide range of texts, which is a main goal in the field of Natural Language Processing (NLP). We introduce a method (algorithm) that learns word lists with meanings (CCG lexicons with semantics) but needs fully detailed CCG outcomes (derivations) in the training examples. We address the difficult task of creating broad meaning-based structures (broad-coverage semantic representations) with CCG, but we do not learn the word list (lexicon).

Semantic Role Labeling Via Integer Linear Programming Inference We present a system for identifying the roles words play in a sentence. The system uses a computer learning method with a decision-making process that uses math rules to include language and structure rules into its decisions. The system is tested on data from a specific competition in 2004 about identifying word roles and performs very well. We describe this task as labeling each part of the sentence one by one.

Determining The Sentiment Of Opinions Identifying sentiments (the emotional parts of opinions) is a tough problem. We introduce a system that, when given a topic, automatically finds the people who have opinions about that topic and the feeling or mood of each opinion. The system has a part for figuring out the mood of words and another for mixing those moods within a sentence. We test different ways of sorting and mixing feelings at both the word and sentence levels, with encouraging results. We attempt to find out the overall mood direction of a sentence by combining mood words in it. We begin with two lists of positive and negative starting words. We use WordNet (a large database of words) to find similar and opposite words to expand the two lists of positive and negative starting words.

Sentence Compression Beyond Word Deletion In this paper, we expand the task of sentence compression. Instead of just shortening a sentence by removing words or parts, as done before, we change it by using other methods like swapping words, changing the order, and adding new words. We introduce a new collection of texts that is suitable for our task and a special model that can handle differences in structure and word choices. The model uses a new way of creating rules, applies a language model to make the sentences flow well, and can be adjusted to fit different ways of measuring compression quality. Unlike earlier studies, we compress sentences using a mix of methods including word removal, swapping, adding, and rearranging based on a statistical model, similar to how we create paraphrases. We provide a model that can both shorten and rephrase single sentences, but it does not create summaries for entire documents. Our new methods give more insight into how people shorten sentences, but they don't always do better than methods that just cut out parts. We include changes, additions, and rearrangements that are automatically learned from matching texts. We introduce the first method that creates new sentences while shortening them.

A Classifier-Based Approach to Preposition and Determiner Error Correction in L2 English In this paper, we present a method to automatically find and fix mistakes with prepositions (like "in," "on," "at") and determiners (like "a," "an," "the") in writing by people learning English as a second language (L2). We show that we can teach computer models to identify the correct use of these words with 70.06% accuracy for prepositions and 92.15% accuracy for determiners in native (L1) English text, and we share initial results for finding errors in L2 writing. We also mention that fixing these errors automatically is often complicated by spelling mistakes.

Estimation of Conditional Probabilities With Decision Trees and an Application to Fine-Grained POS Tagging We present a HMM part-of-speech tagging method which works well for POS tagsets with a large number of detailed tags. It is based on three ideas: (1) breaking down the POS tags into attribute lists and breaking down the context-based POS probabilities of the HMM into a combination of attribute probabilities, (2) estimating the context-based probabilities using decision trees, which are like flowcharts for making decisions, and (3) using high-order HMMs, which means using more complex models that consider more context. In tests on German and Czech data, our tagger performed better than the best POS taggers currently available. Our detailed tag set contains about 800 tags.

Learning Entailment Rules for Unary Templates Most studies on learning entailment rules without supervision have focused on rules connecting templates with two variables, overlooking unary rules - which are rules connecting templates with just one variable. In this paper, we explore two methods for learning these unary rules without supervision and compare them to a method for learning binary rules. The findings show that the unary rules we learned perform better than the binary rules. Also, we introduce a new way to measure similarity for learning entailment called Balanced-Inclusion, which performs the best. We define a unary template as a template with one empty space for an argument and one action or description phrase. We detect unary template entailment using the similarity in how arguments are used. We explore two methods for learning unary rules without supervision (meaning between templates with one variable). In a study by Zhao et al. (2009), a method using a pivot approach to find similar phrases from bilingual texts was presented, while Callison-Burch (2008) improved the quality of finding similar phrases by ensuring that phrases and their similar phrases follow the same grammatical structure. Our method is different in several ways: they aim to find similar phrases, while we aim to find directional entailment rules; they use bilingual texts (using phrases in another language as a reference), while we use changes made to Wikipedia articles in the same language (benefiting from its growing content); the similar phrases they find are more like DIRT, while our approach focuses on finding rules for specific common patterns in entailment pairs not covered by other resources. We attempt to identify the entailment relationship between lexical-syntactic templates using a method called WeedsPrec, but noticed it often suggests unreliable relationships involving uncommon templates.

The Ups and Downs of Preposition Error Detection in ESL Writing In this paper we describe a method for finding preposition mistakes in the writing of people learning English. Our system is right 84% of the time (precision) but only finds about 19% of all mistakes (recall) in a large set of student essays. We also talk about the problem of grading and judging in this area by showing how using just one person to rate can make the system's performance look different. We present a sampling method to avoid some problems that make judging error detection systems difficult. We use data from the TOEFL test. We show that when two native English speakers take a fill-in-the-blank test focused on prepositions, they agree 76% of the time, showing that there are many situations where different prepositions can be used correctly. Our model is trained using word features.

A Uniform Approach to Analogies Synonyms Antonyms and Associations Recognizing analogies (like comparisons), synonyms (words with similar meanings), antonyms (opposite words), and associations (related words) seem like four different jobs, needing different language processing methods. In the past, these four jobs have been handled separately, using many different techniques. However, these four types are just a small part of all language meanings, and we can't make special methods for each one; we need a single way to handle them all. We suggest covering a wide range of these types using analogies. To keep this paper focused, we only look at including synonyms, antonyms, and associations. We introduce a teaching-based computer learning method using written examples to classify word pairs that work like analogies, and we show it can answer multiple-choice SAT analogy questions, TOEFL synonym questions, ESL synonym-antonym questions, and questions from psychology about words that are similar, related, or both. We suggest a simpler algorithm (a step-by-step problem-solving method) based on Support Vector Machine (SVM) for classifying analogies called PairClass. We believe many language tasks can be thought of in terms of analogical thinking, and we apply our PairClass method to various problems like SAT verbal analogy tests, identifying synonyms and antonyms, and telling apart words that are similar in meaning from those that are just related. We support the need for a single method for handling language meaning tasks using written examples.

Top Accuracy and Fast Dependency Parsing is not a Contradiction In addition to being very accurate, having short times for parsing (breaking down language structures) and training (teaching the system) are the most important features of a parser (a tool that analyzes sentences). However, these times are still pretty long. To find out why, we studied how a dependency parser (a tool that looks at how words depend on each other) uses time. We show that connecting features (important details) to their weights (importance) in the support vector machine (a type of computer model) is the main reason for long time usage. To fix this, we used the passive-aggressive perceptron algorithm (a method for training models) as a Hash Kernel (a smart way to organize and access data). The Hash Kernel makes parsing faster and considers the negative examples (unsuccessful cases) during training, which leads to better accuracy. We could make parsing and training even faster by using parallel (simultaneous) feature extraction and parsing. We believe the Hash Kernel and parallel processing can work well in other language processing tasks too, such as transition-based dependency parsers (another parsing method), phrase structure parsers (tools that understand sentence structures), and machine translation (converting text from one language to another). We show that the Hash Kernel speeds up parsing and improves accuracy by also using negative features. The Mateparser is a fast second-order dependency parser that looks at relationships between words like siblings and grandchildren (levels of word connections).

A Monolingual Tree-based Translation Model for Sentence Simplification In this paper, we look at sentence simplification as a type of translation where the complicated sentence is changed into a simple sentence. We suggest a Tree-based Simplification Model (TSM), which we believe is the first statistical model that combines splitting sentences, removing unnecessary parts, changing the order of words, and replacing words all together. We also explain an effective way to train our model using a large set of matching data from Wikipedia and Simple Wikipedia. The testing shows that our model makes sentences easier to read compared to other basic systems. We use a tree-based simplification model which applies methods from statistical machine translation (SMT) with this data. We explore using paired articles from English Wikipedia and Simple Wikipedia to guide the sentence simplification process with data. We suggest that simplifying sentences can be seen as a task of translating within the same language, aiming for an output that is easier to read but has the same meaning as the input.

Robust Sentiment Detection on Twitter from Biased and Noisy Data In this paper, we suggest a way to automatically find feelings in Twitter messages (tweets) by looking at how tweets are written and extra information about the words in these messages. We also use imperfect data from some websites that judge emotions on Twitter as our training data. In our tests, we demonstrate that because our method captures a more general idea of tweets, it works better than older methods and handles imperfect and biased data better, which is the type of data from these sources. We introduce a two-step method to sort the feelings in tweets using SVM classifiers (a type of machine learning model) with general features.

Enhanced Sentiment Learning Using Twitter Hashtags and Smileys Automated identification of different types of feelings can be helpful for many language processing systems like those that summarize reviews or analyze public media. In some of these systems, there is an option to give a feeling value to a single sentence or a very short text. In this paper, we suggest a supervised way to classify feelings using data from Twitter, a widely used microblogging platform. By using 50 Twitter tags and 15 smileys (faces showing emotions) as labels for feelings, this method avoids the need for time-consuming manual labeling, allowing it to identify and classify different feeling types in short texts. We check how different features help in classifying feelings and show that our method can successfully identify feelings in sentences that aren't tagged. The accuracy of identifying feelings was also confirmed by human judges. We also look at the connections and similarities between different feeling types shown by smileys and Twitter hashtags. We used 50 hashtags and 15 emoticons (simple pictures expressing emotions) as rough labels to create a dataset for classifying feelings on Twitter.

D-PATR: A Development Environment For Unification-Based Grammars We explain systems where Feature Structures (FSs) can be changed by default rules so that this characteristic does not always apply automatically.

Categorial Unification Grammars Categorial unification grammars (CUGs) combine key features of both unification and categorial grammar systems. They offer an effective and consistent way to express language knowledge using familiar and widely accepted methods, making them useful for computer applications and language research. This paper will introduce the basic ideas of CUGs and give simple examples of how they are used. It will suggest that the methods and possibilities of CUGs make them worth further study in the larger field of research on unification grammars. The paper will also talk about how CUGs can handle certain language topics like long-distance dependencies (connections between parts of a sentence that are far apart), adjuncts (extra information in a sentence), word order (the arrangement of words in a sentence), and extraposition (moving parts of a sentence to a different place).

A Statistical Approach To Language Translation An approach to automatic translation is explained that uses methods to gather information from large databases. The method relies on having large pairs of texts that are translations of each other. In our case, the texts are in English and French. Key to the technique is a detailed list (glossary) of matching phrases. The steps in the translation process are: (1) Break down the original text into set phrases. (2) Use the glossary and surrounding information to choose the matching set phrases to form the final sentence. (3) Arrange the words of the chosen phrases to form the final sentence. We have created statistical methods to help both make the glossary automatically and carry out these three translation steps, using aligned matching sentences in the two texts. Although we cannot yet show examples of French/English translation, we present promising initial results about creating the glossary and arranging word sequences. We develop certain models, using basic actions like moving, copying, and translating that work on individual words in the original sentence. Generally, a statistical machine translation system has three parts: a language model, a translation model, and a decoder. The language model shows how likely a sentence is in the original language, the translation model shows how likely a target sentence is a translation of the original, and the decoder is what actually translates the original sentence.

Parsing Strategies With 'Lexicalized' Grammars: Application To Tree Adjoining Grammars. In this paper, we introduce a general method for breaking down sentences (parsing) that came from creating a parsing algorithm similar to Earley's for Tree Adjoining Grammars (TAGs) and from recent studies on TAGs. In our method, basic sentence structures are linked with their main words (lexical heads). These structures cover larger areas than traditional grammars, allowing us to set rules about them. These rules either apply to the structure itself or explain how it can connect with other structures. We explain when grammars based on simple rules can be turned into 'lexicalized' ones without changing the original sentence structures. We claim that even if you expand simple grammars to include whole trees, just using substitution doesn't let you choose the main word of each structure freely. We demonstrate how using 'adjunction' (adding extra elements) allows us to easily create 'lexicalized' grammars from simple ones. We then explain how 'lexicalized' grammars come naturally from the larger areas covered by TAGs and discuss some language benefits of our method. We talk about a new general method for parsing 'lexicalized' grammars. First, the parser creates structures corresponding to the sentence, then it analyzes the sentence based on these structures. This method works with any language theory or grammar system, but we focus on TAGs. Since the number of trees needed for parsing a sentence is limited, any search method can be used. Particularly, a top-down method can be used as it avoids problems with repeating patterns. The parser can also use information from outside the immediate area to help with the search. We then describe how the Earley-type parser for TAGs can be changed to use this new method. Lexicalized grammars provide major benefits because the number of steps needed to break down a sentence is clearly limited by the sentence's length.

A Uniform Architecture For Parsing And Generation The idea of using one set of language rules (grammar) for both understanding (parsing) and creating (generation) sentences is attractive and has been considered by many researchers. In this paper, we explore a more daring idea: not only can one grammar be used by different processes working in different ways, but the same system for understanding and creating language can be used to handle the grammar in different modes. Specifically, understanding and creating language can be seen as two tasks performed by a single flexible system that figures out the logical meaning of the structure. We explain our current version of this system, which is designed to work for either task with language rules written in a specific format called PATR. Additionally, this system can be adjusted to match different methods of processing, including models for understanding language that are designed to imitate human language understanding. These adjustments allow the system to work as efficiently as previous systems focused only on understanding, but with much more ability to handle other tasks. We mention that to ensure all possibilities are covered when using a pre-prepared entry in the system's record, the entry must cover the formula being created from the top down.

Feature Structures Based Tree Adjoining Grammars We have combined Tree Adjoining Grammars (TAG), a type of grammar used in language processing, with a system that uses feature structures to unify information. This new system, called Feature Structure based Tree Adjoining Grammars (FTAG), simplifies the way dependencies and repetitive patterns are handled, which is a key aspect of TAGs. We demonstrate that FTAG can describe things more effectively than the original TAG system. We also explore simpler versions of this system and some possible language rules that can be applied. We provide a brief overview of a method to represent the structures used by this system, building upon previous work by Rounds and Kasper, which involved the logical setup of feature structures. A Feature-based TAG is made up of basic tree structures (either auxiliary or initial) and involves two main ways to combine these trees: substitution, which means replacing one part with another, and adjunction, which means adding extra parts.

Word Sense Disambiguation With Very Large Neural Networks Extracted From Machine Readable Dictionaries In this paper, we explain a method to automatically create very large neural networks (VLNNs) from the text of definitions in digital dictionaries and show how these networks help in figuring out the correct meaning of words. Our method combines two earlier, separate techniques for understanding word meanings: using digital dictionaries and spreading activation models (a way to mimic how the brain processes information). Building these VLNNs automatically allows us to run experiments with real-sized neural networks for understanding human language, which helps us understand how they work and how we might make them better. We use traditional spreading activation methods to help determine word meanings.

Constraint Grammar As A Framework For Parsing Running Text Grammars used in parsers (tools that analyze sentence structure) are often taken from grammar theories and practices not specifically designed for parsing. Parsers have been created for English using theories like Government and Binding Theory, Generalized Phrase Structure Grammar, and Lexical-Functional Grammar. We introduce a system for parsing that makes grammar rules closer to real sentences and addresses common problems like ambiguity (when a sentence can have more than one meaning). This system is based on language rules. It indirectly uses probabilities (how likely something is). Probabilities aren't directly included in the rules. The rules (called constraints) don't just define what a 'correct sentence' is. They are more flexible and focus on word structure, helping with the main job of parsing. Parsing here means figuring out the structure of a sentence from individual words and phrases. Constraints are based on extensive study of language use. They can represent strict rules or tendencies that involve some level of risk. Strict rules are preferred. All the constraints for a language make up a Constraint Grammar (CG) for that language. A CG is used by the Constraint Grammar Parser (CGP), which is a program written in Lisp (a programming language). The input for CGP is words that have been analyzed for their structure. One key idea is to use as much word structure information as possible for parsing. The structure is assigned directly through a dictionary, word structure analysis, and simple connections from word structure to sentence structure. The goal of the constraints is to eliminate as many wrong options as possible, ideally leaving a sentence with only one clear meaning. Another key idea is to handle both word structure and sentence labeling by removing incorrect options. A good parsing system should meet several needs: constraints should be rules, not step-by-step instructions, they should handle any real-world text (not just examples made by linguists), they should be separate from the program code they run on, the system should work for any language, it should be easy to set up (ideally as simple automated processes), and it should work efficiently. The CG system meets these needs. We suggest using the Constraint Grammar framework.

Toward Memory-Based Translation An essential problem of example-based translation is how to use more than one translation example for translating one original sentence. This paper proposes a method to solve this problem. We introduce a tool, called matching expression, which shows how parts of translation examples fit together. The translation process has three steps: (1) Create the source matching expression from the original sentence. (2) Change the source matching expression into the target matching expression. (3) Build the target sentence from the target matching expression. This system creates some translation options. To choose the best translation from these options, we define a score for each translation. We combine a way to measure how similar the structure is with a way to measure the difference in words to get an overall distance measure used for matching.

Synchronous Tree-Adjoining Grammars The special features of Tree-adjoining grammars (TAG) make it difficult to use them outside of their typical use in sentence structure, like for understanding meaning or translating languages automatically. We introduce a version of TAGs called synchronous TAGs that show connections between languages. This method is meant to link words and sentences in one language to their meanings in a logical form or to their translations in another language, allowing TAGs to be used for more than just sentence structure. We talk about using synchronous TAGs with real-life examples and briefly mention some computer-related challenges that come up when interpreting them. Synchronous Tree Adjoining Grammars is mainly introduced for understanding meaning but will also be suggested for translation later. A synchronous process for creating sentence structures in both languages shows how similar the sentence structures (trees) are between the two languages.

Typed Unification Grammars We introduce TFS, a computer system that is part of logic systems and includes a strong way to categorize information. Its main data structures are called typed feature structures, which are like organized templates. The type system supports an object-oriented method (a way of organizing information like objects) for describing language by providing a way to share properties (multiple inheritance) and a way to define relations between different language levels, treated as classes of objects. We explain this method starting from a simple DCG (a basic grammar model), and show how to use the type system to apply general rules and divide language descriptions into parts. We also explain how more general ideas lead to a grammar similar to HPSG (a complex grammar model). This approach can make the data structure a bit more complicated.

Automatic Processing Of Large Corpora For The Resolution Of Anaphora References Manual acquisition of semantic constraints in broad domains is very expensive. This paper presents an automatic method for collecting data on patterns that frequently occur together in a large collection of texts. To a large extent, these data show limitations based on meaning, and thus are used to clarify unclear references and sentence structure. The method was carried out by collecting data from the results of other language tools. An experiment was conducted to resolve the references of the pronoun "it" in sentences that were randomly chosen from the collection. Title results of the experiment show that in most cases, the frequently occurring data indeed reflect the limitations based on meaning and thus provide a basis for a useful tool to clarify confusion. We use the distribution of a pronoun's context to determine which possible previous words can fit the context. We present one of the earliest methods for using how often certain words appear together in resolving pronouns.

Word Identification For Mandarin Chinese Sentences Chinese sentences are made up of a series of characters without spaces to separate words. However, the basic unit needed to break down and understand a sentence is a word. So, the first step in processing Chinese sentences is to find the words. The challenges in finding words include (1) identifying complex words, like those that combine a word and a measure word, repeated words, words made from other words, etc., (2) recognizing proper names, (3) solving unclear separations between words. In this paper, we suggest possible solutions for these challenges. We use a matching method with 6 different practical rules to solve the unclear separations and achieve a 99.77% success rate. The data shows that the best matching method is the most effective approach. We suggest using the forward maximum matching method.

Two-Level Morphology With Composition We understand that using a series of combined FSTs (Finite State Transducers, which are tools that map between different sets of symbols) could achieve the two-level model. We notice that the sets of rules can be combined with the dictionary transducers efficiently, and the combined transducer ended up being about the same size as the dictionary transducer itself.

A Fast Algorithm For The Generation Of Referring Expressions We make previous methods for creating algorithms that produce referring expressions (ways to describe things) simpler, while also considering findings from psychology about language and real-world examples. The result is a simple algorithm that is easy for computers to handle, respects what people prefer, and works in many different situations. We explain what resources a computer system needs to use this algorithm, and describe how it was used in the IDAS system. We use general ideas about what makes certain features of objects stand out and rules about which words are used to describe basic characteristics to gradually choose words for a description.

Stochastic Lexicalized Tree-Adjoining Grammars The idea of stochastic lexicalized tree-adjoining grammar (SLTAG) is formally explained. The parameters of a SLTAG relate to the chance of combining two structures, each linked with a word. The features of SLTAG are special and new because it is sensitive to individual words (like models that predict word sequences or models that guess hidden states) and also structured in layers (like grammars that predict sentence structure). Then, two main methods for SLTAG are introduced: a method for calculating the chance of a sentence created by a SLTAG and a repeated process similar to the "inside-outside" method for figuring out the parameters of a SLTAG using a practice set of texts. Finally, we show how SLTAG allows creating a version of stochastic context-free grammars that focuses on words, and we share early tests showing some benefits of SLTAG over regular stochastic context-free grammars. In stochastic tree-adjoining grammar, the problem of not considering context is solved by giving probabilities to larger parts of the structure.

Word-Sense Disambiguation Using Statistical Models Of Roget's Categories Trained On Large Corpora This paper explains a program that figures out the meanings of English words in any type of text using statistical methods based on major groups from Roget's Thesaurus. Roget's groups act like rough categories for different ideas. The groups listed for a word in Roget's index often match up with different meanings; so, picking the most likely group helps us understand the word's meaning better. Choosing the groups is done by finding and giving importance to words that suggest each group when seen in context, using a method based on probability (Bayesian theory). Other statistical methods needed special text collections or examples labeled by hand for many words. Our use of group models avoids this problem of gathering knowledge, allowing us to train on any single-language text without human help. When applied to the 10 million-word Grolier's Encyclopedia, the system correctly figured out the meanings of 92% of the examples of 12 words with multiple meanings that have been studied before. We base our method on the idea that the meanings of words are suggested by the words around them. From the view of creating text, nearby words of a target word are produced by the target word's true meaning.

Automatic Acquisition Of Hyponyms From Large Text Corpora We explain a method to automatically find hyponyms (specific examples of broader categories) from any text. This method has two main goals: (i) to avoid needing pre-existing knowledge and (ii) to work with many types of text. We look for patterns in the text that are easy to spot, happen often, and clearly show the relationship we are interested in. We explain how to find these patterns and suggest that other word relationships can be found in the same way. Part of the method is tested, and the results help improve and review a large manually created thesaurus (a book of words grouped by meaning). We also suggest how this can help in areas like finding information. We identify pairs of hypernyms (general terms) and hyponyms (specific examples) from text using pattern-finding methods.

A Computational Model Of Language Performance: Data Oriented Parsing Data Oriented Parsing (DOP) is a method where instead of using abstract rules, previous language experiences stored in a database called a corpus are used to understand language. When the system analyzes new information, it tries to rebuild it using pieces from the existing database. Clarifying meaning happens as a natural outcome. DOP can be carried out using standard parsing methods. In this study, we examine very strong relationships between other types of statistical language structures. We demonstrate that standard context-free parsing methods can be used to create a collection of possible interpretations, known as a parse forest, for a sentence in DOP1.

Surface Grammatical Analysis For The Extraction Of Terminological Noun Phrases LEXTER is a computer program used to find technical terms. It takes a collection of French texts on any topic and creates a list of possible technical terms for an expert to check. LEXTER identifies these terms by looking at their structure in two main steps: analysis and parsing. In the first step, LEXTER uses a set of rules to find boundary markers in the text to analyze and pull out the longest possible noun groups. In the second step, LEXTER breaks down these long noun groups to find smaller parts that could be technical terms based on their grammar and position within the phrase. This article focuses on the type of analysis used, called surface grammatical analysis, and the experimental methods used to adapt the rules. We introduce a tool that finds the longest noun groups, which are mainly sequences of words like determiners (words like "the" or "a"), premodifiers (descriptive words before the noun), main nouns, and certain types of phrases or adjectives that come after the noun, from French texts for use in finding technical terms. Our approach depends entirely on language details, especially the grammatical characteristics of potential terms.

Part-Of-Speech Tagging With Neural Networks Text collections labeled with part-of-speech information are helpful in many areas of language research. In this paper, a new part-of-speech tagging method using neural networks (Net-Tagger) is introduced and its performance is compared to that of an HMM-tagger (a previous method from Cutting et al., 1992) and a trigram-based tagger (another method from Kempe, 1993). It is shown that the Net-Tagger works as well as the trigram-based tagger and better than the HMM-tagger. The accuracy of tagging has reached 95%, partly by using a very large amount of practice data.

A Stochastic Japanese Morphological Analyzer Using A Forward-DP Backward-A* N-Best Search Algorithm We introduce a new way to break down a sentence into words and label the words with their parts of speech (like nouns or verbs). It uses a statistical model of language and a fast two-step search method to find the best word options. This method doesn't need spaces between words, making it perfect for written Japanese. Our Japanese word analyzer correctly found 95.1% of the words it looked for and was accurate 94.6% of the time when tested on the ATR Corpus, a collection of texts. We suggest a way to find the top N best groups of words.

Comlex Syntax: Building A Computational Lexicon We explain how we created Complex Syntax, a computer-based dictionary that gives detailed grammar information for about 38,000 main English words. We look at the kinds of mistakes that can happen when making this dictionary, and how we can find and fix these mistakes. Our COMLEX syntax dictionary includes details about how verbs are used and different ways to say things in sentences, but because they are organized by words, it is not easy to use them for creating new sentences directly.

PRINCIPAR - An Efficient Broad-Coverage Principle-Based Parser We introduce an efficient English language parser that uses broad rules to understand sentences. The parser is built using C++ and works on SUN Sparcstations with a graphical interface called X-windows. It includes a word list (lexicon) with over 90,000 entries, created automatically by using rules to extract and convert data from digital dictionaries. We also offer MiniPar, a quick and reliable tool for analyzing grammatical connections between words.

Recognizing Text Genres With Simple Metrics Using Discriminant Analysis A simple method for sorting texts into set categories based on their type or style is shown using a statistical method called discriminant analysis, applied to a collection of texts known as the Brown corpus. Discriminant analysis helps to take many different features that might be unique to a specific collection of texts or information and turn them into a few important calculations, giving more importance to features that are better at identifying text types. This method is also talked about in relation to finding information. We use word length as a sign of how formal something is for tasks like identifying text types.

K-Vec: A New Approach For Aligning Parallel Texts Various methods have been proposed for aligning texts in two or more languages like the Canadian Parliamentary Debates (Hansards). Some of these methods create a list of words with their translations as a side effect. We introduce a different way to align texts called K-vec, which begins by figuring out this list of translations. For instance, it finds that the English word "fisheries" matches the French word "peches" by observing that "fisheries" appears in similar ways in English as "peches" does in French. K-vec does not rely on sentence breaks.

A Rule-Based Approach To Prepositional Phrase Attachment Disambiguation In this paper, we describe a new method based on a collection of texts (corpus) to clarify where prepositional phrases belong in a sentence, and show how well this method works compared to other methods based on text collections for this issue. We train a learning algorithm on 12,766 sets of four related items from the Wall Street Journal (WSJ). We use a supervised learning method, which means it learns from examples, and we use word types and meanings from WordNet (a large dictionary), achieving 82% accuracy on 500 randomly chosen examples.

Word Sense Disambiguation Using Conceptual Density This paper presents a way to solve the confusion of meaning for nouns and tests it automatically using the Brown Corpus, a large collection of texts. The method uses WordNet, a database that organizes nouns in a hierarchy, and measures how close concepts are to each other with a special formula called Conceptual Density. This automatic method doesn't need any manual input of words, tagging of text, or training process. The test results are automatically compared with SemCor, a version of Brown Corpus where meanings are already tagged. Our Conceptual Density (CD) is a flexible way to see how similar meanings are, without sticking to one level of the hierarchy. We use a formula that measures how close concepts are based on the shortest path between them, how deep they are in the hierarchy, and how many concepts are around them. To handle different distances between links, we suggest a measure of similarity (called conceptual density) that considers i) the path length, ii) how deep the points are in the hierarchy (deeper points are closer), and iii) how crowded the concepts are in their groups (more crowded groups are closer than less crowded ones).

Anaphora For Everyone: Pronominal Anaphora Resolution Without A Parser We present a method for figuring out what pronouns refer to (anaphora resolution) which is an improved version of the one developed by (Lappin and Leass, 1994). Unlike that earlier work, our method doesn't need a detailed analysis of the sentence structure (full syntactic parsing). Instead, with only a small drop in result quality, our method can work using just the basic information about parts of speech (like nouns, verbs, etc.) and some extra notes on the role each word plays in sentences. Tests show that our method can accurately resolve what pronouns refer to even in systems that don't use strong and reliable sentence structure analysis tools. We also suggest that resolving pronouns is part of figuring out what words in a conversation refer to.

Role Of Word Sense Disambiguation In Lexical Acquisition: Predicting Semantics From Syntactic Cues This paper talks about the problem of words having multiple meanings when gathering information from computerized resources to build large knowledge bases. We explain two experiments: one where we didn't consider different meanings of words, leading to only 6.3% accuracy in sorting verbs by meaning based on (Levin, 1993); and another where we considered different word meanings, achieving 97.9% accuracy. These experiments had two goals: (1) to confirm the main idea from (Levin, 1993) that the meaning of verbs and how they are used in sentences are predictably connected; (2) to show that by separating sentence patterns into groups that match different word meanings, we can improve our understanding of word meanings by 15 times. Finally, we demonstrate that we can effectively learn new word meanings by using a mix of online resources. We show that with perfect knowledge of sentence structures, verbs can be sorted into the right categories almost flawlessly.

Three New Probabilistic Models For Dependency Parsing: An Exploration After introducing a new O(n3) parsing algorithm for dependency grammar, we explore three different ways to make it random and unpredictable. We suggest (a) a lexical affinity model where words try to connect with each other, (b) a sense tagging model where words change unpredictably in their preferences, and (e) a generative model where the speaker develops each word's grammatical and meaning structure without thinking about how the listener will understand it. We also provide early test results from checking how well the three models work on parsing using annotated Wall Street Journal training text (from the Penn Treebank). In these tests, the generative model does much better than the others and performs about the same in assigning part-of-speech tags. The suggested parsing algorithm is capable of searching through all projective trees in O(n3) time.

Message Understanding Conference-6: A Brief History We have recently finished the sixth "Message Understanding Conference," which aims to support and assess research in pulling information from texts. MUC-6 brought in new ideas compared to earlier conferences, especially with different types of tasks being evaluated. We explain why we changed the format and talk a little about the evaluation results. We show that systems based on language rules for information extraction (IE) can work well in many situations. We also introduce tasks for recognizing and sorting names.

HMM-Based Word Alignment In Statistical Translation In this paper, we explain a new way to match words in statistical translation and share test results. The main idea is to base the matching chances on how word positions differ, not where they are exactly. To do this, we use a simple Hidden Markov model (HMM), which is a statistical model, similar to how it's used in speech recognition to match timing. Unlike speech models, this one doesn't require words to stay in order. We explain how the model works and test it on various sets of bilingual texts. We use a helpful feature that checks how well the words match in the original sentence.

Motivations And Methods For Text Simplification Long and complicated sentences can be difficult for current systems that rely on natural language (NL) input. These systems can benefit from methods that make such sentences simpler. To make a sentence simpler, we need to understand its structure to identify parts that can be separated. A parser, which analyzes sentence structure, could be used for this purpose. However, full parsing can be slow and may not work well with complex sentences. In this paper, we look at two alternatives to full parsing for simplification. The first method uses a Finite State Grammar (FSG) to create groups of nouns and verbs, while the second uses a Supertagging model to show how words depend on each other. We explore how these two ways of organizing input affect the simplification process. We introduce a two-step process: first changing a sentence into a syntactic tree (a diagram showing structure), then changing this tree into a new, simpler sentence. Our text simplification techniques are designed not only to help people with reading difficulties but also to assist NLP systems as a tool for preparing data.

Using Semantic Roles to Improve Question Answering Shallow semantic parsing, which is the automatic process of identifying and labeling parts of a sentence, has recently gained a lot of interest. Our study looks at whether using information about semantic roles (the parts words play in a sentence) can help improve question answering. We present a general method for finding answers that takes advantage of semantic role annotations, following the FrameNet model. We treat assigning semantic roles as a problem of finding the best solution in a two-part graph, and we see finding answers as a kind of matching in this graph. Tests on the TREC datasets show improvements compared to the best models available. We demonstrate that using basic semantic information in the form of predicate argument structures (PASs), which are relationships between verbs and their related words, helps automatically find the right answers to a given question. However, we also note that the current version of FrameNet doesn’t cover enough examples, which limits the potential improvement in performance.

What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA This paper introduces a method based on sentence structure to help answer questions, focusing on choosing the right sentence for short answers. Instead of adding sentence structure features to existing statistical models (as done before), we work with the idea that questions and their correct answers have a pattern of predictable changes in structure. We suggest using a probabilistic grammar, which is a structured set of rules, inspired by one used for translating languages (by D. Smith and Eisner, 2006). This grammar is adjusted using a mix of a strong model that doesn't rely on word meanings with an optional model influenced by word meanings. Our model learns flexible connections as an unseen factor during specific training. Tests using the TREC dataset show it performs much better than other advanced methods. We look into using a system called quasi-synchronous grammar (by Smith and Eisner, 2006) to create a clearer model for matching related parts of sentences, while still allowing for some flexibility. We use this grammar to connect all related paths between a question and an answer.

Improving Statistical Machine Translation Using Word Sense Disambiguation We demonstrate for the first time that adding predictions from a word sense disambiguation (WSD) system to a typical phrase-based statistical machine translation (SMT) model consistently makes the translation better across three different IWSLT Chinese-English test sets and also shows notable improvements on the larger NIST Chinese-English machine translation task. Importantly, it never harms performance on any test set, according to the BLEU score and all other eight commonly used automatic evaluation measures. Recent studies have questioned whether WSD systems are helpful for SMT. However, SMT still struggles with choosing the right words. In this paper, we tackle this issue by exploring a new way to combine WSD with an SMT system that handles phrases and multiple words together. Instead of using a traditional WSD system, we redefine the WSD task to suit the same phrase translation challenge faced by phrase-based SMT systems. Our findings provide the first known evidence that understanding word meanings is indeed helpful for SMT, even though some have argued otherwise. We give a machine translation system the WSD probabilities for a phrase translation as additional features in a log-linear model. We use detailed context features based on position, sentence structure, and nearby words to adjust the word choices for each sentence and help choose longer phrases. We use a top-notch WSD tool (which combines naive Bayes, maximum entropy, boosting, and Kernel PCA models) to determine the score of a phrase pair in context, allowing the phrase choice to fit the sentence more effectively.

Why Doesn't EM Find Good HMM POS-Taggers? This paper looks into why Hidden Markov Models (HMMs) estimated by Expectation-Maximization (EM) perform poorly as Part-of-Speech (POS) taggers. We find that HMMs estimated by EM tend to assign a similar number of word tokens (units of text) to each hidden state, while in reality, the distribution of tokens to POS tags is very uneven. This suggests using a Bayesian approach with a sparse prior (a method that favors certain distributions) to guide the estimation towards this uneven distribution. We explore Gibbs Sampling (GS) and Variational Bayes (VB) methods and show that VB reaches results faster than GS for this task and that VB greatly improves the accuracy of matching words to their correct POS tags compared to EM. We also show that EM performs almost as well as VB when the number of hidden HMM states is greatly reduced. Additionally, we note the high variability in the results of all these methods, and that they need more iterations (repetitions of the process) to get close to stable results than commonly expected. We demonstrate effective performance on unsupervised POS tagging (without labeled data, using a dictionary) using a Bayesian model.

V-Measure: A Conditional Entropy-Based External Cluster Evaluation Measure We introduce V-measure, a method to evaluate groupings of data using a concept called entropy, which is about measuring disorder. V-measure effectively addresses several issues found in older methods for judging data groupings, such as 1) relying too much on the specific method or data used to create groups, 2) only checking a part of the data, and 3) accurately assessing and balancing two important features of data grouping: homogeneity (each group should have items from only one category) and completeness (all items of a category should be in the same group). We compare V-measure with other well-known evaluation methods and show that it has several good qualities for judging how well data is grouped, using test data. Lastly, we use V-measure to evaluate two grouping tasks: organizing documents and organizing speech patterns. The F score is not good for comparing results when the number of groups differs. The V-measure is a way to measure how well data is grouped by calculating the average of homogeneity and completeness. A major drawback of the F-Score is that it doesn't look at the content of groups beyond the most common category.

Lexical Semantic Relatedness with Random Graph Walks Many systems, like those used for answering questions, summarizing multiple documents, and retrieving information, need strong ways to measure how related words are. Traditional methods that use a thesaurus to measure how similar word pairs are only look at one direct connection between the words in the thesaurus. In contrast, we suggest a new model that looks at all possible direct and indirect connections between two words in a whole network. Our model uses a random walk, which is like taking a random path, through nodes and links from WordNet (a large database of words) and statistics from texts. We treat this network like a Markov chain, which is a type of mathematical model, and calculate a specific distribution of results for each word using a modified PageRank algorithm (a method originally used to rank web pages). We rate how related a pair of words is using a new method called ZKL, which is better than current methods for some types of data. In our tests, this new way of measuring word relatedness aligns most closely with how humans judge word similarity, with a high accuracy of 90%. We use random walks over WordNet, which includes information like parts of a whole and dictionary explanations.

Online Learning of Relaxed CCG Grammars for Parsing to Logical Form We look at the challenge of teaching a computer to break down sentences into lambda-calculus, which is a way of representing their basic meanings, and we introduce a method that learns a type of grammar called weighted combinatory categorial grammar (CCG). A central idea is to add new, non-traditional CCG combinators that make parts of the grammar more flexible, such as allowing words to be in different orders or adding words, with these changes having learned costs. We also share a new method that updates the grammar as it learns, called an online algorithm, for creating a weighted CCG. When tested on ATIS data, our approach showed an 86% success rate in getting the exact correct meaning and a 95.9% success rate when a partial match was acceptable, which is over 5% better than a previous result of 90.3% reported by researchers He and Young in 2006. We create the ATIS dataset for teaching computers how to understand sentence meanings. This dataset includes features that pay attention to word choices and the structure of the logical meaning that is built. We include the usual methods for combining grammar parts, such as application, composition, and coordination, along with rules that help handle natural, unpolished text.

The Infinite PCFG Using Hierarchical Dirichlet Processes We present a flexible Bayesian model of tree structures based on a method called the hierarchical Dirichlet process (HDP). Our HDP-PCFG model allows the grammar's complexity to increase as more training data becomes available. Besides introducing a complete Bayesian model for the PCFG, we also create an efficient method for estimating the model's parameters. With simulated data, we accurately identify the correct grammar without needing to decide its complexity beforehand. We also show that our methods work well for large-scale parsing applications by proving their effectiveness in learning detailed grammars. We find that because the hidden variable grammars are not clearly simplified, the EM algorithm (a method for finding parameters) keeps adjusting to fit the training data and eventually starts fitting too closely to the specific data.

Large-Scale Named Entity Disambiguation Based on Wikipedia Data This paper introduces a big system that identifies and clarifies the meaning of names using information from a large online encyclopedia and web search results. It explains the method used to clarify the names and how information is taken from Wikipedia. By making sure the information from Wikipedia matches the information in a document and checking if the category tags for possible names agree, the system works accurately for both news stories and Wikipedia articles. We find that keeping the topic consistent between a possible name and other names in the text helps improve accuracy. We use context clues made of phrases and categories from Wikipedia. We introduce a set of data for checking names. We use the clear category information available in Wikipedia. For testing, we use 20 news stories from MSNBC with 642 names manually linked to Wikipedia and another 113 names without a matching Wikipedia link.

Tree Kernel-Based Relation Extraction with Context-Sensitive Structured Parse Tree Information This paper introduces a new method using tree structures to find relationships in data. It fixes two major issues with older methods in two ways. First, it automatically chooses the right part of the tree to look at by adding important information that was left out before. Second, it suggests a new way to look at tree parts by including the paths leading to them as important context. Additionally, the paper tests how well this new method works with another advanced method. Tests on specific data show that our new tree method is better at finding relationships than older methods and is also better than the best current method. It also shows that our method works much better than the best current simpler methods. Finally, it shows that using both our method and another method together works better because they help each other. Our combined method uses both detailed and simpler features from the data.

Chinese Syntactic Reordering for Statistical Machine Translation Syntactic reordering approaches are a useful way to manage differences in word order (the sequence of words) between original and translated languages in statistical machine translation (SMT) systems. This paper presents a method for changing word order to help translate from Chinese to English. We explain a set of reordering rules that take advantage of the regular differences in how words are arranged in Chinese and English. The new system is used before processing both training and test sentences, changing Chinese sentences to be more like English ones in their word order. We tested this reordering method using the MOSES phrase-based SMT system (a specific translation system by Koehn et al., 2007). The reordering method increased the BLEU score (a metric for evaluating translation quality) for the MOSES system from 28.52 to 30.86 on the NIST 2006 evaluation data. We also did several tests to examine how accurate and effective different types of reordering rules are. Our set of rules significantly reduces the total number of times rules are applied by about 60%, compared to a method based on sentence parts. Chinese order is different from English mostly in how clauses (parts of sentences) are arranged.

Online Large-Margin Training for Statistical Machine Translation We reached the best performance in statistical machine translation by using many features with an online large-margin training method. The millions of settings were adjusted using only a small test set with less than 1,000 sentences. Tests on translating from Arabic to English showed that a model trained with specific simple features did better than a regular SMT system with fewer features. We calculate BLUE scores, which measure translation quality, using a group of previously translated sentences. We noticed a risk of overfitting, which means the model might work well only on the test data, especially when there are differences in the topics of the text (like Arabic-English news articles).

Large Language Models in Machine Translation This paper discusses how using large statistical language models can improve machine translation. We suggest a shared system to train on up to 2 trillion pieces of text, creating language models with up to 300 billion groups of words (n-grams). This system quickly gives smooth probabilities for translating text in one go. We introduce a new, cheaper smoothing method called Stupid Backoff, which works well on large data and gets close to the quality of a more complex method called Kneser-Ney Smoothing as more data is used. We train 5-word (5-gram) models in English using different single-language text collections. In language models, we often have to remove rare words due to limited computer power, as the space needed for features (k grams) is huge, sometimes requiring limits even when using a shared system. To handle larger text collections with complex connections, we use shared language models that can expand more easily. Stupid Backoff is much easier to train and use in a shared system than a more complex method like Kneser-Ney. We found that doubling the training data from news articles improves results by about 0.5 BLEU points, a measure of translation quality. We used 1500 computers for a day to calculate how often word groups appear in 1.8TB of web data.

Factored Translation Models We introduce an improvement of phrase-based statistical machine translation models that allows for easy addition of extra information at the word level, like language labels or computer-generated word categories. In several tests, we demonstrate that using factored translation models improves translation quality, both in terms of automated scoring systems and in making sentences sound more naturally correct. Any method that applies language rules will need less data and will lead to more complete models with the same amount of information. We also suggest systems for using different word-level representations at the same time. We recommend closely incorporating detailed language information into the translation model, where the base form of a word and its grammar details are translated separately, and this information is combined to create the final translation. We expand the phrase-based model's way of representing a word from just a string of letters to a collection of features, allowing extra details like parts of speech and word structure to be linked with or to replace the word's basic form during the translation process. Factored translation models encourage a more data-focused method for handling word agreement, which is how words match in a sentence.

The CoNLL 2007 Shared Task on Dependency Parsing The Conference on Computational Natural Language Learning has a shared task where participants train and test their learning systems using the same data sets. In 2007, like in 2006, the shared task focused on dependency parsing, which is about understanding sentence structure. This year, it included a multilingual track (using multiple languages) and a domain adaptation track (adjusting to different topics). In this paper, we explain the different tasks and how the data sets were made from existing language databases called treebanks for ten languages. We also describe the different methods used by participants, show the test results, and give an initial look at these results. We find that languages with flexible word order and complex grammar are the hardest for dependency parsing. Languages with complex grammar bring new challenges because the top parsers (sentence analyzers) used for languages like English, which have a fixed word order and simpler grammar, do not perform as well in languages like Basque, Greek, or Turkish.

Single Malt or Blended? A Study in Multilingual Parser Optimization We explain a two-step process to improve the MaltParser system for ten languages in the multilingual section of the CoNLL 2007 shared task on dependency parsing, which is a way to understand sentence structure. The first step is to adjust a single-parser system for each language by fine-tuning parts of the parsing method, the feature model (which determines what information the parser looks at), and the learning method. The second step is to create a combined system that uses six different parsing strategies, using the best settings for each language. When tested officially, this combined system does much better than the single-parser system and gets the highest average score for correct links between words. We expand this two-step method into a three-step process where the parser and labeler create a list of the best sentence structures, which is then sorted again to find the best one. We note that the official results for Chinese had an error, and our system actually performs much better than reported. We create a left-to-right arc-eager parsing model, which means the parser reads from left to right and quickly connects words to their main words as soon as possible.

Experiments with a Higher-Order Projective Dependency Parser We present experiments with a model that analyzes sentence structure using detailed elements. Our model represents sentence structures with elements that include three types of relationships between words and their related words. We improve upon an existing parsing method by Eisner (1996) to fit our needs and train models using a method called averaged perceptron. Our experiments show that using more detailed information greatly improves accuracy in understanding sentence structures, but it requires a lot of time and computer memory. In a multilingual competition (CoNLL-2007), our system achieved the highest accuracy for English, and the second highest for Basque and Czech. We enhance the basic model to include a combination of scores for pairs of connected word links, creating a more advanced model. Our advanced models include relationships involving the main word, its parent, and grandparent. Our advanced method considers the word between the main and related word, as well as the link from related words to their grandchild. We introduce the first and last grandchild as elements in our model.

Improving Translation Quality by Discarding Most of the Phrasetable It is possible to make phrase-tables, which are used in Statistical Machine Translation, much smaller by using a method that checks how often phrase pairs appear together in a text with two languages. This method can save a lot of space (up to 90%) and does not lower the BLEU score, which measures translation quality. Sometimes, it even improves the BLEU score, but this is less noticeable if advanced phrase table techniques are used. We use a method called Fisher's exact test to do this. We remove translation pairs that are not statistically reliable.

Hierarchical Phrase-Based Translation with Suffix Arrays A big challenge in creating systems that automatically translate languages is finding a way to efficiently manage a very large set of translation rules. In phrase-based models, we can solve this problem by storing the learning data in memory and using a suffix array, which is a tool that helps quickly find and use these rules. Hierarchical phrase-based translation adds another layer of complexity because it deals with source phrases that have gaps. The usual methods for finding these phrases don't work, and the best available methods are too slow, taking several minutes for each sentence. We explain new methods for quickly finding phrases with gaps in hierarchical phrase-based translation, cutting down the time it takes by almost 100 times, making it possible to find phrases with gaps quickly. Our approach is based on looking for sequences of words that appear together using a suffix array and then combining these sequences to find phrases that don't appear together continuously.

A Topic Model for Word Sense Disambiguation We develop a version of a model called latent Dirichlet allocation with WORDNET (LDAWN), which is a system that helps understand word meanings without needing labeled examples, and treats word sense as something hidden. We create a method to help figure out meanings in a group of texts and learn which areas each word belongs in at the same time. By using the WORDNET structure, we incorporate the approach of Abney and Light (1999) into this model and show that automatically learned areas improve the accuracy of figuring out word meanings compared to other methods. We use topics found in documents with Latent Dirichlet Allocation (LDA) as clues to help understand word meanings. We explain a related model, LDAWN, for understanding word meanings that adds an extra layer of hidden elements Z, which affects how the system's parameters are set. We bring understanding of word meanings into the topic model framework.

Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles We introduce a method based on data for dependency parsing, using a variation of the LR algorithm. We enhance it with a best-first search, which is a way to find the most probable outcomes in a list of possibilities, for a more flexible way of parsing sentences. The parser makes decisions using a classifier, which is a tool that chooses actions based on specific features or characteristics of the current parsing situation. We use this parsing method on both parts of the CoNLL 2007 shared task, utilizing multiple models, which are trained using different techniques. In the multilingual part, we create three LR models for each of the ten languages and combine their results using a voting system called maximum spanning tree, which helps decide the best overall outcome. In the domain adaptation part, we use two models to work with data that hasn't been labeled in the new area we're studying, adding it to the labeled data from another area, in a process similar to co-training, where multiple learning methods improve each other. We combine co-learning, where models learn together, and active learning, where we only add data if two different parsers agree on it. We expand the usual straightforward method to include the best probable outcomes using a best-first search strategy.

Extracting Aspect-Evaluation and Aspect-Of Relations in Opinion Mining The technology of opinion extraction helps users find and study people's opinions spread across web documents. We define an opinion unit as four parts: the person giving the opinion, the thing being talked about, the specific part or feature of that thing being discussed, and whether the opinion is good or bad. We use this definition as the base for our opinion extraction task. We concentrate on two key parts of opinion extraction: (a) finding connections between the aspect (part or feature) and the evaluation (opinion), and (b) finding connections between the aspect and what it belongs to, using methods that mix context (surrounding information) and statistical clues (data patterns). Our tests on Japanese blog posts show that using context clues makes both tasks better. We study a labeled set of data for opinion expressions and notice that many opinions are used in different areas. We use a supervised learning technique, which is a method where a computer learns from examples, to find useful sentence structures as context clues.

Indirect-HMM-based Hypothesis Alignment for Combining Outputs from Machine Translation Systems This paper introduces a new way to align and combine results from different machine translation (MT) systems. It uses an indirect hidden Markov model (IHMM) to help match similar words and correct word order in the combined translations. Unlike usual hidden Markov models that are trained by finding the most likely patterns, the IHMM gets its information from various sources such as how similar words are in meaning, how they look, and a penalty for how far words are moved from their original place. The IHMM method works much better than the best existing model, called TER-based alignment, in our tests using NIST standard test sets. Our combined system using this method was the best in translating Chinese to English in a specific competition in 2008. We suggest using the IHMM model to align outputs from different systems two at a time.

Multilingual Subjectivity Analysis Using Machine Translation Although research in other languages is increasing, much of the work in subjectivity analysis (studying opinions or feelings in text) has been applied to English data, mainly due to the large number of electronic resources and tools available for this language. In this paper, we propose and evaluate methods to transfer a collection of resources for studying opinions across languages. Specifically, we try to use the resources available for English and, by using machine translation (automatic translation by computers), create resources for studying opinions in other languages. Through comparisons on two different languages (Romanian and Spanish), we show that automatic translation is a practical option for creating resources and tools for studying opinions in a new language. We show that machine translation can work well when expanding opinion analysis to different languages. We suggest that opinions are expressed differently in various languages due to word choice, formal versus informal language markers, etc.

Dependency Parsing by Belief Propagation We describe dependency parsing (finding relationships between words in a sentence) as a graphical model with a new feature of overall rules. We demonstrate how to use loopy belief propagation (BP), a straightforward and effective method for approximate learning and reasoning. As a parsing method, BP is both theoretically and practically fast. Even with more complex features or hidden elements, which would make exact parsing much slower or extremely difficult (NP-hard), BP only takes O(n3) time with a small added constant. Additionally, these features greatly improve accuracy compared to simpler first-order methods. Adding more features would only slightly increase the time it takes, rather than significantly. We can integrate common problem-solving algorithms within special-purpose factors to effectively control all variable setups. DEP-TREE is a global factor that connects to all Link (i, j) variables and contributes a factor of 1 only if the setup of Link variables creates a valid projective dependency graph.

Revisiting Readability: A Unified Framework for Predicting Text Quality We put together word choices, sentence structure, and how ideas are connected to create a model that can guess how people will judge how easy a text is to read. This is the first research to look at so many language factors and to show with evidence that how ideas connect in a text is closely linked to how good people think the text is. We demonstrate that certain simple measurements, usually thought to be related to readability, don't actually predict how easy a text is to read in our Wall Street Journal study. We also show that what predicts readability can change depending on the goal: whether it's to guess how easy a text is or to compare different texts. Our tests show that how ideas connect is the only feature that stays reliable in both tasks. We suggest a combined approach that uses word choice, sentence structure, how well words fit together, how topics are carried through, and idea connections to judge text quality. When judging readability for adults who are fluent in the language, how ideas connect becomes more important. Five people rated the overall quality of each article on a scale from 1 to 5. We find that the average number of words in a sentence and the average number of letters in a word do not really relate to text quality.

Syntactic Constraints on Paraphrases Extracted from Parallel Corpora We improve the quality of paraphrases (different ways to say the same thing) taken from parallel corpora (collections of text in two languages) by ensuring that phrases and their paraphrases have the same grammatical structure. This is done by analyzing the English text in the collection and changing the way we extract phrases to include phrase labels along with pairs from both languages. To keep a wide range of phrases that are not full sentences, we use complex grammatical labels. A manual review shows a 19% absolute improvement in paraphrase quality compared to the basic method. We demonstrate how the context of a sentence in one language can be used to enhance the quality of the paraphrases obtained. Human evaluators (people who judge quality) are asked to rate each pair of original and paraphrased sentences with two scores: Grammaticality (if the paraphrased sentence follows grammar rules) and Meaning (if the paraphrased sentence keeps the original meaning). A problem with methods that change phrases or acquire term variations is that many of the suggested variations are just larger parts of the original phrase. We automatically create a paraphrase dictionary (a collection of different ways to say things).

Forest-based Translation Rule Extraction Translation rule extraction is a key issue in machine translation, especially for systems based on language structure that need sentence structure trees from one or both languages. The current main method only uses the best single tree, which lowers the quality of rules due to mistakes in analyzing sentence structure. We suggest a new way that extracts rules from a packed forest, a method that efficiently represents many possible sentence structures. Tests show that this new method improves translation quality by more than 1 point on the BLEU scale, a measurement of translation accuracy, in a top-tier system that converts tree structures to strings. It is also 0.5 points better and twice as fast as using the top 30 best sentence structures. When combined with our earlier work on decoding using forests, it results in a 2.5 BLEU points improvement over the basic system and even beats the Hiero system, which is a well-known translation method, by 0.7 points. We use the Viterbi algorithm, a procedure for finding the most likely sequence of hidden states, to simplify the forest.

Online Large-Margin Training of Syntactic and Structural Translation Features Minimum-error-rate training (MERT) is a challenge for improving statistical machine translation because it can't effectively adjust many weights at once. Building on the work of Watanabe et al., we explore using the MIRA algorithm by Crammer et al. instead of MERT. We first show that by using parallel processing and making better use of the parse forest, we can achieve results with MIRA that are as good as or better than with MERT in terms of translation quality and cost. We then test the method on two types of features that fix problems in the Hiero model, which is a system that uses hierarchical phrases: first, we train many of Marton and Resnik's soft syntactic constraints at the same time; second, we introduce a new structural distortion model, which helps with word order changes. In both cases, we see big improvements in translation quality. By optimizing all these together, with a total of 56 feature weights, we improve performance by 2.6 BLUE points (a measure of translation quality) on some of the NIST 2006 Arabic-English test data. We add structural distortion features into a model that uses hierarchical phrases to better handle rearranging parts of a sentence based on the length of the source text. We show that MERT works well with a small number of features compared to MIRA, which can handle many features. Our feature specifically addresses over-counting of translation rules and problems with poorly matched or inserted words.

Cheap and Fast â€“ But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks Human language tagging is important for many tasks in natural language processing, but it can be costly and take a lot of time. We look into using Amazon's Mechanical Turk system, which is a much cheaper and quicker way to gather annotations from a wide range of non-expert paid workers online. We examine five tasks: recognizing emotions, finding word similarities, understanding if one text implies another, figuring out the order of events, and clarifying word meanings. For all five, we find that non-expert annotations from Mechanical Turk agree well with existing top-quality labels given by expert annotators. For recognizing emotions, we also show that using these non-expert labels to train computer programs can work just as well as using the best expert labels. We suggest a way to fix biases that greatly improves the quality of annotations for two tasks. We conclude that many large labeling tasks can be successfully planned and done using this method for much less money. We compare the quality of labels made by non-expert workers with those made by experts for different natural language processing tasks and find that only four responses per item were needed to match expert annotations. We show that getting many low-quality labels (through Mechanical Turk) can come close to high-quality expert labels. We use a system where the most common answer wins, and ties are broken randomly, and we report a match (accuracy) between this system and the expert standard of 89.7%.

Understanding the Value of Features for Coreference Resolution In recent years, there has been a lot of work on the important problem of coreference resolution, which is figuring out when different words in a text refer to the same thing. Most of this work has focused on creating new models and techniques for solving this problem. These studies often show that complicated models do better than simpler ones that just compare pairs of words. However, not much focus has been on how important it is to choose strong features, or characteristics, that help in learning a coreference model. This paper describes a fairly simple method that compares pairs of words for coreference resolution, using a carefully chosen set of features. We show that this creates a top-performing system that does better than systems using complex models. We suggest that our system can be used as a starting point for developing more complex models, which might not be needed if strong features are already being used. The paper also includes a study that removes one feature at a time to see which ones are most important. Our method takes time that increases with the square of the number of mentions, or references, in the text.

Bayesian Unsupervised Topic Segmentation This paper explains a new way using Bayesian methods for dividing topics without needing prior examples. Systems that don't need training often rely on lexical cohesion, which is how well words stick together in a topic. We show that this can be understood using Bayesian methods by treating words in a topic as if they come from a specific language model for that topic; getting the best fit in this model gives a good topic division. This is different from older methods that used manually created measures of cohesion. The Bayesian method also allows us to include other helpful features like cue phrases, which are strong hints about the structure of the conversation and were not used before in such systems. Our model shows consistent improvements over the latest systems on both written and spoken data. We also show that both a way of analyzing unpredictability (entropy) and a well-known previous technique can be seen as examples of the Bayesian method. We present a method for segmenting in a straight line. If the number of segments is known and only a straight-line structure is allowed, then just moving the border between segments is enough. We find the more detailed model is helpful for meeting notes but not for a textbook.

A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers There is increasing interest in using Bayesian methods for Natural Language Processing (NLP) challenges. There are several different methods (estimators) for Bayesian models, and it's helpful to know which tasks each method is good at. This paper looks at different Bayesian methods for Hidden Markov Model (HMM) POS taggers, which use different numbers of hidden states, on data sets of various sizes. Recent studies have shown mixed results when comparing Bayesian methods to a technique called Expectation Maximization (EM) for unsupervised HMM POS tagging. We demonstrate that the difference in these findings is mainly due to the size of the training data and the number of states in the HMM. We examine different sampling methods for HMMs, including some not explored in earlier research. We discover that all Gibbs samplers work well with small data sets and few states, while Variational Bayes performs well on large data sets and matches the performance of Gibbs samplers. In terms of speed, Variational Bayes was the quickest of all methods, especially on large data sets, and the detailed Gibbs sampler (both pointwise and sentence-blocked) was typically faster than their simplified versions on large data sets. We use three evaluation methods: M-to-1 and 1-to-1 measure tagging accuracy with different mapping strategies; VI is a method that doesn’t rely on mapping. We create a many-to-one linking of state identifiers to Parts of Speech (PoS) tags from one half of the data and test on the other half, known as cross-validation accuracy. We show that using sparse priors can improve accuracy by 4% (from .62 to .66 with a 1 million word data set) in cross-validated many-to-one accuracy.

A Tale of Two Parsers: Investigating and Combining Graph-based and Transition-based Dependency Parsing Graph-based and transition-based methods for dependency parsing look at the problem in different ways, each having its own benefits and drawbacks. We examine both methods using a technique called beam-search. By creating a graph-based and a transition-based dependency parser, we demonstrate that beam-search is a strong option for both techniques. More importantly, we introduce a new parser that uses beam-search to combine both graph-based and transition-based parsing into one system for learning and understanding, and it performs better than using just one method alone. When tested on English and Chinese language data sets, the combined system achieved top-level accuracy scores of 92.1% and 86.2%, respectively. We establish rules to change phrase structures into dependency structures. We mix beam search with a globally adjusted model that makes decisions based on differences, using a learning method called structured perceptron learning and an early update strategy from Collins and Roark (2004). We also look into adding graph-based features to a transition-based parser.

Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation We introduce a method called Minimum Bayes-Risk (MBR) decoding that helps choose the best translations from a large set of possible translations stored in a compact form called "translation lattices." We explain how to set up the rules for measuring translation errors so that the MBR method works well with these lattices. We create a simplified version of the BLEU score, which is a way to measure how good a translation is, to fit these rules. The MBR decoding using this simplified BLEU score is done with a tool called Weighted Finite State Automata. Our tests show that this Lattice MBR method gives better and more reliable translation results compared to the older method called N-best MBR decoding when translating between Arabic, Chinese, and English. We run various tests to find out why Lattice MBR is better than N-best MBR and see how different settings affect its performance. We use a mathematical shortcut, called Taylor approximation, to simplify the calculation of the BLEU score. We expand the MBR method to work with word lattices, which further improves its performance over the k-best list MBR method. We make a small change to the BLEU score calculation by replacing a part of it with a simpler multiplication method to make the math easier. We calculate expected values for certain features by combining the translation lattice with another lattice for each part of the translation.

Joint Unsupervised Coreference Resolution with Markov Logic Machine learning methods to solve coreference resolution usually need labeled data, which can be costly. Some methods that don't need labeled data have been tried (like by Haghighi and Klein in 2007), but they aren't as accurate. In this paper, we introduce the first method that doesn’t need labeled data and competes well with those that do. We achieve this by analyzing all mentions together, unlike the separate comparisons usually used in other methods, and by using Markov logic, a system that helps us easily describe relationships like apposition (placing things side by side) and predicate nominals (nouns following verbs like "is"). Using MUC and ACE datasets, our model does better than the one by Haghigi and Klein while using much less data, and often matches or is better than the best methods that use labeled data. We show that methods looking at the whole text perform better than those that look at parts of it one by one. Our method focuses on the entity-mention model. In sentences with a predicate nominative (where a noun follows a verb like "is"), the noun must refer back to the subject.

Lattice-based Minimum Error Rate Training for Statistical Machine Translation Minimum Error Rate Training (MERT) is a useful way to find the best settings for a model by directly improving how well it works using an automatic test. During training, the process checks each feature to see how it affects errors in a list of possible translations. The settings are then changed to reduce errors by looking at all sentences together. Usually, MERT uses N-best lists, which are the top N possible translations from a decoder. This paper introduces a new method that efficiently handles all possible translations in a phrase lattice, rather than just the top ones. This means considering many more translation options. This method is used to train a system that translates languages using a statistical approach. Tests showed it runs faster and slightly better than the usual N-best MERT. However, early attempts to adjust settings often result in poor choices, likely because they focus too much on the set of possibilities. We suggest a way to optimize directly over a group of word choices. We use a method to remove unnecessary functions and options. We guess that the most functions at the end are the same as the number of paths in the lattice. Our MERT method checks errors by looking at all possible translations in a packed forest, which allows for small changes in settings. We improve the MERT method to consider all possible translations, not just the top few. We find that a method called the Downhill Simplex Algorithm doesn't work as well when there are more than 10 factors to consider.

A Generative Model for Parsing Natural Language to Meaning Representations In this paper, we introduce a method for teaching a computer program to understand natural language sentences and their complex meaning in a structured way. The method is used to convert sentences into organized representations that show their true meaning. We use special techniques to make the training and decoding processes fast and efficient. In tests, we show that our method, when combined with a technique that helps choose the best answer, performs better than previous methods on two publicly available datasets. The method still works well even when given new examples that are different from those it learned from. This leads to a noticeable improvement in remembering information compared to older methods. Our method uses a tree-based approach that combines different elements. We introduce a combined process that creates a tree structure including words, grammar structures, and meanings, with meanings in a simple tree structure. We suggest three methods for understanding the meaning of sentences: unigram (single unit), bigram (pairs of units), and mix gram (a combination of the two).

Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis Determining the positive or negative feeling of an expression requires more than just looking at individual words. Words or parts of the expression can interact to create a specific overall feeling. In this paper, we look at these interactions using compositional semantics (how word meanings combine) and introduce a new learning approach that uses this idea in the learning process. Our experiments show that (1) simple rules based on compositional semantics can be better than learning methods that don't use this idea (89.7% accuracy vs. 89.1%), and (2) using compositional semantics in learning works best (90.7% accuracy). We also find that "content-word negators," which are not often used in past research, are important in figuring out the overall feeling of expressions. Surprisingly, we discover that accuracy in classifying expressions decreases when more context is added. Content-word negators are words that aren't basic words but act to reverse meaning. We combine different kinds of negators with words showing feelings using various models, both simple and machine-learned, to improve understanding of phrases. We suggest a method for analyzing phrase sentiment that learns how to decide the sentiment of phrases based on the basic feeling of the words and the correct overall phrase feeling. We manually create rules to model how combining different words affects the overall meaning. We divide words that reverse meaning into two types: function-word negators like "not" and content-word negators like "eliminate."

A Simple and Effective Hierarchical Phrase Reordering Model While current translation systems are good, they struggle with changing word order. Present models can manage swaps between phrases next to each other but aren't good at handling long-distance rearrangements like systems that use grammar rules. In this paper, we introduce a new model that helps with these long-distance changes and works well with existing systems without slowing them down much. We demonstrate that this model can handle key examples that usually need complex grammar-based systems, like moving a group of words around another group. We compare our model with those commonly used and show it improves translation quality for Chinese-English and Arabic-English. Our model uses a method called a shift-reduce algorithm to handle non-local phrase changes. We add a straightforward shift-reduce parser to the translation process so it can always see the biggest part of the sentence already translated. We propose three models for reordering words: based on single words, groups of words, and a hierarchical approach.

Two Languages are Better than One (for Syntactic Parsing) We show that analyzing two languages together can greatly improve the accuracy of understanding sentence structure in both languages. In a model that predicts the structure of sentences in two languages (bitext parsing), we define a method to look at sentence trees in each language and how their parts match up. Features include scores for sentence structure in one language and measures of how different the structures are between the two languages. Using translated parts of a Chinese language dataset, our model is trained in steps to best guess the sentence structures, treating the connections between languages as hidden information. The resulting two-language parser performs better than the best single-language parsers by 2.5 F points for English trees and 1.8 F points for Chinese trees (the best numbers published for these data sets). Also, these better sentence structures improve the quality of translated text by 2.4 BLEU points. In two-language parsing, we use features that look at combinations of sentence structures in both languages and word connections, combined in a model trained to improve accuracy. We use features that measure how well the connected words match with pairings from another word matching tool.

Unsupervised Semantic Parsing We introduce the first method that doesn't need labeled data to teach a semantic parser, using Markov logic, which helps computers understand language meanings. Our USP system changes sentence structures into simple logic expressions, creates lambda forms (a way to express functions) from these, and groups them to focus on the same meaning despite different phrasing. The most likely semantic structure of a sentence is found by matching its parts to these groups and combining them. We test this method by using it to build a database from scientific papers and answer questions about them. USP does much better than TextRunner, DIRT, and a smart starting point in both correctness and coverage for this task. We look at a situation where the aim is to (1) break down the sentence structure into parts, (2) match each part to a group of similar sentence structures, and (3) figure out the relationships between these parts. We use Markov Logic Networks (MLNs) to model the combined probability of the sentence structure and its hidden meaning, choosing settings (weights of rules) to make the observed structures most likely. We organize settings and apply local rules within each group.

First- and Second-Order Expectation Semirings with Applications to Minimum-Risk Training on Translation Forests Many statistical translation models can be seen as logical processes where each step has a weight or importance. In this way, we use weights from a mathematical tool called the expectation semiring to calculate basic statistics, like the expected length of a translation or how often certain features appear, over groups of translations. We then introduce a new tool called the second-order expectation semiring, which helps calculate more complex statistics, like how much the length of a translation varies or how much uncertainty there is. This second-order tool is important for advanced training methods, such as minimum risk training, which aims to minimize mistakes, and other strategies that require understanding changes in uncertainty and risk. We use these tools in a free machine translation software, Joshua, which can improve translation quality by up to 1.0 BLEU point, a score used to measure translation accuracy. We explore minimum risk training using a simplified method to calculate BLEU scores. Important numbers needed for calculating expected BLEU scores on graphs can be found using expectation semirings. We improve on previous work by Smith and Eisner by using a comprehensive chart instead of a simple list to get better estimates of expected features. We conduct training to improve BLEU scores using a method called deterministic annealing on translation forests created by a system called Hiero.

Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora A large part of the world's text is tagged by users on social bookmarking websites. Credit attribution is a problem in these collections of text because most pages have many tags, but the tags don't always match the whole document equally well. Solving this problem means matching each word in a document with the best tags and vice versa. This paper presents Labeled LDA, a topic model that adjusts Latent Dirichlet Allocation (a type of statistical model) by linking each hidden topic to user tags directly. This helps Labeled LDA to learn how words and tags are related. We show how Labeled LDA is better at expressing ideas than traditional LDA using visual examples from a collection of tagged web pages from del.icio.us. Labeled LDA is more than three times better than SVMs (support vector machines, a type of algorithm) at finding document parts related to specific tags. As a tool for sorting text with multiple labels, our model competes well with a strong baseline on various sets of data. L-LDA takes standard LDA and adds guidance for specific target categories, and the process involves a second observed factor, meaning each document is clearly marked with a target category.

Fast Cheap and Creative: Evaluating Translation Quality Using Amazon’s Mechanical Turk Manual evaluation of translation quality is often seen as very time-consuming and costly. We look into a quick and cheap method using Amazon’s Mechanical Turk, where we pay small amounts of money to many non-experts to rate translations. For $10, we recreated the evaluations from a WMT08 translation task. We discovered that combined ratings from non-experts matched well with the top-quality standards and were more similar to expert opinions than Bleu (a common translation evaluation metric). We also show that Mechanical Turk can be used to calculate human-mediated translation edit rate (HTER), do reading comprehension tests with machine translation, and produce high-quality reference translations. We noticed that less careful raters tended to stay longer and do more ratings. We handle evaluation like a weighted voting problem, where each rater's input is given importance based on how much they agree with a high standard or with other raters. We demonstrate the success of using a large group of people (crowdsourcing) to complete demanding language tasks.

An Empirical Study of Semi-supervised Structured Conditional Models for Dependency Parsing This paper describes a practical study of high-performing tools for analyzing sentence structure using a semi-supervised learning method (a way of teaching computers using a mix of labeled and unlabeled data). We explain how we expanded a method called semi-supervised structured conditional models (SS-SCMs) to tackle the problem of dependency parsing (analyzing how words in a sentence relate to each other), originally proposed by researchers Suzuki and Isozaki in 2008. We also introduce two new ways to enhance dependency parsing: The first is by combining SS-SCMs with another semi-supervised method, as explained by Koo and others in 2008. The second improvement applies this method to advanced parsing models, such as those described by Carreras in 2007, using a two-step semi-supervised learning method. We show how effective our new methods are by testing them on two well-known sets of data: the Penn Treebank for English and the Prague Dependency Treebank for Czech. Our best results on these tests show 93.79% accuracy for predicting word relationships in English and 88.05% for Czech. We introduce a highly effective semi-supervised method that combines features from various models trained on unlabeled data into a system that makes structured predictions.

Parser Adaptation and Projection with Quasi-Synchronous Grammar Features We link two learning situations: changing a parser (a tool for understanding sentences) trained on one set of rules to work with a different set of rules, and transferring sentence structures from one language to another. We introduce quasi-synchronous grammar (QG) features, which means we look at pairs of sentences from two different languages and score them based on specific parts of the sentences and how they match. Unlike other methods that require strict matching, our model allows any kind of match between sentences. In adapting to different styles within the same language, we get very accurate results when changing how the same sentence is understood. For the harder task of using one language to help understand another, we create a sentence-understanding tool for a new language by using texts that have been translated, an English sentence tool, and automatic word matches. Our tests show that our method, which doesn’t need direct guidance, does better than methods that only use very precise matches and is much more accurate by over 35% when compared to just using raw text in the new language. When we have a few examples from the new language, our method improves results as if we had twice as many examples. We view adapting across languages as using word-matched translated text to help train tools for the new language without needing direct guidance.

Polylingual Topic Models Topic models are tools that help analyze large collections of text, but until now, they have been used mostly for texts in one language or sometimes two. However, with huge collections of linked documents in many languages, like Wikipedia, there's a need for tools that can understand content in several languages. We introduce a polylingual topic model that finds topics common across different languages. We test the model using two large text groups, each with texts in over ten languages, and show that it helps in translating languages and tracking topic changes between languages. We find possible translations by choosing a few likely words in both languages and combining them for each topic to create translation options. We show that as long as the number of related documents to unrelated ones is more than 0.25, the topic patterns (measured by how similar they are using a method called Jensen-Shannon Divergence) don’t get worse by much. We expand the original idea of LDA (a type of topic model) to work for many languages, both for texts that are exactly the same (like legal documents) and somewhat similar texts (like Wikipedia articles). Polylingual topic models learn about topics in different languages, creating groups of language-specific word arrangements for each topic.

Web-Scale Distributional Similarity and Entity Set Expansion Computing how similar all words are to each other on the Web is very difficult for computers. We need to use methods that allow computers to work faster and smarter. We suggest a way that can handle lots of data, using a method that looks at how words are used similarly. This is done with a system called MapReduce, which can handle huge amounts of data, like 200 billion words from the Web. We compare how similar 500 million words are in 50 hours using 200 powerful computers. We use this information to automatically grow sets of related items and show a study that measures how different factors affect this process, like the size and quality of the data, and the number and type of examples we start with. We share a tool for testing this process that includes many different groups of items from Wikipedia. Our DASH system keeps track of each phrase in Wikipedia. We find that starting with 10 to 20 examples is enough in our method to find many new correct examples. From these starting examples, we create a main example profile using the words that appear around them in the data.

Supervised Models for Coreference Resolution Traditional learning-based coreference resolvers work by teaching a system to identify if two words or phrases (called mentions) refer to the same thing. Recently, two separate research methods have tried to improve these systems: one by ranking earlier mentions to find the right one for a reference (anaphor), and the other by checking if a group of previous mentions (a cluster) matches a current mention. We suggest a new method that combines the best of both these approaches. Our method also helps in identifying new references in a text at the same time as solving coreferences. Tests on the ACE data sets show that our method performs better compared to others. During each query, we include an option that assumes no previous mention, which helps in learning to detect new references. We demonstrate that our new model (CR) is better than the old one (MP). Our model works by looking at the text from left to right and adds the current mention to the most relevant previous group.

Simple Coreference Resolution with Rich Syntactic and Semantic Features Coreference systems rely on rules about sentence structure (syntactic), meaning (semantic), and conversation flow (discourse constraints). We introduce a straightforward method that separates these three areas completely. Unlike many current methods that focus on learning and conversation flow (discourse), our approach is predictable (deterministic) and based solely on how well the sentence structure and meaning match, learned from a large collection of text without labels (unlabeled corpus). Even though it's simple and not focused on conversation flow, our system performs much better than all systems that don't rely on labeled data (unsupervised) and better than most that do (supervised). Our main contributions are (1) offering an easy-to-copy, high-performing starting point and (2) showing that most mistakes are due to sentence structure and meaning issues not related to linking references (coreference) and might be better fixed by systems not designed for coreference. We demonstrate that top systems often make mistakes because they don't handle meaning matching well. In our SYN-CONSTR setup, each reference connects with any earlier mention with the same main word (head) or in a predictable sentence structure (like appositives or sentences that rename the subject). When looking for a previous reference (antecedent) for a mention (mk), possible matches are checked based on their order in the sentence diagram (parse tree).

Bilingually-Constrained (Monolingual) Shift-Reduce Parsing Jointly parsing two languages has been shown to improve accuracies on either or both sides. However, its search space is much bigger than the monolingual case, forcing existing approaches to use complex models and rough estimates. Here we propose a much simpler alternative, bilingually-constrained monolingual parsing, where a parser for the source language learns to use reorderings as extra information, but does not build the tree for the target language. We show specifically how to improve a shift-reduce dependency parser with alignment features to solve shift-reduce conflicts. Experiments on the bilingual part of the Chinese Treebank show that, with just 3 bilingual features, we can improve parsing accuracies by 0.6% (absolute) for both English and Chinese compared to a leading baseline, with very little (around 6%) extra work, making it much faster than parsing both languages together. We keep the chances of a natural rule the same and set those of a virtual rule to 1. We improve English prepositional phrase attachment using features from an unparsed Chinese sentence.

Phrase Dependency Parsing for Opinion Mining In this paper, we introduce a new method for finding opinions in product reviews. This method changes the task of opinion mining into finding product features, opinion expressions, and how they are connected. Noting that many product features are phrases, we introduce "phrase dependency parsing," which expands regular dependency parsing to include phrases. This idea is used to find connections between product features and opinion expressions. Tests show that using phrase dependency parsing improves the opinion mining task. We use the dependency parser to pick out noun phrases (nouns and related words) and verb phrases (verbs and related words) from reviews as possible aspects. For tasks involving one language, we use a simple parser to turn word-level dependencies from a dependency parser into phrase-level dependencies.

On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing This paper introduces dual decomposition as a method for creating problem-solving algorithms for NLP (Natural Language Processing). The approach uses well-known dynamic-programming (a method for solving problems by breaking them down into simpler sub-problems) algorithms as tools for solving smaller parts of the problem, along with an easy way to make sure these tools agree with each other. This method can solve a simplified version of a bigger problem called a linear programming (LP) relaxation. It results in algorithms that are easy to use because they rely on existing problem-solving methods; efficient because they avoid using complete methods for the entire model; and often correct, because in practice, they frequently find the right solution despite using an LP relaxation. We show test results on two issues: 1) combining two detailed sentence structure models; and 2) combining a detailed sentence structure model with a three-word sequence part-of-speech tagger (a tool that labels words with their roles, like noun or verb). We use the highest scoring result from the sentence structure submodel in all attempts.

Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation We explain a new way to improve Statistical Machine Translation (SMT) by giving importance to phrases from other topics based on how relevant they are to the target topic. This relevance is judged by how similar they are to the target topic and whether they are common language or not. This method builds on earlier work by adding more detail and focusing on specific examples rather than large parts of text, while also using a simpler way to train the system. We add this technique of giving importance to examples into a system that mixes different models, and we find it consistently improves results compared to many standard methods. We order the sentence pairs in a general language collection based on scores that measure how well the sentences fit with the specific topic language. We then use a simple method to combine the phrases from other topics with the specific topic model. We suggest a method for translating languages that uses features to show how general or specific they are.

A Multi-Pass Sieve for Coreference Resolution Most models for coreference resolution, which is figuring out if two words refer to the same thing, use one method with rules or features to decide. This can cause mistakes because the less accurate features often overshadow the fewer, more accurate ones. To fix this, we suggest a simple system called a sieve, which uses different layers of coreference models, starting from the most accurate to the least. Each layer improves on the results from before. Also, our model spreads overall information by sharing details like gender and number among words in the same group. This careful sieve makes sure stronger features are prioritized over weaker ones and each decision uses all the available information. The system is very flexible: new parts can be added without changing the rest. Despite being simple, our method performs better than many advanced models on several standard tests. This shows sieve-based methods might work for other tasks in language processing. Our rule-based model gives competitive results quickly. We organize potential matches for pronouns based on how noticeable they are in the text, focusing on sentence structure and closeness in the document. We create accurate unsupervised systems that use basic but strong language rules.

Nouns are Vectors Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space We suggest a method for understanding how adjectives and nouns combine in language using a technique called distributional semantics, which is based on analyzing large collections of text (corpus). In simple terms, we represent nouns as points in space (vectors) and adjectives as processes (functions) that change these points, which are shown as grids of numbers (matrices). Our model works better than others at predicting combinations of adjectives and nouns that it hasn’t seen before. A small follow-up analysis shows that if our model’s prediction is different from what is found in the text collection, it’s often because of unusual examples in the text. We also show that our method offers two new ways to understand what adjectives mean, which are better than the traditional method based on how often words appear together in text. We find that one method, called "mult," works better in the original setup because some mathematical adjustments (SVD) can cause confusing results when multiplying numbers. The "alm" model, which is specific to each adjective, was much better than two other methods ("add" and "mult") at guessing the right combinations of unseen adjective-noun pairs. However, for a different task that looks at the general relationship between words, add and mult were more effective, while alm was successful only when using a more complex method of measuring how closely words are grouped together.

Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification This paper focuses on how to teach a system to convert sentences into logical forms, using examples that pair sentences with their logical meanings. Previous methods were specific to certain languages or meanings; we introduce a more universal technique. Our method creates a probabilistic CCG grammar, which captures the meaning of words and shows how these meanings can be joined to understand full sentences. We use a technique called higher-order unification to define a range of possible grammars that match the examples provided. We also create an online learning method that efficiently explores this range while determining the best parameters for a model that interprets sentences. Tests show high accuracy on standard data sets in four languages with two different meanings. We offer a method for learning that works across languages, replacing custom templates with a universal method to learn word meanings. We start by setting initial weights for learning using overall alignment statistics between words and their meanings.

Using Universal Linguistic Knowledge to Guide Grammar Induction We present a method for teaching computers to understand grammar by using common grammar rules found in many languages to help improve how well they can identify sentence structure in different languages. Our method uses one set of general rules that point out relationships between types of words that are common in most languages. When the computer tries to learn these relationships, we make sure that a certain amount of the connections it finds matches our rules. We also improve the basic word categories we start with automatically. In tests with six languages, our method works better than the best existing methods that don't use prior information. Our system uses a little bit of human guidance, where we use predefined universal grammar rules to guide a statistical model.

A Latent Variable Model for Geographic Lexical Variation The rapid increase in social media posts that include location data (geotagged) opens up new ways to study how language changes in different places. In this paper, we introduce a detailed model that looks at hidden themes and locations at the same time. Big themes like “sports” or “entertainment” are expressed differently depending on the area, showing unique regional features for each topic. When used on a new collection of geotagged short messages (microblogs), our model finds clear topics and their local versions, while also highlighting areas where language is similar. The model can also guess where a writer is located just from their text, doing better than other methods that use direct calculations or guided topic analysis. We collected texts and locations of 9,250 people posting short messages (microbloggers) on Twitter to create a dataset. We gathered around 380,000 short messages (tweets) from Twitter's official data tool (API). We estimate locations using a statistical method (Gaussian distributions) over the earth's surface as part of a structured (hierarchical) statistical approach (Bayesian model). We treat all of a user's tweets as one document and use the first GPS-based location as the correct location.

Dual Decomposition for Parsing with Non-Projective Head Automata This paper introduces methods for non-projective parsing (a way of analyzing sentence structure) using dual decomposition (a strategy to break down complex problems). We focus on parsing methods for non-projective head automata, which extend head-automata models to work with more complex sentence structures. The dual decomposition methods are straightforward and effective, using common techniques like dynamic programming (a method for solving problems by breaking them down into simpler steps) and minimum spanning tree algorithms (a way to connect points with the shortest path). They are proven to solve a simplified version of the non-projective parsing problem. In practice, this simplified version usually works well: for many languages, it finds exact solutions for over 98% of test sentences. The accuracy of our models is better than past research across many types of data. We also look at more detailed features like grand-siblings and tri-siblings (relationships between words in a sentence).

Multi-Source Transfer of Delexicalized Dependency Parsers We present a simple way to move dependency parsers, which help understand sentence structure, from languages with labeled data to those without. We show that delexicalized parsers, which don't rely on specific words, can be moved between languages, leading to better accuracy than unsupervised parsers, which learn without labeled data. We use a learning method driven by rules from similar texts in different languages to build the final parser. Unlike past methods, we show that using multiple source languages can greatly improve the quality of the parsers. Our system's parsers perform the best when compared to other systems that don't use labels or use projected techniques in eight different languages. We show that part-of-speech tags, which label words as nouns, verbs, etc., hold a lot of information for parsing in languages without labeled data. We offer another way to create grammar by transferring parse trees, which show how sentences are structured, from languages with data to those without. Language data from other languages can still help learn useful features. We show that choosing one well-matched language to transfer from can lead to good parsing results.

Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions We introduce a new machine learning method that uses recursive autoencoders to predict how people feel about sentences. Our method creates numerical representations (like summaries in numbers) for phrases with multiple words. In tasks where we guess how people feel, these representations work better than other top methods on popular datasets, like movie reviews, without needing any pre-set lists of emotional words or rules for changing opinions. We also test how well the model can guess people's feelings on a new dataset made from personal stories shared on the Experience Project website. This dataset includes personal stories labeled with various emotions, which together form a pattern showing how people react emotionally. Our algorithm predicts these emotion patterns more accurately than several other strong methods. We introduce a semi-supervised approach that uses recursive autoencoders to learn the sentence's layered structure and how feelings are spread throughout it.

Domain Adaptation via Pseudo In-Domain Data Selection We look at an effective way to improve translation software by picking sentences from a big collection of translated texts that best match the topic we’re interested in. We use simple methods based on cross-entropy, a statistical measure, to choose these sentences, and we share three such methods. Since these sentences aren't exactly like our desired topic's data, we call them "pseudo" in-domain data groups. These smaller groups, which are just 1% of the original size, can be used to train translation systems that work better than systems trained on the entire collection. The performance gets even better when we use these specialized models together with a real in-domain model. The findings show that more data isn't always better; instead, choosing the right data for the topic and combining different sources during translation gives the best results. We have improved the perplexity approach, another statistical measure, and suggest using bilingual cross-entropy difference, which considers both languages involved, to rank and select data. This method captures some context and performs better than using cross-entropy for just one language.

Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora We focus on making cross-language datasets using crowdsourcing, which means getting help from lots of people online. Our aim is to find a cheap and repeatable way to gather data that requires little work from experts, without using special tools or pre-done data in one language. Following recent studies that highlight the need for lots of data for understanding text relationships, our work seeks to: i) deal with the lack of data needed to train and test systems, and ii) encourage the use of crowdsourcing as a cost-effective method to gather data without losing quality. We demonstrate that a difficult data creation task, where even experts often disagree, can be broken down into simple tasks for non-experts. The final dataset, made through a series of tasks on Amazon Mechanical Turk, includes more than 1,600 matched pairs for each set of texts and hypotheses in English, Italian, and German.

Tuning as Ranking We present an easy, effective, and scalable method for adjusting settings in statistical machine translation, using a ranking approach based on comparing pairs (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method can handle many settings and easily work with systems that have thousands of features. Also, unlike recent methods based on the MIRA algorithm by Crammer and Singer (2003) (Watanabe et al., 2007; Chiang et al., 2008b), PRO is simple to set up. It uses ready-made software for linear binary classification and can be added to an existing MERT setup in just a few hours. We show PRO's ability to work on a large scale and its effectiveness by comparing it to MERT and MIRA, and show it works equally well on both phrase-based and syntax-based systems in different languages with large amounts of data. PRO treats the task of tuning as a ranking problem between pairs of translation options. We improve ranking in lists of top choices but learn settings as we go. We reduce logistic loss, which is a measure of prediction error, from the combined top choices, and use sentence-BLEU, a scoring method that measures translation quality, to determine ranks.

Experimental Support for a Categorical Compositional Distributional Model of Meaning Modeling how sentences mean what they do using real-world data methods has been a challenge for language experts who work with computers. We use the theoretical model by Coecke and others (2010) with data from the BNC (British National Corpus) and test it. Our approach is based on teaching a computer system to learn relationships between words without direct instruction and applying these learned relationships to the words they describe. We test this by seeing how well our model can figure out the meaning of words in simple sentences (following the work by Mitchell and Lapata, 2008) and a new test for more complex sentences. Our model performs just as well as others in the first test and does even better in the second test. The fact that our results get better with more complex sentence structures shows how well our model can understand sentence meaning. We suggest that the space where word relationships exist should depend on how many things the word relates to.

Named Entity Recognition in Tweets: An Experimental Study People send over 100 million tweets every day, creating a large amount of messy, informal, but sometimes useful short messages that reflect current trends in a way never seen before. Regular language processing tools don't work well on tweets. This paper solves this problem by improving the language processing steps, starting with identifying parts of speech, then grouping words, and finally recognizing named entities (like names of people, places). Our new T-NER system performs twice as well as the Stanford NER system. T-NER takes advantage of repeated information in tweets to work better, using a technique called LabeledLDA to use Freebase dictionaries as a guide without needing direct supervision. LabeledLDA works better than another method called co-training, improving results by 25% for ten common types of entities. You can find our language processing tools at: http://github.com/aritter/twitter_nlp. We use single words as features, including hashtags, but we skip Twitter mentions, web links, and purely number-based tokens. Our system uses a model called CRF to identify named entities in text and then uses a distantly supervised method based on LabeledLDA to categorize these named entities.

Identifying Relations for Open Information Extraction Open Information Extraction (IE) is about taking statements from large collections of text without needing a specific set of words in advance. This paper explains that current Open IE systems often produce results that are not clear or useful. To fix these issues, we introduce two simple rules based on sentence structure and word choice for connections made by verbs. We added these rules to the REVERB Open IE system, which significantly improves accuracy compared to older systems like TEXTRUNNER and WOEpos. Over 30% of REVERB's results are very accurate (at least 80% correct), while earlier systems had almost none at this level. The paper ends with a detailed look at REVERB's mistakes and suggests ways to improve. We demonstrate that using action phrases reveals many connections between two things while cutting down on irrelevant phrases that don't show any real links. We created a large online collection called the ReVerb corpus, which includes extracted templates of connections and their specific examples. Our ReVerb corpus is a big public data set available on the web, with about 15 million unique extractions, automatically taken from the ClueWeb09 web archive.

A Comparison of Vector-based Representations for Semantic Composition In this paper, we tackle the challenge of figuring out the combined meaning of phrases and sentences using methods that analyze word usage in large text collections. We try different ways of representing and combining words, with some methods being simple and others being more complex and requiring learning from large text collections or understanding sentence structure. We discover that simple methods work just as well as more complex ones for two specific tasks: (1) checking how similar phrases are, and (2) identifying if sentences mean the same thing in different words. The amount of text used for training and the size of word representations are less important than how well the method of combining words matches the way their meanings are represented. We calculate a weighted sum of word meanings for words in a document to classify it. We compare two types of word representations as inputs for combining them. For identifying similar meaning sentences, we use a measure called cosine similarity between pairs of sentences, along with two simple tricks: checking how many words the sentences share and the difference in their lengths. The "Add" and "Mult" methods performed the best with these simple models for both tasks.

A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing Most current dependency parsers assume that input words have been clarified using a part-of-speech tagger before they start working. We present a transition-based system that does both part-of-speech tagging (identifying the role of each word in a sentence) and labeled dependency parsing (analyzing the sentence structure) with non-projective trees (a flexible tree structure). Tests on Chinese, Czech, English, and German show consistent improvements in both tagging and parsing accuracy compared to a step-by-step system, leading to better results for all languages. We introduce a transition-based system that performs both POS tagging and dependency parsing together.

An Efficient Implementation Of A New DOP Model Two seemingly opposite DOP models are discussed in studies: one finds the parse tree (structure of a sentence) with the most common subtrees from a collection of tree patterns, and the other finds the parse tree with the fewest subtrees. This paper suggests combining these two models to perform better than each one alone. By also simplifying DOP with a PCFG (a grammar-based technique), we get better accuracy and speed with the Wall Street Journal tree data. Our findings show an 11% lower error rate than earlier models, with an average processing time of 3.6 seconds for each WSJ sentence. We emphasize that the best parse (sentence structure), not the way it is created, is what we aim for. We demonstrate that DOP models that choose the best parse using the shortest creation method work very well. We adjust subtree probability with a simple correction factor.

Bootstrapping Statistical Parsers From Small Datasets We introduce a useful method called co-training for improving statistical parsers, which are tools that help computers understand sentences. This method uses a small set of sentences that have been manually analyzed and a larger set of plain sentences that haven’t been analyzed yet. Tests show that these plain sentences can help make the parsers work better. We also look into the challenge of improving parsers when the manually analyzed sentences come from a different area or topic than the plain sentences or the sentences we test on. We demonstrate that this method is still helpful even if no analyzed sentences from the specific area are used. We explore a method called self-training for a specific type of parsing (PCFG) when starting with a very small amount of data (less than 1,000 analyzed sentences). Our results show either slight improvements or noticeable problems when using self-training for parsing. We observe that a certain parser (lexicalized tree adjoining grammar parser) performs worse, while another type (Collins lexicalized PCFG parser) shows a small improvement, but only when it was trained with a small set of analyzed sentences.

Combining Distributional And Morphological Information For Part Of Speech Induction In this paper we discuss methods for grouping words into categories from text that hasn't been labeled, using algorithms that don't need prior training, based on how words are used (distributional information) and their structure (morphological information). We show how using word structure information can boost accuracy, especially for uncommon words, and that this works well for many languages. We suggest using a complexity test to judge how good the part of speech (POS) grouping method is. We discover that measuring accuracy by matching many words to one category has some problems.

Investigating GIS And Smoothing For Maximum Entropy Taggers This paper examines two parts of Maximum Entropy tagging: using a correction feature in the Generalised Iterative Scaling (GIS) estimation method, and ways to make models more reliable (smoothing). We show through analysis and testing that the correction feature, thought to be necessary for GIS to work correctly, is not needed. We also look into using a Gaussian prior (a mathematical method) and a simple cutoff (a limit to simplify the model) for smoothing. The experiments are done with two sets of tags: the usual Penn Treebank POS tagset and a larger set of word types from Combinatory Categorial Grammar. Our supertagger (a tool that assigns tags) finds the most likely category sequence for a sentence and uses extra features based on the categories already assigned.

Empirical Methods For Compound Splitting Compounded words are tricky for NLP (Natural Language Processing) applications like machine translation (MT), which is translating text from one language to another. We introduce methods to learn splitting rules from large collections of text in one language and collections in two languages. We test these methods against a high-quality standard and see how they affect the performance of statistical MT systems. Results show a high accuracy of 99.1% and improvements in MT performance of 0.039 BLEU (a score for measuring the quality of machine-translated text) on a task translating German-English noun phrases. We present a method that doesn't need complex language structure analysis to break down compound words. We break down German compound words based on how often the words appear in possible breakdowns.

Using Encyclopedic Knowledge For Named Entity Disambiguation We present a new way to identify and clarify the meaning of specific names or terms in general text. We train a machine learning model, called a disambiguation SVM kernel, to take advantage of the detailed and well-organized information found in an online encyclopedia. This model works much better than a simpler version with less information. We compare how similar the surrounding text of the named entity is to the Wikipedia categories of possible matches. We use context matching to connect groups of nouns into Wikipedia.

Computing Consensus Translation For Multiple Machine Translation Systems Using Enhanced Hypothesis Alignment This paper talks about a new way to find a common translation from the results of different machine translation (MT) systems. The results are mixed together to possibly create a new translation option. Like the well-known ROVER approach by Fiscus in 1997, which combines speech recognition attempts, the common translation is figured out by voting on a confusion network, which is a system that helps organize different options. To make this network, we match words from the original machine translation attempts using a better method that specifically deals with words being in different orders. Instead of just looking at one sentence, we look at the whole document to help with matching words. The new method of matching and voting was tested on many translation tasks, including one with a big vocabulary. It was also tested with translations from different sources and speech translations. In all tests, we saw big improvements in translation quality, like increasing the BLEU score, which measures translation quality, by up to 15%. We match similar words and different word forms to each other by learning from texts in different languages. We use pair-by-pair matching methods based on balanced matches from an HMM (Hidden Markov Model) alignment model. We consider different word orders by training match models using all pairs of attempts as a parallel text group with GIZA++ (a tool for aligning words). We suggest using a statistical word matching method as a stronger way to line up (same language) outputs into a confusion network to combine systems.

Online Learning Of Approximate Dependency Parsing Algorithms In this paper, we expand on the maximum spanning tree (MST) method for understanding sentence structure, originally by McDonald et al. (2005c). We include more detailed features and allow for sentence structures where words can have more than one parent. We acknowledge that these changes make the MST approach very complex and difficult to compute, but we solve this issue with new simpler parsing methods. Our experiments demonstrate that using these simpler methods with online learning techniques gives the highest accuracy for analyzing Czech and Danish languages. We introduce a model that looks at the connections between parts of a sentence in two different ways. We use a method called the Viterbi algorithm to process this in a reasonable amount of time, specifically O(n3). We find that a specific type of sentence analysis, called non-projective parsing with horizontal Markovization, is extremely challenging to solve, known as FNP-hard. We describe a model that allows interactions between nearby sibling words in a sentence.

Making Tree Kernels Practical For Natural Language Learning In recent years, tree kernels have been suggested for automatically teaching computers to understand language. Unfortunately, they have (a) a complex and time-consuming process and (b) are less accurate than traditional methods that use specific features and values. In this paper, we show that tree kernels are useful for understanding language because (a) we have a simple method to compute tree kernels quickly and (b) our research on how different tree kernels classify language shows that combining them always improves on traditional methods. Tests with Support Vector Machines (a type of machine learning model) on classifying predicates and arguments (parts of sentences) support our idea. We introduce a quick way to use tree kernels, where we first create a set of node pairs that follow the same rules.

Determining Term Subjectivity And Term Orientation For Opinion Mining Opinion mining is a new area of study in computational linguistics, which focuses on the opinions expressed in a document rather than the topic it discusses. To help pull out opinions from text, recent studies have addressed how to find out if "subjective" terms (words showing opinions) in text have a positive or negative meaning. This is important for figuring out if a document has a positive or negative opinion about its topic. We argue that just figuring out if terms are positive or negative isn't realistic because it assumes we already know if a term is subjective (showing opinion) or not. This means we would need a resource that labels terms as "subjective" or "objective" (neutral), which usually isn't available. In this paper, we take on the challenge of deciding if a term has a positive, negative, or no opinion at all, which includes finding out both subjectivity and direction (positive or negative meaning). We address this by testing three different versions of a semi-supervised method (a method using both labeled and unlabeled data) that was suggested before for finding direction. Our results show that figuring out both subjectivity and direction is much harder than just finding the direction.

Mining WordNet For A Fuzzy Sentiment: Sentiment Tag Extraction From WordNet Glosses Many tasks needed to label phrases and texts with meaning rely on a list of words marked with certain features. We introduce a way to find adjectives that show feelings from WordNet using a program called STEP. We ran STEP 58 times on different lists of words that were checked by people to be either positive or negative and compared the results to other checked lists. These 58 runs were combined into one group of 7,813 different words. For each word, we calculated a Net Overlap Score by subtracting the number of times the word was marked negative from the number of times it was marked positive. We show that the Net Overlap Score can indicate how strongly a word belongs to the fuzzy category of sentiment: main adjectives with high scores were identified accurately by STEP and humans, while words on the edges had low scores and low agreement among people. We discovered that machines can struggle to correctly label words with feelings if the words are unclear in meaning during training. Adjectives that are not neutral were taken from WordNet and given scores to show how strongly they fit into a fuzzy category of sentiment. WordNet's similar words, opposites, and definitions are used to expand a list of starting words step by step.

CDER: Efficient MT Evaluation Using Block Movements Most advanced methods for judging machine translation give high penalties for moving groups of words around. However, these movements often still create correct or nearly correct sentences. In this paper, we introduce a new way to evaluate that specifically treats moving word groups as an editing step. Our method can be calculated accurately in a time that grows with the square of the number of words. We look at edit distance, which means counting changes needed for word swapping and moving. Our CDER measure is based on edit distance, like the well-known Word Error Rate (WER), but it allows for moving word groups around.

Re-Evaluation The Role Of Bleu In Machine Translation Research We believe that people working on machine translation rely too much on the Bleu score, a tool that measures how well a machine translates. We demonstrate that getting a better Bleu score doesn't always mean that the translation is actually better, and we provide two strong examples where Bleu doesn't match what humans think is good translation. This creates opportunities for new research that was previously seen as not worth pursuing because it couldn't improve Bleu scores. The problems with Bleu include: (1) it only recognizes similar words and phrases if they are already provided in several correct translations; (2) it treats all words as equally important, so missing important content doesn't affect the score much; (3) the "brevity penalty" is a temporary fix for not being able to measure how much of the original content is actually translated. Bleu has limitations when comparing different types of machine translation systems, especially if the systems work in different ways, like those using phrases versus those following specific rules. We find that both BLEU and another metric called NIST tend to favor systems that use short word sequences, such as the Pharaoh model (Koehn, 2004). As a result, translations from rule-based systems get lower automatic scores even though human judges often think they are better than those from Pharaoh.

Discriminative Sentence Compression With Soft Syntactic Evidence We introduce a method for shortening sentences that uses a learning system designed to focus on differences, combined with new tools that work on pairs of words and deep grammatical structures provided by extra tools that look at sentence structure and word relationships. These tools are trained on unrelated topics and include a lot of errors. We believe that because our learning system can focus on differences, it can adjust to any errors in the tools to improve accuracy in shortening sentences. This approach is different from the best current methods (Knight and Marcu, 2000) that consider error-filled sentence structures as perfect when setting up the model. We offer a method similar to one used for finding the best path in decision-making to find the best sequence of word pairs that keeps their order, with or without a specific length limit. We use the results from two tools (one focusing on sentence parts and another on how words depend on each other) as inputs in a model that breaks down sentences into pairs of consecutive words. We use a type of model that allows adding a language model to help with shortening sentences.

Comparing Automatic And Human Evaluation Of NLG Systems We look at the challenge of evaluating Natural Language Generation (NLG) systems and share results from testing several NLG systems that do similar tasks, including one that uses knowledge to generate text and others that rely on statistics. We compare how these systems were evaluated by experts in the field, regular people who are not experts, and several automatic scoring methods, such as NIST, BLEU, and ROUGE. We discover that NIST scores align best (above 0.8) with human opinions, but all the automatic methods we checked seem to favor systems that choose words based on how often they appear. We suggest that automatic evaluation of NLG systems has a lot of promise, especially when there are high-quality example texts and only a few human reviewers. However, generally, it's probably best if automatic evaluations are backed up by human-based reviews, or at least by research that shows a particular method matches human opinions well in a certain area. We use several different evaluation methods (by humans and by text analysis) to assess the results from five NLG systems that created wind descriptions for weather forecasts. We show that automatic scoring methods can match human ratings closely if the data used for training is of high quality. The two automatic methods used, NIST and BLEU, have been shown to match expert opinions well (with Pearson correlation scores of 0.82 and 0.79 respectively) in this area.

A Clustering Approach For Nearly Unsupervised Recognition Of Nonliteral Language In this paper, we introduce TroFi (Trope Finder), a system that automatically sorts verbs into literal (exact meaning) and nonliteral (figurative meaning) uses using methods that require little human guidance. TroFi relies on the context of sentences rather than complex grammatical rules or structures. It uses initial sets of literal and nonliteral examples that are collected and refined without human help to start the learning process. We modify an existing method for understanding word meanings to fit our needs and enhance it with multiple initial learning sets, a system for decision-making, and extra features like detailed grammar tags and context outside the sentence. Tests on data checked by humans show our improved method works 24.4% better than the basic one. With TroFi, we also create the TroFi Example Base, a growing resource of labeled examples of literal and nonliteral uses, which is available to researchers in language technology. For evaluation, Literal recall is calculated as (correct literal meanings found / total correct literal meanings); Literal precision is calculated as (correct literal meanings found / total literal meanings identified). We treat the task of distinguishing literal from nonliteral meanings as a problem of figuring out word meanings and use a grouping method that compares new examples to two sets created automatically (one for literal and one for nonliteral meanings), labeling the example based on the closest match.

Automatically Constructing A Lexicon Of Verb Phrase Idiomatic Combinations We study how flexible idiomatic expressions (phrases with special meanings) are in terms of words used and sentence structure. We create methods that use these language features and show that these methods, based on analyzing large collections of text, can effectively tell apart idiomatic phrases from regular ones. We also suggest a way to automatically find out which sentence structures a specific idiom can be used in, so it can be correctly represented in a dictionary. To check how fixed these expressions are, we use statistical methods to look at word choice, sentence structure, and overall consistency. We identify twelve possible sentence structures for verb-object pairs (like using passive voice, different articles, or plural forms) and use a statistical method based on text analysis to find the standard form(s).

Exploiting Shallow Linguistic Information For Relation Extraction From Biomedical Literature We suggest a method to find connections between things in medical texts using only simple language details. We combine special mathematical tools called kernel functions to bring together two types of information: (i) the entire sentence where the connection is, and (ii) the nearby words around the related things. We tested this on finding how genes and proteins interact in two different sets of data. The findings show that our method works better than most older methods that use complex grammar and meaning details. Besides word features, we also use simple language details like POS tag (which tells what part of speech a word is), lemma (basic word form), and the way words look for extracting PPI (protein-protein interaction).

Personalizing PageRank for Word Sense Disambiguation In this paper, we suggest a new method using a graph (a network of connected points) that relies on a knowledge base (LKB) like WordNet to perform Word Sense Disambiguation without needing any pre-labeled examples. Our method uses the entire graph from the LKB in a smart way, working better than older methods on English all-words datasets. We also demonstrate that it can be easily adapted to other languages effectively, as long as a wordnet (a database of words) is available. Additionally, we examine how well the method works, showing that it is effective and can be adjusted to work faster. We introduce Personalized PageRank (PPR), which aims to balance the amount of word information used with the speed of the process. We start by giving each vertex (point in the graph) an equal initial value, usually 1/N for a graph with N points. We show a new way to use PageRank for figuring out the meanings of words based on their context. The main idea is to change how we start the process to make use of the surrounding information available.

Bayesian Word Sense Induction Sense induction tries to automatically find different meanings of a word by looking at a large collection of texts. In the past, it was believed that the words around a confusing word help to figure out what it means. So, sense induction is usually seen as a task where we group different uses of a word into categories, each showing a different meaning. Our work uses a method called Bayesian to look at this by treating the words around a confusing word as examples from a type of statistical model, which sees word meanings as groups of related words. This Bayesian method offers a clear way to include many different features, not just those that involve nearby words, and to test how useful they are for figuring out word meanings. Our new method does better than the best current systems when tested on a standard set of data. Our approach, which involves hidden variables, sets the stage for creating stronger models for understanding other language features. We take out sections of text that are 10 words long, centered around the word we are studying. We mix different sets of features using a probability-based Word Sense Induction model and discover that only some mixes actually make the system better.

Nonconcatenative Finite-State Morphology Instead of using the usual finite-state transducer (a tool for language patterns), we suggest using a n-tape automaton (a type of machine with multiple tracks) that can handle the mixing patterns seen in Semitic languages. We propose a system where each layer of language structure gets its own track in a multi-track finite state machine, with one extra track for the final word form.

Inference In DATR DATR is a language used to show how ideas are connected in a specific way, allowing for multiple layers of ideas and fallback options when needed. It is mainly used to describe words and their meanings for processing natural language, and examples are taken from this area. In this paper, we explain the rules and methods of using the language. The purpose of creating DATR is to have a simple language that (i) can describe word meanings in a detailed way, (ii) can show all common patterns about these meanings, (iii) has a clear method for drawing conclusions, (iv) is easy to use with computers, and (v) has clear rules for how it works. This paper focuses mainly on (iii), but the examples might also give an idea of how we handle (i) and (ii). We introduce DATR as a formal way to represent word knowledge.

Translation By Structural Correspondences We outline and show a method for machine translation that takes advantage of matching patterns between different levels of language representation, as explained in the LFG (Lexical Functional Grammar) idea of codescriptions. This method is demonstrated with examples from English, German, and French where the original and translated sentences have significant differences in language analysis. The system offers a formal way to describe complex translation connections between source and target languages in a clear manner, using monolingual (single language) grammar rules and word lists that are independently supported and logically sound.

Named Entity Recognition Without Gazetteers It is often said that systems for recognizing named entities (important names like people, companies, or places) need long lists of these names, called gazetteers. Creating these lists can sometimes slow down the development of these systems. We discuss a system that uses both rule-based methods and statistical models (ways to predict based on data) to recognize named entities. We tested the system's performance with different kinds and sizes of gazetteers, using material from the MUC-7 competition. We found that for this competition, using small lists of well-known names works just as well as using large lists of less common names. We also share thoughts on how our findings apply to different areas beyond the competition. We use the context of the text to clarify confusing items. We take advantage of consistency (keeping things the same) within a document using flexible, multi-step labeling methods.

An Efficient Method For Determining Bilingual Word Classes In statistical natural language processing, we often deal with the issue of having too little data. To help solve this, we can group words into similar categories, which is a common method in statistical language modeling. In this paper, we explain a way to find bilingual word groups that work well for statistical machine translation. We create a rule for optimizing based on the best fit approach and explain a method for grouping words. We will demonstrate that using these bilingual word groups can enhance statistical machine translation. We show improvements on the complexity of understanding a bilingual text collection and the accuracy of translating words using a pattern-based translation model. We explain a method for finding bilingual word groups, which helps improve the process of finding alignment patterns through connections between groups, not just between individual words.

Representing Text Chunks Dividing sentences into groups of words is a helpful first step for understanding sentence structure, finding specific information, and searching for information. (Ramshaw and Marcus, 1995) introduced an "easy" way to represent this by turning it into a task where each word is labeled. In this paper, we will look at seven different ways to represent data for finding groups of words that describe things (noun phrase chunks). We will show that the choice of how to represent data has a small effect on how well this works. However, using the best way to represent data, our memory-based learning system was able to get better results than the best ones previously published for a standard set of data. We explain in detail the IOB methods, which are a way of marking the beginning, inside, and outside of these groups.

Inducing Multilingual Text Analysis Tools Via Robust Projection Across Aligned Corpora This paper explains a system and set of methods for automatically creating language tools, like part-of-speech taggers (which label words as nouns, verbs, etc.), noun-phrase bracketers (which find groups of words that act together), named-entity taggers (which find names of people, places, etc.), and morphological analyzers (which study word forms) for any foreign language. Examples include French, Chinese, Czech, and Spanish. Existing English language tools are used on bilingual texts, and their results are transferred to the second language using statistical word matches. Simple direct transfer of these results is often messy, even with the best word matches. Therefore, this paper introduces methods that can handle this messiness and still train accurate language tools from these imperfect initial results. The French part-of-speech tagger achieves 96% accuracy, and the noun-phrase bracketer scores over 91% in precision and recall. The morphological analyzer gets over 99% accuracy in analyzing French verbs. This is impressive because it didn't need any manually labeled data in the new language, and almost no specific knowledge or resources beyond basic text. This performance is much better than what simple result transfer would give. The paper also discusses early work on transferring part-of-speech information from English to French and Czech using aligned bilingual texts.

On Coreference Resolution Performance Metrics The paper introduces a new way to measure how well computers can identify references to the same thing in different parts of a text, called Constrained Entity-Alignment F-Measure (CEAF). This measurement works by matching groups of related words (coreference chains) from both the reference and the computer system, with the rule that each group can only match with one group from the other side. We demonstrate that finding the best match is a type of problem called maximum bipartite matching, which can be solved using a specific method known as the Kuhn-Munkres algorithm. We perform tests that show the commonly used MUC F-measure has significant problems in evaluating these systems. The new measurement is also compared with another standard method used in a task called Automatic Content Extraction (ACE), and we find that our method has advantages like being fair and easier to understand, which the ACE method lacks. We use a structure called a Bell tree to keep track of and score the different ways of searching.

A Discriminative Matching Approach To Word Alignment We present a method that focuses on distinguishing features (key characteristics) to match words from different languages. In this method, each pair of words is given a score based on certain features, like how often the words are used together, how far apart they are in a sentence, how similar they look, and more. Even with just 100 examples that we know are correct and using simple features that count word occurrences from a large text collection that hasn't been specifically labeled, we reach results close to a well-known method called IBM Model 4, but we do it faster. By including predictions from Model 4 as part of our features, we reduce errors by 22% when compared to Model 4 alone. We use a method that focuses on ensuring that the overall alignment rules are followed by making sure each specific word-to-word link follows these rules. We use a rule that each word can only be linked once, meaning each word can connect to only one other word. We approach the task of aligning words as a problem of finding the best match between two groups, with each word acting as a point that needs to be connected to the best possible match in the other group.

A Discriminative Framework For Bilingual Word Alignment Bilingual word alignment is a key part of most methods for statistical machine translation, which is how computers translate languages using data. Current methods for aligning words are mostly based on generative models, which are like recipes that predict word links based on certain patterns. In this paper, we show a different method called a discriminative approach, which is another way to train simple models for aligning words. These simple models work just as well as the more complex generative models usually used. These models are easy to improve by adding new features and they allow quick tuning of model settings with just a little bit of labeled data. LLR, which stands for log-likelihood ratio, can still be used to find positive word connections by first removing words that might have negative connections. We train two models, named stage 1 and stage 2. Both models work by combining weighted feature values, which are characteristics taken from a pair of sentences and their suggested word alignment. We use statistics like log-likelihood-ratio and conditional likelihood-probability to measure how words are connected.

A Maximum Entropy Word Aligner For Arabic-English Machine Translation This paper introduces a method for connecting words between Arabic and English using a technique called maximum entropy, which is based on examples that have been carefully prepared. We show it's possible to create materials to help solve translation problems and that using a mix of guided and self-learning methods works better. The model, which uses probability, directly focuses on deciding which words should be linked. We show that this method is much better than older ways of linking words and improves several translation tests. We compare how well the algorithm works to how well humans do the same task. We introduce a specially trained model that can connect one word to many others, using specific features for Arabic. We trained this model on a set of 10,000 pairs of Arabic and English sentences, and it performs better than a previous standard tool called GIZA++.

Local Phrase Reordering Models For Statistical Machine Translation We explain random models of moving phrases around locally that can be added to a Statistical Machine Translation (SMT) system. These models give well-defined, complete probability patterns for rearranging phrases. They use Weighted Finite State Transducers, which are a way to process sequences. We explain a method to adjust parameters using EM-style (a statistical technique) based on aligning phrases in the full translation model that includes reordering. Our tests show that this reordering model greatly improves translation quality from Arabic to English and Chinese to English. We also show that this method works well even when the text size increases. We offer a method that works efficiently in a reasonable amount of time. We define two local reordering models for their Translation Template Model (TTM): In the first one, called MJ-1, only phrases next to each other can switch places, and the switch must happen within a small range of 2 phrases.

Extracting Product Features And Opinions From Reviews Consumers often have to read through many online reviews to make a smart product choice. This paper talks about OPINE, a system that automatically goes through reviews to find important product features, what reviewers think about them, and how they compare to other products. OPINE does a better job than previous methods, getting 22% more precise results (but 3% less in finding everything) when finding product features. OPINE uses a new way to understand the meaning of words based on their context, which helps it find opinion phrases and whether they are positive or negative. Our method uses Wikipedia to find a page for a phrase or single term in a question. We not only look at whether opinions about product features are positive or negative but also rank them based on how strong they are. We show a method that discovers product features using data from a large collection of texts, connections from WordNet (a large database of words), and hints from word forms. The importance ranking and finding were done using a method called Pointwise Mutual Information, which measures how often two things happen together.

Recognizing Contextual Polarity In Phrase-Level Sentiment Analysis This paper introduces a new method for analyzing feelings in phrases. It first checks if a phrase is neutral (without emotion) or polar (with emotion) and then figures out if the polar expression is positive or negative. With this method, the system can automatically find out the emotional context for many expressions, performing much better than simple methods. We suggest using supervised learning, which means training the system with examples, by splitting resources into initial emotion (prior polarity) and context-based emotion (context polarity). Our tests show that looking up words in a dictionary won't work well for understanding emotions in general writing. We create a list of words with their emotional levels labeled as strong or weak, and their emotion type as positive, negative, or neutral. Our MPQA list includes separate lists for clues that show emotions, words that make emotions stronger, and words that change the feeling, which help in finding the main opinion words, modifying words, and words that show opposites.

Identifying Sources Of Opinions With Conditional Random Fields And Extraction Patterns Recent systems have been made to figure out if something has a positive or negative feeling, recognize opinions, and analyze them (like finding out how strong those feelings are). We are focusing on another part of opinion analysis: finding out who is expressing these opinions, feelings, and thoughts. We see this as a task of finding specific pieces of information and use a mixed method that uses Conditional Random Fields (a tool for predicting sequence data) and a version of AutoSlog (a method for learning patterns to extract information). CRFs help identify who is giving the opinion by looking at sequences, while AutoSlog helps learn the patterns of how information is shared. Our findings show that using both methods together works better than using each one separately. The system we developed can identify where opinions come from with 79.3% accuracy and finds 59.5% of the sources correctly using a method that matches certain key words, and with 81.2% accuracy and 60.6% correct findings using a method that looks for overlapping information.

Domain-Specific Sense Distributions And Predominant Sense Acquisition Distributions of the meanings of words are often very uneven. This fact is used by word meaning understanding systems, which rely on the most common meaning of a word when the context isn't clear enough. The topic of a document strongly affects how words are understood, but it's not practical to create large collections of manually labeled texts for every topic. In this paper, we explain how we built three collections of texts with labeled meanings in different topics for a sample of English words. We use an existing method to automatically find the most common meaning from plain text, and we show that (1) getting this information automatically from a mixed-topic collection is more accurate than getting it from SemCor (a specific text collection), and (2) getting it automatically from text in the same topic as the target topic works best by a large difference. We also show that for a task where all words need to be understood in context, this automatic method works best for words that are important to the topic, and for words that have a different most common meaning in that topic compared to one from a general collection. Our dataset includes 3 collections of documents: a topic-neutral collection (BNC), and two topic-specific collections (SPORTS and FINANCE).

Bidirectional Inference With The Easiest-First Strategy For Tagging Sequence Data This paper introduces a two-way thinking method for labeling sequences, like identifying parts of speech, recognizing names in text, and breaking text into chunks. The method can list all possible ways to break down the sequence and find the most likely one quickly. We also introduce a fast decoding method using an easiest-first strategy, which performs almost as well as the full two-way method but uses much less computing power. Tests on part-of-speech tagging and text chunking show that this two-way method consistently beats one-way methods, and two-way MEMMs perform as well as top learning methods like advanced support vector machines. We suggest using an easiest-first straightforward decoding method.

Non-Projective Dependency Parsing Using Spanning Tree Algorithms We describe weighted dependency parsing as finding the largest spanning trees (MSTs) in directed graphs (a network of points connected by arrows). With this setup, the parsing method by Eisner (1996) works well for finding all projective trees (a type of structured tree) in O(n3) time, which means it takes a certain amount of time based on the number of elements. Surprisingly, this setup can be easily adapted for non-projective parsing (more flexible tree structures) using the Chu-Liu-Edmonds MST algorithm (created by Chu and Liu, 1965; and Edmonds, 1967), leading to a faster O(n2) parsing process. We test these methods on the Prague Dependency Treebank using online large-margin learning techniques (methods for improving prediction accuracy) by Crammer et al., 2003; and McDonald et al., 2005, and find that MST parsing improves efficiency and accuracy for languages with non-projective dependencies (languages where relationships don't follow a simple tree structure). The main idea is to create a complete graph (a network where every point is connected to every other point) using the words of the sentence, where each connection (edge) is given a score by a learned scoring function.

Emotions From Text: Machine Learning For Text-Based Emotion Prediction In addition to information, text contains feelings and emotions. This paper looks into predicting emotions from text using a method called supervised machine learning, specifically with the SNoW learning system. The aim is to identify the emotional tone of sentences in children's fairy tales to help with making text-to-speech sound more expressive. Initial tests on a small set of 22 fairy tales show positive results compared to a simple narrative approach and a Bag of Words (BOW) method for telling apart emotional and non-emotional content, though it depends somewhat on adjusting certain settings. We also talk about results for a model that includes emotional positivity or negativity, and changes to the features used. Additionally, we share plans for a more advanced model that considers a wider range of basic emotions.

Recognising Textual Entailment With Logical Inference We use logical reasoning techniques to identify if one piece of text logically follows from another. Since the performance of proving logical statements relies heavily on extra information that isn't easily accessible, we add model building, a technique from automated reasoning, to show it is a strong method to estimate entailment. We also use machine learning to combine these deep understanding methods with simple word matching; this combined model performs very well on the RTE test set, based on the latest standards. Our results show that the different methods we use work very differently on some parts of the RTE dataset, so it's helpful to consider the type of data as a feature. Often, not having enough language knowledge causes the system to fail in making inferences, so it often predicts "no entailment" for most pairs. Our system is based on logical representation and automatic logical statement proving, but only uses WordNet (a large database of English words) as a language knowledge source.

A Shortest Path Dependency Kernel For Relation Extraction We introduce a new method for finding relationships between two named things in a sentence. This method is based on the idea that the shortest connection between them in a sentence's structure usually contains the needed information. Tests using news articles from the ACE (Automated Content Extraction) dataset show that this new method is better than a recent one that used a different approach to sentence structure. This research indicates that the shortest path between two entities in a sentence effectively captures the information needed to identify their relationship.

OpinionFinder: A System For Subjectivity Analysis We provide a subjectivity lexicon, which is a list of words that show opinions or feelings. We provide a pre-trained classifier, which is a tool that has already learned how to tag or label the phrases in a sentence with their contextual polarity values, meaning it can identify if the phrases are positive, negative, or neutral in the context they are used.

Identifying Word Correspondences In Parallel Texts Motivated by the need to use less computer memory and ensure the method is strong and reliable when guessing chances, we suggest a different method where chances are not guessed and saved for every pair of words. We suggest a way to measure how strongly words from one language match with words from another language. We use a measure called phi2 statistics to show how well word pairs match and demonstrate that it worked better than the mutual information method.

A Procedure For Quantitatively Comparing The Syntactic Coverage Of English Grammars We explain PARSEVAL measures for parsing: labelled precision, labelled recall, and labelled F-measure (Prec., Rec., and F1, respectively). These are based on counting how many parts in the parser's output match the correct or "gold-standard" parse.

Towards History-Based Grammars: Using Richer Models For Probabilistic Parsing. We explain a model for understanding natural language, called HBG, which uses detailed language information to clear up confusion. HBG uses information about words, sentence structure, meaning, and organization from the parse tree (a diagram showing sentence structure) in a new way to help decide the correct meaning of a sentence. We use a collection of labeled sentences, known as a Tree-bank, along with decision trees (a method for making choices) to identify important parts of a sentence tree that help find the right interpretation of a sentence. This is different from the usual method of adjusting grammar by thinking about language in the hope of finding the right interpretation. In direct comparisons with one of the best existing strong language understanding models, called P-CFG, the HBG model performs much better, improving understanding accuracy from 60% to 75%, which is a 37% reduction in mistakes. We introduce history-based parsing, using previous steps in understanding to predict the next move in the process.

One Sense Per Discourse It is well-known that there are words with multiple meanings, like "sentence," where the meaning depends on how it's used in a sentence. We have recently reported on two new systems that figure out which meaning of a word is intended. One system was trained using material in two languages (the Canadian Hansards), and the other used material in one language (Roget's Thesaurus and Grolier's Encyclopedia). As this work was almost finished, we noticed a strong effect related to the overall discussion or context. This means that if a word with multiple meanings, like "sentence," appears more than once in a well-written text, it is very likely that it will have the same meaning each time. This paper describes an experiment that confirmed this idea and found that the likelihood of sharing the same meaning in the same discussion is very high (98%). This result can help improve the systems that figure out word meanings by using this information as a guide. Additionally, it could help evaluate other systems that do not use context to determine word meanings. We claim, based on analyzing large collections of texts, that a word usually keeps the same meaning throughout a text.

Corpus-Based Statistical Sense Resolution The three methods studied here try to figure out the right meaning of a word that has multiple meanings by looking at how words appear together. The methods were based on different techniques: using probability theory (Bayesian decision theory), computer systems that mimic the human brain (neural networks), and analyzing the content for finding information (content vectors). To understand these methods better, we focused on a specific problem: given a few examples where the word "line" is used clearly, create a tool that picks the right meaning of "line" in new situations. To see how having more meanings affects the results, we compared tasks with three meanings and six meanings. The results show that each method can identify six meanings of "line" with over 70% accuracy. Also, the way these tools responded was mostly similar to each other statistically. Comparing the two tasks suggests that how hard it is to decide on a specific meaning is more important than how many meanings a word has. We created a dataset with 2094 instances of the word "line" for working out its meaning.

One Sense Per Collocation Previous work [Gale, Church and Yarowsky, 1992] showed that with high probability a word with multiple meanings (polysemous) has one meaning in a particular piece of writing (discourse). In this paper, we show that for certain ways of grouping words together (collocation), a word with multiple meanings mostly has just one meaning in that group. We test this idea for different meanings and groupings, and find it works 90-99% of the time for situations with two possible meanings (binary ambiguities). We use this in a method to figure out meanings (disambiguation algorithm) that is 92% accurate by looking at the immediate context. We define collocation as when two words appear together in a specific way. To study and compare different word groupings, we measure unpredictability (entropy) and check the results when applying these groupings to new data using organized lists (decision lists). We find that the objects (things affected by verbs) are more important than the subjects (doers of actions) in figuring out word meanings (WSD), and nouns get clearer meaning information from nearby nouns or adjectives.

A Semantic Concordance A semantic concordance is a collection of texts and a dictionary combined in a way that every important word in the text is connected to its correct meaning in the dictionary. This means it can be seen as a collection of texts where words are marked with their grammar and meaning, or as a dictionary where you can find example sentences for many meanings. A semantic concordance is being created to help study how the meaning of words can be understood in different contexts. The Brown Corpus is the text used, and WordNet is the dictionary. Special tags that point to WordNet definitions are added to the text manually using a tool called ConText, which was made to make this job easier. Another tool helps search through the tagged text. Some practical uses for these semantic concordances are suggested. We introduce SemCor, a well-balanced dataset with meanings marked by experts, where all important words are manually tagged.

The Penn Treebank: Annotating Predicate Argument Structure The Penn Treebank has recently added a new way of labeling sentences to show the relationship between verbs and their arguments (like the subject and object). This paper explains important features of this new labeling method. It offers a more consistent way to handle different grammar rules, includes special markers for things like questions, passive voice, and subjects of phrases without "to" verbs, allows for some complex grammar structures to be easily identified, and provides a simple labeling system for some meanings in sentences. Our Switchboard collection has transcripts of natural spoken conversations with labeled sentence structures. The Penn Treebank II identifies subjects, logical objects in passive sentences, some shortened relative clauses, and other grammar details, but it doesn’t label each part of a sentence with its specific grammar role.

Using A Semantic Concordance For Sense Identification This paper suggests standards for systems that automatically figure out word meanings. A collection of written texts, where important words were marked for grammar and meaning, was used to test three strategies for identifying meanings: a guessing method, a most-common method, and a method based on words appearing together. When there was no information on how often meanings were used, the guessing method using the number of possible meanings in WordNet was correct 45% of the time. When data on meaning frequency was taken from a meaning database, assuming each word is used in its most common meaning was correct 69% of the time; for words with multiple meanings, it dropped to 58%. And when a method based on words appearing together used previous occurrences of words in the same sentences, there was little improvement. The meaning database is still too small to determine the potential limits of using words appearing together. We prepare a sense-marked text collection SEMCOR, which includes a large part of the Brown text collection marked with the detailed meanings from WORDNET.

A Maximum Entropy Model For Prepositional Phrase Attachment We create a standard set of 27,937 examples of how prepositional phrases (like "on the table") attach to sentences, taken from the Wall Street Journal articles. We train a model that predicts how these phrases attach by using groups of four elements (verb, noun1, preposition, noun2) from these articles and reach 81.6% accuracy. Our method uses a clustering algorithm, which groups similar things together, based on mutual information, which measures how much knowing one thing tells us about another.

Syntax Annotation for the GENIA Corpus Linguistically annotated corpus based on texts in biomedical domain has been constructed to improve natural language processing (NLP) tools for finding information in biology texts. As the focus of finding information is shifting from "nominal" information like names to "verbal" information like functions and interactions of substances, using parsers (tools that analyze sentence structure) has become important, making a collection of sentences with marked sentence structure needed. A part of the GENIA corpus, containing 500 short scientific articles from MEDLINE, has been marked for sentence structure in an XML-based format using the Penn Treebank II (PTB) method. A test to see how much different people agree on this annotation showed that the style of writing, rather than the content of the research summaries, makes it hard to annotate sentence structure. It also showed that people who study languages can do the annotations consistently without needing much biology knowledge, as long as they have good guidelines for scientific text language issues. Our GENIA Treebank Corpus is estimated to have no command sentences and only seven question sentences.

The Second International Chinese Word Segmentation Bakeoff The second international Chinese word segmentation bakeoff was held in the summer of 2005 to evaluate the current best methods in breaking down Chinese text into words. Twenty-three groups submitted 130 sets of results over two types of tests and four different collections of text. We found that the technology has gotten better over the past two years, but the issue of dealing with new, unknown words remains very important. In the event, two of the top systems in the closed track competition used a CRF model, which stands for Conditional Random Field, a type of statistical model used for predicting patterns.

A Maximum Entropy Approach to Chinese Word Segmentation We took part in the Second International Chinese Word Segmentation Bakeoff. We tested our Chinese word splitter in the open category on all four datasets: Academia Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (MSR), and Peking University (PKU). Using a method called maximum entropy, our word splitter got the highest accuracy score (F measure) for AS, CITYU, and PKU, and the second highest for MSR. We discovered that using an extra dictionary and more training data with different word-splitting rules helped improve how accurately words were split. We introduce a final step method to better split words that are not known. We use patterns that represent things like numbers, dates, and letters.

A Conditional Random Field Word Segmenter for Sighan Bakeoff 2005 We present a Chinese word segmentation system submitted to the closed track of Sighan bakeoff 2005. Our segmenter was built using a model called conditional random field, which allows us to use many language features like character identity (what each character is), how words change form (morphological features), and when characters repeat (character reduplication). Because we got our language features automatically from the training texts, our system wasn't biased towards any specific type of Mandarin. This means our system didn't become too focused on just one familiar type of Mandarin. Our final system got high scores: 0.947 (AS), 0.943 (HK), 0.950 (PK), and 0.964 (MSR). We developed the Stanford Chinese word segmenter.

Using Contextual Speller Techniques and Language Modeling for ESL Error Correction We introduce a flexible system to find and fix mistakes made by people who are learning English. We mainly look at two types of mistakes: using articles (like "a" and "the") incorrectly and picking the wrong prepositions (like "in," "at," or "on"). We use a method similar to spell checkers to find mistakes and suggest corrections. We also use a big language model, trained on a large collection of written texts called the Gigaword corpus, to help remove bad suggestions. We demonstrate how well this system works on text written by English learners and talk about how we can improve it in the future. We combine a language model with a classifier (a tool that sorts data) to make a better decision system. We use a single score from the language model to decide if a correction is likely to be wrong, helping us choose the best correction.

Learning Dependency Translation Models As Collections Of Finite-State Head Transducers The paper explains weighted head transducers, which are machines that process and transform strings from the middle outwards. These machines are more capable than the usual type that works strictly from left to right. Dependency transduction models use groups of these weighted head transducers, applied in a step-by-step manner. The paper describes a smart search method to find the best way to change an input string using this model. It also explains how to automatically train these models using examples of strings that show both input and output. The training method first looks for patterns in the examples using statistical guidance, then builds the head transducers based on those patterns. Experiments were conducted using this training approach to translate English into Spanish and Japanese. The study treats translation as creating linked tree structures from both the source and target languages using head transduction. It shows a two-step process for organizing words and chunks using a collection of these machines. The paper also discusses how to create matching tree structures from pairs of texts that don't have predefined sentence parts, using a machine to generate each part's connections.

Models Of Translational Equivalence Among Words Parallel texts (bitexts), which are texts translated side-by-side, have special features different from other types of similar data. First, most words translate to only one other word. Second, the match between these texts is usually incomplete - many words in each text don't have a clear counterpart in the other text. This article introduces ways to adjust statistical translation models to consider these features. Tests using independent human opinions have shown that translation models adjusted this way are much more accurate than basic models that don't use prior knowledge. This article also explains how a statistical translation model can use any existing knowledge about specific language pairs. Even the simplest language-specific knowledge, like knowing the difference between important words and helper words, can improve the model's performance on certain tasks. Statistical models that use knowledge about the area they are applied to combine the strengths of both logical thinking and experience-based methods. We measure how words look similar using the longest common subsequence ratio (LCSR), which checks how much of a word sequence is shared between words. We define a direct association as a clear match between two words that are true translations of each other. We suggest a Competitive Linking Algorithm (CLA) to connect words and build a confusion network, which helps in translation. We use competitive linking to quickly create pairings where the pair score measures how well two words are connected. We suggest there are ways to find the edges of some phrases made of multiple words, allowing us to treat several words as one unit.

Dialogue Act Modeling For Automatic Tagging And Recognition Of Conversational Speech We explain a method using statistics to understand dialogue acts in conversation speech. Dialogue acts are parts of speech like STATEMENT, QUESTION, BACKCHANNEL (listening sounds like "uh-huh"), AGREEMENT, DISAGREEMENT, and APOLOGY. Our method finds and guesses these acts using word choices, word patterns, voice tone, and how well the conversation makes sense. The method uses a hidden Markov model, which is a way to see conversation structure as hidden steps with dialogue acts as visible actions. We also use a dialogue act n-gram, which is a way to predict the likely order of dialogue acts. The grammar for dialogue is mixed with word patterns, decision trees (which help make decisions based on data), and neural networks (computer systems that learn) to understand the unique way of speaking each dialogue act. We create a way to combine speech recognition with understanding dialogue acts to make both more accurate. The models are trained and tested with a big database of 1,155 conversations from the Switchboard collection of natural telephone talks. We reached good accuracy in labeling dialogue acts (65% with automatically recognized words and voice tone, and 71% with correct word transcripts, compared to a random accuracy of 35% and human accuracy of 84%) and slightly reduced errors in word recognition. We use HMMs (hidden Markov models) as a general way to understand conversation structure applied to speech acts in talks.

A Compression-Based Algorithm For Chinese Word Segmentation. Chinese is written without spaces or markers to show where words end. Even though text can be thought of as a series of words, it's often unclear where one word ends and another begins. Breaking text into words is helpful for tasks like searching through text, compressing data to use less space, and picking out important phrases. We explain a method that guesses where word boundaries should be using a flexible language model, which is commonly used to compress text. This model is trained on already divided text, and when used on new text, it finds where words should be split to make the text as small as possible when compressed. This straightforward and widely applicable method works well compared to other specialized methods for dividing Chinese text. Our approach, based on n-gram language modeling, doesn't rely on specific knowledge about the subject area.

An Empirically Based System For Processing Definite Descriptions We introduce a system that deals with specific descriptions in any subject area. The system's design is based on previous analysis of text collections, showing that many new descriptions appear in newspapers. The marked text collection was used to test techniques for matching specific descriptions to their earlier mentions, dividing text into sections, identifying new descriptions, and suggesting links for connecting descriptions. A big challenge in resolving specific noun phrases with detailed main words is that only a small part of them are actually referring back to something earlier (about 30%). In our system, we use WordNet, a tool for finding synonyms, broader terms, and part-whole relationships, to help resolve these references. We categorize each specific description as either a direct reference, new in the text, or a connecting description. We separate important details from less important ones by ignoring words between commas, which shouldn't start a chain of references. For deciding if something is new in the text, the most important factor is if the main word has been used before.

On Coreferring: Coreference In MUC And Related Annotation Schemes In this paper, it is argued that "coreference" annotations, as done in the MUC community for example, go far beyond just marking when two things refer to the same entity. As a result, it is not always clear what meaning these annotations are showing. The paper talks about several issues with these annotations and concludes that we need to rethink the coreference task before making it bigger. Specifically, it suggests dividing the work so that identifying when two things refer to the same entity is separate from other tasks like marking when a word refers back to something else (bound anaphora) and showing the relationship between a subject and a descriptive noun phrase (predicative NP). However, it has several problems (van Deemter and Kibble, 2000), especially that the one meaning relation defined by the scheme, called ident, mixes up several relations that language experts see as different: besides actual coreference, there are identity anaphora, bound anaphora, and even predication.

Unsupervised Learning Of The Morphology Of A Natural Language This study talks about using a method called minimum description length (MDL) to understand how to break down words in European languages into smaller parts without prior examples, using text collections ranging from 5,000 to 500,000 words. We create a set of rules (heuristics) that quickly build a system to predict word parts, and we use MDL to decide if these changes should be kept. The resulting word analysis closely matches what a human expert would do. In the last part, we talk about how this way of using MDL relates to early ways of judging grammar rules. We suggest a repeated pattern where the main part of a word (stem) can include a smaller part and an ending (suffix). We use a method to show word parts based on "signatures," which are groups of word endings that show how words are related by their word form changes. We notice that less common and shorter word endings are more likely to be mistakes.

Improving Accuracy In Word Class Tagging Through The Combination Of Machine Learning Systems We look at how using different language models, learned by different computer systems doing the same Natural Language Processing (NLP) job, can help achieve better accuracy than using just one system alone. We test this by doing experiments on morphosyntactic word class tagging, which means identifying word types like nouns and verbs, using three different collections of text that have already been tagged. We train four well-known tools (hidden Markov model, memory-based, transformation rules, and maximum entropy) on the same text data. Then, we compare their results and combine them using different voting methods and extra classifiers to improve performance. All the combined systems work better than the best single system. The decrease in mistakes depends on the material used, but can be as much as 24.3% with the LOB text collection. We report an accuracy of about 97% for identifying parts of speech (POS) using the Penn Treebank when the training and testing data are from similar sources.

Probabilistic Top-Down Parsing And Language Modeling This paper explains how a wide-ranging probabilistic top-down parser works and how it helps with creating language models used in speech recognition. The paper starts by explaining important ideas in language modeling and probabilistic parsing and gives a quick overview of how others have used sentence structure for language modeling before. It then introduces a specific type of probabilistic top-down parser that uses word information (lexicalized) and performs very well in terms of how accurately it interprets sentences and how efficiently it does this compared to other top statistical parsers. A new language model that uses this type of parsing is explained, showing through tests that it does better than older methods in understanding test texts (corpus perplexity). Mixing it with a trigram model (a model that looks at sequences of three words) leads to a big improvement, showing that the information our model captures is different from what the trigram model captures. A small test also shows how useful the model is. Our parser reads a sentence from start to finish (left-to-right) and skips using a method called dynamic programming, instead using a method called beam search, which keeps many possible sentence interpretations open by adding the next word, then cutting down the choices. At each word in the sentence, our top-down parser offers a set of partial interpretations with weights, showing how likely each one is.

The Interaction Of Knowledge Sources In Word Sense Disambiguation Word sense disambiguation (WSD) is a task in computer language studies that can benefit from using different types of knowledge, similar to how artificial intelligence research does. A key step in testing this idea is figuring out which types of language knowledge are most helpful and if combining them makes results better. We show a sense tagger, a tool that labels meanings, which uses several kinds of knowledge. Tested accuracy is above 94% on our test data. Our system tries to figure out the meaning of all main words in a text, instead of just focusing on a limited set of words. It's argued that this method is more likely to help make useful systems. We present a framework that combines different methods of figuring out word meanings, like simulated annealing (a problem-solving method), subject codes, and selectional restrictions (rules about which words can go together), using the TiMBL memory-based approach (a method developed by Daelemans and others in 1999). We use the Longman Dictionary of Contemporary English (LDOCE) as our list of word meanings. We use POS (part of speech) tags of the main word to help with disambiguation related to differences in grammar. We suggest that using both grammar-related and word-related features will make it easier to correctly figure out word meanings.

Automatic Verb Classification Based On Statistical Distributions Of Argument Structure Automatic collection of word knowledge is important for many tasks involving how computers understand language. It's especially important to know about verbs, which are the main words that show how actions or states connect people or things in a sentence (like who did what to whom). In this study, we describe experiments using supervised learning (a method where computers learn from examples) to automatically sort three main types of English verbs, based on how they use other words in a sentence (their argument structure) and the roles they give to other words. We use statistical clues inspired by language rules, taken from large collections of text that have been labeled, to train the computer system. We achieved 69.8% accuracy for this task, where guessing would give 34% accuracy, and experts could get up to 86.5%. A closer look at how the system performs and where it makes mistakes shows that the features we used successfully identify how verbs structure sentences. Our findings support our ideas that knowing about these relationships is key for sorting verbs and that this knowledge can be gathered from texts automatically. We show a successful mix of deep language understanding with the strength and ability to handle large amounts of data using statistical methods. We use a Decision Tree (a model that makes decisions based on questions) and specific language clues to sort English verbs into three categories: unaccusative, unergative, and object-drop.

A Machine Learning Approach To Coreference Resolution Of Noun Phrases In this paper, we introduce a method using machine learning to figure out when different noun phrases (like names or things) in a text refer to the same thing. Our method learns from a small collection of text examples that are marked up with information and works on all kinds of noun phrases, not just specific ones like pronouns. It doesn't limit the types of things the nouns can refer to, such as "organization" or "person." We test our method on well-known data sets (called MUC-6 and MUC-7) and get good results, showing that our method is promising and its accuracy is similar to methods that don't use learning. Our system is the first to use learning that performs as well as the best non-learning systems on these data sets. We include all noun phrases identified by their NP (noun phrase) detector and report scores of 62.6% for MUC-6 and 60.4% for MUC-7. We create a graph that connects mentions by learning to decide which previous mention belongs with a new mention, a process known as the pairwise coreference model.

A Critique And Improvement Of An Evaluation Metric For Text Segmentation The Pk evaluation metric, first suggested by Beeferman, Berger, and Lafferty (1997), is becoming the usual way to judge how well text segmentation algorithms work. But, a detailed study of this metric shows several issues: it is harsher on false negatives (missing a boundary) than false positives (adding an extra boundary), it punishes close guesses too much, and it is influenced by changes in the size of text parts. We suggest a simple change to the Pk metric to fix these problems. This new metric—called WindowDiff—uses a fixed-sized window that moves across the text and penalizes the algorithm when the number of boundaries within the window does not match the actual number of boundaries in that part of the text. As a way to measure how well text is divided, we develop WindowDiff, which only looks at where the boundaries are, not what labels are given to them.

Generating Referring Expressions: Boolean Extensions Of The Incremental Algorithm This paper looks at creating ways to talk about things from a logical viewpoint, fixing gaps in current methods. After looking at how we refer to single objects, we talk about how we refer to groups, including using words like "not" or "or" to describe them. To ensure we always create a unique description when possible, the paper suggests improving and expanding on the Incremental Algorithm by Dale and Reiter (1995).

Class-Based Probability Estimation Using A Semantic Hierarchy This article is about estimating a certain type of probability, specifically, the chance of a noun meaning appearing as a specific part of a sentence. To deal with the issue of not having enough data, the idea is to define probabilities using a system of related meanings and take advantage of the fact that these meanings can be grouped into categories with similar meanings. The focus is on how to find the right category for a given meaning or how to decide on the right level of generalization in this system. A method is created that uses a statistical test called the chi-square test to find the right level of generalization. To test how well the estimation method works, we use a task that simulates choosing between meanings, along with two other estimation methods. Each method uses a different way to generalize: the first uses a principle called the minimum description length, and the second uses Resnik’s method for choosing preferences. We also check how well our method works using both the standard and a different version of the chi-square statistic. In short, we fill in the WordNet hierarchy based on how often nouns appear with a verb or slot, and then find the right probability estimate at each point in the hierarchy by using the chi-square test to decide if we should generalize an estimate to a higher level in the hierarchy.

Automatic Labeling Of Semantic Roles We present a system for identifying the meaning-based relationships, or roles, that parts of a sentence play within a context. Given a sentence, a key word, and a context, the system labels parts of the sentence with general roles, like Agent (doer) or Patient (receiver), or more specific roles like Speaker, Message, and Topic. The system uses statistical programs trained on about 50,000 sentences that were manually labeled with these roles by the FrameNet project. We then broke down each training sentence into a structure tree and picked out different word and structure features, including the type of phrase, its role in the grammar, and its spot in the sentence. These features were mixed with information about the main verb, noun, or adjective, and other details like the chances of different role combinations. We used various methods to group similar words to cover all possible role fillers. Test sentences were structured, labeled with these features, and then run through the programs. Our system correctly identifies 82% of the roles for already divided sentence parts. For the harder task of dividing and identifying roles at the same time, the system did 65% correct labeling and 61% in finding all correct labels. Our study also let us compare which features and ways of combining them are most helpful for this task. We also look into mixing role labeling with sentence structure analysis and try to apply it to new situations not covered in training. We propose the first model for this task using FrameNet.

Summarizing Scientific Articles: Experiments With Relevance And Rhetorical Status In this article, we suggest a way to summarize scientific articles by focusing on the importance of statements in an article. The summaries are created to show the new ideas of the article and compare them to past research. We offer a model for these summaries using a large collection of conference articles in computational linguistics, marked with human opinions on the importance and relevance of each sentence. We show several tests on how much our judges agree on these markings. We also introduce a program that uses the marked training material to pick out content from new articles and sorts it into seven fixed categories based on their rhetorical, or persuasive, style. The result of this system can be seen as a summary of a single document; or it can be used as a base to create summaries tailored to specific tasks or users, helping them understand a scientific field. We look at the challenge of summarizing scientific articles by analyzing the persuasive style of sentences. We summarize scientific articles by choosing common persuasive elements found in scientific summaries.

A Systematic Comparison Of Various Statistical Alignment Models We present and compare different methods for finding word alignments using statistical or rule-based (heuristic) models. We look at the five alignment models introduced by Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov model (a statistical model that predicts sequences), techniques to smooth out data, and improvements. These statistical models are compared with two rule-based models based on the Dice coefficient (a mathematical formula to compare how similar two sets are). We show different ways to combine word alignments to make a balanced version of directed statistical alignment models. To judge them, we look at how good the resulting Viterbi alignment (a method for finding the most likely sequence of states) is compared to a hand-made reference alignment. We test the models using tasks that involve translating between German and English (Verbmobil) and French and English (Hansards). We carefully examine different choices in designing our statistical alignment system and test these on training texts of different sizes. An important finding is that improved alignment models that consider the order of words and how often they appear together give much better results than simple rule-based models. In the Appendix, we show a fast training method for the alignment models discussed. The trial and test data were manually aligned at the word level, marking specific word pairs as 'sure' or 'possible' alignments.

Graph-Based Generation Of Referring Expressions This article explains a new way to create referring expressions, which are phrases that describe specific things. We suggest representing a scene (a group of things with different characteristics and connections) as a labeled directed graph, which is a type of diagram. Deciding which details to include in a referring expression is like solving a puzzle where you only use part of this diagram. We use cost functions, which are rules that help us choose the best solution, to make the search easier. This method has four main benefits: (1) Graph structures have been well-studied, so by using graphs we can use many existing theories and techniques; (2) many current methods can be reworked using graphs, allowing us to compare and combine different methods more easily; (3) using graphs helps solve problems that older methods had when creating referring expressions; and (4) using graphs along with cost functions helps mix traditional rule-based methods with newer random-based ones. A key strength of this graph-based method is its ability to create expressions that show how things are related, including where they are in space (like next to or on top of something).

Word Reordering And A Dynamic Programming Beam Search Algorithm For Statistical Machine Translation In this article, we explain an effective method called a beam search algorithm, which helps in translating languages using a system called dynamic programming (DP). This search method uses a translation model from Brown and others (1993). We start with a DP solution to a common problem called the traveling-salesman problem and introduce a new way to limit how words can be rearranged between the original and target language to make the search process better. These word rearrangement limits are especially helpful when translating from German to English. We make these limits more general and introduce four simple settings to control how words are rearranged, which can easily be used for new translation tasks. This beam search method has been tested successfully on the Verbmobil task (translating German to English, with 8,000 different words) and on the Canadian Hansards task (translating French to English, with 100,000 different words). For the moderate-sized Verbmobil task, a sentence can be translated quickly in just a few seconds, with only a few mistakes, and no drop in quality, according to the error measurement used in this article. In our research, we adapted a beam-search method used for TSP to work with an IBM-4 word-based model and a phrase-based model respectively.

Introduction To The Special Issue On The Web As Corpus The Web, full of language data in different forms and languages, is a great place for linguists to explore. This special issue of Computational Linguistics looks at how this exciting idea is being investigated. It is normal to wonder if web data is suitable for research because it can be messy and search engines can add unique quirks that might change the results.

The Web As A Parallel Corpus Parallel corpora, which are collections of texts in different languages that are matched, are important for working with multiple languages in natural language processing (teaching computers to understand human languages). In this article, we share our work using the STRAND system to find parallel texts on the internet. We first look at the original method and results, then show important improvements. These improvements include using supervised learning (a way to train computers) based on the structure of documents to better sort them, a new way to measure how well texts match in translation, and adjusting the system to use the Internet Archive to find parallel texts on the Web in a big way. Finally, we show how useful these techniques are by building a large collection of matched texts for a pair of languages that don't have many resources. We first find parallel documents on bilingual websites and then extract matching sentences using a method to align sentences. We use similarities in how URLs (web addresses) and documents are structured, along with other hints, to find parallel documents on the Web.

Using The Web To Obtain Frequencies For Unseen Bigrams This article shows that the Web can be used to find how often certain pairs of words appear when they haven't been seen in a specific collection of texts. We explain a way to get numbers for pairs like adjective-noun, noun-noun, and verb-object from the Web by using a search engine. We test this way by showing: (a) a strong link between Web numbers and those from a text collection; (b) a dependable link between Web numbers and how believable people think the word pairs are; (c) a dependable link between Web numbers and numbers recreated using a method to fill in missing data; (d) good results from Web numbers in a task to check word pair meanings. Our study shows that the huge amount of information on the Web can be more useful than the issues caused by messy data.

Head-Driven Statistical Models For Natural Language Parsing This article explains three statistical methods for understanding natural language structure. These methods build on techniques from simple grammar rules that involve probabilities, adding more detail by focusing on important words in sentences. This creates a structure where a sentence is broken down step-by-step, starting with the main word. The assumptions made lead to rules that capture different grammar patterns like sentence structure, word order, and connections between words. These rules are expressed using chances based on key words. The models are tested using a collection of sentence structures from the Wall Street Journal, showing that they are as accurate as other known models. To understand the models better, we also look at how they perform with different parts of sentences and how well they identify various connections between words. We study the models' features by checking how accurately they can pick up sentence structures, counting how often different structures appear, and using examples based on language rules. Lastly, we compare these models with others to explain why some work better than others. We suggest creating the main word of a phrase first and then the related words, using a process that takes advantage of connections between these words.

Disambiguating Nouns Verbs And Adjectives Using Automatically Acquired Selectional Preferences Selectional preferences, which are patterns showing how words typically go together, have been used by word sense disambiguation (WSD) systems to help figure out the right meaning of a word. We test WSD using selectional preferences collected for English adjective–noun pairs, and sentence parts like subjects and direct objects, using a standard test set. These preferences are linked to types of verbs or adjectives, not just specific words, so they help clarify which adjectives and verbs go together, not just the main noun in a sentence. We also look at using a method called one-sense-per-discourse, which assumes a word has the same meaning throughout a document, to spread a word’s meaning to all its uses in that document, aiming to improve results. While these preferences work well compared to other systems that don’t rely on pre-labeled data, more information sources are needed to improve accuracy and effectiveness for many tasks. Besides measuring how well the system works, we look at when these preferences are most precise and when using one-sense-per-discourse boosts performance. We find that the Word-Class Model, which groups words by type, works effectively in unsupervised WSD, meaning it doesn't need pre-labeled data.

CorMet: A Computational Corpus-Based Conventional Metaphor Extraction System CorMet is a system that uses large collections of text to find metaphorical connections between ideas. It does this by looking for patterns in how words are used in different subject areas, based on information gathered from the internet. Metaphors take ideas from one area (source) and apply them to another (target), making some ideas seem similar in both areas. The verbs (action words) used with a concept in the source area also often apply to its metaphorical equivalent in the target area. This pattern, which can be spotted with a simple language analysis, helps identify metaphorical links between ideas, which can then suggest the presence of common metaphors. Most other systems use small, manually created databases of word meanings and focus on a few examples. Although CorMet only uses a resource called WordNet, it can find many common metaphorical connections and sometimes recognize sentences that use these connections. CorMet is tested on its ability to find a part of the Master Metaphor List. The CorMet system gathers specific subject area texts to find less common uses and identifies metaphorical ideas. We show how using statistical analysis can automatically find and pull out common metaphors from texts, although coming up with new and creative metaphors is still a difficult task.

The Kappa Statistic: A Second Look In recent years, the kappa coefficient (a measure for how much two or more people agree when they label or tag things) has become the usual standard for checking how much people agree in tagging tasks. In this short article, we point out problems that affect kappa, which many people have mostly ignored. First, we talk about the assumptions or things taken for granted when calculating the expected agreement part of kappa. Second, we talk about how commonness (prevalence) and unfairness (bias) affect the kappa measure.

Statistical Machine Translation With Scarce Resources Using Morpho-Syntactic Information In statistical machine translation, the system learns how words in one language relate to words in another language by using collections of texts that exist in both languages. Often, these systems don't use much language knowledge to build their models. Many current translation systems wrongly treat different forms of a word as if they were unrelated. We can use the bilingual text data more effectively by recognizing how these word forms are connected. We suggest building word models based on groups of similar words. We also introduce changes to sentence structure to make the word order in related sentences more similar. We studied how much bilingual text data is needed to keep translation quality acceptable. Our methods for improving translation with limited data have been tested successfully: we reduced the bilingual text data to less than 10% of the original, losing only 1.6% in quality. The translation improvements are shown using two German-English text collections from the Verbmobil and Nespole! tasks. We break down German words into a structure using basic word forms and language tags, and use a MaxEnt model to combine these for translation. We explain a method that reconnects split verbs in German and rearranges questions in both English and German.

Learning Subjective Language Subjectivity in natural language is about using words to show personal opinions, thoughts, or guesses. It is important for many tasks in natural language processing, like pulling out information or sorting text. This study aims to learn how to identify subjective language from large collections of written text. Signs of subjectivity are found and checked, including rare words, word pairs that often appear together, and descriptions and actions identified by comparing word usage. These characteristics are also analyzed to see how they work together. These characteristics, drawn from various data using different methods, show consistent results, meaning they perform similarly across different data sets. Moreover, this article shows that the amount of subjective clues around a word greatly influences how likely it is to be subjective, and it shares the findings of a study that rated how subjective sentences with many such clues are. Finally, these clues are used to identify opinion-based writing (a way to sort text and detect writing style) to show the usefulness of what was learned in this article. We demonstrate that rare words and some common word pairs are strong signs of subjectivity.

The Alignment Template Approach To Statistical Machine Translation A method called the alignment template approach is explained, which is a way to use computers to translate languages by looking at groups of words together. This method helps understand how words relate to each other in both languages and learns how to change the order of words correctly. The model uses a flexible way of adjusting rules, making it easier to improve than older translation systems. We detail how to learn and find good translations of phrases, the steps used, and how the system searches for translations. This method is tested on three tasks. For the German-English speech task called Verbmobil, we study how different parts of the system work. In the French-English Canadian Hansards task, this system works much better than translating one word at a time. In a Chinese-English test in 2002 by the National Institute of Standards and Technology (NIST), it scores much higher than other translation systems. We explain a method to find pairs of phrases from sentences that have been matched up with their best translation.

Intricacies Of Collins Parsing Model This article shares many details about Collins' parser that were not previously published. Together with Collins' 1999 thesis, this article gives all the information needed to recreate Collins' results. These previously unpublished details explain why there is an 11% increase in errors when not using all the details in a simpler version of Collins’ model. We also present a simpler and equally effective way to handle punctuation and connecting words, and we show some surprising things about how Collins’ parser works. We not only look at the impact of the unpublished details but also reassess some well-known details, showing that the model barely uses word-to-word connections and that choosing the main word is not as crucial as previously thought. Finally, we run tests that show the real strength of adding word details is in using the main word and its type to create sentence structures. The results suggest that the strength of Collins-style parsing models is not mainly due to word-to-word connections, but in predicting structures based on main words. We show that word-to-word information is used in only 1.49% of decisions in Collins' Model-2 parser, and removing this information causes only a very small decrease in performance.

Discriminative Reranking For Natural Language Parsing This article looks at methods to improve the results of a probabilistic parser, which is a tool that analyzes sentence structure based on likelihood. The initial parser gives different possible sentence structures, each with a probability score. A second model tries to make the ranking of these structures better by using extra details in the tree structure. The advantage of our method is that it can use any set of features (details) of a tree without worrying about how they might interact or overlap, and without needing a complex model to explain how they work together. We introduce a new technique to improve ranking, based on a method called boosting, which is a way to enhance results by Freund et al. (1998). We use this boosting method on a set of data from the Wall Street Journal. This method combines a basic model's prediction with extra information from 500,000 additional features that weren't in the original model. The new model achieved a score of 89.75% in F-measure, which is a metric for accuracy, showing a 13% better error rate than the basic model’s score of 88.2%. We also present a new algorithm for the boosting method that efficiently handles the large number of features in the data. Tests show that this new algorithm is much faster than the usual way of using boosting. We believe this method is a good alternative because it's simple and efficient, compared to other methods that choose which features to use in complex models. Although our tests focus on natural language parsing (understanding sentence structure), this method could also be used in other language tasks that involve ranking, like speech recognition, translating languages, or generating text. We demonstrate that enhancing the ranking of the top options from a basic parser can improve how well it works. We suggest a method that only changes the feature values that appear together with a rule feature during each step of the process.

The Proposition Bank: An Annotated Corpus Of Semantic Roles The Proposition Bank project takes a practical approach to showing meaning, adding a layer of information about who is doing what to whom (semantic role labels) to the sentence structures of the Penn Treebank. The resulting resource is considered simple because it doesn't show things like references to earlier mentioned items, amounts, and other complex features, but it is also broad because it includes every use of every verb in the collection of texts and allows for useful statistics to be created. We talk about the guidelines used to define the sets of roles in the labeling process and to study how often sentence structures change their meaning in the collection of texts. We explain an automatic system for tagging these roles, trained using the collection, and discuss how different types of information affect its performance, including comparing complete sentence analysis with a simpler form and the role of the "trace" categories that don't have a direct word in the treebank. As proposition banks are semantically annotated versions of a Penn-style tree bank, they provide consistent labels for roles across different sentence structures using the same verb.

Sentence Fusion For Multidocument News Summarization A system that can create informative summaries by showing shared information from many online articles will help people find what they need without having to read a lot. In this article, we introduce sentence fusion, a new way to create text by combining common information from different documents. Sentence fusion includes a step-by-step process to find phrases that have similar meanings and uses statistics to mix these phrases into one sentence. Sentence fusion improves summarization from just copying sentences to creating new summaries with sentences not found in the original documents, combining information from different sources. We show the input as dependency trees (a way to organize sentence structure), align some words to merge these trees into a network, and then take out a single, connected tree as the final result. We present the challenge of turning multiple sentences into one summary sentence.

Improving Machine Translation Performance By Exploiting Non-Parallel Corpora We introduce a new way to find matching sentences in texts that aren't already matched up. We teach a computer program to decide if two sentences mean the same thing in different languages. We use this method to gather matching sentences from large collections of Chinese, Arabic, and English newspapers that don't already have paired translations. We check the quality of these sentences by showing they make a top-notch translation system work better. We also demonstrate that a good translation system can be created from the ground up by starting with a small set of matched sentences (100,000 words) and making use of a large collection of unmatched texts. This means our method is useful for language pairs with limited translation resources. We use article dates and word similarity (after translating words with a bilingual dictionary) to find related news stories. We remove bad matches that are too different in length or have few words in common (using a bilingual dictionary). We use features mainly based on a specific word alignment method from IBM's research (Brown et al, 1993).

Evaluating WordNet-based Measures of Lexical Semantic Relatedness The measurement of how words are related in meaning has many uses in Natural Language Processing (NLP), and many different methods have been suggested. We review five of these methods, all of which use WordNet (a large database of words) as their main tool, by testing how well they find and fix real-word spelling mistakes. A method based on information content, suggested by Jiang and Conrath, is found to be better than those suggested by Hirst and St-Onge, Leacock and Chodorow, Lin, and Resnik. Additionally, we discuss why using distributional similarity (looking at how often words appear together) is not a good substitute for measuring how closely related words are in meaning. Measures based on WordNet are known to be better at measuring similarity (how alike things are) than relatedness (how things are connected) because of its organized, category-based structure.

Similarity of Semantic Relations There are at least two types of similarity. Relational similarity is a match between relationships, while attributional similarity is a match between characteristics. When two words are very similar in characteristics, we call them synonyms. When two pairs of words have very similar relationships, we say their relationships are analogous, meaning they are similar in a specific way. For example, the word pair mason:stone is similar in relationship to the pair carpenter:wood. This article introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA can be used in many fields, such as extracting information, understanding different meanings of words, and finding information. Recently, the Vector Space Model (VSM) used for finding information has been adjusted to measure relational similarity, reaching a score of 47% on a set of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relationship between a pair of words is shown by a vector, which is a series of numbers, that counts how often certain patterns appear in a large collection of texts. LRA improves the VSM approach in three ways: (1) The patterns are automatically taken from the collection of texts, (2) the Singular Value Decomposition (SVD), a mathematical method, is used to simplify the frequency data, and (3) automatically generated synonyms are used to explore different variations of the word pairs. LRA achieves 56% on the 374 analogy questions, which is statistically similar to the average human score of 57%. On the related problem of classifying semantic relations, which means understanding different kinds of word relationships, LRA shows similar improvements over the VSM. We develop a method based on a collection of texts to model relational similarity, addressing tasks like telling the difference between synonyms and antonyms. We describe a method (Latent Relational Analysis) that finds pattern sequences for noun pairs from a large collection of texts, using query expansion to increase the range of the search and feature selection and dimensionality reduction to make the feature space less complex.

Hierarchical Phrase-Based Translation We introduce a statistical machine translation model that uses hierarchical phrases, which are phrases containing smaller phrases within them. The model is technically a synchronous context-free grammar (a type of grammar used to describe languages) but it learns from a parallel text (text in two languages side by side) without needing any syntax rules. This means it combines basic ideas from both syntax-based (rules of sentence structure) and phrase-based (using groups of words) translation. We explain in detail how our system learns and processes translations, and we test its speed and accuracy. By using BLEU (a measure for checking translation quality), we find that our system works much better than the Alignment Template System, which is a leading phrase-based system. The hierarchical phrase-based model improves statistical machine translation by using hierarchical phrases, which not only helps in learning translations of small parts but also helps in rearranging words and phrases over a larger context.

CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank This article introduces a method for converting the Penn Treebank, a collection of structured sentences, into a set of Combinatory Categorial Grammar (CCG) derivations, which are detailed sentence structures. These derivations are improved with connections between nearby and distant words. The new collection, called CCGbank, covers 99.4% of the sentences in the Penn Treebank. It is available from the Linguistic Data Consortium and has been used to train advanced computer programs that understand sentence structures very well. To make sure the CCG analyses were correct and to remove errors and inconsistencies in the original data, a detailed review of the sentence structures and labels in the Penn Treebank was needed, and many adjustments were made. We explore what our findings mean for extracting other types of detailed grammar from the Treebank and for creating future collections of structured sentences. The CCGbank-style dependency is a detailed map showing how words connect, labeled with the main word's category and the role of the connected word. CCGbank is a collection of CCG sentence structures that was partly converted automatically from the Wall Street Journal section of the Penn Treebank.

Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models This article talks about different methods for analyzing sentences using log-linear models, which are mathematical models, for a type of grammar that is automatically created. These models evaluate entire sentences instead of looking at parts separately. They are trained by comparing correct sentence structures with incorrect ones. The grammar type used is called Combinatory Categorial Grammar (CCG), and it is taken from a resource called CCGbank, a version of the Penn Treebank. Using this method requires a lot of computer memory (up to 25 GB), which is managed by using a special computer system called a Beowulf cluster that uses parallel computing (multiple computers working together). A method called dynamic programming helps solve one of the biggest problems in sentence analysis in less than three hours. An important part of this system is a tool called a Maximum Entropy supertagger, which assigns specific labels to words in a sentence, making the training process efficient and the analysis faster. Surprisingly, despite CCG's tendency for 'spurious ambiguity' (unnecessary multiple interpretations), the process is quicker than other similar systems. We also improve current methods by creating a new model and algorithm to handle all types of sentence structures, even unusual ones, achieving high accuracy in understanding sentence parts and their relationships. The parser, or sentence analyzer, is tested against another called RASP and performs better overall. The testing on DepBank, a resource for evaluating parsers, highlights some issues with how parsers are usually tested. This article serves as a detailed guide for building a comprehensive CCG parser, proving that accurate and efficient sentence analysis is achievable. The C & C parser matches up well with other top-notch sentence analyzers across various tests, including those involving grammatical relations, Penn Treebank sentence structures, and complex dependencies.

Modeling Local Coherence: An Entity-Based Approach This article introduces a new way to show and measure how smoothly ideas connect in a piece of writing. The main part of this method is the entity-grid, which is a way to see how often and where important things (entities) are mentioned in a text. The algorithm in the article turns a text into a series of steps showing how these things change and keeps track of different kinds of information about them. We rethink checking smoothness of ideas as a learning task and show that our method works well for tasks like making good text order and sorting texts. Using this method, we do well in organizing text, checking how well summaries flow, and judging how easy texts are to read. An entity grid is created for each document, shown as a table where each line is a sentence and each column is an important thing. We test on two sets of data: news stories about earthquakes and stories about aviation accidents.

Feature Forest Models for Probabilistic HPSG Parsing Probabilistic modeling of lexicalized grammars (a type of grammar where each word has a specific role) is hard because these grammars use complex data structures, like typed feature structures (organized data with specific types). This makes it hard to use standard methods of probabilistic modeling, where complex structures are broken into smaller parts and assumed to work independently. For instance, breaking down sentence tagging into tagging each word or splitting CFG parsing (a type of grammar parsing) into applying rules. These methods depend on simpler structures like sequences or trees, and don't work well for graph-like structures including typed feature structures. This article suggests the feature forest model as a solution for modeling complex data structures like typed feature structures probabilistically. The feature forest model allows for probabilistic modeling without assuming independence between events when these events are shown as feature forests. Feature forests are general structures that pack multiple possible tree outcomes into one. Feature forest models use maximum entropy models (a type of statistical model) for feature forests. A dynamic programming algorithm (an efficient calculation method) is suggested for estimating maximum entropy without taking apart feature forests. This way, probabilistic modeling of any data structure is possible when using feature forests. The article also explains how to represent HPSG (a type of grammar) and predicate-argument structures with feature forests. Thus, it provides a full strategy for creating probabilistic models for HPSG parsing. The effectiveness of these methods is tested through experiments with the Penn Treebank (a database of annotated sentences), and the potential for using them on real-world sentences is discussed.

A Global Joint Model for Semantic Role Labeling We introduce a model for identifying semantic roles that understands that a group of related words (semantic argument frame) is connected, with strong links between them. We explain how to include these strong links in a statistical model with many features that look at different groups of words. This new model performs much better than a similar advanced model that doesn't consider these links between different groups. We test the improvements from using this connected information on the Propbank dataset, both when using perfect sentence structures (parse trees) and when using automatically generated ones. The improvements result in a 24.1% reduction in errors for all groups of words and 36.8% for main groups when using perfect sentence structures. For automatically generated sentence structures, the error reductions are 8.3% for all groups and 10.3% for main groups. We also show results on the CoNLL 2005 shared task data set. Additionally, we look into using multiple sentence analyses to handle errors and uncertainty from the parser. We introduce a model that ranks these analyses to learn the semantic roles of multiple parts in the Semantic Role Labeling task.

The Importance of Syntactic Parsing and Inference in Semantic Role Labeling We present a basic system for semantic role labeling, which means identifying the roles words play in a sentence. This system uses a computer learning method combined with a special problem-solving process that uses rules about language and structure to make decisions. We examine how important sentence structure information is when identifying word roles. We find that complete sentence structure information is crucial for identifying roles, especially in the first step called the pruning stage, where unnecessary parts are removed. Interestingly, the success of this stage isn't just about how many correct roles it finds (recall) or how accurate it is (precision). Instead, it depends on the types of possible answers it gives, which affects later steps. Based on this, we suggest a simple and effective way to improve by combining different systems that label word roles, which significantly boosts performance. Our system was tested in a competition called CoNLL-2005 and got the highest score out of 19 participants. The verb SRL system has four steps: generating possible candidates, identifying roles, classifying roles, and making final decisions.

Algorithms for Deterministic Incremental Dependency Parsing Parsing methods that go through text from start to finish and create one single structure have often been seen as not good enough for understanding natural language because languages are usually very confusing. However, it's been found that when these methods are combined with tools that help make choices, they can be very good at sorting out language structures, especially those that focus on relationships between words. In this article, we first introduce a basic way to explain and study these methods, known as transition systems. We then discuss two types of these methods: stack-based and list-based. The stack-based method deals only with simpler, straightforward structures and includes two versions: arc-eager and arc-standard. The list-based method includes both straightforward and more complex versions. For each of these four methods, we provide explanations to show they work correctly and how fast they operate. We also test all methods using SVM classifiers, which predict the next step in parsing, across data from thirteen languages. We find that all four methods are fairly accurate, but the more complex list-based method generally works better for languages that have many complex sentence structures. However, the simpler methods can still be effective when used with a special technique called pseudo-projective parsing. The stack-based methods are faster in terms of both learning and parsing, but the simpler list-based method is also quite fast in real situations. When the simpler methods use pseudo-projective parsing, they can become slower in parsing but not in learning compared to the complex list-based method. Although parts of these methods have been discussed before, this is the first detailed review and testing of them all together. We provide a clear explanation of the arc-standard and arc-eager methods, which are currently popular for understanding word relationships in sentences.

Survey Article: Inter-Coder Agreement for Computational Linguistics This article reviews methods to check how much different people agree when labeling a set of language data. It explains the math and ideas behind agreement scores, talking about Krippendorff’s alpha, Scott’s pi, and Cohen’s kappa; looks at how these scores are used in different labeling tasks; and suggests that using weighted scores, like alpha, which are not as common as kappa scores in language studies, might be better for many labeling tasks, but they make understanding the score value more difficult. This work gives a full overview of methods for measuring how well people agree when labeling language data in different areas of language studies.

Articles: Recognizing Contextual Polarity: An Exploration of Features for Phrase-Level Sentiment Analysis Many methods for automatic sentiment analysis start with a large list of words labeled with their usual emotional meaning (also called semantic orientation). However, the emotional meaning of a phrase where a word appears can be very different from the word's usual meaning. Positive words can be used in phrases that express negative feelings, or the other way around. Often, words that are positive or negative outside of a phrase are neutral in a phrase, meaning they do not express any emotion. The aim of this study is to automatically tell the difference between usual and in-context emotional meanings, focusing on what features are important for this. A key part of the problem is figuring out when emotional words are used in neutral contexts, so features that help tell apart neutral and emotional uses are tested, as well as features for telling apart positive and negative in-context meanings. The assessment involves checking how well features work with different machine learning methods. For all but one learning method, using all features together works best. Another part of the assessment looks at how neutral uses affect the performance of features for telling apart positive and negative meanings. These tests show that neutral uses greatly reduce the effectiveness of these features, and perhaps the best way to improve effectiveness for all meaning types is to better identify when a use is neutral. We explore the difference between usual and in-context emotional meanings: words that lose meaning in context, or have their meaning changed because of context.

Generating Phrasal and Sentential Paraphrases: A Survey of Data-Driven Methods The task of paraphrasing, which means rewording a sentence or phrase, is something that people who speak any language are familiar with. Additionally, creating or finding different ways of saying the same thing in words, phrases, and sentences automatically is a key part of natural language processing (NLP), which is a technology that helps computers understand human language better. This is being used more and more to make various language-related applications work better. In this article, we aim to provide a thorough review of methods that use data to create paraphrases of phrases and sentences, regardless of specific applications. We also highlight why paraphrases are valuable and how they can be used in NLP. We look at recent efforts in both manual and automatic creation of collections of paraphrases. We also talk about how paraphrase generation techniques are evaluated and briefly mention some future directions in this area. We review different techniques that use data to create paraphrases, organizing them based on the kind of data they use.

Distributional Memory: A General Framework for Corpus-Based Semantics Research into understanding language using collections of texts (corpus-based semantics) has often focused on creating specific models for each task. Each task is treated as a separate problem, using different types of information from the text collection. Instead of this "one task, one model" method, the Distributional Memory framework collects this information once from the text collection as a set of word connections that are organized in a large data structure called a third-order tensor. From this data structure, different tables are made, where rows and columns help solve different language understanding tasks. This means the same information can be used for various tasks like judging word similarities, finding synonyms, grouping concepts, predicting how verbs will be used, solving analogy problems, identifying relationships between words, discovering typical qualities of concepts, and sorting verbs into action types. Extensive testing shows that this method works well compared to specialized programs designed for these tasks, and also compared to top modern methods. The Distributional Memory approach is proven to be effective despite having to serve multiple purposes. We use a complex data structure (third-order tensors) to create a general system for understanding word meanings in different contexts using a single setup.

A Plan-Based Analysis Of Indirect Speech Act We suggest a way to understand indirect speech acts, like when people hint at requests or information, by assuming that people can identify what others are doing, guess what they want, and work together to achieve it. This teamwork happens naturally and might not always be planned by the speaker. If the listener thinks it's planned, they see the speech as indirect; if not, they take it literally. We propose simple rules to help choose between these interpretations.

Extraposition Grammars Extraposition grammars build on definite clause grammars, which are a type of logical rules. This new version helps describe how parts of a sentence can be moved to the front, which is important for understanding sentence structure in languages. While head grammars explain how verbs can move to the front and handle complex sentence parts, extraposition grammars focus on moving noun phrases (groups of words acting as a noun) around in English.

Coping With Syntactic Ambiguity Or How To Put The Block In The Box On The Table Sentences are much more unclear than you might think. There can be many different ways to structure certain simple English sentences. This has been a big issue for computer programs trying to understand language, especially when many of these structures are considered during understanding the meaning or context of the sentences. In this paper, we suggest some ways to handle this confusion by using patterns found among different sentence structures. These patterns will be shown as combinations of ATN networks (a type of language model), and also as sums and products of mathematical series. We believe this way of dealing with confusion will improve understanding, whether the sentence structure and meaning are handled one after the other or mixed together. The number of possible ways to break down a sentence is defined by the Catalan number, which is a mathematical formula that grows rapidly.

Attention Intentions And The Structure Of Discourse In this paper, we look into a new theory about how conversations are organized, focusing on the purpose and how they are processed. This theory says conversation structure has three parts that work together: the order of what is said (linguistic structure), the goals behind what is said (intentional structure), and what people are focusing on while talking (attentional state). The linguistic structure is about how parts of the conversation fit together naturally. The intentional structure is about the goals in each part of the conversation and how they connect. The attentional state is about where the participants' focus is during the conversation. It changes and records important things, traits, and connections at each moment. These parts are important to explain things in conversations like hints, how we refer to things, and interruptions. The paper shows examples of how attention, intention, and grouping of what is said work together. Different features of conversations are explained, and the paper looks into why certain things like hints, references, and interruptions happen. This theory provides a way to understand how conversations are processed. To process a conversation, you need to see how parts fit together, understand the goals, and follow the conversation by knowing what people focus on. This process uses information from the conversation and what people know about the topic. We suggest a theory to explain why something was said and what it means.

An Efficient Augmented-Context-Free Parsing Algorithm An efficient method for analyzing complex language structures is introduced, and its use in real-time language systems is discussed. The method is a type of LR parsing algorithm, which prepares a table in advance to help with parsing from a given set of language rules. Unlike the regular LR parsing method, it can handle any set of language rules, even confusing ones, while keeping much of the speed by using a "graph-structured stack," a special way of storing multiple possible meanings without repeating work. This method can also be seen as a fast version of another parsing method, guided by pre-made tables. It is quick because it prepares the tables ahead of time. In tests with different English language rules and sentences, it was found to be five to ten times faster than another well-known method called Earley's algorithm. The algorithm processes a sentence as it is typed, without waiting for the whole sentence to be completed. A working version of this parser has been made using the Common Lisp programming language and runs on certain computers. It is used in a multi-language translation project at Carnegie Mellon University (CMU). Additionally, a company called Intelligent Technology Incorporation is developing a version for Japanese, based on CMU's method.

An Algorithm For Generating Quantifier Scopings The structure of a sentence often clearly shows the main verb and its related parts, as well as how different parts of the sentence depend on each other. But understanding which parts of a sentence affect others is not so obvious. Because of this, many systems that try to understand the meaning of sentences have either ignored this aspect or have used unclear methods that either don't show how they choose from different interpretations or allow too many interpretations. This paper presents, along with proofs of some of its important properties, a method that creates interpretations with clear meanings from expressions that show the main verb and its related parts. Unlike other methods that simply rearrange parts, this method is careful and can be a strong base for computer solutions where being complete is less important than being efficient and effective. We expand this method to include operators (like not) and present a more efficient way to list possibilities than simple approaches. We showed a method to create interpretations from representations of how verbs and their parts relate and how different parts of a sentence depend on each other. We introduce a method for creating all possible interpretations of quantifiers.

Grammatical Category Disambiguation By Statistical Optimization Several algorithms have been developed in the past that try to solve the confusion over word categories in natural language text without using grammar or meaning information. A new method (called "CLAWS") was recently created by researchers working with the Lancaster-Oslo/Bergen Collection of British English. This method uses a systematic calculation based on the chances that certain tags appear together. It is very accurate but very slow, and it has been manually improved in several ways. The effects on accuracy of these manual improvements are not known individually. The current paper presents a method for removing confusion over word categories that is similar to CLAWS but works faster and uses less memory, while reducing random improvements. Tests of the method using the million words of the Brown Standard Collection of English are reported; the overall accuracy is 96%. This method can provide a fast and accurate start to any system that processes or analyzes English language.

Temporal Ontology And Temporal Reference A semantics (meaning) of temporal (time-related) categories in language and a theory of their use in defining the temporal relations between events both require a more complex structure on the domain underlying the meaning representations than is commonly assumed. This paper proposes an ontology (a way to categorize things) based on ideas like causation (cause and effect) and consequence, rather than just time-based basic concepts. A central idea in the ontology is an elementary event-complex called a "nucleus." A nucleus can be thought of as a combination of a goal event, or "culmination," with a "preparatory process" (the steps leading to it), and a "consequent state," which follows after. Natural-language categories like aspects, futurates, adverbials, and when-clauses are argued to change the time-related category of ideas based on such a nucleic knowledge representation structure. The same concept of a nucleus plays a central role in a theory of temporal reference, and of the semantics of tense, which we follow McCawley, Partee, and Isard in regarding as an anaphoric category (referring back to something previously mentioned). We claim that any manageable system for describing time in natural language will need to include such an ontology, as will any usable temporal database for knowledge about events that is to be queried using natural language. We describe temporal expressions relating to changes of state.

Tense As Discourse Anaphor In this paper, I look at different English expressions and explain that how they depend on context can be explained by two things: 1. They point out specific things in a changing story the listener is creating in their mind; 2. The specific thing they point out depends on another thing in the part of the "story model" the listener is focusing on. These types of expressions are called anaphors, which means they refer back to something mentioned earlier. I explain how sentences with tenses share these features, usually thought to only apply to anaphoric noun phrases, which are words that refer back to nouns used before. This helps us easily understand the often-mentioned but hard-to-prove idea that tense, or the timing of actions in sentences, also refers back to earlier parts of the story. It also helps us know more about what we need to understand stories better. We improve the previous work by setting rules for how events are connected in a story and Sing and Sing defined limits for how events can be linked (Sing, 1997).

Word Association Norms Mutual Information And Lexicography The term word association is used in a specific way in the study of how our brains process language. (Basically, people respond faster to the word nurse if it comes after the word doctor, because they are closely related.) We will use this term to help describe different interesting language patterns, like the connection between related words like doctor and nurse, and the rules about how verbs and prepositions go together. This paper will suggest a clear way to measure how words are related using a concept from information theory called mutual information, which helps us understand how often words are linked based on large text databases. (The usual way of finding out how words are related involves testing a few thousand people on a few hundred words, which is both expensive and not very reliable.) The suggested measure, called the association ratio, figures out how words are related directly from large text databases, allowing us to find these connections for tens of thousands of words. In our research, the importance of a word pair (x, y) is measured by mutual information I (x, y), which looks at how often x and y appear together compared to how often they appear separately.

Semantic-Head-Driven Generation We present a method for creating sentences from encoded logical forms that is better than older methods because it works with more types of grammar rules. Unlike an older method that builds from the bottom up, this new method can use grammar rules that aren't straightforward, and unlike top-down methods, it allows for certain kinds of repeated steps. The key idea of this method is that it works by following the structure of the sentence in a way that focuses on the main meaning parts first. We introduce a main-idea-focused method for creating sentences from logical forms.

A Statistical Approach To Machine Translation In this paper, we present a statistical approach to machine translation. We explain how we use this method to translate from French to English and share early results. We calculate values for a model that links words and changes their order, using large collections of bilingual texts that match each other.

Lexical Cohesion Computed By Thesaural Relations As An Indicator Of The Structure Of Text In text, lexical cohesion happens when related words connect with each other, helping the text flow smoothly. These connections, called lexical chains, occur because parts of the text talk about the same topic, so finding these chains helps understand the text's organization. Calculating these chains is helpful because they relate to how the text is organized. Figuring out the organization of a text is important for understanding its deeper meaning. In this paper, a thesaurus (a book or tool listing words with similar meanings) is used as the main resource for creating these lexical chains. It is shown that there is a link between these chains and how the text is structured. Because these chains can be calculated and found in any type of text, they are useful for showing how the text is organized. They also help understand the meanings of words, ideas, and sentences. We suggest using lexical chains to indicate how words are connected. We suggest using Lexical Chains to study how a text is organized and flows.

:met*: A Method For Discriminating Metonymy And Metaphor By Computer The met* method helps a computer tell apart examples of metonymy (using one thing to represent another) from metaphor (comparing two things) and from literal (exact) and unusual sentences in short English. The method identifies literal sentences because they fit normal word use while the others do not. Metonymy is separated from metaphor and strange sentences by [1] supporting the idea that metonymy uses one thing in place of another, while a metaphor sees one thing as something else; [2] allowing for sequences of metonymies; and [3] letting metonymies occur with literal, metaphorical, or unusual sentences. Metaphors are different from strange sentences because they have a meaningful comparison, unlike strange ones. The met* method is part of a system for understanding language and is used in a computer program called meta5. Some examples of how meta5 analyzes metaphor and metonymy are provided. The met* method is compared with methods from artificial intelligence, language studies, philosophy, and psychology. We use a technique that notices when word meanings don't match to find metaphors. We made a system called met* to separate literal, metonymy, metaphor, and unusual sentences. We created a system met* to tell metaphor and metonymy apart from literal text, using special methods to handle these types of language. We developed a system called met*, capable of discriminating between literalness, metonymy, metaphor, and anomaly.

The Generative Lexicon In this paper, I will discuss four main topics related to current research in the meanings of words: the methods used, how well they describe meanings, how accurately they show meanings, and how useful these representations are for computers. In addressing these issues, I will talk about some main problems the word meanings research community faces and suggest the best ways to handle these issues. Then, I will provide a way to break down word types and explain a theory of word meanings that includes ideas like cocompositionality (how words combine meaning together) and type coercion (forcing words into certain categories), along with different levels of meaning description, where the meaning work is shared more equally across the dictionary. I argue that breaking down word meanings is possible if done in a creative way. Instead of assuming a fixed set of basic elements, I will assume a fixed number of creative tools that can build meaning expressions. I develop a theory called Qualia Structure, a language for representing word meanings, which makes much word confusion unnecessary, while still explaining the multiple meanings that words have. Finally, I discuss how individual word structures can be included in the larger word knowledge system through a theory of passing down word traits. This gives us the needed rules for organizing the dictionary, allowing us to fully combine our natural language dictionary into a complete concept. We propose the Generative Lexicon Theory (GLT), which can be said to use both language and idea approaches, providing a framework that came from merging language studies and techniques used in artificial intelligence (AI).

Using Multiple Knowledge Sources For Word Sense Discrimination This paper looks at how to figure out what a specific word means in any text, even if we don't have the full details of the sentences. To tell apart word meanings, we can use different kinds of information like grammar tags, how often words appear, common word pairings, meaning context, expected roles, and grammar rules. But current methods only use a small part of this information. Here, we will explain how to use all the available information. Our talk will cover how the hints we use connect to general word and idea knowledge and more specific knowledge about common word pairings and contexts. We will explain a way to combine these hints based on how specific each one is, instead of following a fixed order of importance. We will also talk about using this method in a system that assigns meaning tags to any text, even when it can't find a single grammar or meaning structure for some sentences. We are among the first to use many different features for figuring out word meanings in the TRUMP system. We describe a study of different sources that help figure out word meanings, including word structure information.

TINA: A Natural Language System For Spoken Language Applications A new natural language system, TINA, has been developed for tasks involving spoken language. TINA combines important ideas from simple grammar rules, Augmented Transition Networks (ATN's), and the idea of unification. TINA smoothly connects understanding of sentence structure (syntax) and meaning (semantics) and creates a detailed language model to help improve accuracy in recognizing speech. A starting set of simple rules is first turned into a network format. Probabilities for each part of the network are automatically learned from example sentences. The system uses a method that searches using a stack (like a list) from top to bottom and includes a way to handle complex sentence parts and meaning limitations. TINA can automatically generate sentences, which helps find overly broad interpretations and create a model of word pairs for speech recognition. The system is currently used with MIT's SUMMIT recognizer in two specific areas, helping to check and filter outputs either by whole sentences or during the search for solutions. We suggest the TINA language system, which combines important ideas from simple grammar, network paths for language, and combining information concepts.

Class-Based N-Gram Models Of Natural Language We talk about the challenge of guessing a word based on the words that came before it in a piece of text. Specifically, we look at "n-gram models" which are methods that use groups of words. We also explore different statistical methods (mathematical techniques) to group words together based on how often they appear together. We discover that we can create groups of words that seem to be based on either grammar (how words are used together) or meaning (what words mean), depending on the data we use. We suggest a "window method," which is a new idea that looks at how often two words appear close together, within 500 words, to understand their connection.

A Problem For RST: The Need For Multi-Level Discourse Analysis We note that Rhetorical Structure Theory (RST) mixes together the information being shared and the intended effect on the reader's thoughts or feelings. We argue that both the meaning of the information and the intended effects can exist between sentences at the same time and separately from each other.

Introduction To The Special Issue On Computational Linguistics Using Large Corpora A historical account of this empirical renaissance is provided in this work. Much recent research in the field of natural language processing (NLP), which is about how computers understand human language, has focused on studying real examples from large collections of text, called "corpora."

Generalized Probabilistic LR Parsing Of Natural Language (Corpora) With Unification-Based Grammars We describe work toward building a very broad probabilistic system to analyze natural language (NL) using LR parsing techniques. The system aims to rank many sentence structures made by NL grammars based on how often each rule is used. We talk about an automatic method to create an LR parse table from a unification-based grammar style and consider other LALR(1) table methods for big grammars. The parse table is used for two parsers: an interactive system that helps train the statistical data needed for the probabilistic parser in a way that's easy to manage and doesn't require much manual effort. The second parser is made by directly linking probabilities to the LR parse table. This method is better than those using probabilistic word tagging or simple grammar rules because it considers more context and uses a grammar style that's more accurate for language. We compare the performance of a refined version of Tomita's (1987) generalized LR parsing method with a well-organized chart parser. We share positive results from a small study using 150 noun definitions from the Longman Dictionary of Contemporary English (LDOCE) for training, plus testing on these and another 55 definitions. Finally, we talk about the limits of the current system and potential improvements to handle how often words appear in terms of their meaning and grammar. Our statistical parsing uses a modified system that can work with tagged input, focusing on sequences of tags instead of the actual words. Our statistical parser builds on the ANLT grammar development system.

Accurate Methods For The Statistics Of Surprise And Coincidence Much work has been done on analyzing text using statistics. In some studies, wrong statistical methods have been used, and how important the results are has not been considered. Specifically, assuming data follows a normal distribution (a common bell curve) is often done without reason, causing incorrect results. This normal distribution assumption makes it hard to study rare events, which actually happen a lot in real text. Thankfully, there are better methods based on likelihood ratio tests (a way to compare two hypotheses) that work well even with small samples. These tests can be done quickly and have been used to find combined terms or specific terms related to a topic. Sometimes, these methods work much better than older methods. When traditional methods with contingency tables (a type of table used in statistics) work well, the likelihood ratio tests work almost the same. This paper explains a method using likelihood ratios for analyzing text. Since we first introduced it to the natural language processing (NLP) community, the G log-likelihood-ratio statistic has been widely used in statistical NLP to measure connection strength, especially word connections.

A Program For Aligning Sentences In Bilingual Corpora Researchers in both machine translation (e.g., Brown et al. 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann 1990) have recently become interested in studying bilingual corpora, which are collections of texts like the Canadian Hansards (parliamentary proceedings) available in multiple languages (such as French and English). One helpful step is to align the sentences, meaning to match sentences in one language with sentences in the other language. This paper will describe a method and a program (align) for matching sentences based on a simple statistical model of character lengths. The program uses the idea that longer sentences in one language usually translate to longer sentences in the other language, and shorter sentences translate to shorter sentences. A probabilistic score, which is a likelihood measure, is given to each proposed match of sentences, based on the difference in lengths of the two sentences (in characters) and how much this difference varies. This score is used in a method called dynamic programming to find the best possible match of sentences. It is surprising that such a simple method works so well. An evaluation was done using a trilingual set of economic reports from the Union Bank of Switzerland (UBS) in English, French, and German. The method correctly matched all but 4% of the sentences. Furthermore, it's possible to select a large part of the text with even fewer errors. By choosing the best-scoring 80% of the matches, the error rate drops from 4% to 0.7%. There were more errors in the English-French text than in the English-German text, indicating that error rates depend on the text considered; however, both were low enough to suggest that the method will be useful for many language pairs. To advance research on bilingual corpora, a much larger set of Canadian Hansards (about 90 million words, half in English and half in French) has been matched with the align program and will be available through the Data Collection Initiative of the Association for Computational Linguistics (ACL/DCI). Additionally, to make it easy to replicate the align program, an appendix is provided with detailed c-code of the more complex core of the align program. We present a combined approach, and the main idea is that longer sentences in one language are usually translated into longer sentences in the other language, and shorter sentences into shorter sentences. We suggest a dynamic programming algorithm for matching translations at the sentence level that uses two ideas: the length of translated sentences closely matches the length of the original sentences, and the order of sentences in translated text mostly matches the original order of sentences.

Structural Ambiguity And Lexical Relations We suggest that many unclear cases where prepositional phrases (like "in," "on," "at") could be attached to different parts of a sentence can be solved by looking at how often these prepositions appear with either verbs (action words) or nouns (things or people) based on patterns found in a large collection of text that a computer has already analyzed. This means that by looking at how words are commonly used together, we can find a simple way to solve sentence structure problems that usually require complicated thinking. We are the first to show that using a large collection of text to solve these prepositional phrase attachment issues can give good results. We were among the first to use a text-based method to figure out where prepositional phrases belong in a sentence by checking how often certain verbs and nouns appear with specific prepositions. We used a tool that partially analyzes sentences to pull out groups of verbs, nouns, and prepositions from a large collection of text, where the preposition could be connected to either the verb or the noun.

Text-Translation Alignment We introduce a method for matching texts with their translations using only information from the texts themselves. The process involves figuring out which words in one text match words in the other text based mainly on how often they appear. It uses a partial word match to create the best possible sentence match, which then helps improve the word match in the next round. The method seems to quickly find the correct sentence matches after just a few rounds. Our word structure method is used for breaking down possible word endings and beginnings and for getting the basic word forms.

Retrieving Collocations From Text: Xtract Natural languages are full of collocations, which are pairs or groups of words that frequently appear together more often than by random chance and show specific word usage. Recent work in the study of dictionaries shows that collocations are common in English and appear in all kinds of writing, whether technical or not. Several methods have been suggested to find these word combinations by analyzing large amounts of text. These methods automatically find many collocations and use numbers to show how important these word pairings are. However, these methods do not provide useful information about the collocations themselves. Also, the results sometimes include incorrect word pairings influenced by irrelevant details in the training text, which are not true collocations. In this paper, we explain techniques that use statistical methods to find and identify collocations from large text collections. These techniques can find a wide variety of collocations and use special filtering methods to produce better and more accurate results. These techniques have been used to create a dictionary tool, Xtract. The techniques are explained and some results are shown using a 10 million-word collection of stock market news reports. An evaluation of Xtract as a tool to find collocations has been done, and its accuracy is estimated to be 80%. We developed Xtract, a system to extract terms. We suggest a statistical model that measures how often pairs of words appear together with strong connections. For finding multi-word expressions (MWEs), we suggest a well-known approach that uses statistical methods based on how often words appear together to find MWEs in text collections.

From Grammar To Lexicon: Unsupervised Learning Of Lexical Syntax Imagine a language that you know nothing about; the only way to study it is with a basic grammar book and a huge collection of text. There is no dictionary. How can you use easily noticed grammar rules to gather as much information as possible about how individual words are used in sentences from this collection of text? This paper describes a method based on two ideas. First, use small grammar hints to structure sentences instead of trying to understand whole sentences at once. Second, treat these hints as likely clues rather than definite rules of sentence structure. Use statistical methods to analyze the data collected from these hints, rather than making a firm conclusion from just one example. The success of this method in figuring out how verbs fit into sentences is shown by experiments on English text using a program called Lerner. Lerner starts with no knowledge of the main words--it begins with basic words like determiners (e.g., the, a), helping verbs (e.g., is, have), modals (e.g., can, will), prepositions (e.g., in, on), pronouns (e.g., he, she), words that join clauses (e.g., that, because), coordinating conjunctions (e.g., and, but), and punctuation marks. Our study is focused on automatically gathering large amounts of information about sentence structures (subcategorization frames).

The Mathematics Of Statistical Machine Translation: Parameter Estimation We explain five different statistical models (ways to understand and use numbers) for translating languages and provide methods (step-by-step plans) for finding the parameters (important numbers) in these models using pairs of sentences that are translations of each other. We introduce the idea of matching words one-by-one in these sentence pairs. For any given pair of sentences, each model gives a probability (chance) for every possible word match. We have a method to find the most likely word matches. Even though this method is not perfect, it does a good job of showing how words relate in the sentence pairs. We have a lot of data in French and English from the Canadian Parliament's records. Therefore, we focus only on these two languages, but we believe our methods, which don't rely heavily on language rules, could work well for other language pairs too. We also believe that because our methods don't depend much on specific language rules, it's fair to say that word-by-word matches can be found in any large enough collection of texts in two languages. Our approach to Statistical Machine Translation (SMT) focuses on translating word by word and is based on the noisy channel model (a method used to understand how information can get messed up as it travels).

Building A Large Annotated Corpus Of English: The Penn Treebank The Penn Treebank is a big collection of English sentences that have been organized and labeled to help computers understand human language better. It includes lots of sample sentences where each word is tagged with its grammatical role, like noun or verb, making it useful for developing language processing tools.

Lexical Semantic Techniques For Corpus Analysis In this paper, we outline a research plan for computational linguistics, which involves using large collections of text. We show how a system for understanding word meanings can reveal deeper connections between words in text, not just words that often appear together. The work explains how language features like metonymy (using a related word) and polysemy (words with multiple meanings) can be used to tag words with their meanings. Unlike basic statistical analyses of word pairings, our meaning-based approach can predict complex relationships between words that often appear together. We demonstrate this method for gathering information about different types of nouns and how it can improve the word structures obtained from a computer-readable dictionary. Besides traditional word meaning connections, we show how information about word assumptions and preferences can also be gathered from text collections when using the right tools. Finally, we discuss how studying large text collections can provide valuable data for language research and help confirm or challenge language theories. We introduce an interesting method for finding word associations from text collections, not just using statistics but also guided by theoretical principles. We explain how statistical tools, like mutual information (a measure of word connection), can help automatically gather information about the link between a noun and a verb. We use general sentence patterns to extract detailed meaning structures from a partly analyzed text collection.

Coping With Ambiguity And Unknown Words Through Probabilistic Models From spring 1990 through fall 1991, we conducted several small tests to see how well adding probability-based methods to our existing knowledge-based techniques would work. This paper shares our tests focused on predicting the part of speech (like noun, verb, etc.) of words that can have multiple meanings, figuring out the correct meaning of a sentence when it can be understood in different ways, and learning how verbs are used through examples. From these tests, we believe that probability-based methods using labeled text examples can help clear up confusion when processing text and can be used to gather word information from a large collection of text, along with current techniques. Based on what we learned, we created a new computer system called PLUM that helps pick out information from text, such as news articles. Our model also deals with words we haven't seen before using the same probability method.

Empirical Studies On The Disambiguation Of Cue Phrases Cue phrases are words or expressions like "now" and "well" that help show how a conversation or text is organized. For example, "now" might mean starting a new topic or going back to an old one, while "well" could show a reply or explanation. However, these phrases can have different meanings. For instance, "incidentally" can be used as an adverb to add extra information, but in conversation, it might start a side note. Understanding the difference between these uses is important for understanding and creating conversations, but it’s not often studied. This paper shares results from studies on how these phrases are used in conversation and in sentences, looking at text and sound features to see how they help understand meaning. The studies suggest that changes in voice tone, like pitch and rhythm, help tell the difference between the uses. A model describing these voice changes is identified. This model is linked with text features like spelling and grammar, so the findings can be used to create the right voice tones for using cue phrases in computer-generated speech. There is no clear definition for discourse markers in research. We find that voice tone and pitch help clarify the meaning of cue phrases, aiding in understanding conversation structure.

Tagging English Text With A Probabilistic Model In this paper, we talk about some tests using a probabilistic model, which is a method that uses chances or probabilities, to tag English text. Tagging means giving each word the right label, like noun or verb, based on the sentence it is in. What is new about these tests is using text that hasn't been tagged before to train the model. We used a simple three-part Markov model, which is a tool that predicts based on past information, and we are trying to find the best way to figure out the model's settings based on different types and amounts of training data. We compare and combine two main methods: using text that has been manually tagged and counting how often tags appear, and using text that hasn't been tagged and training the model in a hidden way, like a mystery, using a method called Maximum Likelihood, which is about finding the most likely explanation. Tests show the best results come from using as much tagged text as possible. They also show that training with Maximum Likelihood, a common way to set up hidden Markov models, doesn't always make tagging more accurate. In fact, it usually makes it worse unless there is only a small amount of tagged text. We tried to make HMM, a type of model for tagging parts of speech (POS), better by using unlabeled data with a method called expectation maximization, which is a way of adjusting guesses to improve them. We introduced a common method now that uses a bigram HMM, which looks at pairs of words, trained with expectation maximization. In POS tagging, we introduced a method called maximum likelihood tagging, which is about picking the most likely label for each word.

Japanese Discourse And The Process Of Centering This paper has three goals: (1) to explain a computer-based method called CENTERING, which helps understand conversations, (2) to use this method for understanding Japanese conversations in computer systems that translate languages or help understand them, and (3) to share some thoughts on how grammar affects understanding conversations in Japanese. We believe that understanding conversations involves making guesses, but grammar clues limit these guesses; we show this using examples of ZEROS, which are missing parts of a sentence, in Japanese. The grammar clues we study in Japanese conversations are words that show the main topic, like the word "wa," and words that show grammatical roles like "ga" for subject, "o" for object, and "ni" for another object. We also look at the speaker's EMPATHY, which is the angle from which a story is told. This is shown through verb combinations, like using extra verbs such as "kureta" or "kita." Our findings come from asking native speakers how they understand short conversations with small changes. We show that grammar clues affect how ZEROS are understood, but being the main topic before and showing up as a ZERO also makes a part of the conversation stand out. We suggest a rule for assigning ZERO as the main topic and show that CENTERING guides when a ZERO can be seen as the main topic. We suggest an order of importance for topics in Japanese conversations.

Regular Models Of Phonological Rule Systems This paper introduces a set of mathematical and computer-based tools for working with and understanding regular languages (patterns of sounds) and regular relations (connections between sound patterns). It argues that these tools are a strong foundation for computer-based phonology (study of sound patterns). It explains in detail how this approach works with ordered sets of context-sensitive rewriting rules (rules that change sounds based on their surroundings) and also with grammars in Koskenniemi's two-level formalism (a method for analyzing languages). This study provides a shared way of showing phonological constraints (limits on sound patterns) that allows for easy generation and understanding by one simple interpreter (program). We provide a step-by-step method for converting into transducers (tools that transform input into output). We describe a general method for showing a replacement process as finite-state transduction (a simple and predictable change process).

A Syntactic Analysis Method Of Long Japanese Sentences Based On The Detection Of Conjunctive Structures This paper explains a way to analyze sentence structure by first finding conjunctions (words like 'and', 'or') in a sentence by looking for patterns in two groups of words. Then, it uses this information to understand how the parts of the sentence connect to each other. Analyzing long sentences is very challenging in processing languages with computers. This is mainly because it's hard to figure out the sentence structure where conjunctions are present. People can understand conjunctions because of certain, sometimes subtle, similarities between the parts connected by conjunctions. To address this, we created a method to compare two groups of words on either side of a conjunction and find the most similar ones to identify the conjunction structure. This is done with a technique called dynamic programming, which is a way to solve problems by breaking them into simpler steps. By identifying conjunctions, we can simplify long sentences into shorter ones. As a result, we can understand the whole sentence structure using simple rules about which words depend on others. One major challenge with conjunctions, apart from the confusion about what they connect, is that sometimes parts of the sentence are left out. Our process can find these missing parts and fill them in. We show the results of examining 150 Japanese sentences to demonstrate how well this method works. We suggest a way to find conjunctions by comparing scores of similarity between two sets of 'bunsetsus' (units of words in Japanese). We offer a way based on similarity to solve both tasks for Japanese. We propose a Japanese sentence analysis method that includes finding coordinated structures.

An Algorithm For Pronominal Anaphora Resolution This paper presents a method for identifying the nouns that pronouns refer to. The method works with sentence structures created by McCord's Slot Grammar parser and uses importance measures based on sentence structure and a simple model of where attention is focused. The method, like the parser, is created using Prolog, a programming language. The authors tested it on computer manuals and did a test on 360 pronoun uses. The method correctly found the noun for the pronoun 86% of the time. The authors looked at how different parts of the method contributed to its success. They also tested an improved version of the method that used data about meaning and real-world connections, but it only improved results by 2%. The method was compared to other ways of solving pronoun references mentioned in research. Specifically, they tried another method, Hobbs' algorithm, within the Slot Grammar system on the test sentences. Their method was 4% more successful than Hobbs' method. The paper also discusses how their method relates to the centering theory, which considers various information factors for choosing the right noun. In their approach, they group related nouns and calculate an overall importance score by adding up the scores of each noun. They describe a method for resolving pronouns that correctly analyzes 85% of cases.

Word Sense Disambiguation Using A Second Language Monolingual Corpus This paper introduces a new way to clarify word meanings in one language by using statistics from a collection of text in another language where only one language is used. This method uses the differences in how words match up with meanings in different languages. The paper focuses on choosing the right word in machine translation, where this method can be directly applied. The method described finds connections between words using a tool that analyzes the grammar of the source language and matches these connections to another language using a dictionary that includes two languages. The best meanings are then chosen based on statistics about word relationships in the target language. This choice is made using a statistical model and a method that deals with all unclear meanings in a sentence at the same time. The method was tested with examples in Hebrew and German and was found to be very helpful for understanding word meanings. The paper also includes a detailed comparison of methods that use statistics to figure out word meanings. We suggest using a collection of text in one language, a dictionary for two languages, and a grammar tool for the source language to understand word meanings.

Machine Translation Divergences: A Formal Description And Proposed Solution. There are many times when translating from one language to another changes the form a lot compared to the original. The differences in how languages work (called translation divergences) mean that simply copying from one language to another doesn't work well. Many translation systems have ways to handle these differences but don't have a general method to make use of the connection between word meanings and sentence structure. This paper shows that a systematic solution to these differences can be created by formalizing two types of information: (1) the language-based groups that cause differences in word meanings; and (2) the methods used to solve these differences. This formalization is helpful because it makes designing and building the system easier, helps evaluate how well the system is working, and provides a way to prove some important features of the system. We identify the reasons why sentence structures differ between languages.

An Efficient Probabilistic Context-Free Parsing Algorithm That Computes Prefix Probabilities We explain how we improved Earley's parser, a tool for analyzing sentences, to work with stochastic (random) context-free grammars. This improved parser can calculate these things when given a grammar and a sentence: a) the chances of different sentence beginnings being created by the grammar; b) the chances of parts of the sentence being created by the grammar's building blocks, including the whole sentence; c) the most likely explanation (Viterbi parse) of how the sentence was created; d) how often each rule in the grammar is used, which is needed to update rule chances. Chances (a) and (b) are calculated step-by-step in one pass from left to right over the sentence. Our method is better than the usual step-by-step methods for working with grammars because it works well with grammars that don't have many rules by using Earley's top-down method. It can handle any grammar rule format without needing to change it to a special form, and it calculates everything from (a) to (d) using one method. Also, the method can be easily adjusted to handle sentences with some parts marked and to find possible explanations and their chances for incorrect sentences. An Earley chart, a tool for keeping track of sentence explanations, is used to keep track of all possible ways the sentence could have been created.

Centering: A Framework For Modeling The Local Coherence Of Discourse This paper is about how focus of attention, choice of words to refer to something, and how smooth and logical sentences are within a part of a conversation relate to each other. It introduces a basic idea called centering to explain how attention works locally. The paper looks at how the smoothness of conversation and the choice of words to refer to something interact; it suggests that how smooth a conversation seems depends partly on the effort needed to understand different kinds of referring words, given a particular focus. It shows that the qualities of attention explained by centering can explain these differences. Our centering model ranks things mentioned in sentences and calculates changes between sentences to help understand how clear and well-structured texts are. Our centering theory suggests strong connections between what people focus on when understanding sentences and the way sentences are structured and refer to things. Our centering theory focuses on how certain things mentioned in a sentence are more important than others, which affects how a speaker uses certain words to refer to things. Our centering theory has been a key idea for understanding how we keep things connected in language studies over the past twenty years.

Transformation-Based-Error-Driven Learning And Natural Language Processing: A Case Study In Part-Of-Speech Tagging Recently, there has been a renewed focus on using real-world data in the field of natural language processing (NLP). Instead of manually programming language rules, people are using computers to learn from large collections of text (called a corpus) to help NLP systems understand language. While these methods have worked well in many areas, they often store language information in complex and confusing statistics tables. This can make it hard to understand and improve how these methods work with language. In this paper, we will explain a simple method that uses rules to help computers learn language information automatically. This method has been successful in several tasks by capturing information in a more understandable and straightforward way without losing effectiveness. We provide a detailed example of this learning method used in part-of-speech tagging, which is identifying the role of each word in a sentence, like noun or verb. We explain a system that learns guessing rules from existing tagged text data. We suggest a new way of applying this learning method that doesn’t rely on processing text in a set order. We introduce a symbolic machine learning method, which is a type of learning that focuses on clear and logical steps, using a method called transformation-based learning.

Translating Collocations For Bilingual Lexicons: A Statistical Approach Collocations are groups of words that often go together, and they are tough for non-native speakers to translate because they don't make sense if you translate them one word at a time. We talk about a program called Champollion, which takes texts in two different languages and a list of these word groups in one language to automatically find their translations. Our aim is to create a tool that helps build bilingual dictionaries for groups of words, not just single words, in many languages and fields. The method we use relies on statistical techniques and creates translations where the number of words in each language doesn't have to match. For instance, Champollion translates phrases like make ... decision, employment equity, and stock market into the French equivalents prendre ... decision, equite en matiere d'emploi, and bourse. When tested on three years of the Hansards text collection, it found 300 French translations of these word groups each year, with an average accuracy of 73%. In this paper, we explain the statistical techniques, the method, and how Champollion works, along with our findings and evaluation. The connection between a statistical measure called pointwise Mutual Information and another called the Dice coefficient is discussed. We suggest a method based on text collections to create bilingual word lists. We also suggest using the Dice coefficient to help solve the problem of translating these word groups.

A Maximum Entropy Approach To Natural Language Processing The concept of maximum entropy, which is an idea about evenly spreading out information, has been around for a long time, even back to old times. But only recently have computers become strong enough to use this idea widely to solve real-world problems, like guessing and identifying patterns. In this paper, we explain a way to create statistical models (which are like mathematical representations of data) using maximum entropy. We show a method that helps build these models automatically and explain how to do this efficiently, using some examples from natural language processing (how computers understand human language). We suggest a method for choosing the best options by considering what we gain.

Assessing Agreement On Classification Tasks: The Kappa Statistic Currently, experts in computer language and the study of thinking who focus on conversation say their personal opinions are consistent using various statistics, which are hard to understand or compare with each other. At the same time, people studying content analysis have faced similar challenges and have found a solution with the kappa statistic. We talk about the problems with current methods for checking reliability in conversation studies in computer language and thinking science, and suggest that this field should use methods from content analysis. Our method, the kappa statistic, is widely used in real-world studies of conversation (Carletta, 1996).

A Stochastic Finite-State Word-Segmentation Algorithm For Chinese The first step in analyzing text for any Natural Language Processing (NLP) task usually involves breaking the input text into individual words, called tokenization. For languages like English, you can generally assume that spaces or punctuation marks show where words end. However, in many Asian languages like Chinese, spaces aren’t used to separate words. Instead, you have to use word information to figure out where words begin and end. In this paper, we introduce a model that uses random processes within a fixed set of rules (stochastic finite-state model) to break down Chinese text into words found in the dictionary and new words formed by different processes. The main part of this model is a tool called the weighted finite-state transducer. Since this model is mainly designed to help turn written text into spoken words (text-to-speech synthesis), it also provides how to pronounce these words. We test how well the system works by comparing its way of separating words with how a group of people do it, and it shows good results. We created a basic word model using a method called Viterbi re-estimation, which started with guesses based on how often each word appears in a collection of texts. We also suggested a way to guess initial word frequencies without needing to split the text collection into separate words first.

The Reliability Of A Dialogue Structure Coding Scheme This paper explains how reliable a system is for organizing conversations based on what is being said, the flow of interaction, and the overall conversation pattern. This system was used on a collection of casual, goal-focused spoken conversations. We measured how much the people coding the conversations agreed on breaking them into bigger parts that were built from smaller pieces. We checked how well they agreed on where these bigger parts began and ended.

TextTiling: Segmenting Text Into Multi-Paragraph Subtopic Passages TextTiling is a method for breaking down texts into sections with several paragraphs that show different topics or ideas. The clues for finding big changes in topics are patterns of words that appear together and how they are spread out. The method is completely developed and has been shown to divide text in a way that matches how people naturally identify topic changes in 12 different texts. Breaking text into multi-paragraph sections should help with tasks like analyzing text, finding information, and making summaries. We calculate how often people agree by chance on whether they think a boundary between segments exists (segt) or not (unsegt).

Discourse Segmentation By Human And Automated Means The need to understand how the structure of conversations relates to language features is widely accepted. However, there's little agreement on what these conversation units are or how to identify them. We share detailed findings from a two-part study using a collection of natural, storytelling speeches. The first part explains a method to test conversation units called discourse segments. We found strong results from people without specialized training, using common sense to decide where segments begin and end. In the second part, we use the data from these people to test two sets of computer programs that look at speech features to find segments. For the first set, we check how well these programs match with three language signals (like names, connecting words, and pauses). We then create a second set using two techniques: error checking and teaching computers to learn. Testing these new programs on fresh data shows they work better when they use different types of language knowledge at the same time. We also conducted an experiment where seven people without training were asked to find conversation segments in written stories about a movie.

Finite-State Transducers In Language And Speech Processing Finite-state machines, which are simple computational models, have been used in many areas of natural language processing, which involves understanding and generating human language. We focus on using a specific type of machine called sequential transducers, which are known for running very efficient programs or tasks. We review well-known rules and introduce new ones that describe how these sequential machines transform one string of characters into another. Machines that also produce numerical values, or weights, are important for processing language and speech. We examine string-to-weight machines in detail, including methods for making these machines run more efficiently by simplifying them, and rules for identifying which machines can be simplified in this way. We also explain how these methods are used in speech recognition, which is the technology that allows computers to understand spoken words, with examples. The use of a series of weighted string machines, known as weighted string transducers, is thoroughly explored in this study.

Stochastic Inversion Transduction Grammars And Bilingual Parsing Of Parallel Corpora We introduce (1) a new way of using stochastic inversion transduction grammar, which is a method that uses randomness to model pairs of sentences in two languages, and (2) the idea of analyzing bilingual text with various uses in studying texts written in two languages at the same time. Unlike traditional methods, which often use simpler models called finite-state transducers, our method has three main features: it starts with a more advanced model called context-free, it allows some flexibility in how sentences are ordered, and it uses probabilities to allow a more effective way to analyze bilingual text. We show that there is a standard way to use this method. Our analysis suggests that this method is especially good at handling the way languages can change order, providing the right balance between flexibility and complexity. We provide several examples of how using this method helps solve difficult problems in analyzing bilingual texts, like breaking down text into parts, aligning phrases, and parsing, which means understanding the structure. We use a specific type of training called an inside-outside algorithm to learn how to translate sentences in a structured way. Our method of Bilingual Bracketing is one of the ways studied to align words between Chinese and English, which is a type of shallow parsing that doesn't go into too much detail. We offer a solution that works within a reasonable time frame for aligning text based on matching structures called synchronous binary trees.

Automatic Rule Induction For Unknown-Word Guessing Words unknown to the dictionary create a big problem for Natural Language Processing (NLP) tools, like those that identify parts of speech (e.g., noun, verb) or analyze sentence structure. In this paper, we show a method to automatically create rules to guess the possible parts of speech for unknown words by looking at the beginning and end parts of the words. This learning comes from a general dictionary and word counts gathered from a collection of text. Three types of word-guessing rules are created using statistics: rules based on the start of the word (prefix), the end of the word (suffix), and just the ending. Using this method, sets of rules for guessing unknown words were created and added to a tagger that uses chance (stochastic tagger) and one that uses set rules, which were then used on texts with unknown words. Our model, LTPOS, does both identifying sentences and tagging parts of speech. Our LTPOS is a statistical tool that combines tagging parts of speech and figuring out where sentences start and end.

Stochastic Attribute-Value Grammars Probabilistic analogues (similar versions using probability) of regular and context-free grammars (rules for arranging words) are well known in computational linguistics (study of language using computers) and are being heavily researched. However, so far, no satisfactory probabilistic version of attribute-value grammars (a type of grammar involving pairs of characteristics) has been created because previous attempts couldn't define a good method for estimating parameters (important numbers in a model). In this paper, I explain stochastic (random) attribute-value grammars and provide a method for finding the best estimation of their parameters. The method for estimation is based on work by Della Pietra, Della Pietra, and Lafferty (1995). To estimate model parameters, it is necessary to figure out the expected values of certain functions in random settings. In the study by Della Pietra, Della Pietra, and Lafferty (about English spelling rules), Gibbs sampling (a statistical method) is used to calculate these expected values. Because attribute-value grammars create restricted languages, Gibbs sampling doesn't work, but I show that sampling can be done using a broader method called the Metropolis-Hastings algorithm (a technique for generating samples). We propose a Markov Random Field or log-linear model for SUBGs (a type of structured data).

Introduction To The Special Issue On Word Sense Disambiguation: The State Of The Art We present a short overview of the history of ideas used in word sense disambiguation, which is figuring out which meaning of a word is being used. In general, the different methods for figuring out word meanings from the past can be divided into two types: those based on data and those based on knowledge. We believe that understanding word meanings is a major issue for many well-known human language technology (HLT) applications, like translating languages by computer, pulling specific information from texts, and finding information.

Using Corpus Statistics And WordNet Relations For Sense Identification Corpus-based methods for figuring out the meaning of a word are flexible and can be used in many situations, but they have a problem with getting enough knowledge. We show how methods that rely on existing knowledge can help solve this problem by automatically finding useful text collections. We explain a tool that uses statistics to combine the general topic and specific hints to identify what a word means. This tool is used to figure out the meaning of a noun, a verb, and an adjective. A database like WordNet, which shows how words are related, is used to automatically find examples in a general collection of texts. We compare test results with results from examples that were tagged by hand. We introduce a way to get examples with specific meanings using words that only have one meaning in a certain context.

A Corpus-Based Investigation Of Definite Description Use We present the results of a study on how definite descriptions (phrases like "the book" that refer to something specific) are used in written texts. We wanted to see if it was possible to label large collections of text (called corpora) with information about how these descriptions are understood. We conducted two experiments where people were asked to identify different uses of these descriptions in a collection of 33 newspaper articles, containing 1,412 definite descriptions. We checked how much the participants agreed on the categories they assigned to the definite descriptions and how much they agreed on what previous part of the text (antecedent) these descriptions referred to. The most interesting discovery was that the agreement among participants was quite low (K = 0.63) when using complex classification methods by Hawkins and Prince. However, a simpler method suggested by Fraurud, which uses only two categories - first mention and subsequent mention - got better agreement (K = 0.76). Agreement on what the descriptions referred to was also not complete. These findings suggest that it's problematic to judge systems for understanding definite descriptions by comparing them to a standard set of labels. From a language perspective, we noticed many new, unfamiliar definite descriptions in our text collection (in one experiment, around 50% were new, 30% referred back to something mentioned earlier, and 18% were related or connected in another way). Some descriptions didn't need full clarification. We suggest a labeling method based on this study showing that more than 50% of definite descriptions in the text are new or unfamiliar.

Generalizing Case Frames Using A Thesaurus And The MDL Principle A new method for automatically finding patterns in language structures, called case frames, from large collections of text is introduced. The challenge of simplifying the values in a case frame for a verb is seen as figuring out the likelihood of different words being used, using a new method based on the Minimum Description Length (MDL) principle, which is a way to simplify information. To make this process quicker, the method uses a thesaurus and focuses only on sections that already exist in the thesaurus, making it easier by focusing on a specific part, known as a "tree cut model," of the thesaurus. A fast algorithm is provided, which is guaranteed to find the best tree cut model for the given word usage data, according to MDL. The patterns found by this method were used to solve confusion about where prepositional phrases (like "in the park") should go in a sentence. Tests show that this new method is better or at least as good as other current methods. We use an MDL-based algorithm to find the best tree cut over WordNet, a large thesaurus, for each problem, showing improvements over previous methods that focused on word connections (Hindle and Rooth, 1993) and idea connections, matching the best transformation-based results. We propose a model where the best cut is chosen based on MDL, which carefully balances between being general and being accurate by minimizing the total length of the model and the data descriptions.

New Figures Of Merit For Best-First Probabilistic Chart Parsing Best-first parsing methods for natural language aim to analyze sentences efficiently by looking at the most probable parts first. A specific way to measure how likely these parts are is needed, which greatly affects how well the parser works. Although some sentence analyzers mentioned in studies have used these methods, there isn't much information published about how well they actually work, and even fewer efforts to compare their effectiveness. We suggest and test several ways to measure effectiveness for best-first parsing and identify a method that is easy to calculate and works very well across different tests and grammar rules. We introduce best-first parsing with these effectiveness measures that allow adapting the decision-making process based on patterns in the input sentence.

Generating Natural Language Summaries From Multiple On-Line Sources We present a method for creating short summaries of news about current events, which also include important background (historical) information. Our system, called SUMMONS, uses results from systems made for the DARPA Message Understanding Conferences to create summaries of several documents about the same or related events. It shows similarities and differences, disagreements, and general ideas among different sources of information. We explain the different parts of the system, showing how information from various articles is put together, organized into a paragraph, and then turned into English sentences. A special part of our work is pulling out details about people and places to use again, making the summaries better. We combine work in getting information and processing natural language.

Machine Transliteration It is difficult to translate names and technical terms between languages that use different alphabets and sounds. These terms are often transliterated, meaning they are replaced with similar-sounding letters. For example, "computer" in English is written as "konpyuutaa" in Japanese. Translating these back from Japanese to English is even harder and important because these transliterated terms are often not found in bilingual dictionaries. We explain and test a method for doing reverse transliterations using a machine. This method uses a step-by-step model that includes different parts of the transliteration process. We suggested a Japanese-English transliteration method based on the chances of matching sounds between English and Japanese katakana.

PCFG Models Of Linguistic Tree Representations The types of tree structures used in a collection of annotated text, called a treebank, can greatly impact how well a computer program that predicts sentence structure, called a parser, works. This is because the predicted likelihood of a tree structure might be very different from how often it actually appears in the training data. This paper highlights that the Penn II treebank, a specific set of tree structures, is expected to affect parser performance in this way. It also describes a straightforward way of renaming parts of the tree that can boost the parser's accuracy and ability to retrieve information by about 8%, which is roughly half the gap between a basic PCFG model and the top-performing parsers available today. These changes in performance happen because any PCFG, which stands for Probabilistic Context-Free Grammar, assumes certain things about how words and phrases are distributed. The specific assumptions about independence in a tree structure can be explored theoretically and tested practically by changing and then reversing changes in the tree. We add information to each part of the tree about its parent category, leading to significant improvements compared to the original PCFGs on the Penn Treebank.

Bitext Maps And Alignment Via Pattern Recognition Texts that are available in two languages (bitexts) are becoming more and more abundant, both in private data storage areas and on publicly accessible websites on the internet. Like other types of data, the value of bitexts largely depends on how effective the available data analysis tools are. The first step in getting useful information from bitexts is to find matching words or sections in their two parts (bitext maps). This article improves the current methods of bitext mapping by seeing the problem as one of pattern recognition, which is identifying patterns or regularities in the data. From this perspective, the success of a bitext mapping method depends on how well it does three things: creating signals, removing unwanted data (noise), and finding information. The Smooth Injective Map Recognizer (SIMR) method introduced here combines new techniques for each of these tasks. Testing has shown that SIMR's accuracy is consistently high for language pairs as different as French/English and Korean/English. If needed, SIMR's bitext maps can be easily turned into section alignments using the Geometric Segment Alignment (GSA) method, which is also explained here. SIMR has created bitext maps for over 200 megabytes of French-English texts. GSA has changed these maps into alignments. Both the maps and the alignments are available from the Linguistic Data Consortium, which is a group that provides language data. We standardize the Longest Common Subsequence (LCS) by dividing the length of the longest common sequence of characters by the length of the longer text and call it the longest common subsequence ratio (LCSR).

Supertagging: An Approach To Almost Parsing In this paper, we have proposed new methods for strong parsing that combine flexible word descriptions based on language rules with strong statistical methods. Our main idea is that understanding sentence structure can be simplified if words are linked to detailed descriptions (supertags) that set specific rules in a small area around them. The supertags are made so that only the parts affected by the word's rules appear in each supertag. Each word can have as many supertags as the different sentence structures it can fit into. This means there are many more descriptions for each word compared to simpler descriptions, increasing confusion for a computer analyzing the sentence. But this confusion can be sorted out by using patterns of how supertags appear together, collected from a large set of analyzed sentences. We have tested these ideas in the Lexicalized Tree-Adjoining Grammar (LTAG) system. In LTAG, supertags combine both sentence structure and word relationships in one form. Solving the supertag puzzle gives a result that is almost a full sentence analysis, and the computer program needs to just put the supertags together. This parsing method can also be used for parts of sentences, like in spoken language, where the supertag sequence might not fit into one structure. We show that correctly figuring out the supertags, which means choosing word entries before analyzing, allows effective LTAG parsing.

Functional Centering Grounding Referential Coherence In Information Structure Considering real-life evidence from German, a language where word order is flexible, we suggest changing the rules that decide how topics are ordered in the centering model. We believe that instead of using grammatical roles, we should use rules that show the important information structure of sentences. These new rules focus on whether the topics are already known or new to the listener. We show that this functional approach can be used to understand different ways topics are referred to in text, such as using pronouns or names. Our ideas are supported by two studies. The first study compares how well two different methods solve pronoun references: one based on grammar roles and the other on our functional approach. The second study introduces a new way to evaluate centering data based on cost, which is explained by the mental effort ideas in the centering model. We present Functional Centering, a version of Centering Theory that uses the difference between what the listener already knows and what is new to them.

Semiring Parsing We combine ideas from parsing methods, logical parsing, and the use of math in formal languages to create a system that describes parsers. Each parser does basic calculations using a structure called a semiring. This system lets us use a simple method to describe different parsers by changing the semiring operations to get recognition, derivation trees, Viterbi (best paths), top results, internal values, and more. We also explain how to find external values using the same method but in a different way. This system can describe many kinds of parsers, like Earley's method, tree joining grammar parsing, a method by Graham Harrison Ruzzo, and prefix value calculation. We explain how to mix parsing rules with different semirings to find various details about the input. We add semiring weights to these rules, providing a math-based reason for why similar algorithm classes are connected.

Decoding Complexity In Word-Replacement Translation Models Statistical machine translation is a new way to solve the old problem of computers translating human languages. Current methods find translation rules from texts in two languages and use these rules to translate new texts. The main structure is the source-channel model: an English sentence is created using statistics (source) and then changed into French using statistics (channel). To translate a French sentence, we find the most likely English version. We show that for the simplest statistical models, this problem is very hard to solve (NP-complete), meaning it likely takes a lot of time as the sentence gets longer. We find this complexity comes from factors not in other translation problems. We proved that the Exact Decoding problem is very hard (NP-Hard) when using a bigram model, which looks at pairs of words. We show that the problem of translating in statistical machine translation (SMT) and some bilingual matching problems are NP-complete, meaning there is no quick way to solve them in general.

The Penn Discourse TreeBank 2.0. We introduce the second version of the Penn Discourse Treebank, PDTB-2.0, explaining its word-based notes on discourse connections (ways sentences and parts of text relate to each other) and their two abstract object arguments (the ideas or topics they connect) using the 1 million word Wall Street Journal text collection. We explain all parts of the notes, including (a) the argument structure of discourse relations (how these connections are built), (b) the sense annotation of the relations (labeling the meaning or function of connections), and (c) the attribution of discourse relations and each of their arguments (who is responsible for these connections). We list the differences between PDTB-1.0 and PDTB-2.0. We show sample numbers for different parts of the notes in the text collection. We present The Penn Discourse Treebank (PDTB), which is a collection that adds a layer of discourse-level notes on top of the Penn Treebank, using a predicate argument approach (a method focusing on the main action and its participants).

A Model-Theoretic Coreference Scoring Scheme This note explains a scoring method for the coreference task in MUC6. It makes the original method better by: (1) basing the scoring on a model; (2) giving more understandable recall and precision scores; and (3) not needing to calculate all possible connections for coreference. The main change is that we switched from a syntax-based scoring model, which followed coreference links, to a method based on the model theory of these links. In simple terms, the method works by comparing groups of equivalent items defined by the links in the key and the response, instead of the links themselves (so, this is currently only defined for identity links). These groups are the models of the IDENT equivalence relation, and this approach is better for several reasons, one being that the scores don't depend on the specific links used to show the equivalence relation. The scores are found by figuring out the smallest changes needed in the response to make its groups of equivalent items match those of the key. Specifically, the recall (or precision) errors are found by counting the smallest number of links needed to be added to the response (or the key) to make the groups match. Although this seems very complex at first, because of references to minimal spanning subsets of the equivalence relation, it turns out it can be done with a simple counting method. We introduce the link-based MUC evaluation metric for the MUC-6 and MUC-7 coreference tasks.

MITRE: Description Of The Alembic System Used For MUC-6 As with several other experienced MUC participants, MITRE's Alembic system has gone through big changes in the last two years. This change started during a dinner talk at the last MUC conference, MUC-5. Back then, some of us admitted that our main problem in getting better results was depending on the usual language rules for sentence structure. We realized we needed a different way than the usual language rules, even the somewhat unique fake-parser we had at the time. But the question was, what should we use instead? The answer was rule sequences, an idea Eric Brill explained in his work on identifying parts of speech [5, 7]. Rule sequences now form the basis for all the main processing steps in Alembic: identifying parts of speech, understanding sentence structure, making logical connections, and even some of the processing in the Template Element task (TE). We found this method gives many benefits, with speed and accuracy being the most obvious. Also, most of our rule sequence processors can be trained, usually from small examples. The rules created this way allow us to easily mix rules made by people and those learned by machines. We have used this chance to apply both machine-learned and hand-made rules a lot, sometimes using mostly machine-learned sequences, and other times using sequences made entirely by hand. Our usual machine learning methods for English Named Entity (NE) are transformation-based learning, which is a method of teaching computers by changing rules.

Transformation Based Learning In The Fast Lane Transformation-based learning, a method used for solving language problems, has been very successful. It performs very well on tasks related to understanding language and doesn't get worse with too much training. But, it has a big issue: it takes a very long time to learn, especially with large text data often used in language processing. In this paper, we introduce a new and practical way to speed up the learning time for this method without losing effectiveness. We compare the time and performance of our improved method with two other systems: a regular transformation-based learner and the ICA system (a method from Hepple, 2000). Our tests show that our method can learn much faster while still working as well as the regular method. This is important for systems and processes that use transformation-based learning at any step. We suggest using the fnTBL toolkit, which includes several improvements to make the learning process much quicker.

Text And Knowledge Mining For Coreference Resolution Traditionally, coreference, which is determining when different words refer to the same thing, is resolved by meeting a mix of importance, sentence structure, meaning, and conversation rules. Learning this knowledge takes a lot of time, is hard, and can have mistakes. So, we introduce a simple method to find coreference rules from labeled text collections. Evidence of meaning consistency, which is a type of knowledge needed for coreference, can be easily obtained from WordNet, a large database of words and their meanings. More consistency knowledge is found using a smart algorithm applied to texts without labels. We use connections through WordNet, using not just synonyms and "is-a" relationships, but also parts, word forms, explanation texts, and words with multiple meanings, which are given importance based on the type of relation and how many steps are in the connection. The connection patterns in WordNet are used to calculate the meaning consistency between Noun Phrases (NPs).

A Decision Tree Of Bigrams Is An Accurate Predictor Of Word Sense This paper describes a method to figure out the meaning of a word that can have more than one meaning (word sense disambiguation) by using a decision tree, which makes choices based on pairs of words (bigrams) that appear close to the word in question. This method was tested using a collection of texts where the meanings of words were already identified from an event in 1998 called SENSEVAL, which focused on understanding word meanings. The method was more accurate than the average results for 30 out of 36 words tested and even better than the best results for 19 out of those 36 words. We looked at how decision trees, simple decision stumps (a basic form of decision tree), and a Naive Bayesian classifier (a simple way to make predictions) perform, showing that bigrams are very helpful in figuring out what a word means.

Edit Detection And Parsing For Transcribed Speech We introduce an easy method for understanding transcribed speech where an edit detector first takes out unnecessary words from the sentence, and then a normal statistical parser, which is specially trained for this kind of speech, analyzes the remaining words. The edit detector makes mistakes with only 2.2% of the edited words. (A simple model that assumes everything is unchanged has a 5.9% error rate.) To check how well our parser works, we use a new way to measure it that doesn't care much about the exact position of the edited parts. Using this measure, the parser is 85.3% accurate and 86.5% complete in its analysis. Our study of understanding spoken conversations has looked at how well a parser works when it removes unnecessary parts early on.

Multipath Translation Lexicon Induction Via Bridge Languages This paper introduces a way to create translation word lists using models that transform similar word pairs through intermediate languages. Bilingual word lists within language families are created using models that predict changes in word spelling. Translation word lists for languages that are very different from each other are made by combining these models with online dictionaries that work across different language families. This approach achieves up to 95% accuracy when matching exact words in the target language (covering 30-68% of test pairs from different language families). This means that large parts of translation word lists can be created accurately for languages that don't have a bilingual dictionary or similar text resources. We explain a method for creating translation word lists using transformation models of similar word pairs through bridge languages.

A Probabilistic Earley Parser As A Psycholinguistic Model In human sentence processing, cognitive load, or mental effort, can be defined in many ways. This report looks at defining cognitive load based on the total likelihood of sentence structures that have been ruled out as you read: the surprise of reading a word wi given the words before it w0...i−1 in a language structure model. These loads can be calculated effectively using a probabilistic Earley parser (Stolcke, 1995), which helps predict how long it takes to read each word. Based on grammar rules and data about word usage, Stolcke’s probabilistic Earley parser accurately predicts issues like garden path sentences (sentences that lead you to an unexpected meaning) and differences in understanding subject and object in sentences. Since the idea of using a parser to measure surprise, statistical methods have become common to model how hard reading is and the complexity of language.

Applying Co-Training Methods To Statistical Parsing We introduce a new Co-Training method for statistical parsing, which is a way to break down sentences to understand their structure. The method starts with a small set of 9695 sentences that have been marked with their grammatical structure, a list of possible word structures from the training set, and a large amount of text without labels. The method repeatedly assigns grammatical structures to the entire set of data. From testing with the Wall Street Journal text, we demonstrate that training a statistical parser using both labeled and unlabeled data performs much better than using only labeled data. Our co-training method mostly runs without human help by using two or more parsers to assign labels to training examples for each other.

Knowledge-Free Induction Of Inflectional Morphologies We suggest a method to automatically figure out the word forms (morphology) of languages that change word forms (inflectional languages) by using only large collections of written text (text corpora) without any help from humans. Our method uses hints from how words are spelled (orthography), what they mean (semantics), and how they are used in sentences (syntactic distributions) to find relationships in word forms in German, Dutch, and English. By using CELEX, a trusted database, as a standard for checking accuracy, we show that our method is better than any other method that doesn't use prior knowledge. We use a technique called latent semantic analysis to identify beginnings (prefixes), endings (suffixes), and combined forms (circumfixes) in German, Dutch, and English.

Chunking With Support Vector Machines We use Support Vector Machines (SVMs) to find basic parts of English sentences, called chunks. SVMs are good at handling complex data and still make accurate predictions. Thanks to something called the Kernel principle, SVMs can learn effectively without needing a lot of computer power, even with complex data. We use a method where 8 different SVM systems, each trained with different ways of identifying chunks, vote on the results. Tests show that our method is more accurate than older ones. In this paper, we create a tool called YamCha that uses SVMs for chunking.

Inducing Multilingual POS Taggers And NP Bracketers Via Robust Projection Across Aligned Corpora This paper explores how to transfer language labels like part-of-speech tags (labels that tell the role of words in a sentence) and base noun phrase bracketings (groupings of words that form basic noun phrases) from one language to another using texts that are aligned word-for-word in two languages. Initially, tests measure how accurate it is to directly copy these labels and groupings from English to French and Chinese, using both rough machine-aligned texts and accurate human-aligned texts. Then, by applying techniques that work well with messy data, accuracy significantly improves, reaching 94-96% accuracy for French part-of-speech tags and 90% accuracy for French groupings, even when tools are trained without any human help in the target language. We create a tool to label French words and detect basic noun phrases in both French and Chinese by transferring them from English tools. We are the first to suggest using parallel texts (texts in two languages that match each other) to start developing these labeling tools.

Learning To Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment We tackle the challenge of changing sentences into different words with the same meaning, which is harder than changing just words or phrases. Our method uses a technique called multiple-sequence alignment on sentences from similar but unmarked text collections, learning sets of patterns for changing sentences and figuring out how to use these patterns to rewrite new sentences. Our tests show that our system creates accurate sentence changes, doing better than basic systems. We suggest using multiple-sequence alignment (MSA) for traditional sentence changes. We create networks over sentence changes using a step-by-step pairwise multiple sequence alignment (MSA) method. We propose a multi-sequence alignment method that takes sentences with similar structures and builds a simple network representation that shows small differences. We introduce a method for creating sentence-level changes by learning similar ways of expression from data and finding pairs to change using a similar text collection.

Inducing History Representations For Broad Coverage Statistical Parsing We introduce a method using neural networks to create representations (models) of parse histories (records of how sentences are broken down) and use these models to calculate the chances needed by a specific type of parser (a tool that breaks down sentences). This parser performs well, achieving a score of 89.1% (measured by a standard called F-measure) on a well-known dataset, the Penn Treebank. This score is just 0.6% less than the best parser available, even though it uses fewer words and less language knowledge. The key to our success is using gentle guidance based on structure to create the parse history models, and not assuming things are independent when they are not. Among previous studies using neural networks for understanding natural language, our approach with Simple Synchrony Networks (SSN) has been the most successful based on experiments. We also examine how having a larger set of words affects SSN performance by adjusting the number of times a word must appear to be included in the input.

A* Parsing: Fast Exact Viterbi Parse Selection We introduce a way to improve the classic A* search method for parsing sentences using PCFG (probabilistic context-free grammar). A* search helps speed up finding the best sentence structure by guessing how likely different parts of the sentence are to complete correctly. We explain different ways to make these guesses and provide quick methods to calculate them. For average-length sentences from the Penn Treebank (a collection of English sentences used for language research), our best guessing method cuts the amount of work down to less than 3% of what would be needed if we checked every possibility. A simpler method, needing less than a minute to set up, reduces the work to less than 5%. Unlike other methods that might only find a close answer quickly, A* is guaranteed to find the most accurate sentence structure. Our parser, which is easier to build than other complex methods, works well with many ways of controlling the parsing process and takes a predictable amount of time even in the worst cases. We talk about allowed guessing methods and an A* system for parsing sentences.

Statistical Phrase-Based Translation We propose a new method for translating languages using phrases, along with a new process for decoding, which allows us to test and compare different methods for phrase-based translation that were suggested before. In our study, we perform many tests to understand and explain why methods using phrases translate better than those using single words. Our practical results, which apply to all languages we tested, show that the best performance is achieved with simple techniques: guessing phrase translations based on aligning words and giving importance to the meaning of phrases. Surprisingly, learning phrases longer than three words and using very accurate word matching doesn't greatly improve performance. Focusing only on phrases based on grammar rules makes our systems perform worse. We introduce STIR, a step to rearrange words, in a top-performing system that translates from English to Japanese.

Automatic Evaluation Of Summaries Using N-Gram Co-Occurrence Statistics Following the recent use by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct a detailed study of a similar idea for evaluating summaries. The results show that automatic evaluation using unigram (single words) matches between summary pairs surprisingly correlates well with human evaluations, based on various statistical measures; while directly using the BLEU evaluation method doesn't always give good results. We are the first to systematically point out problems with the large-scale DUC evaluation (a specific evaluation method) and to look for solutions by finding more reliable automatic alternatives. We propose ROUGE, a partly automatic method, which mainly focuses on matching sequences of words (n-grams) between automated and human summaries.

Multitext Grammars And Synchronous Parsers Multitext Grammars (MTGs) create many parallel texts using flexible rules. Both standard MTGs and a special type called bilexical MTGs have relatively fast parsers. However, MTGs can express more complex ideas than other systems, for which parsers have been mentioned in research papers. Because MTGs are both powerful and efficient, they are a good choice for models that find equivalent meanings in translations. We introduce methods for parsing more complex grammars, focusing on how to handle grammars with more than two branches and those with words that have specific meanings. We also explore using a method known as the hook trick to parse bilexical multitext grammars.

COGEX: A Logic Prover For Question Answering Recent TREC results have shown that we need better ways to understand text deeply. This paper introduces the idea of using automated reasoning, a computer method of solving problems, for answering questions and shows it's possible to add a logic prover, a program that checks logic, into a Question Answering system. The method is to change questions and the parts of text with answers into logic formats. General knowledge rules and language rules are given to the logic prover to help it understand the connection between the question and answer text. Plus, the steps of the logic process give reasons for the answers. The results indicate that the logic prover improves the Question Answering system's performance on TREC questions by 30%. COGEX uses its logic prover to find word connections between the question and possible answers.

Syntax-Based Alignment Of Multiple Translations: Extracting Paraphrases And Generating New Sentences We explain a method that uses sentence structure to automatically create word patterns (called Finite State Automata) from groups of translations that mean the same thing. These word patterns are useful for finding different ways to say the same thing (paraphrases) and can also create new sentences that have the same meaning as those in the original groups. Our method can also check if different ways of saying something are correct, which helps in assessing how good translations are. We explain a method that creates word patterns from similar translations to form new paraphrases. We suggest a method to match groups of similar sentences by focusing on their structure.

Statistical Sentence Condensation Using Ambiguity Packing And Stochastic Disambiguation Methods For Lexical-Functional Grammar We introduce a way to make sentences shorter using techniques to handle multiple meanings and choosing the best option, specifically for a type of grammar called Lexical-Functional Grammar (LFG). Our system uses a language processor for LFG, a tool to simplify sentence structures efficiently, and a model that selects the best output based on probability. We also suggest using common methods for evaluating parsers to automatically check how well the system summarizes sentences. Testing shows that our automatic evaluation closely matches human judgment on the quality of summaries. The system creates high-quality summaries and ensures correct grammar because it uses a precise language processor. We have developed a tool to choose the best sentence shortening from multiple options given by an LFG parser. This tool uses detailed LFG grammar rules to improve a system that makes sentences shorter.

Shallow Parsing With Conditional Random Fields Conditional random fields (a type of computer model that helps label parts of a sequence) are better than older models like Hidden Markov Models (HMMs) and other methods that label each part separately. In language processing tasks like shallow parsing (finding simple sentence parts), there's been a lot of focus, with many tests and comparisons among different methods. We show how to train a conditional random field to do as well as any known noun-phrase chunking method for the CoNLL task and even better than any single model. New training methods using recent improvement techniques were key to these great results. We compare different models and training methods thoroughly, confirming and enhancing past findings on shallow parsing and training for maximum-entropy models (another type of computer model). CRFs have been used with great success for breaking sentences into noun phrases in this work.

Sentence Level Discourse Parsing Using Syntactic And Lexical Information We introduce two probabilistic models (models that use probability to make decisions) that can be used to identify basic parts of text and build sentence-level structure trees. The models use syntactic (relating to sentence structure) and lexical (relating to words) features. A discourse parsing algorithm (a method for analyzing sentences) that uses these models creates sentence structure trees with an 18.8% improvement over a leading decision-based sentence analyzer. Tests show that our sentence analysis model is advanced enough to produce sentence trees with accuracy similar to human performance. Within Rhetorical Structure Theory (a way to analyze text structure), we have developed two probabilistic models for identifying basic sentence parts and generating sentence trees. We introduce a statistical tool for breaking down sentences, which is trained on RST DT (a database of sentence structures) to label words with either boundary (end of segment) or no-boundary labels.

Feature-Rich Part-Of-Speech Tagging With A Cyclic Dependency Network We introduce a new tool that labels words in a sentence as nouns, verbs, etc., using these ideas: (i) it looks at both the tags before and after a word using a special network method, (ii) it uses a wide range of information about words, including looking at several words together, (iii) it effectively uses prior knowledge in advanced statistical models, and (iv) it closely examines features of unfamiliar words. By combining these ideas, our tool is very accurate, correctly labeling 97.24% of words in a well-known language dataset, improving by 4.4% over the previous best tool that learned automatically. We present a supervised (trained with examples) model that labels words by using information from both the left and right side of a word.

Factored Language Models And Generalized Parallel Backoff We introduce factored language models (FLMs) and generalized parallel backoff (GPB). An FLM uses words as collections of features (like word parts, word roots, or groups of similar words) and creates a probability model for sequences of these features instead of just individual words. GPB improves the usual method of handling missing data by allowing for more flexible ways to predict the likelihood of events, even when variables are of different types or there's no clear order to follow. These methods were added to the SRI language modeling toolkit during the JHU 2002 workshop. This paper shows initial results on how well the models predict, tested on CallHome Arabic and Penn Treebank Wall Street Journal articles. Importantly, FLMs with GPB can create pairs of words (bigrams) that are easier to predict than the more complex word groups (trigrams) that are usually very accurate. In multi-step speech recognition, where these word pairs are used to make first guesses or shortlists, these results are very important. We show that factored language models can do better than standard methods in predicting word sequences. A factored language model (FLM) involves representing words as groups of features and can incorporate various additional information such as part-of-speech tags, word structure details, or word meanings in a comprehensive way.

Precision And Recall Of Machine Translation Machine translation can be judged using precision (how accurate the translated words are), recall (how many correct words are captured), and the F-measure (a balance between precision and recall). These usual ways of measuring have a much stronger connection with what humans think than some new methods. More importantly, these usual ways are easy to understand, which can help us think of ways to make machine translation systems better. The necessary software can be found for free by anyone. We create a way to measure translation quality using precision and recall directly, instead of using precision and a penalty for short translations.

A Statistical Model For Multilingual Entity Detection And Tracking Entity detection and tracking is a relatively new task in understanding language. In this paper, we introduce a statistical method that doesn't depend on any specific language to find and follow names, descriptions, and pronouns referring to things or people in any kind of text, then group them together for each logical item mentioned. Both the mention detection model and the new entity tracking model can use any type of feature, allowing them to incorporate a wide range of word-based, grammatical, and meaning-related features. Additionally, the mention detection model importantly uses information from different systems that recognize named entities. We tested this method in several experiments using Arabic, Chinese, and English texts; a system based on our method and entered into the latest Automatic Content Extraction (ACE) evaluation achieved top-level results in all three languages tested.

Speed And Accuracy In Shallow And Deep Stochastic Parsing This paper discusses some tests that compare how accurate and fast two different sentence analyzing systems are. The well-known Collins parser is a "shallow parser," meaning it analyzes sentences in a simpler way but gives more useful information about the meaning than other similar systems. The XLE parser is a "deep-parsing system," which means it analyzes sentences in a more detailed way. It uses something called Lexical Functional Grammar (LFG) and a special part to help decide between different meanings, giving a much deeper understanding. We checked how accurate both systems were by comparing them to a set standard from the PARC 700 dependency bank and also timed how long they took to process. We found that the deep parsing system using LFG is quite fast, processing about 1.9 sentences every second for 560 sentences from a specific part of the Penn Treebank.

Training Tree Transducers Many probabilistic models for natural language are now written using hierarchical tree structures, which means they organize information in a branching way, like a family tree. However, tree-based modeling still lacks many of the standard tools that are commonly used in string-based modeling, which deals with sequences of symbols. The theory of tree transducer automata, which are mathematical machines that transform trees, offers a possible framework to use, as it has been detailed extensively in scientific writings. We explain why using tree transducers for natural language is beneficial and tackle the challenge of training probabilistic tree-to-tree and tree-to-string transducers, which are systems that convert one tree structure into another or into a sequence of symbols. We define methods for training and decoding, which means interpreting the data, for both generalized tree-to-tree and tree-to-string transducers.

Catching The Drift: Probabilistic Content Models With Applications To Generation And Summarization We look at how to understand the structure of text content in a specific area, focusing on what topics the texts talk about and the order these topics show up. We start by introducing an effective way that doesn't rely on much existing knowledge to learn content models from documents that haven't been annotated or labeled, using a new version of methods for Hidden Markov Models (a statistical model that represents systems with hidden states). Next, we use our approach for two related tasks: arranging information in the right order and picking out key pieces to summarize. Our tests demonstrate that adding content models to these tasks greatly improves the results compared to methods suggested before. We created a domain-specific HMM (Hidden Markov Model) to understand how topics change in a text, where topics are shown as hidden states (invisible factors) and sentences are the visible parts.

The Web As A Baseline: Evaluating The Performance Of Unsupervised Web-Based Models For A Range Of NLP Tasks Previous work showed that counting words on the web can help guess how often pairs of words appear together, which can be useful for many language processing tasks. Until now, only two tasks (choosing words for machine translation and telling apart confusing word sets) have been tested with large web data. This paper looks into whether these findings apply to tasks involving both sentence structure and meaning, creating and understanding language, and using a wider range of word groups. For most tasks, we find that simple models that don't need training data do better when word group frequencies come from the web rather than a large text collection. However, in most cases, web-based models do not do better than the best advanced models trained on smaller text collections. We suggest that web-based models should be used as a starting point or comparison, not as a replacement for usual models. Our web-based model, which doesn't need training data, classifies pairs of nouns based on Lauer's list of 8 linking words and uses the web as the learning material.

Evaluating Content Selection In Summarization: The Pyramid Method We present a method based on real-world evidence for checking what information is chosen in summarization. It includes the idea that there's no one perfect summary for a group of documents. Our method measures how important each piece of information is. We believe it is trustworthy, can predict outcomes, and helps identify issues, making it much better than the human evaluation method currently used in the Document Understanding Conference. We suggest a manual way to evaluate, based on the idea that no one summary is the best for a bunch of documents.

A Smorgasbord Of Features For Statistical Machine Translation We explain a method for quick testing in statistical machine translation, which we use to add many new features to a basic system by using features from different levels of sentence structure. Feature values were combined in a mathematical model (log-linear model) to pick the best translation from a list of options (n-best list). The importance of each feature was adjusted to improve results based on the BLEU evaluation metric, which measures translation quality, using a separate set of data. We show results for a few features at each level of sentence structure. At the 2003 Johns Hopkins summer workshop on statistical machine translation, many features were tested to see which ones could make a top-quality translation system better, and the only feature that made a 'truly significant improvement' was the Model 1 score. The impact of adding sentence structure into a top-quality statistical machine translation system is studied.

Minimum Bayes-Risk Decoding For Statistical Machine Translation We introduce a method called Minimum Bayes-Risk (MBR) decoding for machine translation, which uses statistics to reduce the chances of translation mistakes based on how well the translation performs. We explain a range of ways to measure translation quality that use different levels of language information, like basic word strings, how words align between languages, and the grammatical structure of sentences in both the original and translated languages. We tested the MBR approach on translating Chinese to English. Our findings indicate that MBR decoding can help improve translation performance by focusing on specific quality measures. The MBR method works by aiming to make the system's translations as similar as possible to the best possible translations according to the model. In statistical machine translation (SMT), MBR decoding helps reduce mistakes in a single translation system by rearranging a list of the top translation options generated by an initial translation step.

Discriminative Reranking For Machine Translation This paper talks about using special reranking methods to improve machine translation. For each sentence in the original language, we get a list of possible translations from a basic translation system, ranked by quality. We introduce two new reranking methods, inspired by a simple learning model, that make translations better than the basic system, as measured by the BLEU score, which checks translation quality. We show test results from a Chinese-English translation challenge in 2003. We also explain our methods in theory and show through tests that our methods perform as well as the best available in machine translation. We compare different methods for adjusting weights in a reranking setup and get results similar to a popular training method. We show ways to improve translation results using sentence structure information.

A Language Modeling Approach To Predicting Reading Difficulty We show a new way to solve the problem of figuring out how hard a text is to read by using language models (tools that predict words) statistically. We create a method based on a simple classification technique called multinomial naive Bayes that uses several language models to guess the likely school grade level for a piece of text. This tool can work with any subject and doesn't need a lot of labeled examples to learn. We test it by predicting the difficulty of individual web pages in English and compare our results to popular traditional methods that use word meaning. We demonstrate that with small adjustments, the tool can be retrained to work on French web pages. For both English and French, the tool accurately matches the expected grade level (between 0.63 and 0.79 on a scale of 0 to 1) across all tests. Some traditional methods, like counting unique words (type-token ratio), worked best on standard test texts, but our approach was more accurate for web pages and very short texts (under 10 words). We use a simple method called a smoothed unigram language model to estimate how difficult web pages and short texts are to read.

Shallow Semantic Parsing Using Support Vector Machines In this paper, we suggest a machine learning method for shallow semantic parsing, adding to the research done by Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others. Our method uses Support Vector Machines (a type of machine learning model) which we demonstrate provides better results than older models. We show these improvements by using new features (specific characteristics or data points) and test their effectiveness with a new set of data from the AQUAINT corpus (a collection of text data). We start by creating a parse tree (a structure that shows the grammatical parts of a sentence), and then we label these parts using specific features that we manually extract and input into a machine learning system.

Improvements In Phrase-Based Statistical Machine Translation In statistical machine translation, the best systems currently use phrases or groups of words. We explain the basic phrase-based translation system and various improvements. We describe a very fast monotone search method that works in a straightforward way with the length of the sentence. We show translation results for three projects: Verb-mobil, Xerox, and the Canadian Hansards. For the Xerox project, it takes less than 7 seconds to translate all the test data, which has over 10,000 words. The translation results for the Xerox and Canadian Hansards projects are very encouraging. The system even does better than the alignment template system. In our method, improved phrase probabilities are created from word-pair probabilities and combined in a log-linear model, which is a mathematical model, with an unadjusted phrase-table.

What's In A Translation Rule? We suggest a theory that provides a clear explanation for how words match up in texts that are available in two languages. Using this theory, we bring in a simple method to find the smallest group of language-changing rules from these word matches that make sense of how humans translate languages. We explain how to discover many transformation rules from a structured and matched Chinese/English text, and how to estimate the likelihood of these rules being correct. Our translation rules are learned from bilingual texts where the original language side has been analyzed for grammar structure using a special tool.

Automatically Labeling Semantic Classes Systems that automatically find groups of similar meaning words have been created partly to fix the limits of big word databases like WordNet and Cyc. The best current methods can find many groups but struggle to name their ideas. We suggest a method to name these groups and use them to find "is-a" relationships (like saying a dog is an animal) using a top-down method. The relationships our system learns on its own include appositions (phrases that rename a noun next to them), nominal subjects (the main noun in a sentence), and connections like "such as" and "like." Our method, which looks at how words appear together, has a worst-case time complexity of O(n2k), where "n" is the number of words in the text, and "k" is the number of features we look at. When we use a set of news articles that are cleaner and smaller compared to collections from the internet, we use a syntactic parser (a tool to analyze sentence structure) on the sentences to find and use sentence dependencies to help pick possible labels for the groups.

Accurate Information Extraction From Research Papers Using Conditional Random Fields With the growing use of research paper search tools like CiteSeer for finding literature and making hiring choices, it's crucial that these systems are very accurate. This paper uses a method called Conditional Random Fields (CRFs) to pull out common details from the headers and references of research papers. While the basic idea of CRFs is well-known, figuring out the best way to use them with real-life data needs more study. This paper looks into different factors, such as changes to mathematical methods like Gaussian, exponential, and hyperbolic-L1 priors for better model regulation, and different types of features and Markov order, which is a way to consider the order of events. On a standard test data set, we reached the best performance ever, reducing the error in average F1 score by 36%, and word error rate by 78% compared to the previous best results using another method called SVM. Our accuracy is even better when compared to HMMs. CORA includes two sets: one with research paper headers marked for details like title, author, and institution, and another set of references marked with BibTeX details like journal, year, and publisher.

Name Tagging With Word Clusters And Discriminative Training We present a method to improve training data by adding groups of similar words (word clusters) that are automatically created from a large collection of text that hasn't been labeled. These word groups are used as features in a model trained to identify names in the text. We use active learning, which means we pick the most useful examples for training. We test this method for finding names in text. Compared to a top-performing method that uses Hidden Markov Models (HMM), our method needs only 13% of the labeled data to perform just as well. With a large set of 1,000,000 labeled words, our method cuts errors by 25% compared to the HMM method trained on the same data. We use parts of a system called the Brown cluster hierarchy to make these word groups at different detail levels. We apply the Brown algorithm (an earlier method from 1992) for creating these clusters.

WordNet::Similarity - Measuring The Relatedness Of Concepts WordNet::Similarity is a free software package that lets you measure how similar or related two ideas (or synsets) are in meaning. It offers six ways to measure similarity and three ways to measure relatedness, all based on the WordNet word database. These measurements are done through Perl modules, which take two concepts and give back a number showing how similar or related they are. A lot of research on adding meaning-based knowledge to Natural Language Processing (NLP) for English has likely grown because of the WordNet::Similarity package. The methods range from simple counting of links to considering special features of the network structure like link direction, relative path, and density, and include vector, lesk, hso, lch, wup, path, res, lin, and jcn.

Morphological Analysis For Statistical Machine Translation We present a new technique for analyzing word forms that creates a balance between two languages, even if one has more complex word structures. This helps improve machine translation, which is the process of computers translating text. The technique requires breaking down words from the more complex language into parts like prefixes (beginning of a word), stems (main part of a word), and suffixes (end of a word), and also labeling parts of speech (like noun or verb) in both languages being compared. The method finds parts of words that can be combined or removed in the more complex language to create a balance between the languages. This approach makes translations from Arabic to English much better when using certain computer models that learn from a large collection of sentence pairs, from as few as 3,500 to as many as 3.3 million. We demonstrate that breaking down and removing some small words in Arabic sentences helps match them better with English sentences, which improves the overall translation quality.

A Unigram Orientation Model For Statistical Machine Translation In this paper, we introduce a unigram segmentation model for statistical machine translation where the segmentation units are blocks: pairs of phrases without any internal structure. The segmentation model uses a new orientation part to manage the swapping (changing places) of neighboring blocks. During training, we count how often a block appears to the left or right of another block. The orientation model is proven to improve translation performance over two models: 1) no block re-ordering is used, and 2) block swapping is controlled only by a language model (a system that understands how language is usually structured). We show test results on a typical Arabic-English translation task. This work introduces word-level features for modeling distortion (changes in order).

Automatic Tagging Of Arabic Text: From Raw Text To Base Phrase Chunks To date, there are no completely automatic systems that meet the community's need for basic tools to process Arabic text. In this paper, we introduce a method using Support Vector Machine (SVM), a type of computer algorithm, to automatically break down text into smaller parts (separating short words attached to others), identify parts of speech (like nouns and verbs), and label simple word groups in Arabic text. We modify very accurate tools originally made for English text and use them for Arabic text. Khoja (2001) first created a tool for marking parts of speech in Arabic, which had 131 labels, but this work has reduced the number of labels to make the process easier. We explain a tool for identifying parts of speech based on support vector machines that is trained on separated data (small words attached to others are separate), showing a 95.5% accuracy in tagging.

Improved Statistical Machine Translation Using Paraphrases Parallel corpora, which are collections of text in two languages, are essential for teaching Statistical Machine Translation (SMT) systems. However, for many pairs of languages, these collections are available only in very small amounts. For these language pairs, many phrases that appear during translation will be unfamiliar or unknown. We show how using techniques from paraphrasing, which involve rewording phrases, can help handle these unknown phrases from the original language. Our results demonstrate that adding paraphrases to a top-performing SMT system significantly improves both the range of phrases it can translate and the quality of the translation. For a training set of 10,000 sentence pairs, we improve the coverage of unique words from a test set from 48% to 90%, with more than half of the newly translated words being accurately translated, compared to none with current methods. We introduce a new method that replaces an unknown word or phrase from the original language with a paraphrase, and then uses the translation of that paraphrase to create the final translated sentence.

Learning To Recognize Features Of Valid Textual Entailments This paper suggests a new way to understand text by first matching the text and then checking if one text logically follows from the other. Current methods try to solve this by finding the best match between the question and the text, using a score that breaks down into parts. We believe this method has problems, like wrongly assuming that the relationship is straightforward and local. Instead, we suggest a step-by-step method where matching is followed by a sorting step. Here, we identify key features that show important aspects of the problem and use these features in a computer program trained on sample data. We show results on a test from the 2005 Pascal RTE Challenge that are better than previous methods focused on matching. We highlight that understanding if a text logically follows involves more than just matching words or structures: things like negations (opposite statements), models, verbs that aren't factual or suggest something, and other language elements can change whether a text is valid in ways that are hard to capture just by matching.

Named Entity Transliteration And Discovery From Multilingual Comparable Corpora Named Entity recognition (NER) is a key part of many tasks that involve understanding human language. Most current methods use computer programs that learn from examples, but many languages don't have enough examples to learn from. This paper introduces a method to find Named Entities (NEs), like names of people or places, in languages that don't have many examples, using texts in two languages that are loosely matched over time with a language that has many examples. We noticed that NEs often appear around the same times in these texts and are often written similarly, so we created a method that uses these patterns repeatedly. The method uses a new way to measure how often NEs appear over time and a way to match NEs that doesn't need many examples. We tested the method on a set of English and Russian texts and showed that it found many NEs in Russian. We used single letters and pairs of letters to create a model that recognizes patterns in how NEs are written and combined this with how similarly they appear over time.

Alignment By Agreement We introduce a method that doesn't need labeled data to match words in two languages. It involves training two basic models together to ensure they agree with each other while also fitting the data well. This approach improves accuracy by 32% compared to the usual method where models are trained separately and then combined. Additionally, using a straightforward and fast pair of HMM (Hidden Markov Model) aligners improves accuracy by 29% over the traditional IBM model 4. We apply a selective translation method on a large amount of data, training with 1.5 million features on 67,000 sentences.

Effective Self-Training For Parsing We present a simple, but surprisingly effective, way to train a two-step system that analyzes sentences using easily available data that hasn't been labeled. We show that it's possible to improve sentence analysis using a technique called bootstrapping, which involves using initial results to improve further analysis, especially when a reranker (a tool that helps decide the best analysis) is used. Our improved model scores 92.1%, which is 1.1% better (or 12% fewer mistakes) than the best previous result for analyzing Wall Street Journal texts. Lastly, we share some insights to help understand why this method works. We introduced a very effective way to train a two-step system for analyzing sentences, where the first step uses a specific type of parser (a tool that breaks down sentences), and the second step uses a reranker to improve results. Self-training can sometimes make the same mistakes worse in the new model as in the original model.

Exploiting Semantic Role Labeling WordNet And Wikipedia For Coreference Resolution In this paper, we present an improvement of a computer-based system that finds coreferences, which means determining when different words refer to the same thing. This system uses clues (features) taken from different sources of meaning. These clues come from WordNet (a dictionary-like tool) and Wikipedia, and also involve understanding roles in sentences. We show that these clues help improve how well the system identifies different groups of words, like pronouns (he, she) and common nouns (cat, table). We also show that using pairs of related actions and things in sentences as clues makes the system better. We teach a computer model to decide if two words are referring to the same thing or not. We suggest getting related meanings from Wikipedia, which helps solve the problem of not having enough data when using WordNet alone.

Synchronous Binarization For Machine Translation Systems based on linked grammar rules and tree converters promise to improve the quality of statistical machine translation output, but often require a lot of computing power. The difficulty increases dramatically with the size of individual grammar rules because of the random rearrangements between the two languages, and rules taken from matching text samples can be quite large. We create a simple and quick method for organizing the rearrangements by breaking down the linked rules into smaller parts when possible and show that the new rule set greatly improves the speed and accuracy of a top syntax-based machine translation system. Synchronous binarization breaks down both the original and translated sides of a linked rule at the same time, ensuring that related sections remain together on both sides whenever possible.

Preemptive Information Extraction Using Unrestricted Relation Discovery We are trying to expand what Information Extraction (IE) systems can do. Current IE systems need a lot of time and human work to adjust to new situations. Preemptive Information Extraction aims to automatically set up all possible IE systems ahead of time without needing people to help. We suggest a method called Unrestricted Relation Discovery that finds all possible connections from texts and shows them as tables. We show an early version of a system that gets fairly good results. We use Named Entity Recognition (NER), coreference resolution (tracking mentions of the same thing in text), and parsing (analyzing sentence structure) to a collection of newspaper articles to find two-part connections between Named Entities (NEs). We also use supervised methods (techniques that learn from examples), defining features using a detailed sentence analysis, and use multiple descriptions of the same story in news articles to find useful connections. Preemptive IE is a method that first groups documents using pairwise vector clustering (a way to organize data) and then uses more clustering to group entities based on document groups.

Prototype-Driven Learning For Sequence Models We explore a method called prototype-driven learning for mostly unsupervised sequence modeling, which involves predicting sequences of data without much guidance. Instead of detailed instructions, we use a few clear examples of what each label should look like. This minimal example information is spread across a larger set of data by looking at similarities in the data using a specific type of model. In tasks like identifying parts of speech (categories like noun, verb, etc.) in English and Chinese, and extracting information, these example-based features help reduce errors significantly compared to other methods and do better than past attempts. For instance, we can correctly identify parts of speech in English 80.5% of the time using just three examples of each category without relying on a dictionary. We also look at how this compares to methods that use some guidance and talk about where the system tends to make mistakes. Prototype-driven learning (PDL) improves how well we can predict data by focusing on the likelihood of data labeled with these example features. We ask users to give a few example cases for each category and use those examples as key features.

Learning For Semantic Parsing With Statistical Machine Translation We introduce a new statistical method called WASP for understanding the meaning of sentences in a complete and formal way. A semantic parser, which is a tool to understand sentence meanings, is trained using sentences that have been marked with their correct meanings. The key new idea in WASP is using advanced statistical methods from machine translation (turning one language into another). A word alignment model helps in learning the meaning of words, and the parsing model itself works like a translation model that focuses on sentence structure. We demonstrate that WASP is both accurate and thorough compared to other methods that need similar levels of guidance, and it handles different task difficulties and word arrangements better. We utilize the maximum-entropy model, which is a way to calculate the likelihood of different interpretations based on a given normal language sentence.

Paraphrasing For Automatic Evaluation This paper looks at how changing the words in sentences (paraphrasing) affects the accuracy of automatic evaluation. We try to find a way to reword a reference sentence so it sounds more like a machine-created sentence than the original version. We use this rewording method to help evaluate machine translations. Our tests show that using these reworded reference sentences makes automatic evaluations more accurate. We also noticed that if humans think the automatic rewording is good, it helps the evaluation process. We demonstrate that making new reference sentences by swapping words with their synonyms (similar words) from a tool called Wordnet helps match more words exactly with a machine translation output, which improves the accuracy of BLEU scores, a measure used to judge how good translations are.

Arabic Preprocessing Schemes For Statistical Machine Translation In this paper, we look at how different ways of preparing Arabic words affect the quality of translating it into another language using computers. Our findings show that when we have a lot of training examples, it's best to only separate small parts of words (proclitics). But when we have only a few examples, it's better to break down the text like English using grammar tags and detailed word analysis to understand meanings better. Also, choosing the right way to prepare the text makes a big improvement in translation quality, especially if the topics of the training and test data are different. We demonstrate that different ways of breaking down words lead to better results, but the improvements get smaller as the amount of training data increases.

OntoNotes: The 90% Solution We explain the OntoNotes method and its result, which is a big collection of texts in different languages that has been carefully labeled and built with 90% agreement among people doing the labeling. A first part (300,000 words of English news articles and 250,000 words of Chinese news articles) will be shared with the public in 2007. OntoNotes includes many types of data sources like TV news, news articles, magazines, online text, etc. In the OntoNotes project, people who label the data (called annotators) use small-scale text analysis to create lists of word meanings by grouping similar meanings from WordNet (a dictionary of word meanings), making sure that 90% of them agree on the labels.

Parser Combination By Reparsing We present a new method for combining parsers (tools that analyze sentence structure) by analyzing sentences again after they have been processed by multiple parsers. We use this method for two types of sentence analysis: dependency (how words relate) and constituent parsing (breaking sentences into parts), achieving better accuracy than any single parser alone. We set a limit on the number of sentence parts and look for the arrangement with the most parts from all possible combinations. By combining five parsers, we achieve a score of 92.1, compared to the best single parser score of 91.0.

First-Order Probabilistic Models for Coreference Resolution Traditional noun phrase coreference resolution systems only look at features of pairs of noun phrases. In this paper, we suggest a machine learning method that looks at features over groups of noun phrases, leading to a first-order probabilistic model for coreference. We describe some shortcuts that make this method practical and use it on the ACE coreference dataset, reducing errors by 45% compared to a similar method that only looks at pairs of noun phrases. This result shows how using first-order logic (a way to structure reasoning) in a probabilistic model can work well and be efficient. We present a system that uses online learning (a way to continuously train a machine) to teach a classifier (a tool that decides) whether two entities (things or people) refer to the same thing or not. We introduce a first-order probabilistic model that considers features over groups of mentions, so it works directly on entities.

Bayesian Inference for PCFGs via Markov Chain Monte Carlo This paper introduces two MCMC methods for Bayesian inference (a way to make predictions) of probabilistic context-free grammars (PCFGs are rules for sentence structure) from basic word strings. These methods offer an alternative to the maximum-likelihood estimation (a way to find the best explanation for data) using the Inside-Outside algorithm. We show these methods by estimating a simple grammar that explains the word structure of the Bantu language Sesotho. This proves that with the right starting assumptions, Bayesian methods can figure out language rules even when maximum-likelihood methods, like the Inside-Outside algorithm, only create simple and unhelpful grammar rules. We explain how to use Gibbs samplers (a specific technique for Bayesian inference) to determine the chances of PCFG rules. We also introduce adaptor grammars, a tool for single-language grammar that allows a non-terminal part (a larger sentence part) to be rewritten all at once as a whole tree of phrases.

Lexicalized Markov Grammars for Sentence Compression We introduce a system that shortens sentences using a method called synchronous context-free grammars (SCFG), inspired by a successful approach from 2000. We create a new way to apply SCFG rules that cut out parts of the sentence, which helps us predict more accurately when these parts should be removed. We also use a strong method to match sentence structures in different documents, which allows us to train our models with a lot more data than older methods that relied on limited resources. Finally, we test different models and find that our best model uses a technique to clearly tell apart extra information from main parts of the sentence, and it creates sentences that are considered more grammatically correct than those from previous methods.

Combining Outputs from Multiple Machine Translation Systems Currently, there are several methods for machine translation (MT), which means converting text from one language to another, based on different styles; for example, phrasal (using phrases), hierarchical (using a layered approach), and syntax-based (using sentence structure). These three methods give similar translation quality even though they use different amounts of language knowledge. Having different systems has sparked interest in finding better translations by mixing results from different systems. This paper explains three ways to combine MT systems. These methods work on sentence, phrase, and word levels, using information from lists of top translations (N-best lists), system scores, and matching phrases from the target language back to the source language. The word-level combination gives the most reliable improvements, but the best results on test sets (NIST MT05 and the newsgroup part of GALE 2006 trial) were achieved by using all three methods together. We use a method called minimum Translation Error Rate (TER) alignment (a way to measure translation accuracy) to build a confusion network (a way to organize possible translations). We gather connections from the source language to the target language from the input systems, create a new list of translation options using only these phrases, and re-process the source sentence to create better translations.

Joint Determination of Anaphoricity and Coreference Resolution using Integer Programming Standard pairwise coreference resolution systems, which connect words in a text that refer to the same thing, often make mistakes because they try to identify anaphora (words like "he" or "it" that refer back to something mentioned earlier) as part of this connection process. In this paper, we suggest a method using integer linear programming (ILP), which is a math approach, to solve coreference problems by considering anaphoricity and coreference together, so each helps the other for final decisions. This combined ILP method improves the accuracy score by 3.7-5.3% compared to a basic coreference system on the ACE datasets (a set of data used for testing). By working on anaphoricity and coreference together, we avoid mistakes caused by doing things in steps, without having to separately adjust limits.

Multiple Aspect Ranking Using the Good Grief Algorithm We tackle the challenge of examining several connected opinions in a text. For example, in a restaurant review, these opinions might be about the food, atmosphere, and service. We treat this task as a multi-part ranking issue, aiming to assign a number score to each part. We introduce a method that learns how to rank each part by considering how the rankings relate to each other. This method helps predict individual rankings by looking at relationships between opinions, like when they agree or differ. We show that our model, which focuses on agreement, is more detailed than models that rank each part separately. Our test results further show that our method is better: it improves greatly over both individual rankings and the latest joint ranking model. We combine an agreement model based on contrasting relationships with a local model focused on specific parts to make a smarter overall decision for understanding opinions.

Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme Conversion Letter-to-phoneme conversion usually needs training data where letters and sounds are matched up. Normally, these matches are one-to-one, meaning one letter to one sound. We introduce a new method that allows multiple letters to be matched to multiple sounds. A system predicts pairs of letters and sounds automatically without needing predefined lists. We also use a special method called Hidden Markov Models (HMM) along with a local prediction model to predict the entire set of sounds for a word. This new approach of matching many-to-many shows much better results than the traditional one-to-one matching. Our system performs as well as or better than any other current systems for several languages and data. The M2M-aligner uses a method called the expectation maximization (EM) algorithm, which helps in efficiently matching multiple letters to multiple sounds.

Improved Inference for Unlexicalized Parsing We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs. First, we introduce a new method that simplifies parsing by using a grammar's own structure step-by-step, allowing us to cut down unnecessary parts efficiently without needing a large collection of sentence examples (treebank). In our tests, this method speeds up parsing significantly without making it less accurate. Second, we look at different ways to make predictions using state-split PCFGs, focusing on minimizing mistakes and considering their real-world benefits and drawbacks. Lastly, we share experiments in multiple languages showing that this method of splitting states in a hierarchy is both quick and precise, even without adjustments for specific languages. A successful technique for simplifying the parsing process is using a training method based on probability (likelihood-based hierarchical EM training).

ISP: Learning Inferential Selectional Preferences Semantic inference helps computers understand language better. But the current methods for finding these inference rules, which help computers understand if one statement logically follows from another, haven't worked well in tasks like understanding if two texts mean the same thing or answering questions. This paper introduces ISP, a set of methods to automatically learn which values are suitable for inference rules, called inferential selectional preferences, and ways to remove incorrect conclusions. We test ISP and show evidence that it works well. Context-sensitive extensions of DIRT aim to make DIRT rules adaptable to different situations by adding the right meaning categories to the X and Y parts of these rules. We create groups of meanings using WordNet, a dictionary-like database, and CBC clustering, a method to group similar things. For each rule, we look at how much the examples from the input match to find the right meaning categories. We add specific types of entities to each relationship to understand different meanings of words better.

TextRunner: Open Information Extraction on the Web Traditional information extraction systems have focused on satisfying specific, narrow, pre-planned requests from small, similar collections of text. In contrast, the TEXTRUNNER system shows a new kind of information extraction, called Open Information Extraction (OIE), where the system goes through the entire text collection once and pulls out a large set of relational data pairs, without needing any human help. (Banko et al., 2007) TEXTRUNNER is a fully working, very scalable example of OIE. TEXTRUNNER's extractions are indexed, allowing a fast way to search. Our first public demonstration of the TEXTRUNNER system shows the results of performing OIE on a set of 117 million web pages. It shows the power of TEXTRUNNER by the large number of facts it has collected, as well as its accuracy using our new assessment method. And it shows the ability to automatically find similar relations and objects using large sets of extractions. We have built a faster user interface for searching the results. We provide an online demo of TextRunner.

A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches This paper explains and compares two methods: WordNet-based, which uses a large database of words and their meanings, and distributional similarity, which looks at how words are used in texts. The paper talks about what each method does well and not so well when it comes to figuring out how similar or related words are, and suggests using both together. Each of our methods, when used alone, gives the best results in their category on the RG and WordSim353 word tests, and when we combine them using a supervised method (where a model learns from data), it gives the best results ever recorded on all tests. Lastly, we are the first to explore using these methods for comparing words in different languages, showing that they can be easily adjusted for this and only lose a little accuracy. We create a WordNet-based measure using PageRank (a method to rank web pages) and mix it with several text-based models using SVMs (a type of machine learning model). By looking at the connections between words in each pair, we further divide this dataset into similar pairs (WS-sim) and related pairs (WS-rel), where the first group includes words that mean the same, opposite, are exactly the same, or are more or less specific, and the second group includes other types of word connections.

Shared Logistic Normal Distributions for Soft Parameter Tying in Unsupervised Grammar Induction We present a set of guidelines for probabilistic grammar weights called the shared logistic normal distribution. This set builds on the partitioned logistic normal distribution, allowing linked variability between the chances of different outcomes in the probabilistic grammar, offering a new way to include prior knowledge about an unknown grammar. We explain a method called a variational EM algorithm for learning a probabilistic grammar using these guidelines. We then test unsupervised dependency grammar learning and demonstrate significant improvements with our model for learning in one language and across languages using a non-parallel, multilingual dataset. Our biggest improvements come from linking parameters for various broad categories of parts-of-speech within one language, with only moderate gains by allowing influence between languages on top of the within-language sharing.

Improving Unsupervised Dependency Parsing with Richer Contexts and Smoothing Unsupervised grammar induction models, which try to understand sentence structure without labeled examples, usually use simpler methods compared to models that have been trained with labeled examples. These unsupervised models are kept simple because they need to handle complex calculations and often have limited data. In this paper, we add basic patterns and word information to a model that tries to figure out sentence structures without guidance and show how this extra information can be used effectively by a technique called smoothing, which helps deal with gaps in data. Our model achieves the best results so far for figuring out grammar without guidance, improving previous best results by nearly 10%. We use words that appear more than 100 times and define a grammar model using tied probability (a system that shares parameters) and a method called Dirichlet priors, which helps improve accuracy. We also apply a technique to connect parameters for the E-DMV (a specific model) by learning a backup method for estimating the probabilities of parts of the sentence.

11001 New Features for Statistical Machine Translation We use an improved algorithm called the Margin Infused Relaxed Algorithm by Crammer and others to add many new tools to two machine translation systems: the Hiero system, which uses a hierarchy of phrases, and our system that uses syntax, which is the arrangement of words. On a large Chinese-English translation task, we see clear improvements in quality, with scores increasing by +1.5 and +1.1 on the Bleu scale, which measures translation accuracy. We look at how these new tools affect the systems and how well the learning algorithm works. We use only the 100 most common words for understanding word context. We add features to a model called SCFG for Chinese/English translation, which are in two categories: The first type specifically reduces overestimates of rule counts, or rules with poor connections, bad changes, or unwanted additions of words on the target side.

Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages We introduce a new way to rearrange words using a tool called a dependency parser, to help computer systems translate languages better. Like other methods that change word order before translation, our method can use language rules without making the translation process more complicated. For five languages that typically use the order subject-object-verb (SOV), we show big improvements in a measure called BLEU scores when translating from English, compared to other methods, in advanced translation systems that use phrases. We demonstrate that translation between languages like English, which uses subject-verb-object (SVO), and languages like Pashto, which uses SOV, can be improved by changing the word order on the English side of the texts used for translation. On online texts, we show big improvements by using a set of carefully designed rules to translate from English to each of five SOV languages: Korean, Japanese, Hindi, Urdu, and Turkish.

Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars One of the reasons nonparametric Bayesian inference, which is a method for making predictions without assuming a fixed number of parameters, is gaining attention in computational linguistics (the study of language using computers) is because it offers a systematic way to learn patterns and their likelihoods. Adaptor grammars are a method for creating different types of complex Bayesian models without fixed parameters. This paper looks into the different decisions made when creating adaptor grammars and how these decisions affect the methods used to make predictions, showing that they can greatly influence how well we can separate words in a text without guidance. By using the right adaptor grammars and prediction methods, we achieve an 87% accuracy in identifying words in the well-known Brent version of the Bernstein-Ratner collection of language data, which reduces errors by over 35% compared to the best results reported before for this data set. We discover that using an adaptor grammar that includes rules for syllable structure and three levels of how words commonly appear together results in the highest accuracy for identifying words.

Joint Parsing and Named Entity Recognition For many applications that use language technology, like question answering systems, the overall system uses several separate tools to analyze data (like a tool to identify names of people or places, a tool to link related words, and a tool to understand sentence structure). This can lead to mixed-up results, which can reduce how well the whole system works. We start solving this issue with a combined model that does both parsing (understanding sentence structure) and named entity recognition (identifying names of things), using a specific type of tool that focuses on recognizing parts of sentences. Our model creates results that match well together, so the identified names don't clash with parts of the sentence structure. This combined approach also means that the information from one type of analysis helps improve the other. In tests with a specific dataset called the OntoNotes corpus, we saw improvements of up to 1.36% in one measure called F1 for sentence structure analysis, and up to 9.0% F1 for identifying names. While doing named entity recognition and analyzing sentence structure together improves both tasks, the only part of sentence structure that helps the name identification tool is finding noun phrases (groups of words that act like a noun).

Exploring Content Models for Multi-Document Summarization We explore different methods to create summaries from multiple documents using models that predict the likelihood of words appearing together (generative probabilistic models). Starting with a basic model that counts how often words appear (Nenkova and Vanderwende, 2005), we develop a series of models that add more detail to how we understand the content of a group of documents, and we see improvements in summary quality measured by a standard called ROUGE. Our final model, HIERSUM, uses a layered approach similar to hierarchical LDA (Latent Dirichlet Allocation) (Blei et al., 2004), which organizes topics and their related words in a tree-like structure. In creating general summaries like those for DUC (a summarization competition), HIERSUM achieves top-level performance and is rated better by users compared to another leading method by Toutanova et al. (2007). We also look at how HIERSUM can create several summaries focused on different topics to help users find and explore content. In TOPICSUM, we generate each word based on one topic, which could be a general set of common words, words specific to a document, or the main ideas of a specific group. We create a summarization system using topic models, learning about both broad topics for documents and more specific topics within them.

Using a maximum entropy model to build segmentation lattices for MT Recent work has shown that translating segmentation lattices (which are structures that show different ways to split the input into words for a machine translation system) instead of just using one specific way of splitting the text, makes translations better for languages where writing doesn't clearly show where word parts begin and end. However, a lot of this work has depended on using several different tools to split words, which work differently on the same text, to create a variety of ways to split the source text. In this study, we explain a maximum entropy model (a type of mathematical model) for splitting compound words that uses some general rules to create segmentation lattices for many languages with complex word formations. Using a model specifically tuned for translating German, we show that there are significant improvements in the quality of translations from German to English, Hungarian to English, and Turkish to English compared to the best available methods. We find that using unigram weights (a simple counting method for words) is a useful feature in our word splitting model.

Semantic Roles for SMT: A Hybrid Two-Pass Model We share results on a new mixed method for translation that combines the best parts of understanding sentence roles and translating with statistics. This method avoids being too complicated by using two steps. The first step uses a regular translation model that works with phrases. The second step changes the order of the words, guided by simple tools that understand sentence roles and structures. Testing on a Wall Street Journal news set showed that this method improves the translation score by about half a point compared to a strong regular phrase-based translation, marking the first time understanding sentence roles has been successfully used in translation. We analyze the roles of words in the translated sentence and change their order to better match the meaning between the original and translated sentences.

Multi-Prototype Vector-Space Models of Word Meaning Current models that use vector-space, a type of mathematical model, create one "prototype" or main representation to show what a word means. But because words can have more than one meaning (lexical ambiguity), using just one representation can be a problem. This paper shows a method that groups word meanings into different "sense-specific" representations, meaning each word can have several meanings based on the group it belongs to. This method gives a meaning to words that changes based on the situation, making it easier to handle words that sound the same but have different meanings (homonymy) and words with multiple meanings (polysemy). Tests comparing this method to how humans judge word meanings, both alone and in sentences, show it works better than older methods that use only one main representation or examples. We introduce a new multi-prototype model where we first group the situations where words are used, and then create main representations using these grouped word situations.

Using Mostly Native Data to Correct Errors in Learners’ Writing We present results from various tests on fixing mistakes with articles (like "a" or "the") and prepositions (like "in" or "on") made by people learning English. First, we compare a language model (a computer system that understands language) and special tools designed to find and fix these mistakes. We see how well each one works. Then, we mix the language model and these tools in a combined approach, using information from both to make better decisions. This combined system is trained using examples of learner mistakes, to improve its ability to find and fix errors. This combined approach works much better than using just the language model or the tools alone. Since the combined system needs learner mistakes to learn from, we check how much of this training data is needed to see improvements over not using this combined system. We test everything on a large set of learner English that has been marked for errors. We take out sentences where other mistakes are right next to an article or preposition mistake.

Unsupervised Modeling of Twitter Conversations We introduce the first method that doesn't need pre-labeled data to understand the roles of messages (dialogue acts) in open discussions. Our technique, trained on messy Twitter conversations, groups similar messages together to find out their roles. By considering the order of these roles, the model helps us understand how people communicate in this new way. We tackle the problem of checking how well this model works by using a visual representation and a task that orders conversations. This study is based on a large collection of 1.3 million Twitter chats, which we will share with the public. This large amount of data, possible because Twitter mixes chatting with posting publicly, shows the importance of adapting to new ways of communication. We suggest an evaluation method that uses a rank correlation coefficient, which checks how similar two arrangements of data are. We narrow our data by selecting Twitter chats with 3 to 6 messages, making it easier to list all possible arrangements. We present a method that uses probability to find out the roles of messages in Twitter chats and to categorize tweets in a chat based on those roles. In the block HMM (Hidden Markov Model), messages in a chat follow a sequence determined by a Markov process, where the words in messages are created based on language patterns linked to a state in the hidden Markov model.

For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia We are working on a method to find simpler versions of words (like changing “collaborate” to “work together”) by looking at changes made in Simple English Wikipedia. We are trying two main methods: (1) calculating how likely a word change is a simplification by looking at different kinds of edits, and (2) focusing on changes that are probably simplifications by using extra information about the edits. Our methods work better than a basic approach and find many good simple word changes that aren’t on a list made by people. We find simpler words without considering the sentence structure.

Coreference Resolution in a Modular Entity-Centered Model Coreference resolution, which is about linking words that refer to the same thing, is controlled by grammar rules, meaning-based rules, and conversation context. We present a new approach that uses a model where each of these rules is separately handled and learned mostly without direct instruction. Our model guesses some hidden categories of entity types that create specific entities, which then connect to individual mentions or references. By sharing word information at the level of these general entity categories, our model reduces mistakes related to meaning, leading to the best results so far in fully connecting all related references in text.

Extracting Parallel Sentences from Comparable Corpora using Document Level Alignment The quality of a statistical machine translation (SMT) system, which is a computer program that translates languages, depends a lot on how many matching sentences, called parallel sentences, are used during its training. Recently, different methods have been created to find these matching sentences from data that aren’t exactly matching, like news articles from the same time or similar web pages. One source that hasn't been fully explored is Wikipedia, which is an online encyclopedia with articles connected in many languages. We improve how we find parallel sentences by looking at entire documents, based on the idea that matching sentences are often close to each other. We also use extra information provided by Wikipedia and a tool that automatically creates a dictionary. We show results that prove both the accuracy of finding matching sentences and how it helps improve the translation system. We found big improvements by searching Wikipedia articles for matching sentences using better hints, more detailed information, and understanding how sentences relate to each other within a Conditional Random Fields (CRFs) model, which is a complex statistical tool.

An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing We introduce a new straightforward method for dependency parsing, focusing on creating the simplest connections first, without following a specific direction. Traditional methods use a shift-reduce strategy: they move through a sentence from left to right, choosing from a set of actions until they form a complete tree. A downside of this method is that it focuses too much on the immediate next word or words: while it can use complex structures on the left side, it only considers a few words ahead on the right. Our method, on the other hand, builds the tree by repeatedly choosing the best pair of neighboring words to connect at each step. This means it can use information from both sides of the connection point that has already been created. The parser learns both where to attach words and the order in which to do it. The result is a straightforward, best-first parser that works in O(nlogn) time, which is much more accurate than other similar methods and almost as good as the most advanced models. We find that most of the parsing time is spent on finding and calculating the features.

The viability of web-derived polarity lexicons We look at how possible it is to create large lists of positive or negative words (polarity lexicons) from the internet, doing it partly by machine. We start by explaining a method that spreads information through a network of words, based on past research on making these lists from word connections (like Kim and Hovy, 2004; Hu and Liu, 2004; Esuli and Sabastiani, 2009; Blair-Goldensohn et al., 2008; Rao and Ravichandran, 2009). We use this method to create a much larger English word list than those studied before. Importantly, this list made from the internet doesn’t need special language tools like WordNet (a database of English words), tools that identify parts of speech (like nouns or verbs), or other resources usually used in systems that analyze opinions and feelings. This means the list isn't just limited to certain types of words, like adjectives in WordNet, and actually includes slang, typos, phrases with multiple words, etc. We test this list made from English texts both by looking at its quality and by measuring its performance, showing that it's better than lists studied before, even one made from WordNet. We make a network where each point is one of 20 million possible words or phrases, chosen using rules based on how often words appear and information about where words start and end.

Batch Tuning Strategies for Statistical Machine Translation There has been a lot of recent work on improving SMT (Statistical Machine Translation) tuning methods that can manage more features than the old MERT (Minimum Error Rate Training) method. We look at some of these methods based on how they handle errors in sentences, which leads us to try out new methods, like a Structured SVM (Support Vector Machine). We compare eight different tuning methods, including MERT, in different situations. One important finding is that a straightforward and effective group version of MIRA (Margin Infused Relaxed Algorithm) works just as well as learning one-by-one and often does better than other methods.

Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure It has been shown that using word groupings (word cluster features) from large collections of text without labels (unlabeled corpora) can greatly improve the way we predict language patterns. Previously, research mostly focused on English, but we are now showing these benefits for other languages in two ways. First, we demonstrate that these benefits apply to many different languages. Second, more excitingly, we introduce a method (algorithm) to create language groupings (clusters) that work across different languages and show that using these groupings makes predicting language patterns across languages much better. Specifically, we show that by adding these clusters to systems that directly transfer language tools, the error rate of tools that understand sentence structure (delexicalized dependency parsers), trained on English and used for other languages, can be reduced by up to 13%. When we use the same approach for tools that recognize names of people, places, etc. (named-entity recognizers), we see improvements of up to 26%.

Better Evaluation for Grammatical Error Correction We introduce a new way to judge how well grammatical errors are fixed. Our main tool, called MaxMatch (M2), is a step-by-step process that quickly figures out the best changes needed to match a corrected sentence to the original, correct version. This best sequence of changes is then graded using a method called F1 measure, which is a way to balance accuracy and completeness. We try out our M2 tool on a specific set of data for fixing grammar errors, called the Helping Our Own (HOO) task, and find that it gives a more precise evaluation of how well errors are corrected. We suggest a different way to evaluate, focusing on small parts of words (tokens) instead of the positions of individual characters.

Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters We look at the challenge of identifying parts of speech (like nouns, verbs, etc.) in casual online chat. We test using big groups of similar words found without manual labeling and new word details to make tagging more accurate. Using these methods, our system achieves top results in identifying parts of speech on both Twitter and IRC. For Twitter, accuracy improves from 90% to 93%, which is over a 3% increase. By studying these groups of words, we learn more about language and how it's used in these types of conversations. Plus, we create the first set of rules for marking parts of speech in this kind of text and provide a new collection of English tweets marked following these rules. The tagging software, rules, and big groups of words can be found at: http://www.ark.cs.cmu.edu/TweetNLP This paper explains version 0.3 of the CMU Twitter Part-of-Speech Tagger and the marked data.

Linguistic Regularities in Continuous Space Word Representations Continuous space language models have recently shown excellent results in different tasks. In this paper, we look at word representations in a special way that are automatically learned by the first layer of the model. We discover that these representations are very good at understanding grammar and meaning patterns in language, with each pattern having a specific way of showing the difference between words. This lets us reason with words based on these differences. For example, the difference between male and female is learned automatically, so "King - Man + Woman" gives a result very close to "Queen". We show that these word representations understand grammar patterns using grammar analogy questions (included in this paper) and can correctly answer almost 40% of the questions. We also show that these word representations understand meaning patterns by using the method to answer questions from a test called SemEval-2012 Task 2. Surprisingly, this method does better than the best systems used before. We achieve top accuracy on the grammar-focused part (an syn) with a model called CBOW predict.

Robust Temporal Processing Of News We present a system for marking time-related phrases in news, and explain how we identify these time phrases in newspapers and TV news. The system uses both rules written by humans and those learned by computers, and it is correct 83.2% of the time when compared to manually checked data. We also talk about the first steps in marking the order of events. The main tool in the system is a tagger for time phrases that uses a series of steps based on rules written by humans. We focus on news and explain a system for marking time phrases and a method to use these phrases to set time frames for the whole article. More than half of the mistakes in our basic method are due to spreading a wrong event time to nearby events.

Rule Writing Or Annotation: Cost-Efficient Resource Usage For Base Noun Phrase Chunking This paper compares two methods for creating a tool that identifies base noun phrases: writing rules by hand and using active learning, which involves real-time human feedback. Several new methods of active learning are tested, and the costs of each method are compared. Results show that using active learning is more efficient and effective than writing rules by hand, even with a similar amount of human effort. We use a mix of techniques, like bagging (combining results from multiple models) and partitioning (dividing the data), for active learning in identifying base noun phrases.

Minimally Supervised Morphological Analysis By Multimodal Alignment This paper introduces a method that uses a collection of written texts to figure out how words change form, like turning "brought" into "bring," by analyzing patterns in a lot of single-language text without direct guidance. The method uses four unique ways to line up text based on how often words appear, how similar their context is, how similar their spelling is, and probabilities that are updated over time on how words change form. It starts without any examples pairing changed words with their base form and without any pre-set rules on how words can change form. The method achieves over 99.2% accuracy in analyzing 3888 past-tense examples in English, more than 80% accuracy on the hardest-to-change forms, and 99.7% accuracy on forms that change by adding parts to the end of words. We get excellent results in figuring out the past tense in English after starting with a list of basic word forms, a table showing how words change depending on their part of speech, and standard endings for each part of speech. We suggest a method that finds rules for how base words and their changed forms relate, especially for verbs, but it can also be extended to other word changes. The method described here treats word change as altering the end of the word and adding an ending from a possible list of endings, which might be empty.

An Improved Error Model For Noisy Channel Spelling Correction The noisy channel model, which helps correct spelling mistakes, has been used in many areas. These models have two parts: a source model and a channel model. Not much research has focused on making the channel model better for spelling correction. This paper talks about a new channel model for fixing spelling mistakes using general changes from one string to another. This new model works much better than older models. We offer an improved error model that fixes more than just simple errors like adding, removing, changing, or swapping letters. We show that including a source language model, which understands the language better, makes the corrections much more accurate. We describe the error model by calculating the likelihood of different changes happening step-by-step. We present a model that looks at sequences of letters, not just single letters, and considers where these sequences appear in a word.

Headline Generation Based On Statistical Translation Extractive summarization techniques can't create document summaries shorter than one sentence, which is often needed. An ideal summarization system would understand each document and make a suitable summary directly from that understanding. A more practical way to solve this problem is to treat summarization like statistical machine translation, which is a method that translates text using mathematical models. The challenge then is to create a short version of a document from a longer one. This paper shows results of experiments using this approach, where statistical models are used to choose and arrange words to make summaries in a style learned from a set of training examples. We estimate the length of summaries using a bell curve (Gaussian). We take ideas from Machine Translation and create headlines using statistical models for picking content and forming sentences.

Improved Statistical Alignment Models In this paper, we present and compare different models that match single words for computer-based language translation. We talk about five IBM models for matching, a Hidden-Markov model, methods to smooth out data, and some changes to these models. We explain different ways to join these matches together. To judge how well these models work, we compare the final Viterbi match to a match made by a person. We show that models that consider the order of words and how many words are matched at once give much better results than simpler models like IBM-1 or IBM-2, which don't consider order. The Alignment Error Rate (AER) checks how much the computer-made match is different from the match made by a person.

Statistical Parsing With An Automatically-Extracted Tree Adjoining Grammar We talk about the benefits of using a special type of grammar called lexicalized tree-adjoining grammar (LTAG) instead of another grammar called lexicalized PCFG for understanding sentence structures using statistics. We explain how we create a probabilistic (based on chance) LTAG model using a resource called the Penn Treebank and check how well it works in understanding sentences. We find that our method of creating this model is better than a previous method (by Hwa, 1998) and gives similar results to the lexicalized PCFG. We create a type of grammar called stochastic tree-insertion grammar (STIG) from a news source database called WSJ, achieving 86.6% accuracy for parts of sentences (LP) and 86.9% accuracy for complete sentences (LR) when the sentences are 40 words long.

Automatic Labeling Of Semantic Roles We introduce a system that figures out the meaning-based relationships, or roles, that parts of a sentence have within a specific context. We use different word-based and sentence structure features taken from sentence diagrams to create statistical models from manually labeled example data. The task is divided into two steps: first, argument recognition checks if a part of the sentence has a meaning-based role; then, argument labeling gives a name to the parts that do have roles. We describe a system that uses only sentence structure features to sort the Frame Elements (parts of a sentence) in a database called FrameNet.

The Structure And Performance Of An Open-Domain Question Answering System This paper explains the design, how it works, and the results from using the LASSO Question Answering system created at the Natural Language Processing Laboratory at SMU. To find answers, the system uses a mix of sentence structure and meaning-based methods. It searches for answers using a new way of sorting information called paragraph indexing. In the TREC-8 competition, it scored 55.5% for short answers and 64.5% for long answers. We change a question in everyday language into a search query. We pick out keywords from all named entities, which are names of people, places, or organizations.

Scaling To Very Very Large Corpora For Natural Language Disambiguation The amount of easily accessible online text has reached hundreds of billions of words and keeps growing. However, for most main tasks in natural language processing (understanding), algorithms are still being improved, tested, and compared after being trained on collections of text (corpora) that usually have only one million words or less. In this paper, we assess how well different learning methods perform on a typical task of figuring out the correct meaning of words in context, called confusion set disambiguation, when trained on much larger amounts of labeled data than previously used. We are lucky that for this specific application, correctly labeled training data is free. Since this is not always the case, we look at ways to make the most of very large text collections when labeled data costs money. We propose that creating very large training text collections may be more beneficial for advancing practical Natural Language Processing than just improving methods that use the existing smaller collections. We demonstrate that even with a very basic algorithm, the results keep getting better in a predictable way as more training data is used, extending even to a billion words.

Extracting Paraphrases From A Parallel Corpus While changing the wording but keeping the meaning is important for understanding and creating natural language, current systems rely on manual or partly automatic methods to gather these reworded phrases. We introduce a fully automated learning method to find paraphrases from a collection of different English translations of the same original text. Our method produces both short phrases and single word rewordings, as well as changes in sentence structure. We include information about word types and other clues about word structure and order into our shared learning method.

Immediate-Head Parsing For Language Models We introduce two language models based on an "immediate-head" parser—our term for a parser that uses the main word (head) of a sentence part (constituent) to predict what happens below it. Although the most precise statistical parsers use this type, no previous language model based on grammar uses it. The difficulty level (perplexity) for both of these models is much better compared to the basic trigram model and the best earlier grammar-based language model. For our better model, these improvements are 24% and 14% respectively. We also propose that enhancing the main parser should greatly improve the model's difficulty level and even in the short term, there is a lot of room for improvement in immediate-head language models. The model identifies both sentence structure (syntactic) and word choice (lexical) relationships that help in understanding language. These contexts include sentence structure like parent and grandparent category labels as well as words like the main word (head) of the parent or a sibling part.

An Algebra For Semantic Construction In Constraint-Based Grammars We create a system to build meanings in grammars (rules for structuring sentences) using typed feature structure logics, such as HPSG (Head-driven Phrase Structure Grammar). This method offers an alternative to the lambda calculus; it keeps the useful adaptability of unification-based methods (combining parts smoothly) but limits operations to capture basic patterns and make it easier to manage. The meanings are shown using Minimal Recursion Semantics (MRS), which helps to represent meanings simply and broadly using terms from basic logic and general quantifiers (words that express quantity like "all" or "some"). An MRS includes a collection of basic labeled predicates (statements) and their arguments (details), a list of rules for how they relate, and two connections for linking the representation: a label (a marker that must be more important than all others) and an index (a reference point).

Methods For The Qualitative Evaluation Of Lexical Association Measures This paper shows methods to fairly compare how well words are associated with each other, looking at adjective-noun pairs and preposition-noun-verb triples from German texts. We compare a full list of word combinations, ranked by different methods, to a set of correct examples identified by people. We explain how to estimate the large numbers of words that appear only once (hapaxlegomena) or twice by using random samples. We gather German preposition-verb pairs from a processed version of the Frankfurter Rundschau Corpus. We use four methods to measure word pairings: Point-wise mutual information (PMI) which measures how often things occur together compared to separately; T-Score which helps find unusual word pairings; log-likelihood which shows how likely it is the words occur together; and the raw count of how often two words appear together in the text. The t-test measure, which is one of the standard methods, gives the best results in studies that look at usual word pairings in general language.

Fast Decoding And Optimal Decoding For Machine Translation A good decoding algorithm is very important for the success of any statistical machine translation system. The decoder's task is to find the translation that is most likely correct based on a set of previously learned rules (and a method for combining them). Since there are so many possible translations, typical decoding methods can only look at a small part of them, which means they might miss good solutions. In this paper, we compare the speed and quality of translations from a traditional stack-based decoding method with two new decoders: a fast greedy decoder and a slow but perfect decoder that sees decoding as a math problem to solve. We compare translations from a multi-stack decoder and a greedy hill-climbing method against those from a perfect decoder that treats decoding like a version of the traveling-salesman problem, a well-known math puzzle.

A Statistical Model For Domain-Independent Text Segmentation We suggest a statistical way to divide text into parts that are most likely to be correct. This method doesn't need prior data to learn from because it calculates chances directly from the text itself. So, it can work on any text, no matter the subject. A test showed that this method is either more precise or just as precise as the latest advanced text segmentation system. We think of text segmentation (TS) as finding the cheapest path in a network and use a step-by-step calculation method called dynamic programming. We present one of the first methods that use chances and dynamic programming, named U00.

A Syntax-Based Statistical Translation Model We present a syntax-based statistical translation model. Our model changes a source-language parse tree (a structure that shows how a sentence is put together) into a target-language string by using random operations at each part of the tree. These operations deal with language differences like word order (how words are arranged) and case marking (how words change depending on their role in a sentence). Model parameters are estimated in polynomial time using an EM algorithm (a method to find the best estimates for the model). The model creates word alignments (matching words from one language to another) that are better than those by IBM Model 5. We use a parser in the target language to train probabilities on a set of 609 operations that change a target parse tree into a source string. We present an algorithm for estimating probabilistic parameters for a model which shows translation as a series of re-ordering steps over parts of nodes in a syntactic tree, using automatic parser output for the starting tree structures.

Parameter Estimation For Probabilistic Finite-State Transducers Weighted finite-state transducers, which are mathematical models used to process sequences, don't have a clear way to be trained or taught. It's even more challenging to train them when they are created using complex methods like combining, simplifying, joining, and repeating, as this makes it difficult to manage the settings or parameters. We propose a new way to handle these transducers, called "parameterized FST," and provide methods to teach them, including a clever method ("expectation semirings") that helps calculate important values and changes in an organized and effective way. We use methods that combine different parts of the transducer, working within the "expectation semiring" system, before using another method called the forward-backward algorithm. We suggest that using the expectation semiring is similar to a known method (Inside-Outside algorithm) used for another type of grammar models (PCFGs). We present a general method (EM algorithm) for figuring out the best settings for these probabilistic finite-state transducers. We explain how the expectation semiring helps with learning the parameters.

Learning Surface Text Patterns For A Question Answering System In this paper, we look into how useful simple text patterns are for systems that answer questions on any topic. To find the best set of patterns, we've created a way to learn these patterns automatically. We create a tagged collection of text from the Internet by starting with a few examples made by hand for each question type and using Altavista, a search engine. Patterns are then automatically taken from the search results and made uniform. We measure how accurate each pattern is and find the average accuracy for each question type. These patterns are then used to find answers to new questions. Using the TREC-10 question set, we share results for two situations: answers found in the TREC-10 text collection and from the web. We offer a different way to classify question types and explain a method for using this classification to find specific answers using simple text patterns.

Improving Machine Learning Approaches To Coreference Resolution We present a system that identifies when different noun phrases (like names or things) refer to the same person or object. This system builds on the work of Soon et al. (2001) and, as far as we know, it gives the best results so far on two standard tests for this task, called MUC-6 and MUC-7, with scores of 70.4 and 63.4. We improved the system in two main ways: by making changes not related to language in how the system learns and by greatly increasing the number of features, or characteristics, it uses to include more advanced language understanding. During testing, we used a method called best-first clustering, which groups similar items together. We increased the number of features used by Soon et al. (2001) from 12 to 53. We suggest a system that creates rules and then simplifies them by removing unnecessary ones.

A Generative Constituent-Context Model For Improved Grammar Induction We introduce a model that helps computers understand the structure of language without needing labeled examples. It works by looking at parts of sentences and their surroundings. Using a method called EM to adjust the model's settings, we achieve better results than other methods, especially when tested on the ATIS dataset. When we tried it on another set of sentences similar in length, we got an F1 score of 71%, which measures how well the model finds sentence parts. We compare different ways of labeling words and look at ways to improve the model. We talk about mistakes the system makes, how it stacks up against older models, and discuss its potential and limitations. We also identify word types from a large set of Wall Street Journal articles.

A Simple Pattern-Matching Algorithm For Recovering Empty Nodes And Their Antecedents This paper explains an easy method for finding missing parts, called empty nodes, in sentence structures and identifying the parts they relate to, called antecedents, even when the sentence structures don't show this information. The method uses small connected pieces of the sentence structure that include the empty node and all related parts. The paper also suggests a way to evaluate how well different methods find these empty nodes, without focusing too much on specific sentence details, allowing comparison with a high-quality example set. Testing the method using Charniak’s parser (Charniak, 2000) and the Penn treebank (Marcus et al., 1993) reveals that the simple method works surprisingly well on the most common types of empty nodes. We suggest a method that can identify distant connections between parts of a sentence after the initial analysis is done. Although Charniak's parser doesn't recognize empty parts, we've created a method to find patterns in the Treebank to add these missing parts to the parser's results. It's the first method to find long-distance connections after the main analysis by using a simple pattern-finding method on basic sentence structures.

Pronunciation Modeling For Improved Spelling Correction This paper presents a method for adding word pronunciation information to a model that helps correct spelling mistakes. The proposed method creates a clear error model for how words sound. By understanding the similarities in how words are pronounced, we greatly improve the performance over the best models used before for correcting spelling. We use a pronunciation variation model to create multiple ways of pronouncing each standard pronunciation in a pronunciation dictionary. We expand on the work by Brill and Moore (2000) to consider changes in both letter sequences and sequences of sounds (phones) in the pronunciations of the word and its misspelling. We use the noisy channel model approach to figure out the types and importance (weights) of editing actions. Since a spelling correction model needs to rank candidate words instead of candidate pronunciations, we create an error model that calculates the chance that a word w was spelled as the non-word r based on how they sound.

GATE: A Framework And Graphical Development Environment For Robust NLP Tools And Applications In this paper, we introduce GATE, a system and visual tool that helps users create and use language processing tools and resources in a reliable way. The GATE design allows us to make various successful tools for language tasks (like Information Extraction), build and label text collections (corpora), and test the tools we create. The system can create tools and resources in many languages because it supports Unicode, which is a standard for text encoding. We provide the ANNIE IE system with GATE for breaking down text, splitting sentences, and tagging parts of speech (identifying words as nouns, verbs, etc.). We suggest ways to help different language tools work together using a shared XML format (a standard way to organize data).

The Necessity Of Parsing For Predicate Argument Recognition Broad-coverage text collections marked with information on semantic roles (roles in a sentence) are now available. Statistical systems have been trained to automatically label these roles using the results from statistical parsers (tools that analyze sentence structure) on unmarked text. In this paper, we measure how parser accuracy affects these systems' performance and explore whether a simpler "chunked" (grouped) version of the input can be just as effective for identifying semantic roles. We note that this deep syntax feature (detailed sentence structure) is important for linking semantic roles with surface grammatical functions (basic sentence parts). We test with a set of features: Pred HW (main word of the predicate), Arg HW (main word of the argument), Phrase Type, Position, Path, Voice.

An Unsupervised Method For Word Sense Tagging Using Parallel Corpora We introduce an unsupervised way, meaning it doesn't need human guidance, to figure out the meaning of words by using translations in parallel texts. This method uses the idea that translating the same idea into different languages usually keeps the main meaning but can change based on translator choices and context. Using parallel texts (same content in two languages) can be tricky to evaluate because it's hard to find texts that are both tagged for meaning and translated into another language. So, we use fake translations made by machine translation systems to test our method with a standard set of examples. The findings show that translating words is a useful tool for understanding their meanings. We introduce a method that doesn't need human guidance to figure out word meanings by using translations in parallel texts created by using commercial machine translation systems on a set of English texts that have been tagged for meaning. Our goal is to tag words in both the original and translated texts with their meanings using the WordNet sense inventory, which is a database of word meanings.

New Ranking Algorithms For Parsing And Tagging: Kernels Over Discrete Structures And The Voted Perceptron This paper introduces new methods for teaching computers to understand human language, using a simple learning method called the perceptron algorithm. We explain how these methods can be used effectively with very large representations of sentence structures, like the "all subtrees" method (a way to represent all parts of a sentence tree) described by Bod in 1998, or a method that keeps track of all small sentence parts. We provide test results showing big improvements in two tasks: understanding Wall Street Journal text, and finding specific names or terms in web data. Convolution kernels (a tool to analyze data) are used to define the space of tree parts without making them explicitly. The tree kernel is suggested for improving the order of possible sentence structures. Tree kernels compare how similar two sentence trees are by looking at how many common parts they share.

Parsing The Wall Street Journal Using A Lexical-Functional Grammar And Discriminative Estimation Techniques We present a system for analyzing sentences that uses a Lexical-Functional Grammar (LFG), which is a set of language rules, a parser that follows these rules, and a method to choose the best interpretation based on probability. We discuss how this system was used to analyze sentences from the UPenn Wall Street Journal (WSJ) database. The system uses methods to fully and partially understand sentences to cover all grammar rules for new data. The WSJ database, which has some labeled sentence data, helps in training the system using statistical methods to improve accuracy. We evaluate how well the system removes confusion by checking if it correctly identifies relationships between actions and things in sentences on two different test sets. For a subset of WSJ data with manually checked sentence structures, the system achieves a 79% F-score, which measures accuracy. For Brown corpus data, which is another set of sentence examples, it scores 76% F-score. We explain a model that learns to understand sentences using standard sentence examples by considering each sentence as having a visible structure and a hidden meaning structure. XLE, a tool within the system, picks the most likely interpretation from many possibilities using a probabilistic method that relies on a mathematical model to handle complex sentence data.

Discriminative Training And Maximum Entropy Models For Statistical Machine Translation We present a framework for statistical machine translation (changing text from one language to another using statistics) of natural languages based on direct maximum entropy models (a way to predict things based on given information), which includes the commonly used source-channel approach as a special case. All sources of information are treated as feature functions (specific rules or patterns), which depend on the source language sentence, the target language sentence, and possible hidden variables (unknown factors that might affect the outcomes). This approach allows a basic machine translation system to be improved easily by adding new feature functions. We show that a basic statistical machine translation system is significantly improved using this approach.

A Decoder For Syntax-Based Statistical MT This paper explains a decoding method for a syntax-based translation model, originally by Yamada and Knight in 2001. This model has been improved to include group translations, as shown here. Unlike a traditional word-for-word statistical model, a decoder for this syntax-based model creates an English sentence structure (parse tree) from a sentence in another language. As the model becomes very large in real-world use and the decoder looks at different word arrangements, several trimming methods are needed. We tested our decoder in a Chinese-to-English translation system and got better outcomes than IBM Model 4. We also talk about the connection between this decoder and a language model, which helps predict the next word in a sentence. We suggest a syntax-based decoder that limits changing the order of words by using changes on the sentence's structure.

Bleu: A Method For Automatic Evaluation Of Machine Translation Human evaluations of machine translation take a lot of time and cost a lot. Human evaluations can take months to complete and require human effort that can't be reused. We suggest a way to automatically evaluate machine translation that's fast, cheap, and works with any language. It matches well with human evaluation and doesn't cost much extra each time you use it. BLEU is a system that automatically checks machine translation. BLEU works by comparing how similar the translated text is to a reference translation.

Building Deep Dependency Structures Using A Wide-Coverage CCG Parser This paper talks about a statistical parser, which is like a computer program that analyzes sentences, that uses Combinatory Categorial Grammar (CCG) to create structures showing how words depend on each other. Unlike most other parsers that cover a lot of language, this one can understand complex sentence parts that are far apart, like when words are joined together (coordination), when something is taken out (extraction), when subjects and verbs are switched (raising), and when actions are controlled by other verbs. It also handles the usual nearby word relationships. The parser is trained and tested using a set of these word dependency structures, which come from a collection of CCG sentence analyses that were partially automatically created from the Penn Treebank, a big database of sentences. The parser can correctly figure out over 80% of the labeled word connections and about 90% of the unlabeled ones. We give examples to show how main words can fit into dependency slots while analyzing a sentence, and how far-apart connections can be understood by matching linked main word variables. We explain the structure of how verbs and their arguments relate in CCG using the dependencies between words with special categories and their arguments.

Generative Models For Statistical Parsing With Combinatory Categorial Grammar This paper compares different methods for creating probability models to help a computer understand language using a system called Combinatory Categorial Grammar (CCG). These methods are trained and tested using a set of language examples that are converted from another language system called the Penn Treebank into a form that CCG can use. When checking how well these methods understand relationships between words, our best method got a score of 89.9%, which is similar to results from a simpler language system by Collins (1999). Unlike Gildea (2001), we found that focusing on how words relate to each other significantly improves results. The CCG rules, which help combine words and phrases, are represented by specific examples, along with extra rules for punctuation and changing word types. The features for understanding word relationships are defined by looking at these specific examples and adding important words from the categories being combined.

Bootstrapping This paper improves the study of co-training, explains and tests a new co-training method that has a solid reason for its use, explains the reasoning behind the Yarowsky method, and shows that co-training and the Yarowsky method rely on different independence ideas. We demonstrate that the idea of independence can be loosened, and co-training still works well with a less strict independence idea. We improve Dasgupta et al's finding by loosening the requirement of view independence with a new condition. We introduce the Greedy Agreement Algorithm, which uses two separate perspectives of the data to create two simple decision-makers from a set of hand-created starting rules. We demonstrate that if certain independence conditions between these decision-maker rules are met and the accuracy of each rule is higher than a specific level T, then the accuracy of the final decision-maker is higher than T. We claim that the assumption of conditional independence is very strong and is not often found in real data sets, indicating that a less strict independence assumption is enough.

An Unsupervised Approach To Recognizing Discourse Relations We present a method that doesn't require pre-labeled data (unsupervised approach) to identify connections between parts of text, like CONTRAST (showing differences), EXPLANATION-EVIDENCE (providing reasons or proof), CONDITION (if-then situations), and ELABORATION (adding details). We demonstrate that classifiers, which are like smart sorting programs, trained on examples automatically gathered from large amounts of text, can tell some of these connections apart with up to 93% accuracy, even if there are no obvious hint words. We use a pattern-based method to find examples of these connections, such as Contrast and Elaboration, from large collections of text that haven't been labeled. We suggest a way to find connections between parts of text using a simple statistical method (Naive Bayes classifiers) trained on a very large set of text.

Evaluating Translational Correspondence Using Annotation Projection Recently, computer programs that translate languages have started using more advanced language structures, like how words depend on each other. These programs assume that sentences in different languages match up directly, but we don't fully understand if this assumption is correct or useful. In this study, we measure how well these word dependencies stay the same when we translate directly from English to Chinese. Our findings indicate that although assuming direct matching is often too strict, making a few basic language adjustments can improve the quality of Chinese translations by 76% compared to no improvements. The method called dependency projection (DPA) based on the Direct Correspondence Assumption means that if two words are connected in one language, their matching words in the other language are assumed to be connected in the same way. We match sentences in both languages using translation models based on phrases and then apply these matches to sentence structure diagrams.

Translating Named Entities Using Monolingual And Bilingual Resources Named entity phrases are some of the hardest to translate because new ones can appear suddenly, and many are specific to certain areas or topics and aren't in bilingual dictionaries. We introduce a new method for translating these phrases using easily available resources in one language and two languages. We tested this method on translating Arabic named entities into English. We also compared our results with human translations and a commercial translation system doing the same job. We show that using extra language resources like online counts of possible spellings can significantly improve translation accuracy. We describe a model based on spelling that directly changes English letter sequences into Arabic ones, with a probability that is learned from a small list of English and Arabic names, without needing English pronunciations. The models based on sounds (phonetics) and spelling have been combined into one translation model. We use information from the Web to check the translation options created by the language model, achieving an accuracy of 72.6% in translating Arabic words not found in English dictionaries (OOV words).

Thumbs Up Or Thumbs Down? Semantic Orientation Applied To Unsupervised Classification Of Reviews This paper introduces a simple method that doesn't need human supervision to decide if reviews are good (thumbs up) or bad (thumbs down). The decision is made by looking at the average meaning of phrases with describing words (adjectives or adverbs) in the review. A phrase is seen as positive if it has good meanings (like “subtle nuances”) and negative if it has bad meanings (like “very careless”). The paper explains that to find out if a phrase is positive or negative, they compare how often a phrase appears with the word “excellent” versus the word “poor”. A review is marked as good if the average meaning of its phrases is positive. This method correctly identifies whether a review is good or bad 74% of the time when tested on 410 reviews from different areas like cars, banks, movies, and travel spots. The success rate is highest at 84% for car reviews and lowest at 66% for movie reviews. We explain how to automatically create a word list by checking which words often appear together with words known to have certain feelings or opinions.

Named Entity Recognition Using An HMM-Based Chunk Tagger This paper suggests using a Hidden Markov Model (HMM) and a special tool called an HMM-based chunk tagger to create a system that can find and classify names, times, and numbers. With the HMM, our system can use and combine four types of clues: 1) simple internal word features like if a word starts with a capital letter or is a number; 2) important word meanings that act as triggers; 3) lists of names and terms (gazetteer feature); 4) the larger context around the words. This approach helps solve the named entity recognition problem effectively. When tested on specific English tasks, our system achieved scores of 96.6% and 94.1%, which are very high. This means our system works better than any other machine-learning system reported so far. It even performs better than systems that use detailed human-made rules. Our system can identify different types of named entities like organizations, locations, people, dates, times, money, and percentages.

Ranking Algorithms For Named Entity Extraction: Boosting And The Voted Perceptron This paper talks about methods that reorder the best N guesses from a tool that labels parts of text, specifically for finding named entities (like names and places) in web data. The first method uses a boosting algorithm, which is a technique to make predictions better. The second method uses the voted perceptron algorithm, which is another technique for improving predictions. Both methods show similar and important improvements compared to the original labeling tool. The voted perceptron method can be much quicker to train, but it might take more time to process test examples. We explain a way to group words with similar letter patterns into categories.

Offline Strategies For Online Question Answering: Answering Questions Before They Are Asked Recent work in Question Answering has focused on web-based systems that find answers using simple word and grammar patterns. We present a different strategy where patterns are used to gather very accurate relational information ahead of time, creating a data collection that is used to quickly answer questions. We test our strategy on a difficult group of questions, like "Who is …" questions, against a top web-based Question Answering system. Results show that the gathered relations correctly answer 25% more questions and do so a thousand times faster than the top system. We use word type patterns to find a group of "type of" relations involving specific names. The accuracy of the gathered information can be greatly improved by using machine learning methods to remove irrelevant data.

Using Predicate-Argument Structures For Information Extraction In this paper, we present a new, adaptable method for Information Extraction (IE) that uses predicate-argument structures, which are sentence patterns showing relationships between actions and entities. We also introduce a new automatic method to identify these patterns, which is crucial for our IE approach. It is based on: (1) a larger set of characteristics; and (2) using decision tree learning, a method that helps computers make decisions. The test results support our claim that precise predicate-argument patterns lead to high-quality IE results. We use semantic parsing, a technique to understand the meaning of sentences, to identify the predicate-argument structure in sentences.

A Noisy-Channel Approach To Question Answering We introduce a method that uses probability, called a noisy-channel model, for answering questions and show how it can be used in a complete question-answering system. Our noisy-channel system works better than an advanced rule-based question-answering system that uses similar information. We also show that our model is flexible enough to include many resources and techniques specific to question answering, such as using WordNet (a large database of words), structured and semi-structured databases, reasoning, and rephrasing. We have created a noisy-channel model for question answering that shows how a sentence with an answer can be changed into the question through a series of random steps. We also suggest another way to combine strategies for advanced question answering: (1) a system based on understanding sentence structure and meaning is combined using a method that finds the most likely outcome with (2) a statistical noisy-channel method for question answering and (3) a pattern-based method that learns from data on the Web.

Fast Methods For Kernel-Based Text Analysis Kernel-based learning, like Support Vector Machines, has been used successfully for difficult tasks in Natural Language Processing (NLP). In NLP, choosing the right combinations of features (important characteristics) is essential for better performance, but they are often chosen based on trial and error. Kernel methods improve this by automatically combining features effectively without making the process more complicated or slow. Kernel-based text analysis is very accurate, but it's usually too slow for analyzing large amounts of text. In this paper, we modify a Basket Mining algorithm to turn a kernel-based classifier into a simple and fast linear classifier. Tests on English BaseNP Chunking, Japanese Word Segmentation, and Japanese Dependency Parsing show our new classifiers are about 30 to 300 times faster than the usual kernel-based classifiers. We introduce polynomial kernel inverted (PKI). PKI - Inverted Indexing, keeps track of each feature and the important data points where it appears. The PKE approach uses a basket mining method to remove unnecessary features from the expansion. We use a version of the PrefixSpan algorithm to quickly find features in a simple polynomial kernel space.

Clustering Polysemic Subcategorization Frame Distributions Semantically Previous research has shown that grouping similar things together (clustering) helps in finding groups of verbs with similar meanings from data that hasn't been sorted by meaning. We explain a new method that groups patterns of how verbs are used with their different structures (subcategorization frame distributions) using two techniques: Information Bottleneck and nearest neighbour methods. Unlike past studies, we focus on grouping verbs that have multiple meanings (polysemic verbs). We introduce a new way to evaluate, which considers how having multiple meanings affects these groups, giving us a better understanding of how well we can sort these unsorted verb structures by meaning. We test our groups (hard clusterings) against a standard that includes multiple categories for each verb. We create a test sample by looking at how verbs have multiple meanings based on Levin's classes and the LCS database (Dorr, 1997). Prepositional phrases (pp) are set up for two common verb structures (NP and NP PP), and we use the raw counts of these structures, without filtering, to represent a verb.

Reliable Measures For Aligning Japanese-English News Articles And Sentences We have matched Japanese and English news articles and sentences to create a large collection of aligned text (parallel corpus). First, we used a method that helps find information across languages (cross-language information retrieval, CLIR) to match the articles, and then we used a method called dynamic programming (DP) matching to align the sentences within those articles. However, there were many mistakes in the matches. To fix this, we suggest two ways (scores) to check if the matches are correct. To align articles, we compare how similar the sentences matched by DP are, and to align sentences, we compare how similar the articles matched by CLIR are. These methods help each other to make the matches more accurate. With these methods, we've created a large collection of matched articles and sentences that anyone can use. We have automatically created a matched Japanese/English Yomiuri newspaper collection with 180,000 pairs of sentences. We use a similarity measure called BM25.

Loosely Tree-Based Alignment For Machine Translation We improve a translation model that rearranges parts of sentence structures (syntactic trees) so that it can also handle alignments that don't fit the original structure, while keeping the process manageable and not too complex. This is done by adding a new method called subtree cloning to the existing algorithms that align parts of one tree to a string of words or another tree. We discovered that using two trees made the alignment too strict, and we got better results when aligning a tree to a string than when aligning two trees. We trained a system using pairs of sentence structures from the Korean-English Treebank, checking how well it matched with alignments done by people. The "clone" method helps align words even when the structures are very different, but it decreases the likelihood of the alignment being accurate.

A Probability Model To Improve Word Alignment Word alignment is very important in statistical machine translation, which is a method for automatically converting text from one language to another. Collections of text that are aligned word-by-word have been found to be a great source of knowledge for translation. We introduce a statistical model, which is a mathematical way, to calculate the likelihood (probability) of how words align in a pair of sentences. This model makes it easy to include features that are specific to the context, or situation. Our tests indicate that this model can effectively help improve current word alignment. We suggest a direct way to align words and claim it would be easy to figure out the necessary parameters using a supervised alignment corpus, which is a collection of texts with already aligned words. We use the idea that phrases stay together in a dependency tree, which shows how words are connected, as a rule for a beam search aligner, a method for finding the best alignment.

Probabilistic Parsing For German Using Sister-Head Dependencies We present a model that predicts sentence structure in German using probabilities, trained on a data set called the Negra treebank. We find that current models, which focus on word-to-word connections, work well for English but don't do better than basic models for German. Learning curves, which show how performance improves with more data, reveal that this issue isn't due to a lack of data. We suggest a different model that uses connections between a word and its "sister" words, rather than just direct word-to-word connections. This new model does better than the basic ones, reaching up to 74% accuracy in labeling and finding the right sentence parts. This suggests that sister-word connections work better for data sets with simple structures like Negra. We demonstrate that fully focusing on word-to-word connections doesn't work well across different languages, types of data, or subject areas. We also show that figuring out the correct grammatical roles is harder than just understanding the basic structure of sentences.

A Comparative Study On Reordering Constraints In Statistical Machine Translation In statistical machine translation, creating a possible translation is very demanding for computers. If we allow words to be rearranged in any order, the search becomes extremely difficult to solve. However, if we limit how words can be rearranged in a smart way, we can find a solution faster. In this paper, we compare two different ways to limit word rearranging, called ITG constraints and IBM constraints. We discuss how many ways words can be rearranged with each method. We show a link between ITG constraints and Schroder numbers, a mathematical concept known since 1870. We test these methods on two tasks: the Verbmobil task and the Canadian Hansards task. The testing has two parts: First, we check how many of the best word matches in the training data fit each method. Second, we limit the search to each method and compare the translations we get. The tests will show that the basic ITG constraints aren't enough for the Canadian Hansards task. So, we offer an improvement to the ITG constraints. These improved ITG constraints raise the alignment match from about 87% to 96%. We show that ITG constraints provide much better matches than those used in IBM translation methods for both German-English (Verbmobil data) and French-English (Canadian Hansards data). We introduce a standard form of ITG that prevents counting errors.

Minimum Error Rate Training In Statistical Machine Translation Often, the training process for statistical machine translation models relies on maximum likelihood, which is a method of estimating the best-fit parameters, or similar methods. A common issue with this method is that it doesn't closely relate to how well the translation works on new, unseen text. In this paper, we examine different training methods that aim to directly improve how good the translation is. These methods use new automatic ways to evaluate translations. We explain a new method for effectively training without using an adjusted error count. We demonstrate that much better results can often be achieved if we use the final evaluation measure directly as part of the training process. In our model, the importance of different features is adjusted using Minimum Error Rate Training (MERT) to increase the BLEU score, which is a way to measure translation quality.

A Machine Learning Approach To Pronoun Resolution In Spoken Dialogue We use a decision tree method to figure out pronouns in spoken conversations. Our system handles pronouns that refer to both noun phrases (NP) and other types of words. We introduce specific characteristics to help identify pronouns in spoken dialogue and find out which ones work best. We test the system on twenty Switchboard conversations and show that it works as well as Byron's (2002) manually adjusted system. We want to identify a group of the best characteristics that make the system correctly group related words when tested on new data.

Coreference Resolution Using Competition Learning Approach In this paper we suggest a new way of using competition learning for coreference resolution, which is a method to identify when words in a text refer to the same thing. Usually, machine learning techniques use a single-candidate model, where they look at one option at a time. However, this method isn't very good at figuring out which option is best. Our method uses a twin-candidate learning model, which means it looks at two options together to better decide which one is preferred. This model helps in reliably choosing the best option. Additionally, we use a candidate filter to make the process faster and reduce errors during training and resolution. Tests on specific data sets, MUC-6 and MUC-7, show that our method works better than the single-candidate model. We use examples of non-anaphors (words that don't refer back to something else) to create a special group of training examples in the twin-candidate model, which improves our results by 2.9 and 1.6, leading to scores of 67.3 and 67.2 in F1-measure, a performance metric, on the MUC-6 and MUC-7 data collections, respectively.

An Improved Extraction Pattern Representation Model For Automatic IE Pattern Acquisition Several methods have been explained for automatically finding patterns to pull out information without human guidance. Each method uses a specific way of looking at patterns, like how actions and their details are structured, or how words depend on each other. The impact of these different ways has not been looked at before. In this paper, we compare previous models and present a new model called the Subtree model, which is based on any part of dependency trees. We explain a way to find these patterns and show through experiments that this model improves recall, meaning it can pull out more relevant information. Our method involves three steps to learn how to extract patterns from the source documents for a situation given by the user. We use common parts of dependency trees, identified using TF*IDF (a way to find important words) to spot named entities (specific names or terms) and key patterns for a certain area. We also suggest new ways to represent these patterns that expand on the SVO (Subject-Verb-Object) format.

Improved Source-Channel Models For Chinese Word Segmentation This paper introduces a system that breaks down Chinese sentences into words using improved models that predict how sentences are formed. Chinese words are categorized into four types: words found in dictionaries, words formed by adding prefixes or suffixes (morphologically derived), basic facts or statements (factoids), and specific names for people, places, or organizations (named entities). Our system offers a single method to handle four key tasks: (1) breaking sentences into words, (2) analyzing word structure, (3) identifying basic facts, and (4) recognizing names. We tested the system using a set of sentences that were manually checked, and we also compared its performance with other top systems, keeping in mind that different systems might define Chinese words differently.

Counter-Training In Discovery Of Semantic Patterns This paper introduces a method to find patterns in meaning without using labeled examples. These patterns help in understanding text, like finding events in text for information extraction. The method builds on earlier methods that also find patterns without guidance. Usually, these methods keep producing patterns, but they become less accurate over time. Our method is different because it makes multiple scenarios compete against each other. This helps decide when to stop the process while keeping accuracy high. We talk about experiments with different scenarios and look at various parts of the new method. We created Counter-Training to find incorrect rules in a specific area or category by learning from different areas or categories at once. We use a model called predicate-argument (SVO), which considers only the parts of a sentence with a verb, its subject, and its object as possible pattern candidates.

Language Model Based Arabic Word Segmentation We simplify Arabic's complex word structure by using a model that breaks a word into parts called morphemes, which can include prefixes (beginning parts), stems (main parts), and suffixes (ending parts). Our approach starts with a small set of Arabic words that have been manually broken down and uses this to help an automated process learn from a large set of unbroken Arabic words. The process uses a three-part language model to guess the most likely sequence of word parts. This model is first created from about 110,000 words that have been manually broken down. To make the word breaking more accurate, we use an automated method to find new main word parts from a large collection of 155 million unbroken words and update the model with this new information. The final system for breaking Arabic words achieves about 97% accuracy when tested on a set of 28,449 words. We think this is a top-level result and the method can be used for other languages with complex word forms if there is a small set of manually broken down words available. We show how this method can be used to break down Arabic words as a step in translating languages with machines.

Accurate Unlexicalized Parsing We show that a grammar model called an unlexicalized PCFG (Probabilistic Context-Free Grammar without specific word information) can understand sentences more accurately than thought before. This is done by using simple changes based on language rules, which fix wrong assumptions in a standard grammar model. Its accuracy of 86.36% is better than early models that included specific words and almost as good as the best current models. This means that unlexicalized PCFGs could be useful because they are simpler, easier to copy, and easier to understand than more complex models that use specific words. The methods for using them are simpler, more known, less complex, and easier to improve. We also introduce a manual way to improve the symbols used.

Is It Harder To Parse Chinese Or The Chinese Treebank? We examine the problems faced when using models made for English to work with Chinese. We create a statistical parsing tool for the Penn Chinese Treebank, highlighting the big statistical differences between the WSJ (Wall Street Journal) and Chinese data collections, which affect how well general parsing methods can be adapted. We analyze the main reasons for errors in this data set, showing their causes and how often they occur. Some errors are because of tricky grammar issues in Chinese, while others happen due to the way the data is organized. We explain how to fix each error type with simple changes to the assumptions in our PCFG (Probabilistic Context-Free Grammar) model, improving our accuracy score from 80.7% to 82.6% on our test data, nearing the best accuracy reported for Chinese parsing. We suggest that sorting errors carefully can lead to better performance. Mistakes in tagging nouns and verbs often occur in our PCFG model with PCTB data, made worse in Chinese by the lack of specific grammar words and word changes. There are many language and structure differences between Chinese and English and their data sets, making it more challenging to process Chinese.

Exploiting Parallel Texts For Word Sense Disambiguation: An Empirical Study A main issue with word sense disambiguation (WSD) is not having enough manually labeled examples to teach the computer. In this paper, we look at a way to automatically get labeled examples from English-Chinese text pairs, which we then use to figure out the meanings of nouns in a specific English task called SENSEVAL-2. Our research shows that this method of getting labeled examples is promising. For the hardest SENSEVAL-2 nouns, the success rate difference between the two methods is only 14.0%, and it could get smaller to 6.5% if we ignore the advantage of manually labeled examples having more detailed meanings. Our study also points out how important it is to consider the topic area when testing WSD programs. We solve word sense disambiguation by manually linking WordNet meanings with their translation in Chinese, then automatically pulling out examples using IBM Models from a two-language text collection. When different meanings of an English word are translated into the same Chinese word, we can combine these meanings into a bigger, shared meaning group.

Probabilistic Text Structuring: Experiments With Sentence Ordering Ordering information is a crucial task for creating natural language applications. In this paper, we suggest a method for organizing information that works well for generating text from other text. We explain a model that learns rules about the order of sentences from a collection of texts focused on a specific subject and a method that finds the most likely order out of several options. We test the automatically created sentence orders by comparing them to original texts from our collection and to people who try to do the same task as the model. We also check how suitable this model is for summarizing information from multiple documents. We develop a model that looks at words across sentences that are next to each other, especially focusing on words with specific meanings or roles. We propose a method that calculates how likely two sentences are to be next to each other to help order sentences. For features, we suggest using the combination of important words in sentences that are next to each other.

Discourse Segmentation Of Multi-Party Conversation We introduce a general method to divide conversations with multiple people into topics. Our method uses features, which are like clues, about the content and form of speech. It combines a text-based system that looks at what is being said and clues from speech sounds to spot changes in topics. This method uses automatically created rules to mix these different clues together. The text-based system looks at how words stick together and works as well as the best current systems that rely on word information. By using both content and form clues, we make fewer mistakes. We ensure high quality by checking if at least three human reviewers agree on the topic divisions. We suggest using a method that looks at word connections without training (LCSeg) and one that uses training for dividing meeting notes into topics. Our LCSeg is the only system that looks at how words are spread out, tested on ICSI meeting data.

Automatic Error Detection In The Japanese Learners' English Spoken Data This paper explains a way to find grammar and word choice mistakes made by Japanese people learning English, and other methods that help make finding these mistakes more accurate even with a small amount of practice data. In this paper, we show how well these methods work by doing tests using our collection of learner mistakes. We use edited transcripts of Japanese speakers in a spoken English test to train a computer program to identify 13 different types of grammar and word choice mistakes, like errors with prepositions (words like "in" or "at"). In the Japanese Learners of English collection of data (Izumi et al., 2003), mistakes with verbs are one of the most common types. Using articles (words like "a," "an," or "the") has been found to be the most common mistake in the JLE (Japanese Learner English) data collection.

Learning Non-Isomorphic Tree Mappings For Machine Translation Often, someone might want to learn how to convert one tree structure into another, using examples that don't perfectly match or using a mix of trees and simple text. Unlike older methods that only work with matching tree shapes (isomorphic trees), a new method called synchronous TSG allows for changes in the tree structure. We change this method to work with dependency trees, and outline some algorithms (step-by-step procedures) for matching, training, and converting. We suggest that if you have the structure of the original sentence, converting can also be seen as a problem of analyzing the tree structure. We look at synchronous tree substitution grammar, a method that can handle differences in structure and is trained using a method that distinguishes between different options.

A TAG-Based Noisy-Channel Model Of Speech Repairs This paper talks about a system that can find and fix mistakes in spoken words written down. It uses a tool that checks sentence structure as the main model and a new kind of machine (TAG-based transducer) as the second model. This method uses the idea that the mistake is a "rough copy" of the correction. The system is tested with a special set of conversations that have mistakes marked in them. These models are good at finding mistakes. Even though the basic model works well, using a special tool called a log linear re-ranker can make it even better. Our TAG system is very good at fixing errors because it keeps track of words that overlap between mistakes and corrections.

Discriminative Training Of A Neural Network Statistical Parser Discriminative methods, which focus on distinguishing between different categories, have shown big improvements over older methods in many machine learning areas. However, applying them to understanding human language has been challenging. One issue is that many studies on discriminative methods mix up changes to how the learning is done with changes to how the problem is set up. We demonstrate how a language processor (parser) can be trained using a discriminative method while still organizing the problem with a traditional model that predicts based on past examples. We introduce three methods for training a neural network to guess the chances for a statistical parser: one follows the traditional model, one uses the distinguishing method, and one uses the traditional model for predictions but the distinguishing method for training. This last approach performs the best, reaching top levels of accuracy (90.1% F-measure on parts of sentences). We discuss why traditional models are better than those trying to directly predict the probability after something has occurred. We use neural networks to create hidden states in a type of parser known as left-corner. We find that training with the distinguishing method was too slow, but it reports better accuracy than traditional models by re-evaluating the results of the traditional model using the distinguishing approach.

Parsing The WSJ Using CCG And Log-Linear Models This paper explains and tests log-linear parsing models, which are methods for understanding sentences, for Combinatory Categorial Grammar (CCG), a type of language structure. It talks about a way to run the L-BFGS optimization algorithm, which helps improve models, on a group of computers called a Beowulf cluster, allowing the use of the complete Penn Treebank, a large language database, for estimation. We also create a new fast method for parsing CCG that aims to capture the most sentence connections correctly. We compare different models that use all CCG sentence structures, even unusual ones, with standard models. Both models perform similarly well and compete with other CCG parsers that work with all kinds of texts. Our CCG parser is very accurate and fast, finding sentence connections with an overall F-score, a measure of accuracy, of over 84% on Wall Street Journal text, and can process up to 50 sentences per second. Our parsing success depends on a super tagger, a tool that labels words with their types, having at least a 97% accuracy for each word and at least 60% accuracy for whole sentences (using 1.5 categories per word). Our parsing performance shows how accurate word labeling relates to how well we can understand sentence connections.

Incremental Parsing With The Perceptron Algorithm This paper explains a step-by-step way to break down sentences using a learning method called the perceptron algorithm. This method uses a technique called beam-search, which helps find the best outcome during both learning and understanding phases. The perceptron method used the same key details as a previous model by Roark, 2001a, and tests show it works just as well at understanding the Penn treebank, a collection of sentence structures. We show that when we train the perceptron model to work together with the previous model during the search, it improves accuracy by 2.1 percent, reaching an 88.8 percent success rate. We suggest a new way to stop and adjust the model when a correct step is not in the current choices. Our parser, which breaks down sentences, works best (with scores of LR=88.4%, LP=89.1%, and F=88.8%) when it uses information about punctuation marks and what comes next in a sentence. This new stopping and adjusting method helps make the model more accurate and quicker to learn.

A Mention-Synchronous Coreference Resolution Algorithm Based On The Bell Tree This paper introduces a new method for coreference resolution, which is figuring out which words in a text refer to the same thing. It uses a Bell tree, a type of structure to organize choices, to explore different options, treating the problem as finding the best path from the start of the tree to the end points. A Maximum Entropy model, a statistical tool, is used to rank these paths. The results of coreference performance on data from the 2002 and 2003 Automatic Content Extraction (ACE) will be shared. We also trained a system using MUC6 data and got good results. During testing, we use beam search, a method to narrow down choices, but stick to a fixed way of matching references and train a model using batch learning, which means training with groups of data at a time. We use a Bell tree to represent different ways to group mentions into entities and trained a model to search this tree. To handle the complexity of calculations, we use a smart way to find the most likely groups by performing a beam search through the Bell tree. We show that you can get a high MUC score, a measure of success, just by putting all mentions together. We used the ANY predicate, a type of condition, to create features for their model that looks at groups of mentions, but it did not work as well as the model that looks at pairs of mentions.

A Joint Source-Channel Model For Machine Transliteration Most foreign names are changed into Chinese, Japanese, or Korean using sounds that are close but not exact. This change (transliteration) usually goes through a middle step of sound mapping. This paper presents a new method that lets us directly connect the writing systems of two different languages using a combined source-channel model, also known as an n-gram transliteration model (TM). With the n-gram TM model, we automate the alignment of writing systems to get matched transliteration parts from a dictionary with two languages. The n-gram TM in this new method makes it much easier to develop systems and significantly improves accuracy in transliteration compared to other advanced machine learning methods. This model is tested with several English-Chinese language experiments. We discover that translating English to Chinese without using Chinese sound units is better than using them. Our approach, which treats transliteration like a translation task with a straightforward rule, tries to get a direct writing connection to reduce mistakes from multiple changes. Sound-based methods are often not good enough because names come from different origins, and transliterations are not always based on how they sound. Many translated words are names, and how they sound can change depending on their original language. Our direct writing method using individual Chinese characters helps solve this issue by directly choosing the right characters.

A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based On Minimum Cuts Sentiment analysis is about figuring out the opinion or feeling behind a piece of writing, like deciding if a movie review is "thumbs up" or "thumbs down." To figure out if the sentiment is positive or negative, we suggest a new machine-learning method that uses text-categorization (sorting text into categories) techniques, focusing only on the parts of the text that are opinion-based. Finding these parts can be done using efficient methods for finding minimum cuts in graphs (a way to simplify complex data), which helps include connections between sentences. We made a movie-review dataset to detect opinions. We believe that identifying the opinionated parts before analyzing the sentiment gives better results. We demonstrate that analyzing sentences individually can improve the analysis of the whole document. In our method for detecting opinionated parts, we create soft local consistency constraints (gentle rules) between every sentence in a document, and solve it using a min-cut algorithm (a method to separate data efficiently).

Finding Predominant Word Senses In Untagged Text In figuring out the meaning of words (word sense disambiguation or WSD), a simple rule of picking the most common meaning is very effective because the meanings of a word are often not evenly spread. The issue with using the most common meaning, aside from ignoring the words around it, is that it needs some amount of manually labeled data. While there are some manually labeled text collections available for certain languages, it's expected that how often meanings appear in words, especially common topics, depends on the type and subject of the text. We show how to use a thesaurus (a book of words with synonyms) made from natural text and a WordNet similarity tool to automatically find the main meanings of nouns. The main meanings we find are correct 64% of the time in a test called SENSEVAL-2 for English nouns. This is a very good outcome since our method doesn't need any manually labeled text, like SemCor. Moreover, we show that our method finds the right main meanings for words in two specific subject text collections. The first meaning rule, often used as a basic comparison for trained WSD systems, does better than many systems that do consider the words around. We limit the task of word meaning clarification to finding the main meaning in a specific subject area.

Long-Distance Dependency Resolution In Automatically Acquired Wide-Coverage PCFG-Based LFG Approximations This paper explains how we can automatically solve long-distance dependency (LDD) problems using a simple method for broad, reliable, probability-based Lexical-Functional Grammar (LFG) resources that we get from treebanks, which are large collections of sentences. We take LFG subcategorisation frames (the structure of verb phrases) and paths connecting LDD links from f-structures (functional structures) that are automatically created for the Penn-II treebank, and use them in a method to analyze new text. Unlike previous methods by Collins and Johnson, our method handles LDDs at the f-structure level (which are like organized maps of sentence structure) without using empty productions, traces, or matching indexes in the context-free grammar (CFG) trees. Our best automatically created grammars currently achieve an 80.97% score for parsing f-structures in a specific section of the Wall Street Journal part of the Penn-II treebank, and 80.24% against another standard called the PARC 700 Dependency Bank, performing as well or slightly better than the best hand-made grammars. Our method for adding f-structures uses information about sentence structure, categories, function tags, and traces from the Penn-II treebank for English. We automatically connect c-structures (constituent structures) to f-structures by giving grammatical roles to parts of the sentence based on their type, the type of their parent part, and their position related to the main word. Our parser works automatically to label input text with c-structure trees and f-structure links, achieving high accuracy and completeness.

A Study On Convolution Kernels For Shallow Statistic Parsing In this paper, we created and tested new methods called convolution kernels for automatically identifying predicate arguments, which are parts of sentences that give more information about the action or state described by the verb. These methods are especially good at handling complex data structures. Using Support Vector Machines (SVMs), which are a type of computer algorithm, with these new methods and another method that looks at simpler features, we can more accurately identify parts of sentences than current top methods. Also, tests using FrameNet data, which is another set of language information, showed that SVMs are useful for recognizing different roles in sentences, even if our new methods didn't make it better. We can identify these roles by looking at the smallest part of a sentence structure that includes the main verb and one of its related parts, known as the PAF structure. Our convolution kernel focuses on two things: the meaning of the sentence structures and a function that checks how similar they are.

Discovering Relations Among Named Entities From Large Corpora Discovering important connections hidden in documents would be very helpful not just for finding information but also for answering questions and creating summaries. Previous methods for finding these connections needed large collections of labeled documents, which took a lot of time and effort. We suggest a method that doesn't require labeled data (unsupervised method) to find connections in large collections of text. The main idea is to group pairs of named entities (like people, places, organizations) based on the similarity of the words found between them. Our tests using a year's worth of newspaper articles show that not only can we find connections between named entities accurately, but we can also automatically assign suitable labels to these connections. We introduce a fully unsupervised system called Open IE, which clusters pairs of entities that share the same connections. We use large text collections and a tool called an Extended Named Entity tagger to discover new types of connections and their participants.

Dependency Tree Kernels For Relation Extraction We build on earlier work to measure how similar the sentence structures (dependency trees) are. Using this method with a machine learning tool called Support Vector Machine, we identify and categorize connections between pieces of information (entities) in a set of news stories (ACE corpus). We test different features like Wordnet hypernyms (a tool to find word meanings), parts of speech (like nouns, verbs), and types of entities, and discover that our method improves accuracy by 20% compared to just counting word occurrences. To compare connections in two sentences, we suggest looking at the smaller tree structures formed by the relation parts, which means calculating similarities between the shared points (lowest common ancestors) in the sentence structure. We also apply this method to identify types of important names (Named Entity classes) in everyday language.

Collective Information Extraction With Relational Markov Networks Most information extraction (IE) systems treat each piece of information they find as separate and not connected to others. However, in many cases, thinking about how different pieces of information affect each other could make the system more accurate. Statistical methods using undirected graphical models, like conditional random fields (CRFs), have been shown to work well for creating accurate IE systems. We introduce a new IE method that uses Relational Markov Networks (an advanced form of CRFs), which can show any kind of connection between pieces of information. This allows for "collective information extraction" that takes advantage of the way possible pieces of information can influence each other. Tests on learning to find protein names from scientific text show the benefits of this method. We introduce AImed, a collection of data used to test systems that extract protein-protein interaction information.

Corpus-Based Induction Of Syntactic Structure: Models Of Dependency And Constituency We introduce a model that can learn dependency structures without needing labeled examples. We also explain how this dependency model can be combined with another model that looks at the order of words. This combined model works better than each model on its own, achieving the best results for learning both dependency and word order without labeled data. We show that the combined model is effective across different languages, using either how words are linked or patterns in the data. Our work includes a new Dependency Model with Valence (DMV), which is a way to learn these structures. We believe that having consistent ways to represent sentence structures is important for evaluating how well these models can learn without examples.

Improving IBM Word Alignment Model 1 We look at some easy ways to make IBM Model 1 better at matching words correctly. We show that we can make mistakes in word matching go down by about 30% by (1) giving more importance to matching with a word that doesn't exist (null word), (2) adjusting probabilities for words that don't appear often, and (3) using a simple guessing method to start or replace the usual EM training for setting up the model. One problem with IBM Model 1 is that each word in the sentence we are translating to can only come from one word in the original sentence. We also suggested adding extra empty words to the sentence we are translating to in IBM Model 1. Our method also helps with another problem by allowing translations between groups of words in both the question and the documents.

Multi-Criteria-Based Active Learning For Named Entity Recognition In this paper, we introduce a method that uses multiple factors to improve active learning and apply it successfully to named entity recognition (a task in computer science to identify names in text like people, places, or organizations). Active learning aims to reduce the amount of human effort needed for labeling data by choosing the most useful examples for labeling. To make sure the chosen examples are helpful, we look at different factors: how informative they are (how much new information they offer), how representative they are (how well they reflect the data set), and how diverse they are (how varied they are), and we come up with ways to measure these factors. We combine all these factors using two methods to choose examples, both of which are cheaper than using just one factor. The results in specific tests, MUC-6 and GENIA, show that we can cut labeling costs by at least 80% without reducing effectiveness. We focus on Active Learning for entity recognition using a technique called Support Vector Machines (a type of machine learning model). Our approach to ensure variety (diversity-motivated intra-stratum sampling) looks at a diverse group of similar examples (K-diverse neighbors) and aims to make the most out of all examples from a group (stratum).

Automatic Evaluation Of Machine Translation Quality Using Longest Common Subsequence And Skip-Bigram Statistics In this paper we explain two new automatic ways to measure how good a machine translation is. The first way uses the longest common subsequence, which compares a translation with several reference translations to find the longest matching word sequences in the same order. This method naturally considers how similar the overall sentence structure is. The second way is less strict and uses skip-bigram matching, which looks at any two words in the correct order but not necessarily next to each other. Skip-bigram statistics measure how often these word pairs occur in both the translation and the reference translations. Our results show that both methods match human opinions well in terms of how well the translation conveys the right meaning (adequacy) and how smoothly it reads (fluency). We tested these methods against many other evaluation measures, including NIST, WER (a measure of word errors), PER (another error measure), and different forms of ROUGE, BLEU, and GTM.

Statistical Machine Translation By Parsing In a typical sentence analyzer, the input is a sequence of words, and the rules apply to sequences of words. This paper looks into expanding typical parsing methods to handle inputs made of multiple sequences of words and/or apply rules to multiple sequences. These methods can discover matching patterns in texts that are translated side by side. It turns out that these expanded parsers can handle most tasks needed to create and use a translation system that understands sentence structure. When the rules of a parser have fewer parts than the input, we call it a synchronizer. We describe the machine translation problem as matching patterns in multiple texts using rules for multiple texts.

Identifying Agreement And Disagreement In Conversational Speech: Use Of Bayesian Networks To Model Pragmatic Dependencies We explain a method using statistics to understand when people agree or disagree during conversations. Our method first finds pairs of related speech parts, called adjacency pairs, by using a ranking system that looks at words, how long they are spoken, and the structure of the conversation. This system checks both what has already been said and what's coming next. Then, we decide if a part of the conversation shows agreement or disagreement by looking at these pairs and considering how past agreements or disagreements affect what's being said now. Our method is correct 86.9% of the time, which is 4.9% better than older methods. An adjacent pair is defined as two connected parts spoken in order by different people. We also improved identifying who is speaking by 8%. We analyze changes in speakers, how long parts of the conversation last, and the order of adjacency pairs using a dynamic Bayesian network, which is a type of statistical model. We believe that adding more features can make this method even better.

Combining Lexical Syntactic And Semantic Features With Maximum Entropy Models For Information Extraction Extracting semantic relationships between entities is challenging because there isn't enough labeled data, and mistakes happen when finding entities. We use Maximum Entropy models to mix different word, sentence structure, and meaning-based features from the text. Our system did well in the Automatic Content Extraction (ACE) evaluation. Here, we explain our general method and describe our ACE results. We use two kinds of features: ones based on sentence structure and ones based on words, like the path of the given pair of Named Entities (NEs) in the sentence structure tree and the sequence of words between NEs. We get better results when we use many different features together. We achieved an F-measure score of 52.8 for the 24 relation types in the ACE RDC 2003 dataset.

A High-Performance Semi-Supervised Learning Method For Text Chunking In machine learning, a key question is whether we can make a more accurate system for classifying data by using data that isn't labeled (this is called semi-supervised learning). Although many semi-supervised methods have been suggested, how well they work for natural language processing (NLP) tasks isn't always certain. This paper introduces a new semi-supervised method using a learning approach we call structural learning. The idea is to understand "what makes a good classifier" by studying thousands of automatically made extra classification problems using unlabeled data. By doing this, the common patterns that help predict outcomes in these problems can be identified and used to perform better on the main problem we want to solve. This method achieves better results than the previous best methods on CoNLL’00 syntactic chunking and CoNLL’03 named entity chunking (in English and German). We use a system that learns from multiple tasks within our semi-supervised algorithm to find features that are helpful for many related tasks. Our structural learning method applies a technique called alternating structural optimization (ASO). For reasons related to both computing speed and statistical analysis, we calculate a simplified version of the space where important predictors are found.

Probabilistic CFG With Latent Annotations This paper describes a method for creating parse trees, which are structures used in understanding sentences, called PCFG-LA. This method improves upon a basic version known as PCFG by adding hidden elements to the symbols used in the trees. Detailed rules for constructing these trees are automatically learned from a collection of example sentences by training the PCFG-LA model using a technique called the EM-algorithm. Since finding an exact solution with PCFG-LA is very complex and difficult (NP-hard), the paper discusses several simplified methods and compares them through experiments. In tests with a well-known set of sentences (Penn WSJ corpus), our model, which was trained automatically, performed quite well with an accuracy of 86.6% for sentences up to 40 words long, similar to another model that required a lot of manual adjustment. We use a special method (markovized grammar) to get better results when interpreting sentence structures but do not apply this method to the training examples. We also change the structure of example sentence data to make it simpler, using only two types of constructions (unary and binary).

Probabilistic Disambiguation Models For Wide-Coverage HPSG Parsing This paper discusses the creation of log-linear models, which are mathematical tools, to help choose the correct meaning in wide-coverage HPSG parsing (a type of language processing). Making these models requires a lot of computer power, especially with large grammar structures. By using methods to make this process less demanding, we trained the models using 20 sections from the Penn Treebank, a large collection of sentences used for training language models. Through a series of tests, we checked how well these methods worked and how effective the models were when analyzing real-world sentences. Our HPSG parser, a tool for analyzing language, can understand complex structures like who is doing what to whom in a sentence. We also present a mixed model where we combine the probabilities from our previous model with probabilities from super tagging (a method to quickly assign parts of speech to words) instead of using an initial probability model, to make it easier to estimate by removing unlikely word choices.

Online Large-Margin Training Of Dependency Parsers We present an effective training method for parsers that analyze sentence structure, using an online large-margin multi-class approach (a way to make decisions with clear boundaries) based on efficient techniques for dependency trees (a type of sentence structure analysis). The trained parsers perform well in understanding sentence structures for both English and Czech without needing special adjustments for each language. We have developed parsers that work with a time complexity of O(n3), which describes how the processing time increases with more data, without needing extra grammar rules. Instead of using whole words, we use the beginning parts of words as features for analysis. Our parser is as accurate as Charniak's model from 2000 and is ten times faster than Collins' model from 1997 and four times faster than Charniak's model.

Pseudo-Projective Dependency Parsing In order to fully use dependency-based syntactic parsing (a way to understand sentence structures), it's best to allow non-projective dependency structures (structures that can have crossings). We show how a data-driven deterministic dependency parser (a tool that builds sentence structures) that usually works with projective structures (no crossings) can be combined with graph transformation techniques (methods to change graphs) to create non-projective structures. Tests using data from the Prague Dependency Treebank (a database of sentence structures) show that this combined system can handle non-projective constructions well enough to significantly improve overall parsing accuracy. This results in the best reported performance for strong non-projective parsing of Czech. In our pseudo-projective approach, non-projective links (connections that cross) are moved up in the tree to make it projective (no crossings), and special labels are used to bring back the non-projective links when needed. We demonstrate how to remove the limit to projective dependency graphs by using graph transformation techniques to prepare training data and adjust the parser's results, known as pseudo-projective parsing. For dealing with non-projective relations, we suggest using a preprocessing step on a dependency parser, which involves lifting non-projective arcs (connections) to their head repeatedly until the tree becomes pseudo-projective. We point out that since there are fewer non-projective dependencies compared to projective ones, it's not efficient to perform non-projective parsing for every case.

Seeing Stars: Exploiting Class Relationships For Sentiment Categorization With Respect To Rating Scales We tackle the challenge of figuring out the exact rating a review gives, like one to five stars, instead of just saying if it's positive (thumbs up) or negative (thumbs down). This is more complex because there are different levels of similarity between ratings; for instance, three stars is more like four stars than one star. We start by seeing how well humans can do this task. Next, we use a special method that changes how a rating system works to make sure similar things get similar ratings. We demonstrate that this method can greatly improve results compared to other systems when we use a new way to measure similarity that fits this task. We developed a set of movie reviews with ratings to train a tool that identifies positive sentences in a full review.

Extracting Semantic Orientations Of Words Using Spin Model We propose a method for identifying if words have positive or negative meanings. We think of these meanings like tiny particles called electrons, and use a simplified calculation method to estimate how likely certain meanings are, instead of using a complex calculation. We also suggest a way to choose the best settings for this calculation based on how these particles behave. With just a few example words, our method can accurately identify word meanings in English tests, and the results are as good as the best ones reported. We create a network of words using not only word pairings but also other sources like a thesaurus. We decide the meaning of words (for Japanese) using a model that imagines electrons, which can point in two directions and influence each other until everything is stable.

Modeling Local Coherence: An Entity-Based Approach This paper looks at the problem of automatically checking how logically connected a text is. We introduce a new way to represent text structure that is based on Centering Theory, which can be automatically calculated from plain text. We treat the task of checking coherence as a ranking problem, meaning we try to order or prioritize different parts of the text, and demonstrate that our text structure method helps in effectively learning how to rank these parts. Our tests show that this new model achieves much better accuracy than the best existing model for checking coherence. To improve how we compute the sequence of parts in text, we split the matrix, which is a grid of data, into important and less important parts. We prove that our model, which focuses on entities (people, places, or things in the text), can correctly identify the original order of a text compared to a mixed-up version. We use the patterns and connections of these entities to make summaries clearer and more logical.

Machine Learning For Coreference Resolution: From Local Classification To Global Ranking In this paper, we treat coreference resolution (figuring out when different words refer to the same thing) as a problem of ranking different groups created by various systems. We suggest using special features of these groups to teach a model how to pick the best ones. Our method performs well compared to two top coreference systems when tested on three common data sets. We focus on improving the overall ranking of groups formed by simpler steps. We approach coreference resolution as a task of identifying and grouping: (1) train a model to see if two words refer to the same thing, and (2) use a method to group these words based on the pairwise results. Our method ranks different base models by how well they do on a separate test set and then uses the best one for final predictions. Most coreference systems that use learning can be broken down into four parts: the algorithm to train the model, how we create examples for training, the features used to describe these examples, and the method to organize the final decisions.

Coarse-To-Fine N-Best Parsing And MaxEnt Discriminative Reranking Discriminative reranking is a technique used to make statistical parsers, which help computers understand sentences, work better (Collins, 2000). A discriminative reranker needs a list of possible sentence interpretations. This paper explains a new and simple way to create sets of the 50 best sentence interpretations using a step-by-step generative parser (Charniak, 2000). This method produces better quality lists of interpretations than before. We used these interpretations as input for a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002), which picks the best interpretation for each sentence, achieving a score of 91.0% on sentences with 100 words or less. We use pruning, which means removing less important parts in a rough initial analysis to allow a detailed analysis. We show improved accuracy using detailed local features added to a basic word-based parser. To make the system work better and be more reliable, features are trimmed so that the chosen features must clearly identify the best interpretation, which has the highest score, from one that is not as good by at least five times.

A Hierarchical Phrase-Based Model For Statistical Machine Translation We present a statistical phrase-based translation model that uses hierarchical phrases - phrases that contain smaller phrases within them. The model is technically a synchronous context-free grammar, but it is learned from a pair of aligned texts (bitext) without using any grammar rules. This means it shifts to using formal grammar rules like syntax-based translation systems but without relying on language rules. In our tests, using a scoring method called BLEU, the hierarchical phrase-based model shows a 7.5% improvement compared to Pharaoh, which is a top-notch phrase-based system. We use the k-best parsing method in a grammar-based model to find the best feature weights that improve BLEU scores. We note that when combining two parts of a translation, we need to check how smooth they sound together by including the score of any language model features that affect the overall flow of the sentence. To better use grammar rules but still allow for translations that don't follow those rules, we keep a count for each proposed translation and add it whenever it fits perfectly with the grammar rules of the original text. Our hierarchical phrase models for machine translation build on from the traditional word-based models (Brown et al, 1993) and phrase-based models (Koehn et al, 2003a).

Dependency Treelet Translation: Syntactically Informed Phrasal SMT We explain a new way to use computers to translate languages by mixing grammar details from the original language with modern methods of translating phrases. This technique needs a tool that breaks down the sentence structure of the original language, splits words in the target language, and finds word connections without human help. We match sentences that say the same thing in two languages, apply the original language's grammar structure to the target language sentence, pull out small parts of sentences that match, and teach a model to arrange these parts correctly. We talk about a fast program that uses these small sentence parts together with usual translation models to create a strong method that mixes phrase translation with the broad understanding of language from grammar tools. Our system is trained using about 4.6 million pairs of sentences from different places like books in two languages, dictionaries, and online articles. We expand from paths to treelets, which are any connected parts of sentence structures, and suggest a model based on these pairs. We show how using small parts of the target language's grammar, called treelets, helps make phrase translation better.

Supervised And Unsupervised Learning For Sentence Compression In Statistics-Based Summarization - Step One: Sentence Compression, Knight and Marcu (Knight and Marcu, 2000) (K&M) present a noisy-channel model for sentence compression. The main challenge in using this method is the lack of data; Knight and Marcu use a collection of 1035 training sentences. More data is not easily available, so besides improving the original K&M noisy-channel model, we create models that don't need labeled data (unsupervised) and models that need only some labeled data (semi-supervised) for the task. Finally, we point out problems with modeling the task in this way. They suggest areas for future research. We estimate the rules of compression from a collection of unrelated sentences (e.g., the Penn Treebank) by considering grammar rules with matching expansions. We argue that the noisy-channel model is not a good fit for compression since it uses a source model trained on full sentences and therefore tends to view compressed sentences as less likely than full ones. We show that applying handmade rules for shortening sentences can improve both the content and the quality of language.

Contrastive Estimation: Training Log-Linear Models On Unlabeled Data Conditional random fields (Lafferty et al., 2001) are very good at tasks where you need to label parts of a sequence, like breaking down sentences into smaller parts (Sha and Pereira, 2003) and finding names of people or places in text (McCallum and Li, 2003). CRFs use a type of math model called log-linear, which allows adding any kind of information to improve performance. To train on data without labels, we need methods that don't rely on pre-labeled examples for these models; there aren't many such methods. We introduce a new method called contrastive estimation. This new method can be easily understood as using hidden clues to improve learning and works quickly. When used for tagging parts of speech (POS tagging) with a dictionary and unlabeled text, contrastive estimation works better than another method called EM, even when the dictionary isn't perfect, and improves by using more information. For estimating log-linear models, contrastive estimation uses a way to create nearby examples to help calculate its math model. We create negative clues by changing or removing a word in a sentence. We define broad categories of POS tags on a dataset called PTB. We start with all weights set to zero (a basic starting point) and choose the best model without using labeled examples by testing on new, unlabeled data. We try to find examples that look wrong but are similar to correct ones to make the tool that sorts them better. We show that it's possible to learn sentence structure without labeled examples, even when using very small sets of similar examples. Our contrastive estimation method is designed to handle any type of information.

Incorporating Non-Local Information Into Information Extraction Systems By Gibbs Sampling Most current statistical natural language processing models use only local features because it allows them to calculate results quickly and effectively, but this limits their ability to understand the overall structure that is common in language use. We demonstrate how to fix this issue using Gibbs sampling, a simple method that performs approximate calculations in complex probability models. By using a process called simulated annealing instead of Viterbi decoding in sequence models such as Hidden Markov Models (HMMs), Conditional Markov Models (CMMs), and Conditional Random Fields (CRFs), it is possible to include overall structure while keeping calculations manageable. We apply this technique to improve an existing CRF-based information extraction system with models that consider long-distance relationships, ensuring consistency in labels and extraction templates. This method leads to an error reduction of up to 9% compared to leading systems on two well-known information extraction tasks.

A Semantic Approach To IE Pattern Induction This paper introduces a new method for gathering Information Extraction patterns. The method assumes that useful patterns will have meanings similar to patterns already known to be important. Patterns are compared using a modified version of the standard model, where information from a knowledge system (ontology) is used to measure how similar the meanings are. Testing shows this method works well compared to an earlier method that focused on documents. We suggest a lightly guided method for selecting sentences that uses meaning similarity and a technique called bootstrapping to get IE patterns. We use subject-verb-object groups as the features.

Extracting Relations With Integrated Information Using Kernel Methods Entity relation detection is a way to find specific connections between pairs of things or names in a text. This paper explains a method to find these connections by using clues from different levels of sentence structure analysis, called kernel methods. It looks at information from three levels: breaking down text into words (tokenization), understanding sentence structure (sentence parsing), and analyzing word relationships (deep dependency analysis). Each of these information sources is represented by special mathematical tools called kernel functions. Then, these tools are combined to make sure mistakes at one level can be corrected by information from another level. We tested these methods on a task from 2004 called ACE relation detection, using a technique called Support Vector Machines (SVM), and showed that each level of sentence analysis gives helpful information. When tested with official data, our method scored very well. We also compared SVM with another method called KNN using different kernels. We created several combined tools called composite kernels to mix various features for finding relations and achieved a success rate of 70.4% on 7 types of relations in the ACE RDC 2004 dataset. We showed that adding nearby information to detailed sentence analysis improved information extraction results. We also looked at pairs of words between two mentions to give more order information about the words between them.

Exploring Various Knowledge In Relation Extraction Extracting meaningful connections between pieces of information (entities) is difficult. This paper looks at using different types of word meaning, sentence structure, and overall meaning in a method that identifies these connections using a tool called SVM (Support Vector Machine). Our study shows that using basic sentence parts (base phrase chunking) is very helpful for finding these connections and gives most of the improvement in understanding sentence structure, while using detailed sentence structure (full parsing) offers only a little extra help. This means that the most important information for finding connections is simple and can be found by looking at basic sentence parts. We also show how using tools like WordNet (a dictionary of word meanings) and Name Lists can help improve the process. Testing on the ACE dataset shows that using a variety of features effectively allows our system to perform better than the best systems known before on the 24 ACE connection types and performs much better than systems using complex methods by over 20% in accuracy on the 5 ACE connection types. We demonstrate that finding new useful characteristics to further improve accuracy is challenging. We use a collection of straightforward features such as words, types of entities, levels of mention, overlap, basic sentence parts, dependency tree, sentence structure tree, and overall meaning information.

Log-Linear Models For Word Alignment We present a system for matching words from different languages using log-linear models, which are mathematical formulas. All the information we use is turned into feature functions, which are like tools that use the original and translated sentences, and sometimes extra details. Log-linear models make it easy to add grammar-related information to the word matching process. In this paper, we use IBM Model 3, which is a method for matching words, along with parts of speech (POS) matching and coverage from a bilingual dictionary as tools. Our tests show that log-linear models work much better than IBM translation models. We introduce a log-linear model that mixes IBM Model 3, trained to match words both ways, with simple rules, resulting in a one-to-one word matching.

Stochastic Lexicalized Inversion Transduction Grammar For Alignment We introduce a type of Inversion Transduction Grammar (a method for comparing languages) that uses word-specific probabilities throughout the analysis process, with methods to cut down unnecessary parts for faster training. The alignment, or matching, results are better than a non-word-specific version for short sentences where complete Expectation-Maximization (EM, a statistical method) is possible, but cutting down parts negatively affects longer sentences. We describe a model where language rules are detailed with word pairs from English and another language, making the changes rely on specific word information. We suggest a method called Tic-tac-toe pruning, which relies on the basic probabilities of word pairs both within and outside a section of text. The Tic-tac-toe pruning method uses a step-by-step approach to calculate scores for a section of text in a time-efficient manner (proportional to four times the length of the text, noted as O(n4)).

Reading Level Assessment Using Support Vector Machines And Statistical Language Models Reading skill is an essential part of being good at a language. However, it's hard for teachers to find suitable texts for learners of foreign or second languages. This problem can be solved using technology that helps understand language to check the reading level. Current ways to measure reading levels are not very effective for this job, but past research and our own small tests show that using statistical language models (ways to predict text patterns) can help. In this paper, we also use support vector machines (a type of computer algorithm) to combine different features from traditional reading level methods, statistical language models, and other language tools to create a better way to evaluate reading levels. We create a system using support vector machines that combines a text classifier based on trigram language models (which looks at three-word combinations for each difficulty level), some features like average tree height from parsing (breaking down sentences into parts), and variables usually used to determine readability. We use sentence structure features, such as the height of parse trees (a visual breakdown of sentence structure) or the number of passive sentences, to estimate reading grade levels.

Clause Restructuring For Statistical Machine Translation We explain a way to include grammar information in systems that automatically translate languages. The first step is to break down the sentence from the language being translated. The second step is to make changes to the sentence structure, rearranging the words to match the word order of the target language better than the original. This rearranging is done before both learning and translating phases in a system that translates language phrases. We did tests translating German to English, showing an improvement in the translation quality from a 25.2% Bleu score (a quality measure) for a basic system to 26.8% with rearranging, a noticeable improvement. We use a sign test to check if the score improvement is meaningful. We point out it's unsure if the conditions needed for another method, called bootstrap resampling, apply to Bleu scores, so we suggest using the sign test. We use six specific rules to rearrange verbs, subjects, small words, and negative words.

Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars Syntax-based statistical machine translation (MT) uses statistical models to understand and translate structured language data. In this paper, we introduce a system that uses these statistical methods with a specific type of grammar called probabilistic synchronous dependency insertion grammar. This grammar is a version that works with dependency trees, which show how words in a sentence relate to each other. We explain how we create this grammar using collections of translated texts known as parallel corpora. Next, we describe our method for translation using a visual model that acts like a tree-to-tree converter, changing one language structure into another using probability. We also developed a fast method to decode, or interpret, this model. We tested our translation system with software that automatically checks translation quality, called NIST and Bleu. The results showed our system translates faster and better than a basic system using older methods from IBM. Our method needs some assumptions about how similar the two languages are, either in vocabulary or structure. We propose a translation model based on Synchronous Dependency Insertion Grammar (SDIG), which can handle some differences between languages but still needs the structure of both the original and translated sentences.

Arabic Tokenization Part-Of-Speech Tagging And Morphological Disambiguation In One Fell Swoop We present a method that uses a tool to break down and label Arabic words with their grammatical roles, all in one step. We train computer programs to recognize specific word features and decide which analysis to choose from the tool's results. Our accuracy for all tasks is over 90%. To select the best results from the Buckwalter morphological analyzer (BAMA), we count how many times each predicted feature appears in the possible analyses.

Semantic Role Labeling Using Different Syntactic Views Semantic role labeling is the task of marking parts of a sentence with labels that show their meaning and role. In this paper, we introduce a top-level system for semantic role labeling using Support Vector Machine classifiers. We enhance this system by: i) adding new details, including information from dependency parses (which show how words in a sentence are related), ii) choosing and adjusting the details carefully, and iii) merging results from different methods of analyzing sentence structure. An error check of the basic system showed that about half of the mistakes in identifying roles came from errors in sentence structure analysis. To fix this, we combined different sentence structure analyses from Minipar and a simplified method with our original system based on Charniak's analysis. All these methods led to better results. We mix systems based on different ways to analyze sentence structure, like phrase-structure, dependency, and shallow parsing. We use features related to parts of the sentence and their roles to improve performance. We combine results from multiple analyzers to gather accurate sentence structure information, which is used in a machine learning test to assign meaning roles.

Joint Learning Improves Semantic Role Labeling Despite much recent progress in accurately identifying and labeling the roles words play in sentences (semantic role labeling), previous methods mostly used separate tools that worked independently, sometimes combining these with other models that sequence labels using a method called Viterbi decoding. This approach is very different from the linguistic idea that the main parts of a sentence (core argument frame) are connected and depend on each other. We demonstrate how to create a combined model that takes into account these connections, using new features that capture these interactions in advanced log-linear models (a type of mathematical model). This system reduces errors by 22% for all arguments and 32% for core arguments compared to a top independent classifier when using perfect sentence structures from PropBank (a resource for linguistic analysis). We present a combined approach for semantic role labeling (SRL) and show that a model assessing the complete structure of a sentence can significantly reduce errors compared to using separate classifiers for each part of the sentence. To make training efficient, we use a method called decomposition, which means we train the models using only a part of the training examples, specifically those that have a case marker (a word indicating the grammatical function of a noun).

Paraphrasing With Bilingual Parallel Corpora Previous work has used collections of similar texts in the same language to find and create paraphrases, which are different ways of saying the same thing. We show that this can be done using collections of texts in two different languages, a more common resource. By using methods from computer programs that translate languages, we show how you can find paraphrases in one language by using a phrase in another language as a middle step. We define a way to measure how likely something is a paraphrase, which helps organize paraphrases found in two-language text collections using how likely they are to translate, and show how to improve this by considering surrounding words. We test our methods for finding and organizing paraphrases using a set of carefully matched words, and compare the quality with paraphrases found using automatic matching. We define how likely two phrases are paraphrases based on their chance of being translated through all possible middle phrases.

Randomized Algorithms And NLP: Using Locality Sensitive Hash Functions For High Speed Noun Clustering In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data. We apply these algorithms to generate noun similarity lists from 70 million pages. We reduce the running time from quadratic (which grows quickly as data increases) to practically linear (grows slowly with more data) in the number of elements to be computed. We show that by using the LSH (Locality Sensitive Hashing, a method to find similar items) nearest neighbors calculation can be done in O(nd) time (a way to describe how fast an algorithm runs). Our method can produce over 70% accuracy in extracting synonyms (words with similar meanings).

Using Emoticons To Reduce Dependency In Machine Learning Techniques For Sentiment Classification Sentiment Classification tries to figure out if a text shows a positive or negative feeling from the writer towards a topic. Traditional machine learning methods have been used for this and have worked fairly well, but they only work well if the training data (examples used to teach the computer) and test data (new examples to check the computer) are similar in topic. This paper shows that having similar data in terms of subject area and timing is also important. It shares early experiments using training data with emoticons (like smiley faces or frowns), which might not depend on subject, topic, or time. In sentiment analysis research, we use emoticons in online group posts to find examples useful for training emotion detectors. We notice that when people use emoticons in messages, they are actually adding emotional signals to their text.

Multi-Engine Machine Translation Guided By Explicit Word Matching We describe a new way to combine the outputs of different Machine Translation (MT) systems working on the same text. The aim is to create a combined translation that is better than all the original translations. Our method treats each MT system as a separate unit and doesn't need them to work together. A decoding process uses specific word matches, along with confidence levels for each system and a three-word language model, to evaluate and rank different sentence options that mix words from the original systems. The best sentence option is chosen as the final translation. Tests with several Arabic-to-English systems of similar quality show a big improvement in translation quality. We suggest a method based on rules that allows flexible word matching between the different sentence options.

Minimum Cut Model For Spoken Lecture Segmentation We are working on dividing lectures into sections without any prior training. We treat this task like solving a puzzle by cutting a "graph" into parts in the best way possible. Our method looks at the big picture instead of just small parts and considers how well everything sticks together over long stretches. Our findings show that this broader view helps us cut the lecture more accurately, even when there are mistakes in the speech-to-text process. We focus on finding the best way to cut based on how similar the sentences are to each other, using a method similar to cosine similarity (a way to measure how alike two things are). Our goal is to identify where new topics start in the written version of the lectures. We collected a set of lectures that were divided into sections by four different people, each doing it in their own detailed way.

Bootstrapping Path-Based Pronoun Resolution We present a method for figuring out what pronouns refer to using sentence structure paths. Through a simple learning process, we determine how likely it is that a pronoun refers to a specific noun based on the path between them in a sentence structure diagram. This path information helps us solve difficult pronoun references and also effectively deals with traditional sentence structure rules for pronouns. Highly connected paths also allow us to gather detailed information about gender and number. We mix statistical knowledge with well-known features in a machine learning tool for resolving pronouns. We see major improvements in performance across several data sets. Using an automatically processed text collection, we take from each sentence structure a path showing relationships, represented as a series of points and labels linking a pronoun and a possible noun it refers to, and gather statistical data from these paths to determine how likely they are linked. We show that learning about gender is the most important part of their pronoun resolution systems. We reach top-level performance in identifying noun gender, and we share the database of the noun genders we found online. We create a statistical model from paths that include the base form of middle words, but replace the end points with noun, pronoun, or a special pronoun type for nouns, pronouns, and reflexive pronouns, respectively.

Discriminative Word Alignment With Conditional Random Fields In this paper, we introduce a new way to match words from sentences in two different languages. We use a Conditional Random Field (CRF), which is a special method trained with a small set of examples. The CRF looks at both the original and translated texts, allowing it to use different and overlapping characteristics from these texts. Additionally, the CRF is quick and effective in learning and finding the best solutions. We use this model to match words in both French-English and Romanian-English language pairs. We show that many useful characteristics can easily be added to the CRF, and that even with just a few hundred training sentences, our model is better than the current best methods, with error rates of 5.29 and 25.8 for the two tasks, respectively.

Named Entity Transliteration With Comparable Corpora In this paper, we explore how to change Chinese names into English using collections of texts in both languages that are on similar topics but are not direct translations of each other. We introduce two different methods for changing names: one uses sound-based transliteration and the other looks at how often the name pairs appear together over time. Both methods work well, but combining them gives even better results. We then suggest a new way to improve these results further by using a method that spreads scores based on how often name pairs appear together in related documents. This new method improves the results even more. We compare names from similar and current English and Chinese texts by using a learning program that checks how the sounds of the names match and also considers how often the names appear over time.

Extracting Parallel Sub-Sentential Fragments From Non-Parallel Corpora We introduce a new way to find matching parts of sentences in texts that are similar but not exactly the same in two different languages. By looking at sentence pairs that might be similar using a method inspired by signal processing, we figure out which parts of the original sentence match parts in the translated sentence and which do not. This allows us to gather useful data for training translation machines, even from texts that don't have any matching sentences. We test the quality of this data by showing it makes a modern translation system work better. We start by using a tool called GIZA++ (with a specific method called grow-diag-final-and) to match words between the original and translated texts, then measure how strongly these words are connected. We first find possible matching sentences from the similar texts and then accurately pick out matching parts of sentences using a special bilingual dictionary. We use standard methods to search for information along with simple word-based translation to find information across languages (CLIR), and we identify phrases from the search results using a clear bilingual dictionary and a method to smooth out the data. We identify phrases by combining clean word matching dictionaries for initial signals with strategies to refine the matches for final extraction of sentence parts.

Meaningful Clustering Of Senses Helps Boost Word Sense Disambiguation Performance Fine-grained sense distinctions, which means very detailed differences between word meanings, are one of the main challenges in successfully telling apart different meanings of a word. In this paper, we present a method for making the WordNet sense inventory, a collection of word meanings, less detailed by connecting it to a manually created dictionary that organizes meanings, specifically the Oxford Dictionary of English. We check how good this connection is and how well the groupings work, and we test how well systems that handle less detailed word meanings perform in the Senseval-3 English all-words task. In our task, we first grouped the sense inventory into clusters, or groups, semi-automatically, with each group representing a set of similar meanings. We present an automatic way to connect different collections of word meanings; here, we use similarities in word explanations and the organized relationships between the two collections of meanings to connect WordNet meanings with the broader categories in the Oxford English Dictionary. We believe that automatically creating new connections is hard because of word ambiguities, which means words can have multiple meanings, different levels of detail in meanings, or ideas that are specific to a language.

Espresso: Leveraging Generic Patterns For Automatically Harvesting Semantic Relations In this paper, we introduce Espresso, a computer program that is not heavily supervised, is versatile, and accurately finds connections between meanings. Our main contributions are: i) a way to use broad patterns by removing wrong examples with help from the Internet; and ii) a careful method to judge how trustworthy patterns and examples are, which helps in filtering. We compare Espresso with other top systems on various texts of different sizes and types, to find different general and specific connections. Tests show that using broad patterns greatly improves the system's ability to find connections without much loss in accuracy. In the pattern creation step, our system calculates a trust score for each possible pattern using a method called weighted pointwise mutual information, PMI, which looks at how often the pattern appears with all examples found so far. We create specific trustworthy patterns step-by-step for finding connections between entities. Our program, which doesn't need much supervision, starts with a single set that mixes different types of examples, like leader-panel and oxygen-water, which relate to member-of and part-of relationships as described by Keet and Artale in 2008.

Correcting ESL Errors Using Phrasal SMT Techniques This paper presents an initial study on using phrasal Statistical Machine Translation (SMT) methods to find and fix writing mistakes made by people learning English as a Second Language (ESL). By using examples of errors with mass nouns (words like "information" that don't usually have a plural form) found in the Chinese Learner Error Corpus (CLEC), we created a special training set. We show that using the SMT approach can find mistakes that common proofreading tools for native speakers often miss. Our system was able to fix 61.81% of errors in real-life examples of mass noun mistakes found online, indicating that collecting examples of before and after edits of ESL writings can help develop SMT-based writing tools. These tools can fix many complicated grammar and word choice problems in ESL writing. We use phrasal SMT methods to correct ESL writing errors and show that this approach, which needs a lot of data, is very promising. However, we also note that the SMT approach depends on having a large amount of training data.

Efficient Unsupervised Discovery Of Word Categories Using Symmetric Patterns And High Frequency Words We introduce a new way to find groups of words that share similar meanings. We use common patterns of frequently used words and important words to find potential patterns. Then, we identify balanced patterns using methods based on graphs, and word groups are formed based on connected sets in these graphs. Our method is the first to use patterns without needing any pre-marked text or starting words. We test our method on large text collections in two languages, using both human feedback and checks against WordNet, a language database. Our method, which doesn't need any pre-labeled text, works better than past methods that used labeled texts and is much faster for large text collections. We demonstrate that words that often appear together in balanced patterns usually belong to the same group, meaning they have similar meanings.

Reranking And Self-Training For Parser Adaptation Statistical parsers, which are computer programs that analyze sentence structure, have gotten much better over the last 10 years when they are trained and tested using the Penn Wall Street Journal (WSJ) language data set. This improvement is mostly because they are learning from more and more details in the WSJ data. However, there is a worry that these programs might work too well only on this specific data and not as well on other types of writing. These concerns are valid. The commonly used "Charniak parser" scores 89.7% accuracy in understanding sentences in the WSJ test but drops to 82.9% in a different set of sentences from the Brown data set. This paper aims to ease these worries. It shows that by using a technique called reranking, which was explained by Charniak and Johnson in 2005, the parser's performance on the Brown data improves to 85.2%. Additionally, using another method called self-training, mentioned in a 2006 study by McClosky and others, raises the accuracy to 87.8%, which means 28% fewer mistakes, and this is done without using labeled Brown data, which are pre-identified examples. We used self-training, a method that learns from unlabeled or raw data, to improve sentence understanding and saw impressive results when adapting this technique to new types of writing.

Learning Accurate Compact And Interpretable Tree Annotation We introduce an automatic method for marking trees, where basic parts of a structure (nonterminal symbols) are repeatedly split into smaller parts or combined to improve how well the method can predict data from a training set of tree structures. We start with a simple set of rules (X-bar grammar) and learn new rules using smaller parts of the original structure symbols. Unlike past methods, we can split different parts of the structure to varying levels, depending on how complicated the data is. Our learned rules automatically discover the same kinds of language differences that were found in previous work done by hand. However, our rules are more straightforward and much more accurate than past automatic methods. Even though the rules are simple, our best set of rules scores 90.2% on a well-known language test (Penn Treebank), which is better than methods that use full word details. We use a step-by-step improvement method called hierarchical EM training. We show that in the area of analyzing sentence structures using statistical rules (probabilistic context-free grammars), our automatically improved rules can do better than advanced methods that rely on a lot of hand-crafted structure. We introduce a method called split-merge-smooth estimation, which helps improve these rules.

Maximum Entropy Based Phrase Reordering Model For Statistical Machine Translation We introduce a new way to rearrange phrases for a type of language translation tool called statistical machine translation (SMT). It uses a method called maximum entropy (MaxEnt) to predict how parts of sentences (phrase pairs) should be reordered. This model allows rearranging phrases based on their content and structure using features that it learns automatically from real bilingual texts. We explain a method to find all the ways parts of sentences can be reordered using bilingual data. In our tests with translating Chinese to English, this MaxEnt-based method greatly improved the translation quality, shown by better BLEU scores in specific translation tasks. We enhance how phrases are reordered in SMT by looking at words that appear near each other. We suggest a model for predicting how parts of sentences will change order when creating new sentence structures, based on a grammar system called bracketing transduction grammar (BTG). In our MaxEnt-based model, called MEBTG, three rules help translate parts of sentences: the lexical rule, the straight rule, and the inverted rule.

Distortion Models For Statistical Machine Translation In this paper, we argue that n-gram language models (a type of model that predicts the next word in a sequence) are not enough to manage the rearrangement of words needed for Machine Translation. We propose a new distortion model (a model to improve word order) that can be used with current phrase-based SMT (Statistical Machine Translation) systems to fix these limitations of n-gram language models. We present real-world results in translating Arabic to English that show clear improvements when our model is used. We also suggest a new way to measure how similar or different the word order is between any two languages, using word alignments (connections between words in both languages). Our lexicalized distortion model predicts the jump from the last translated word to the next one, with a category for each possible jump distance. We find that simple, rule-based word rearrangement is too complex to improve and cannot be corrected by the translation system.

Annealing Structural Bias In Multilingual Weighted Grammar Induction We first show how focusing on nearby connections (structural locality bias) can enhance the accuracy of advanced models for understanding sentence structures, which are trained using a method called EM (Expectation-Maximization) from examples without labels (Klein and Manning, 2004). Next, by gradually adjusting the control of this bias (annealing the free parameter), we achieve even better results. We then describe another type of structural bias, which involves incomplete structures over parts of sentences ("broken" hypotheses), and show it also leads to improvements. We connect this approach to a method called contrastive estimation (Smith and Eisner, 2005a), apply it to understanding sentence structures in six languages, and show that our new method increases accuracy by 1-17% more than contrastive estimation and 8-30% more than EM, achieving what we believe to be the best results so far. Our method, called structural annealing, is a general technique useful for discovering hidden patterns. We limit the predicted connections between words in a sentence structure task to prevent distant word connections. We suggest structural annealing (SA), which strongly prefers nearby word connections at the start and then loosens this preference over time. Our annealing approach gradually changes the hidden outcomes during the E-step to help learning in the M-step.

Tree-To-String Alignment Template For Statistical Machine Translation We introduce a new translation model that uses a method called tree-to-string alignment template (TAT). This method shows how a structured source language (parse tree) aligns with a target language (string of words). A TAT can create both direct words (terminals) and placeholders (non-terminals), and it can rearrange words in simple or complex ways. The model is based on language structure (syntax) because TATs are taken automatically from texts that have been aligned by words and have a structured format on the source side. To translate a sentence, we first use a tool to create a structured format (parse tree) from the source language, and then use TATs to change this structure into a string of words in the target language. Our tests show that the TAT-based model is much better than Pharaoh, which is a top-performing tool for phrase-based translation. We combine different types of translation rules within one process to improve results. We also incorporate phrases from non-structured phrase-based statistical machine translation (PBSMT) into our tree-to-string translation system.

An Unsupervised Morpheme-Based HMM For Hebrew Morphological Disambiguation Morphological disambiguation is the process of picking the correct set of word features for each word in a text. When a word is unclear (it can mean several things), a procedure based on surrounding words must be used to clarify it. This paper focuses on Hebrew language disambiguation, which combines word parts in different ways. We introduce a model that doesn't rely on pre-labeled data - we only use a tool that breaks words into parts - to tackle the issue of limited data because of Hebrew's complex word structure. We describe a way to encode text for languages with complex word parts, using knowledge of Hebrew's word-building rules to help clarify meaning. We modify HMM algorithms to learn and search this text format, allowing for identifying and labeling word parts at the same time. Large evaluations show this learning method enhances clarity for complicated word label sets. Our method can be used for other languages with complex word structures. For each word, we provide not only the part of speech but also full features like gender, number, person, form, tense, and word parts' details. We offer a modified version of the Baum-Welch algorithm to deal with unclear word part separation.

Contextual Dependencies In Unsupervised Word Segmentation Developing better ways to break down continuous text into words is important for improving how we process Asian languages, and it can also help us understand how people learn to break down spoken language into words. We suggest two new methods based on Bayesian statistics for word segmentation, which assume two types of word relationships: unigram (single word) and bigram (pair of words) models. The bigram model performs much better than the unigram model (and previous models based on probability), showing how important these word relationships are for breaking text into words. We also show that earlier models based on probability depend heavily on not-so-great search methods. We begin with a random breakdown of the text, and in each step, we adjust this breakdown by making small changes at specific points. We use hierarchical Dirichlet processes (HDP), which is a statistical method, to create models that consider word context.

A Discriminative Global Training Algorithm For Statistical MT This paper introduces a new way (training algorithm) to improve a model that translates block sequences (pieces of text) and scores them using a simple formula. The main part is a new method to improve the overall scoring function used by SMT (Statistical Machine Translation) software. Unlike earlier methods, we don't use the chances (probabilities) of translation, language, or changes in order. This makes our method, which uses less specific information, simpler and easier to expand than older methods. Also, the new method treats the decoder (the part that translates) as a black-box, meaning it can work with any translation method. We tested this training on a common task of translating Arabic to English. We use a special technique (BLEU oracle decoder) to train a model that changes the order of words. We use a perceptron style method (a simple algorithm) to train many features. We find high BLEU scores (a measure of translation quality) by using a regular decoder to improve sentence-by-sentence BLEU-4 scores with a simple way to change word order. We show a way to directly improve the overall scoring function used by a phrase-based decoder to make translations more accurate.

Machine Learning Of Temporal Relations This paper looks at using machine learning to figure out the order and timing of events in written language. To handle the problem of not having enough data, we used a technique called temporal reasoning to create more training examples, which helped us reach a 93% accuracy in predicting event links using a Maximum Entropy classifier on data marked by humans. This method did well compared to other complex techniques based on human intuition. While traditional machine learning tries to improve accuracy by selecting important features, we added a temporal reasoning element to significantly increase the training data. We used the connections created by closure (a method of linking data) to increase the training data for a tlink classifier (a tool that predicts temporal links).

Semi-Supervised Training For Statistical Word Alignment We introduce a semi-supervised method for training in statistical machine translation. This method switches between a traditional step called Expectation Maximization, used on a large set of training data, and a decision-making step targeting better word matching on a small set of data that has been manually matched. We demonstrate that our method not only improves word matching but also results in better quality machine translation. If data aligned by humans is available, the EMD algorithm provides better starting alignments than GIZA++, leading to improved machine translation performance. We mix a model that creates word alignments with a decision-making model trained on a few sentences that are hand-matched. We treat the alignment task like a search in a decision-making space with features from IBM alignment models. We suggest an EMD algorithm for word matching that includes a decision-making step in every round of the traditional Expectation-Maximization process used in IBM models.

Semantic Taxonomy Induction From Heterogenous Evidence We propose a new method for creating semantic taxonomies, which are like family trees for words that show how they are related in meaning. Previous methods focused on finding single connections between words using patterns found in text, either by hand or automatically. Our method is different because it uses information from several sources about different kinds of word relationships to improve the whole structure. It uses knowledge about similar words to figure out what broader categories they belong to, and vice versa. We test our method on the task of finding specific types of nouns, combining predictions about word categories with existing knowledge from a tool called WordNet 2.1. We successfully add 10,000 new word groups to WordNet 2.1 with high accuracy, making 70% fewer mistakes compared to older methods. Our new method improves the accuracy by 23% over WordNet 2.1 when tested on a separate set of word category examples. We create many patterns to find broader categories for nouns and use these to suggest where new words might fit in WordNet. We use known word pairs to teach a computer system to recognize patterns in language. We add new words by choosing those that best fit the patterns we have, based on the evidence available. We use specific sentence structures as clues for teaching the computer to recognize when words are similar or belong to a larger category, using examples from WordNet.

Weakly Supervised Named Entity Transliteration And Discovery From Multilingual Comparable Corpora Named Entity recognition (NER) is a key part of many tasks in understanding human languages using computers. Current methods often use machine learning, which needs labeled data to learn. But many languages don't have these resources. This paper introduces a nearly unsupervised learning method to automatically find Named Entities (NEs) in languages without resources, using bilingual text collections that are loosely matched over time with a language that has more resources. NEs tend to appear around the same times in these texts, and sometimes parts of multi-word NEs are converted from one script to another. We created a method that takes advantage of these patterns repeatedly. This method uses a new way to measure time similarities and a method to identify script conversion without extra resources. Starting with a few known script conversion examples, our method finds multi-word NEs and uses a dictionary (if available) to handle NEs that are fully or partly translated. We test the method using English and Russian texts, showing it finds many NEs in Russian. We look at using a model that ranks to find name conversions across similar texts in different languages. Our features are pairs of short word sequences (n-grams) from both languages. We discover that transferring NER tags is simpler than transferring other labels like parts of speech (POS) and some other linguistic categories (BPC). We introduce a Russian dataset made of news articles matched over time.

A Composite Kernel To Extract Relations Between Entities With Both Flat And Structured Features This paper introduces a new method called a composite kernel to find connections between entities. The composite kernel is made up of two parts: one part focuses on features related to the entities themselves, and the other part uses a model called a convolution parse tree kernel to understand the grammar structure of examples. The goal of this method is to make use of the advantages of kernel methods to explore different kinds of information for finding relationships. Our research shows that this composite kernel can effectively capture both simple and complex features without needing a lot of extra work to create features. It can also easily be expanded to include more features. Tests on the ACE dataset show that our method performs better than the best methods reported before and does much better than two earlier models using dependency trees for finding relationships. We use a method called a convolution tree kernel (CTK) to look into different structured information for finding relationships and discover that using the Shortest Path enclosed Tree (SPT) achieves an accuracy rate, or F-measure, of 67.7 on the 7 types of relationships in the ACE RDC 2004 dataset.

An All-Subtrees Approach To Unsupervised Parsing We look into expanding the all-subtrees "DOP" method for unsupervised parsing, which means analyzing the structure of sentences without pre-labeled data. Unsupervised DOP models try all possible ways to break down sentences into parts and then use a large random selection of these parts to figure out the most likely sentence structures. We will examine both a method that uses relative frequencies and a method based on maximum likelihood, which is a statistical approach that ensures reliability. We show top-level results using data in English (WSJ), German (NEGRA), and Chinese (CTB). As far as we know, this is the first study testing the maximum likelihood method for DOP on the Wall Street Journal data, revealing that this unsupervised method performs better than a popular supervised method, which uses pre-labeled data (a treebank PCFG). We find that using a grammar structure that isn't broken into smaller parts scores an average of 72.3% accuracy on WSJ sentences with up to 40 words, while the broken-down version scores only 64.6% accuracy.

Methods For Using Textual Entailment In Open-Domain Question Answering Work on understanding the meaning of questions has suggested that the link between a question and its answer(s) can be explained using logical entailment, which means if one statement is true, another related statement must also be true. In this paper, we show how computer systems that identify textual entailment, which is understanding if one piece of text logically follows from another, can improve the accuracy of current open-domain automatic question answering (Q/A) systems. In our tests, we demonstrate that using textual entailment information to either sort or filter answers given by a Q/A system can improve accuracy by up to 20% in total. We used a Textual Entailment (TE) component to reorder possible answers returned by a search step for the task of Question Answering.

Using String-Kernels For Learning Semantic Parsers We introduce a new method for converting regular sentences into their formal or structured meanings using special tools called string-kernel-based classifiers. Our system learns these tools for each rule in the formal language structure. For new sentences, it figures out the most likely structured meaning using these string tools. Our tests on two real-world examples show this method works well compared to other systems and handles errors well. We use a process that looks at parts of words to find how similar two parts are. Our model, KRISP, builds meaning structures from regular sentences in a layered way. KRISP (Kernel-based Robust Interpretation for Semantic Parsing) (Kate and Mooney, 2006) is a system that learns by example for understanding sentence meanings, using pairs of sentences and their meanings as learning material.

Scalable Inference And Training Of Context-Rich Syntactic Translation Models Statistical Machine Translation (MT) has improved a lot recently, but current translation models struggle with changing word order and making the translated language sound natural. Syntactic (structure-based) methods try to solve these issues. In this paper, we use a method for getting multi-level syntactic translation rules from aligned tree-string pairs (Galley et al., 2004) and introduce two main improvements: first, instead of just finding one simple way to explain a sentence pair, we create many ways that use contextually richer rules and consider different meanings of unaligned words. Second, we suggest ways to estimate probabilities and a training process to prioritize these rules. We compare different methods using real examples, show that our estimates using multiple sentence interpretations prefer word re-orderings that make more sense, and prove that our larger rules improve translation quality by 3.63 BLEU points compared to minimal rules. We use xRS formalism to apply translation rules with multiple levels of target tree annotations and source phrases that aren't continuous. Our method combines two or more basic GHKM or SPMT rules with shared elements to create larger rules.

Empirical Lower Bounds On The Complexity Of Translational Equivalence This paper describes a study of the patterns of how translations match up between two languages, found in different sets of bilingual texts. The study found that these matching patterns were more complicated than what was previously thought. These findings help explain why certain language rules haven't improved statistical translation methods, like phrase-based models that use limited state changes, and models that convert between different tree structures. The paper also provides evidence that certain grammar systems can't handle some translation matches, even in simple bilingual texts of languages that have similar sentence structures and strict word order. Our method checks how complicated it is to match words by counting the gaps needed for a special parser that can handle breaks in sentence order. Translational equivalence is when language expressions mean the same thing. We argue that allowing for breaks in sentence structure (more advanced than basic Synchronous Context-Free Grammars) is necessary to match human-annotated word pairs, while keeping rules simple with a limit of two parts.

A Hierarchical Bayesian Language Model Based On Pitman-Yor Processes We suggest a new structured approach using Bayesian n-gram models to understand natural languages better. Our model uses an advanced version of the usual method called Dirichlet distributions, specifically the Pitman-Yor processes, which create patterns that look more like how words naturally appear in languages. We demonstrate that a simplified version of our hierarchical Pitman-Yor language model matches the precise setup of interpolated Kneser-Ney, known as one of the best ways to smooth out n-gram language models. Tests show that our model achieves better results in measuring unpredictability (cross entropy) than the interpolated Kneser-Ney and is similar to the modified Kneser-Ney. We offer a Bayesian (a method to make predictions using probabilities) explanation for smoothing techniques like Kneser-Ney and Witten-Bell back-off schemes. Nonparametric Bayesian modeling can create starting points (priors) that are particularly helpful for tasks in natural language processing (NLP). While the Dirichlet process is just a simpler form of the Pitman-Yor process with a specific setting (d=0), we find that adjusting the discount parameter helps in better modeling of the long-tailed distributions, which are common in natural language.

Word Sense And Subjectivity Subjectivity (personal feelings or opinions) and meaning are both important properties of language. This paper looks at how they interact and provides real-world evidence for the ideas that (1) subjectivity can be linked to different meanings of a word, and (2) figuring out a word's meaning can benefit from knowing if it's subjective or not. We look at the difference between objective (fact-based) and subjective (opinion-based) meanings of a word, and their effects in understanding opinions and emotions in text. We show that knowing if a word sense is subjective can help improve the process of figuring out its correct meaning. We demonstrate that even words with clear subjective meanings can have factual meanings. We show that knowing if a word is subjective or objective can help when a word has both types of meanings. We study how people label 354 word meanings with positive or negative feelings and find that people mostly agree on these labels. We define subjective expressions as words and phrases used to express mental and emotional states, like guesses, judgments, feelings, and beliefs.

A Phrase-Based Statistical Model For SMS Text Normalization Short Messaging Service (SMS) texts act very differently from regular written texts and have some unique features. To convert SMS texts, traditional methods try to directly address these differences in Machine Translation (MT). However, these methods face a customization problem because it takes a lot of work to change the language model of the current translation system to handle the SMS style. We offer a different solution to fix these differences by adjusting SMS texts before using MT. In this paper, we treat the task of SMS normalization as a translation problem from SMS language to the English language, and we suggest using a phrase-based statistical MT model for this task. Testing with 5-fold cross-validation (a method to check accuracy) on a collection of 5000 SMS sentences shows our method can achieve a BLEU score of 0.80702, compared to the baseline score of 0.6958. Another test of translating SMS texts from English to Chinese on a different SMS text collection shows that using SMS normalization before MT can greatly improve SMS translation performance from a BLEU score of 0.1926 to 0.3770. We also use Phrase-based SMT (Statistical Machine Translation) techniques at the character level. We use a phrase-based statistical machine translation model by dividing sentences into their k most likely phrases.

Evaluating The Accuracy Of An Unlexicalized Statistical Parser On The PARC DepBank We check how correct an unlexicalized statistical parser is. It's trained on 4,000 sentences from a balanced set of data and tested on the PARC DepBank. We show that a parser can be accurate and fast without needing large, manually-made treebanks from specific fields, making it easier to use in tools that need to understand sentence structures. Comparing systems with DepBank is complicated, so we improved and checked DepBank, pointing out some issues with how results are represented and scored. We show that the system is just as accurate as the PARC XLE parser when considering certain language features in the original DepBank standard. We also provide detailed notes on the internal structure of noun phrases. We suggest looking at accuracy by the type of dependency to see what a parser is good at. We updated DepBank using a GRs scheme and used it to test the RASP parser.

Soft Syntactic Constraints For Word Alignment Through Discriminative Training Word alignment methods can improve by making sure their word pairings stay connected with the phrases determined by a single-language dependency tree (a structure showing how words depend on each other). However, strictly following this rule can sometimes ignore correct pairings, and its usefulness lessens as pairing models become more advanced. We use a publicly available structured output Support Vector Machine (SVM) to create a max-margin syntactic aligner, which allows some flexibility in keeping phrases together. This aligner is the first, as far as we know, to use a learning method that focuses on differences to train an ITG bitext parser (a tool for analyzing bilingual text). We use dependency structures as flexible rules to improve word pairings in an ITG framework (a method for managing translations). We add flexible syntactic ITG (Wu, 1997) rules into a model that focuses on differences, and use an ITG parser to limit the search for the best possible word pairing.

Mildly Non-Projective Dependency Structures Syntactic parsing, which is the process of analyzing sentence structure, needs to find a balance between being detailed and being simple enough to work efficiently. Dependency-based parsing is a method that uses rules to limit the types of sentence structures that are allowed, such as projectivity (which means not allowing crossing branches), planarity (flat structure), multi-planarity, well-nestedness (properly ordered), gap degree (distance between elements), and edge degree (connection limit). Although projectivity is often seen as too limiting for natural language, it's unclear which of the other rules works best. In this paper, we review and compare these different rules and test them with data from two collections of tree structures, to see how many structures fit under each rule. The results show that using the well-nestedness rule along with another rule that controls gaps provides a very good match with the language data.

On-Demand Information Extraction At present, adapting an Information Extraction system to new topics is an expensive and slow process, requiring some knowledge engineering (specialized understanding) for each new topic. We propose a new paradigm (model) of Information Extraction which operates 'on demand' in response to a user's query (question). On-demand Information Extraction (ODIE) aims to completely eliminate the customization effort (work needed to adjust it). Given a user's query, the system will automatically create patterns to extract salient (important) relations in the text of the topic, and build tables from the extracted information using paraphrase discovery technology (finding different ways to say the same thing). It relies on recent advances (new improvements) in pattern discovery, paraphrase discovery, and extended named entity tagging (recognizing and classifying names). We report on experimental results in which the system created useful tables for many topics, demonstrating the feasibility (possibility) of this approach.

Minimum Risk Annealing For Training Log-Linear Models When training the settings for a natural language system, you want to reduce the main mistake on a test group. Since the mistake pattern for many language problems is uneven and full of tricky spots, many systems instead focus on log-likelihood, which is easier to calculate and has a nice, smooth shape. We suggest training to reduce the expected mistake, or risk. We do this by using a probability model over possible answers that we gradually narrow down to focus on the best answer. Besides the simple mistake methods used before, we also explain ways to improve complex methods like precision or the BLEU score, which measures translation quality. We show tests with models for sentence structure analysis and for translating languages. In translating, this new method leads to better BLEU scores than the usual error reduction method. We also see better results in sentence structure analysis. We use a method to roughly estimate the expected log BLEU score. We introduce a planned approach to training that aims to minimize expected mistakes, using a technique to manage unpredictability. We see better test results with this method, which uses a factor to balance how smooth the goal is and how well it shows the real mistake pattern.

Unsupervised Part-Of-Speech Tagging Employing Efficient Graph Clustering An unsupervised system for part-of-speech (POS) tagging, which doesn't require labeled data, uses graph clustering techniques. Unlike current leading methods, it automatically determines the types and number of tags. We create and combine two groupings of word graphs: one based on how often words appear together for common words, and another using statistical measures for less common words. The groups of words formed are used as a dictionary to train a Viterbi POS tagger, which is improved with a word structure analysis part. The method is tested on three languages by checking how well it matches existing taggers. We directly compare the results with taggers that use labeled data for English, German, and Finnish using methods that measure the amount of information. We create a network of words to understand how often words appear together. We group the 10,000 most common words using the contexts formed by the 150-200 most common words.

The Second Release Of The RASP System We describe the new version of the RASP system, which is used for adding grammatical structure to text. This new version has an improved way of showing results that considers meaning more, a better list of grammar rules and word types, and a more adaptable way to train the system using some guidance for ranking sentence structures. We test this version on the Wall Street Journal using a method that compares relationships and explain how users can improve results by using specific vocabulary related to their field.

Tailoring Word Alignments to Syntactic Machine Translation Extracting tree transducer rules for syntactic MT (Machine Translation) systems can be made difficult by word alignment mistakes that disrupt the matching of sentence structures. We suggest a new method for automatically matching words that specifically considers the sentence structure of the target language, while still being as strong and fast as the HMM (Hidden Markov Model) alignment method. Our method's predictions help to get more rules for tree transducers without losing the quality of word matching. We also talk about the effects of different methods that balance word alignments from both languages. We improve the part of the HMM aligner that deals with word order to consider the distance between parts of sentence structures instead of just the order of words. We use a method called hard union competitive thresholding. We apply a distance based on sentence structure in an HMM word alignment model to prefer alignments that fit well with sentence structures.

Transductive learning for statistical machine translation Statistical machine translation systems are usually trained using large collections of two-language texts (bilingual) and single-language texts (monolingual) in the language being translated into (target language). In this paper, we look into using a method called transductive semi-supervised learning, which helps make better use of single-language text from the language being translated from (source language) to improve the quality of translations. We suggest several methods to achieve this and discuss what each method does well and what it doesn’t. We provide detailed tests using French-English texts from the EuroParl data set and Chinese-English texts from the NIST large-data track. We demonstrate a noticeable improvement in translation quality for both sets of tasks.

Word Sense Disambiguation Improves Statistical Machine Translation Recent research shows mixed results on whether systems that help identify the correct meaning of words (WSD systems) can make statistical machine translation (MT) systems better. In this paper, we successfully combine an advanced WSD system with an advanced translation system called Hiero. We demonstrate for the first time that adding a WSD system enhances the performance of a top-level statistical MT system during a real translation task. Moreover, this improvement is clearly significant. We train a model that learns to distinguish word meanings using nearby word patterns within the same sentence and also in nearby sentences. This helps in choosing the right phrase pairs by using scores from the WSD system. We use a classifier method called SVM to figure out word meanings, which is included in the translation process through extra features that are part of a combined model approach.

Domain Adaptation with Active Learning for Word Sense Disambiguation When a system that figures out the meaning of words (WSD) is trained for one topic but used for a different topic, it often becomes less accurate. This shows why it is important to adjust WSD systems for different topics (domain adaptation). In this paper, we first demonstrate that using an approach where the system actively selects useful examples (active learning) can help adjust WSD systems for different topics. Then, by using the most common meaning predicted by a method called expectation-maximization (EM) and combining word counts, we make the original adjustment process better than just using active learning alone. We carry out supervised domain adjustment on a carefully chosen group of 21 nouns from the DSO collection. We notably show that noticing changes in the most common meaning, as modeled by topic-based sense expectations (domain sense priors), can enhance the ability to figure out meanings, even after adjustment with active learning.

Forest Rescoring: Faster Decoding with Integrated Language Models Efficient decoding, or quickly turning input text into another language, has been a big challenge in translating languages, especially when using an integrated language model (a tool that helps improve the quality of translations). We create faster methods for this issue using k-best parsing algorithms (techniques to find the best translations) and show their effectiveness in both phrase-based and syntax-based (structure-focused) translation systems. In both cases, our methods significantly speed up the process, often more than ten times faster, compared to the usual beam-search method (a common technique) without losing quality or making more mistakes. We make assumptions about how much rearranging the language model can cause to limit how much we explore different possibilities. We introduce cube pruning and its variation, cube growing (both are techniques to cut down unnecessary options). We use the idea of a forest to describe the range of possibilities in decoding with integrated language models.

A Simple Similarity-based Model for Selectional Preferences We suggest a new, easy way to automatically figure out selectional preferences, which are choices made by words about what other words they go with, by using data from texts and measuring how similar words are. We focus on identifying roles that words play in sentences and calculate preferences for these roles. In tests, our similarity-based model makes fewer mistakes compared to Resnik's model, which uses a large dictionary called WordNet, and another model based on grouping words, but our model sometimes misses some words. We gather main words from texts that have been tagged with these roles and use just one type of data representation. We consider what other words appear with a word to understand its context. We pick a few roles from a resource called FrameNet to test and use all examples labeled within these roles.

Fully Unsupervised Discovery of Concept-Specific Relationships by Web Mining We introduce a method using web data to find and improve relationships involving a specific idea or category of words. Instead of looking for general and known links, we find many different connections focused on the specific idea. Our approach involves grouping patterns that have words related to the idea and other connected words. We test this method on three different detailed ideas and find that it consistently identifies a wide range of relationships accurately. We use how often terms appear together to discover these connections. Before finding relationships among pairs of categories, our method identifies examples of these categories from web pages by sending pairs of examples as searches and examining the contents of the top 1,000 results from a search engine. We suggest a method that finds specific connections related to ideas without supervision, starting with some initial example words.

Adding Noun Phrase Structure to the Penn Treebank The Penn Treebank does not show details inside simple noun phrases (NPs), only showing flat structures that ignore how complex English NPs can be. This means that tools trained with Treebank data cannot learn the correct details inside NPs. This paper explains the process of adding high-quality brackets inside each noun phrase in the Penn Treebank. We then check how consistent and reliable our added notes are. Finally, we use this resource to figure out NP structure using several methods based on statistics, showing how useful the collection is. This adds important details to the Penn Treebank needed for many natural language processing (NLP) applications. Our way of adding notes includes NML and JJP brackets to show the correct NP structure. We use NE tags during the note-taking process, as we find that features based on recognizing names (NER) will be helpful in a statistical model.

Formalism-Independent Parser Evaluation with CCG and DepBank A key question for those studying computer programs that analyze sentence structure is how to compare different programs (parsers) that use various grammar systems and create different outputs. Testing a parser with the same data it was built with can result in misleadingly high accuracy scores and give an overly positive impression of its performance. In this paper, we test a CCG parser with DepBank and show the challenges of changing the parser's results to match DepBank's grammar rules. We also introduce a way to measure how well this conversion works, which sets a limit on how accurate the parsing can be. The CCG parser gets a score of 81.9% for correctly identifying sentence parts, compared to a best possible score of 84.8%. We compare the CCG parser with the RASP parser, finding that CCG performs better by more than 5% overall and in most types of sentence parts. We create a set of rules to change the output from a Combinatorial Categorial Grammar parser to match Grammatical Relations (GR). We show how methods like adaptive super tagging, doing multiple tasks at once (parallelization), and a specific type of algorithm (dynamic-programming chart parsing) help make the C & C parser, a very efficient CCG parser that does well compared to those using other grammar systems.

Instance Weighting for Domain Adaptation in NLP Domain adaptation is a key issue in natural language processing (NLP) because there isn't enough labeled data in new areas. In this paper, we look at this problem by focusing on how we can adjust the importance of each example, or instance, from one area to another. We carefully study and describe the problem of adapting from one domain to another by looking at the differences in how data is spread out. We show that there are two main reasons for needing to adapt: the differences in the types of data (instances) and how categories are determined (classification functions) in the starting area (source) and the new area (target). We suggest a broad approach to adjusting the importance of each instance to help with domain adaptation. Our practical results in three NLP tasks show that using more information from the target area by adjusting instance importance works well. We adjust the importance of training examples based on how similar they are to data from the target area that hasn't been labeled. We discover that a balanced method of gradually adding new examples (balanced bootstrapping) works better for adapting to new areas than the regular method (standard bootstrapping). In our approach, we give more importance to examples that can be easily used in the new area, so the model trained in the starting area can adjust more effectively to the new area.

Guiding Semi-Supervision with Constraint-Driven Learning Over the last few years, two main research areas in machine learning for understanding language have been finding ways to train systems when we don't have much labeled data, and using existing knowledge and information for tasks that need structured learning. In this paper, we propose a method to add specialized knowledge into these learning techniques. Our new approach combines and uses different task-specific rules. The tests we did in the field of extracting information show that using these rules helps the system learn better, allowing it to perform well with much less training data than before. We introduce a method called constraint-driven learning, CoDL. We apply rules at different levels, like sentence-level rules to define field limits and overall rules to make sure relationships are consistent.

Supertagged Phrase-Based Statistical Machine Translation Until recently, making Phrase-based Statistical Machine Translation (PBSMT) include grammar structures made the system work worse. In this work, we show that using detailed grammatical descriptions called supertags can greatly improve PBSMT systems. We describe a new PBSMT model that includes supertags in both the language being translated to and the translation process. We use two types of supertags: one from Lexicalized Tree-Adjoining Grammar and another from Combinatory Categorial Grammar. Even though these two methods are different, they both improve results similarly. Besides using supertags, we also look at a general grammar check based on combination operators. We conduct several tests on translating Arabic to English using the NIST 2005 test set, focusing on problems like data scarcity, system growth, and the usefulness of different system parts. Our best result (0.4688 BLEU score) is 6.1% better than a top PBSMT model, which is very good compared to the best systems in the NIST 2005 task.

Improved Word-Level System Combination for Machine Translation Recently, a method called confusion network decoding has been used to combine outputs from different machine translation systems. However, mistakes in matching sentences can cause incorrect and awkward results. This paper talks about a better method using confusion networks to mix results from multiple translation systems. In this method, different features can be added in a way that improves the translation quality by allowing for more language model testing and scoring. Additionally, a new way to automatically choose which translation to align others with is introduced. A general method for adjusting weights can be used to improve various automatic evaluation scores like TER, BLEU, and METEOR. Tests with translations from Arabic to English and Chinese to English in 2005 show much better BLEU scores than older methods using confusion networks. We use a script called tercom that uses strategies and a method called dynamic programming to find a set of changes needed to transform one sentence into another. We suggest a framework called multiple confusion network or super-network, where each system's results are used as a main structure to create confusion networks based on a matching measure called TER. Each word in this network is linked to a chance of being correct.

Fast Unsupervised Incremental Parsing This paper explains a step-by-step parser, which is a tool that figures out sentence structure, and a learning method that teaches this tool by using regular text. The parser uses a way to show sentence structure similar to dependency links, which are connections that show how words relate to each other, and it works well with step-by-step parsing. Unlike older parsers that learn without guidance, this parser doesn’t rely on part-of-speech tags (labels like noun or verb). It learns and works quickly without needing to group words into categories or find the best overall solution. The parser's results are checked by turning them into a format called bracketing, which groups words together, and it performs better than previous methods for understanding text structure without guidance. Our step-by-step method uses a new way called common cover links, which can be changed into group brackets. Even though punctuation is usually ignored in this type of research, we focus on punctuation that often shows the boundaries of phrases within a sentence.

Structured Models for Fine-to-Coarse Sentiment Analysis In this paper, we look into a structured model that classifies the sentiment (feelings) of text at different detail levels. The model uses usual sequence classification methods with a restricted Viterbi algorithm to make sure solutions are consistent. The main benefit of this model is that it lets decisions made at one text level affect decisions at another level. Tests show that this method can greatly lower mistakes in classification compared to models that are trained separately. We demonstrated that learning both detailed (sentence) and broad (document) sentiment together improves predictions at both levels.

**Biographies Bollywood Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification** Automatic sentiment classification, which is a way to automatically figure out if people's opinions or feelings are positive or negative, has been studied a lot and used in recent years. But since people express their feelings differently in different areas, it's not realistic to label every possible area we're interested in. We look into how to make sentiment classifiers, which are tools that help us understand emotional tones, work better across different areas, focusing on online reviews for various products. First, we improved a new method called the structural correspondence learning (SCL) algorithm for figuring out sentiment, cutting down mistakes by 30% compared to the original SCL method and by 46% compared to a basic supervised method. Second, we found a way to measure how similar two areas are, which helps predict how well a tool for one area can be used in another. This method can help choose a few areas to label so their tools can work well in many other areas. We also created a dataset that includes opinions from multiple areas.

Statistical Machine Translation for Query Expansion in Answer Retrieval We present a method to make questions better for finding answers by using Statistical Machine Translation (SMT) techniques to help match words between questions and answers. SMT-based query expansion is done by i) using a full-sentence paraphraser to add synonyms that fit the whole question, and ii) by changing question words into answer words using a full-sentence SMT model trained on question-answer pairs. We test these smart query expansion techniques on tfidf retrieval from 10 million question-answer pairs taken from FAQ pages. Test results show that using SMT-based expansion improves how well we find answers compared to using simpler methods or not expanding at all. We show the benefits of using a translation-based method for finding answers by using a more advanced translation model, also trained from a lot of data collected from FAQs on the Web.

Randomised Language Modelling for Statistical Machine Translation A Bloom filter (BF) is a special tool that uses random methods to check if something is part of a group. It uses less space than what is usually needed, but it can sometimes say something is there when it isn't (false positives), and we can measure how often this happens. Here, we look at using BFs to help with language modelling in translation done by computers. We explain how a BF that holds word sequences called n-grams can help us use much bigger collections of text and more complex models alongside a regular n-gram language model in a statistical machine translation system. We also look at (i) how to add rough frequency details efficiently into a BF and (ii) how to lower the mistake rate of these models by first checking smaller parts of candidate n-grams. Our methods keep the one-sided error promise of the BF while using how often words appear in a Zipf-like pattern to take up less space. We describe a way to attach fixed frequency details with a group of n-grams in a BF efficiently.

Learning to Extract Relations from the Web using Minimal Supervision We present a new method to find connections between pieces of information that only needs a small number of examples to learn from. Given a few pairs of specific names or things that show or don't show a certain connection, groups of sentences with these pairs are taken from the internet. We improve an existing method for finding these connections to work with less guidance, and we show test results proving our method can consistently find connections in online documents. We offer a collection of data that includes many examples of how two entities (like people or things) relate in a specific way, which is different from similar data collections used in past research.

A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation Inspired by earlier ways to prepare data for SMT (Statistical Machine Translation), this paper suggests a new method that uses probability to rearrange words. This method combines the strengths of using grammar rules (syntax) and phrases in SMT. Given a sentence and its structure (parse tree), our method changes the order of words using tree operations to create a list of the best possible rearrangements. These are then used by a standard phrase-based translator to find the best translation. Tests show that for the NIST MT-05 task, which involves translating from Chinese to English, this approach improves translation quality by 1.56% as measured by BLEU, a tool that evaluates translation accuracy. We use a system called maximum entropy to learn rules for rearranging binary trees (deciding whether to keep the current order or change it for each part). We apply this system to rearrange parts of the sentence structure using both the visible sentence features and hidden grammatical features to improve Chinese-to-English translation.

Machine Translation by Triangulation: Making Effective Use of Multi-Parallel Corpora Current phrase-based SMT (Statistical Machine Translation) systems do not work well with small training data sets. This happens because the translation guesses are not reliable and there is not enough variety in the phrases used to translate from one language to another. This paper introduces a way to fix this issue by using many different translations of the same phrase in the original language. The key idea is triangulation, which means translating from one language to another by going through a third language. This method lets us use a larger variety of translation examples for training and can be mixed with a regular phrase list using typical methods to fill in missing data. Tests show that triangulated models improve BLEU scores, which measure translation quality, compared to a regular phrase-based system.

A fully Bayesian approach to unsupervised part-of-speech tagging Unsupervised learning of language structure is a tough challenge. A common method is to create a model that can generate data and increase the chances of guessing the hidden structure based on what we observe. Usually, this is done by finding the most likely values for the model's settings, called maximum-likelihood estimation (MLE). We demonstrate, using part-of-speech tagging (identifying words as nouns, verbs, etc.), that a fully Bayesian approach can significantly enhance results. Instead of picking one set of settings, the Bayesian method considers all possible settings. This ensures that the learned structure is likely correct across many different settings and allows using 'priors,' which are initial beliefs that favor simpler patterns commonly found in language. Our model is structured like a regular trigram HMM (a model that considers sequences of three words), but its performance is almost as good as the best models available (like the one by Smith and Eisner, 2005), and up to 14% better than using MLE. We see improvements whether we train from data alone or use a dictionary for tagging. In our model, we use Dirichlet priors (a way to set initial beliefs about data) to control how words and their categories are connected, and a technique called a token-level collapsed Gibbs sampler (a method for making educated guesses) is used to draw conclusions.

Guided Learning for Bidirectional Sequence Classification In this paper, we introduce guided learning, a new way to teach computers to understand sequences in both directions. We combine the tasks of figuring out the order of reasoning and training the part of the computer that makes decisions into one simple learning method, similar to a Perceptron (a basic type of artificial neuron). We use this new learning method for POS tagging, which is labeling parts of a sentence like nouns and verbs. It achieves a 2.67% error rate on the standard PTB test set, meaning it makes fewer mistakes than previous methods on the same data set, and it does this with less information. Our model is as accurate as CRF (a popular method for tagging) but takes much less time to train. We create new methods based on starting with the easiest parts and a simple learning strategy.

Exploiting Syntactic and Shallow Semantic Kernels for Question Answer Classification We examine how using sentence structure (syntactic) and basic meaning (shallow semantic) information can help in automatically sorting questions and answers and improving answer organization. We introduce (a) new ways to organize information based on simple meanings found in Predicate Argument Structures (PASs), and (b) new mathematical tools (kernel functions) to take full advantage of these structures using a machine learning method called Support Vector Machines. Our tests show that understanding sentence structure helps with sorting questions and answers and that basic meaning information is very useful when we can reliably pull out PASs, like from answers. Our basic meaning representations, which are more straightforward, can avoid the problems of being too specific or too broad in other methods.

Chinese Segmentation with a Word-Based Perceptron Algorithm Standard methods for Chinese word segmentation treat it like a labeling task, where each character is given a label to show if it is the end of a word. Models trained to spot differences use nearby character traits to decide on the labels, and Viterbi decoding is a method that picks the most accurate word breaks. In this paper, we suggest a different method that focuses on whole words and sequences of words, not just characters. We use the generalized perceptron algorithm to train our model to spot differences, and a beam-search decoder helps find the best word breaks. Tests from the SIGHAN bakeoffs, which are competitions, show that our system is as good as the top methods in research, getting the highest F-scores, which measure accuracy, for several text groups. We also offer a set of guidelines for identifying words in Chinese.

Unsupervised Coreference Resolution in a Nonparametric Bayesian Model We introduce a method that doesn't rely on pre-labeled data (unsupervised) and uses a flexible statistical approach (nonparametric Bayesian) to solve the problem of identifying when different words refer to the same thing (coreference resolution). This method looks at the overall identity of entities (like people or things) across a whole set of documents (corpus) and how words refer back to things mentioned earlier within each document (sequential anaphoric structure). Unlike most existing methods that make decisions by comparing pairs of words, our approach generates each reference based on a mix of overall entity features and local focus of attention. Even though our system works without labeled data, it scores 70.3 in a measure (MUC F1) that evaluates how well it matches with some recent methods that use labeled data (supervised results) on a specific test set (MUC-6). In our approach, we differentiate between types of words like pronouns (he, she), nominals (common nouns), and proper nouns (names). We test how well our model groups similar references together (clustering properties of DPMMs) by resolving references to earlier words (anaphora resolution) with good outcomes.

Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus This paper shows the first practical results we know of for learning synchronous grammars that create logical forms. By using techniques from statistical machine translation (a method for converting one language to another using statistics), we create a semantic parser (a tool for understanding sentences) based on a specific type of grammar that uses lambda-operators (special symbols used in logic) and is learned from a set of training sentences with their correct logical forms. The parser we created is shown to be the best at understanding questions in a database. We show that our way of representing meaning is better than FUNQL, which is another way to represent meaning.

Learning Multilingual Subjective Language via Cross-Lingual Projections This paper looks at ways to create tools for analyzing opinions or feelings in a new language by using the resources available in English. By using a connection between English and the chosen new language (like a bilingual dictionary or matching texts in both languages), these methods can quickly make tools for analyzing opinions in the new language. We talk about the problems with using a word-based translation for the more complex task of analyzing opinions. Instead, we suggest using matching texts in both languages, applying the tool in English, and then using the matching sentences in the new language to teach a new tool. We use a list of words in both languages and a manually translated set of matching texts to create a tool that sorts sentences by how much they express opinions for Romanian.

Weakly Supervised Learning for Hedge Classification in Scientific Literature We explore how to automatically sort speculative language, or "hedging", in biomedical text using a type of machine learning that requires minimal supervision. Our work includes a clear explanation of the task with guidelines for marking text, analysis and discussion, a probabilistic model for weakly supervised learning, and tests of the methods we describe. We demonstrate that sorting speculative language is possible with this kind of machine learning and suggest ideas for future research. We use individual words as input to decide if sentences in biological articles are speculative or not. We build on the work of Light et al. (2004) by improving their guidelines and creating a public data set (called the FlyBase data set) for classifying speculative sentences. We find that our model struggles to identify strong statements about the lack of knowledge, which are usually marked by sentence structure rather than specific words.

Moses: Open Source Toolkit for Statistical Machine Translation We describe a free software toolkit for statistical machine translation, which is a way for computers to translate languages using statistics, with new features including (a) support for language-related elements, (b) a method for dealing with uncertain input called confusion network decoding, and (c) efficient ways to organize data for translation and language models. Besides the SMT decoder, which is the main translation program, the toolkit also has many tools for setting up, adjusting, and using the system for different translation jobs. Our Moses decoder uses the factored phrase-based translation model, which is a specific method for translating phrases using factors or elements related to language.

The Tradeoffs Between Open and Traditional Relation Extraction Traditional Information Extraction (IE) uses a specific relation name and examples that are manually tagged as input. Open IE is a method that doesn't depend on specific relations and is designed for large and varied collections of text like the internet. An Open IE system can find many different types of relationships from text without needing specific information about each one. How does Open IE work? We look at English sentences and show that many relationships are shown using a small set of language patterns that don't rely on specific relations, which an Open IE system can learn. What are the differences between Open IE and traditional IE? We look at this question with two tasks. First, when there are many relations and they are not defined beforehand, Open IE becomes important. We introduce a new Open IE model called O-CRF and show that it is more accurate and finds almost twice as many relations as TEXTRUNNER, the previous best Open IE system. Second, when there are few target relations and their names are known ahead of time, O-CRF can match the accuracy of a traditional system, though it finds fewer relations. Finally, we show how to combine both systems into a hybrid that is more accurate than a traditional system, with a similar ability to find relations. We use a Conditional Random Field (CRF) classifier to improve Open Relation Extraction, achieving over 60% better performance than the Naive Bayes model in the TextRunner system. Our system is trained with a CRF classifier on subject-verb-object (S-V-O) patterns from a processed text collection as positive examples, and patterns that break language rules as negative ones.

Bayesian Learning of Non-Compositional Phrases with Synchronous Parsing We combine the strengths of Bayesian modeling, which is a way of making predictions based on past data, and synchronous grammar, which is a set of rules for matching phrases, to learn basic translation phrase pairs without needing labeled examples. The structured space of a synchronous grammar naturally helps estimate how likely phrase pairs are, but the number of possibilities can be extremely large. Therefore, we look for smart methods to narrow down these possibilities, which leads to practically useful results. By using a method called Variational Bayes, which is a way to simplify models, we guide the models to choose simple and widely applicable parameter sets, which significantly improves the matching of words between languages. This choice for simpler solutions, along with smart narrowing methods, creates a way to match phrases that results in better overall translations than traditional word matching methods. We suggest "tic-tac-toe pruning," which uses a basic model to decide which parts are unnecessary to compute.

Forest-Based Translation Among syntax-based translation models, the tree-based approach, which uses a parse tree (a diagram that shows the structure) of the source sentence, is promising because it is faster and simpler than the string-based (line by line) method. However, current tree-based systems have a big problem: they only use the top 2-best parse (best interpretations) to guide translation, which can lead to mistakes due to errors in interpreting the sentence structure. We suggest a forest-based method that translates a packed forest (a collection) of exponentially many parses, offering many more options than the usual n-best lists (top interpretations). Large-scale tests show an improvement of 1.7 BLEU points (a score for translation accuracy) over the 1-best baseline (basic standard). This result is also 0.8 points better than using 30-best parses, and it is even faster. When translating, we turn input sentences into trees and convert them into a translation forest by matching rule patterns. We suggest the first direct use of a packed forest.

A Discriminative Latent Variable Model for Statistical Machine Translation Large-scale discriminative machine translation, which uses advanced methods to improve translation quality, has not yet outperformed current systems that rely on simple counting techniques. We believe this is because these systems don't handle multiple ways of translating the same text. We introduce a translation model that considers hidden factors during both learning and translating, and it is designed to make the best overall decisions. The results indicate that considering multiple translation methods does indeed boost performance. Moreover, we show that using regularisation, a technique to prevent errors in statistical models, is crucial to avoid poor solutions. We demonstrate that considering all possible ways to divide text during translation improves results. We offer a model that explains how translation and the way it's built are connected. For the method that uses hierarchical phrases, we introduce a model that uses specific rules and explain the difference between relying on just one method of alignment during training and considering all possible methods.

Vector-based Models of Semantic Composition This paper proposes a way to show the meaning of phrases and sentences using vectors, which are like arrows in math. Our main method uses vector composition, which means putting vectors together, using simple math actions like adding and multiplying. In this method, we create many composition models and test them to see how well they understand sentence similarity. The tests show that using multiplication works better than just adding when compared to what people think. We suggest a general method where the meaning of complex phrases is found by mixing the vectors of each word in that phrase.

Refining Event Extraction through Cross-Document Inference We use the idea called "One Sense Per Discourse" (from Yarowsky, 1995), which means a word or phrase usually has the same meaning throughout a discussion, to improve information extraction, and we expand "discourse" to mean a group of related documents, not just one. We use a similar method to keep event details consistent across sentences and documents. By combining overall information from related documents with specific decisions, we create a simple way to make cross-document inferences to enhance the ACE event extraction task. Without using any extra labeled data, this new method achieved a 7.6% higher F-Measure in identifying event triggers and a 6% higher F-Measure in identifying event arguments compared to a top IE system that analyzes each sentence separately. We use a rule-based method to maintain consistent event triggers and details across documents that are about similar topics.

A Joint Model of Text and Aspect Ratings for Sentiment Summarization Online reviews often come with number ratings given by users for different parts or features of a service or product. We suggest a statistical method that can find and match topics in text and pull out supporting text from reviews for each of these ratings, which is a key issue in summarizing feelings or opinions about specific aspects. Our method works very accurately without needing any specially labeled data, except for the ratings given by users. This approach is versatile and can be applied to dividing up information in other areas where data that follows a sequence comes with related signals. Unlike other methods, MLSLDA uses techniques that treat feelings as something that can be measured based on the topics in a document, similar to a method called supervised latent Dirichlet allocation or in more detailed parts of a document. We suggest a combined model of text and aspect ratings that uses a changed LDA topic model to create topics that represent aspects you can rate, and it also creates a set of tools to predict sentiment.

A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing Morphological processes in Semitic languages create words separated by spaces that introduce several different grammar parts into the sentence. These words are often unclear, making it hard for most grammar analyzers to predict the sentence structure ahead of time. We suggest a unified model that handles both breaking down words into parts (morphological segmentation) and clarifying the sentence structure (syntactic disambiguation) without the usual complications. Our model uses a special type of grammar (treebank grammar), a data-based word list (lexicon), and a smart way to deal with unknown words. It works better than previous systems that handled the tasks separately or in parts, reducing errors by 12% compared to the best results so far for Hebrew language processing. Goldberg and Tsarfaty (2008) found that combining word breakdown and sentence structure analysis improves outcomes compared to handling them one after the other. We show that using a method called lattice parsing, which processes both tasks together, is effective for Hebrew text.

A Tree Sequence Alignment-based Tree-to-Tree Translation Model This paper introduces a translation model that uses tree sequence alignment, which means arranging a sequence of small tree structures to cover a phrase. The model combines the advantages of both phrase-based (using word groups) and syntax-based (using sentence structure) methods. It automatically learns pairs of tree sequences with mapping probabilities from texts that have been parsed (analyzed grammatically) and word-aligned (matching words across languages). Unlike earlier models, it captures non-structured phrases and phrases that are spread out, using linguistically structured features, and also supports rearranging structures at different levels with a wider range. This makes our model more powerful than other models. Tests on a 2005 Chinese-English translation task show that our method performs better than standard systems. Our method creates all possible tree fragments (small parts of a tree structure) starting from each point in the source tree or forest, and then matches these fragments with the source parts of translation rules to find useful rules.

A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model In this paper, we propose a new method for translating languages using a string-to-dependency algorithm in statistical machine translation. With this new approach, we use a target dependency language model while translating to understand long-distance word connections, which a traditional n-gram model cannot do. Our experiments show that this string-to-dependency method improves translation quality by 1.48 points in BLEU (a measure of translation accuracy) and 2.53 points in TER (a measure of translation errors) compared to a standard system on the NIST 04 Chinese-English test. We introduce a model that ensures the target part of each translation rule forms a proper dependency tree fragment, and uses a dependency language model to make the translation more grammatically correct.

Forest Reranking: Discriminative Parsing with Non-Local Features Conventional methods that pick the best option from a list of top choices often miss out on many other good options. We suggest forest reranking, which looks at a large group of possible solutions all at once. Because figuring out the exact best choice is too hard with complex factors, we offer a simpler method inspired by forest rescoring that makes it possible to improve training over the entire dataset. Our best result, a score of 91.7, beats both the top 50 and top 100 choices and is better than any system trained on the dataset before. We demonstrate that using complex factors really helps improve the performance of the parser. To narrow down the large group of possible solutions, we use calculations to find how far the best solution that follows a certain path is from the overall best solution.

Simple Semi-supervised Dependency Parsing We introduce an easy and effective way to train dependency parsers (tools that analyze sentence structure) using both labeled and unlabeled data. We focus on understanding words better by using word groups created from a large collection of texts without labels. We prove this method works well through tests on different datasets, like the Penn Treebank and Prague Dependency Treebank, showing that using word groups significantly improves the accuracy of parsing in many situations. For example, in English parsing without labels, accuracy improved from 92.02% to 93.16%, and for Czech, from 86.13% to 87.13%. Additionally, our method works well even with small amounts of training data, reducing the need for labeled data by about half to achieve the desired accuracy. We demonstrate that for analyzing sentence structure, combining word groups with word forms or part-of-speech tags (labels that indicate word types) results in high accuracy, even with limited training data. We suggest using word groups to enhance the statistical analysis of sentence structure for English and Czech.

Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data This paper shows that using a lot of unlabeled data (data without labels or tags) in semi-supervised learning—a way of teaching computers with some labeled data and lots of unlabeled data—can make tasks in Natural Language Processing (NLP) better, like identifying parts of speech, breaking sentences into chunks, and recognizing names of people or places. We first suggest a simple but strong semi-supervised model that works well with large amounts of unlabeled data. Next, we talk about tests we did using well-known datasets, like PTB III data, CoNLL’00 and ’03 data for the above three NLP tasks. We use up to 1 billion words of unlabeled data, the most ever used for these tasks, to see how much better it can perform. Our results are better than the best results previously reported for all of these datasets. We use an initial classifier on unlabeled data to create fake examples that help train another classifier for the same task. We use the automatically labeled data to train Hidden Markov Models (HMMs), which are tools to predict sequences, like words in sentences.

Unsupervised Multilingual Learning for Morphological Segmentation For centuries, the strong link between languages has led to important discoveries about how humans communicate. In this paper, we explore how this valuable information can be used for learning languages without direct guidance. Specifically, we focus on breaking down words into their smallest parts, called morphemes, in multiple languages. We introduce a flexible statistical model that simultaneously figures out these word parts for each language and identifies patterns that are common across different languages. We test our model on three related languages: Arabic, Hebrew, and Aramaic, as well as on English. Our findings show that learning these word structures together reduces mistakes by up to 24% compared to learning each language separately. Additionally, we show that our combined model works better when used on languages from the same family. We use information from two languages, but the word breakdown is learned separately from translation.

EM Can Find Pretty Good HMM POS-Taggers (When Given a Good Start) We tackle the challenge of unsupervised POS (Part of Speech) tagging, which means identifying parts of speech in text without pre-labeled examples. We show that you can get good results using a strong learning method called EM-HMM, especially when you start with good initial settings, even if your word lists (dictionaries) are not complete. We introduce a group of methods to make good starting guesses for which part of speech matches each word (p(t|w)). We test this approach on a complex task in Hebrew, reducing mistakes by 25% compared to a basic method. We also try it on a common English text task (WSJ) and get results similar to the best recent methods, while keeping the learning process simple and efficient. We use language knowledge to pick a good starting point for the EM method. We point out that fixing incorrect word lists manually is quite doable, and suggest focusing more on using human knowledge than just improving algorithms.

Distributed Word Clustering for Large Scale Class-Based Language Modeling in Machine Translation In statistical language modeling, one way to handle the problem of not having enough data is to group words into similar categories. In this paper, we look into how this method works when used with advanced n-gram models that are trained on large text collections. We present a change to the exchange clustering method that makes it run faster for some types of models that use word groups, and we also offer a version of this method that works across multiple computers to quickly sort words into categories when dealing with very large sets of words (more than 1 million words) and large text data (more than 30 billion words). These word groupings are then used to train language models that partly use these word groups. We demonstrate that when these models are combined with word-based n-gram models in a top-performing machine translation system, it results in better translation quality, as shown by the BLEU score (a measurement of translation accuracy). We introduce a new model called the predictive class bigram model.

Learning Bilingual Lexicons from Monolingual Corpora We present a method for creating bilingual translation dictionaries from text written in only one language. Words in each language are described using features that only look at one language, like how words are used in sentences or parts of words. Translations are found using a model that creates results based on a method called canonical correlation analysis, which matches words from different languages by finding hidden connections. We show that accurate dictionaries can be made for different language pairs and types of text. We start with a small dictionary of 100 word pairs. Having language parsers is more challenging, but our findings suggest that simpler language processing tools might work well enough to find bilingual dictionaries. In this study, we used a set of starting translations, unlike some other studies. We present a model based on canonical correlation analysis, considering features like word context and parts of words.

Unsupervised Learning of Narrative Event Chains Hand-coded scripts were used in the 1970-80s as knowledge frameworks that helped with understanding and other Natural Language Processing (NLP) tasks requiring deep comprehension of meaning. We suggest learning similar frameworks called narrative event chains from raw news stories without supervision. A narrative event chain is a somewhat ordered list of events connected by a shared main character. We outline a three-step process to learn narrative event chains. The first step uses methods that don't need supervision to find relationships between events that share common references. The second step applies a time-based classifier to sort these connected events in a partial order. Lastly, the third step trims and groups independent chains from the pool of events. We present two evaluations: the narrative cloze to test how events relate to each other, and an order coherence task to check the sequence of events. We demonstrate a 36% improvement over the starting point for predicting narratives and a 25% improvement for maintaining the sequence order. We explore learning narrative event sequences without supervision using pointwise mutual information (PMI), which measures how often certain syntactic (sentence structure) positions appear together.

Joint Word Segmentation and POS Tagging Using a Single Perceptron For tagging parts of speech (POS) in Chinese, splitting words is a basic first step. To reduce mistakes and improve splitting by using POS details, both can be done at the same time. A problem with this combined method is the large number of possibilities to consider, making it hard to do efficiently. Recent studies have looked at combining word splitting and POS tagging by simplifying the process to make it easier. In this paper, we suggest a model that combines splitting and tagging without strict rules on how words and POS details interact. We achieve fast results by using a new method called a multiple-beam search algorithm. This system uses a decision-making model based on statistics, trained with a method called the generalized perceptron algorithm. The combined model reduces errors in word splitting by 14.6% and in POS tagging by 12.2%, compared to the usual step-by-step method. We use a method for finding solutions that keeps track of partially completed tasks for each character, similar to a simplified plan that is made smaller by removing unnecessary parts.

A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging We suggest a step-by-step method for Chinese word splitting and labeling words with their types (like noun or verb) together. It uses a system that learns based on characters as its main part, along with additional tools like language models that use numbers. This setup can make use of information that is hard to put directly into the main system. Tests show this method is better at splitting words and doing both tasks together. On the Penn Chinese Treebank 5.0, we made 18.5% fewer mistakes in splitting words and 12% fewer mistakes in doing both tasks together compared to using only the basic system. For CTB-5, we use the division done by Duan et al (2007) called CTB-5d, and the one by Jiang et al (2008) called CTB-5j.

Integrating Graph-Based and Transition-Based Dependency Parsers Previous studies about computer programs that help understand sentence structure have shown that the mistakes these programs make are linked to the ways the programs are designed to learn and make decisions. In this paper, we show how these findings can be used to make these programs better at understanding sentences by combining two different types of models. By allowing one program to help the other with hints or suggestions, we consistently make both programs more accurate, leading to a big improvement when tested on specific data collections. We first demonstrate how two specific programs, the MST Parser and the Malt Parser, can be made better by using each program's predictions to help the other. In earlier work, researchers Nivre and McDonald combined the MST Parser and Malt Parser by using the output of one as input hints for the other.

Efficient Feature-based Conditional Random Field Parsing Discriminative feature-based methods, which focus on identifying specific characteristics in data, are commonly used in processing language naturally, but sentence parsing (breaking down sentences into parts) is still mainly done by generative methods (which create data based on rules). Previous feature-based parsers using dynamic programming have been limited to short sentences for training and testing, but we introduce the first general parser rich in features and based on a conditional random field model (a statistical tool), which works with the full WSJ parsing data (a large set of text data). We achieve efficiency mainly through stochastic optimization techniques (randomness-based methods for improvement), parallelization (using multiple processors at once), and chart prefiltering (sorting data before processing). On the WSJ15 dataset, we achieve a top-level F-score of 90.9% (a measure of accuracy), which is a 14% reduction in errors compared to previous models and is 100 times faster. For sentences with 40 words, our system achieves an F-score of 89.0%, a 36% reduction in errors compared to a basic generative model. In our approach, distributed online learning (learning while processing data) is done synchronously, meaning that a small set of data is split across several CPUs (computer processors), and the model is updated after all have finished processing (Finkel et al, 2008).

Soft Syntactic Constraints for Hierarchical Phrased-Based Translation In adding syntax (rules for sentence structure) to statistical Machine Translation (MT), there is a balance between using linguistic analysis (studying language structure) and allowing the model to use patterns not based on language rules from training data. Some past efforts have addressed this balance by focusing on language-based analysis first and then finding ways to relax that focus. We introduce a method that approaches the balance differently, by starting with a simple translation model directly from aligned text in two languages, then adding flexible rules about sentence parts based on language structure of the source text. This results in significant improvements in translating from Chinese and Arabic to English. We update this method by identifying different types of sentence parts and defining rules for each to check if a phrase matches or goes beyond these boundaries. We discover that these rules are affected by the specific languages being translated.

Generalizing Word Lattice Translation Word lattice decoding has been helpful in translating spoken language; we believe it also offers a strong method for translating different types of written texts. We explain how earlier methods of translating using simple state-based systems can be easily improved to use more advanced grammar-based systems. Furthermore, we address a major challenge that comes with non-linear (not straightforward) word lattice inputs in rearranging words. Our tests on this method show significant improvements in translating from Chinese to English and Arabic to English. In our model, several different methods for breaking down Chinese are combined to create the lattice (a network of possible translations). All the systems we discuss use the lattice input format compatible with Moses (Dyer et al, 2008), including the baseline systems that do not require it.

Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs We present a new method for learning groups of related words from the web with little supervision, using a strong pattern and graph structures, which capture two important features: popularity (how often something is found) and productivity (how often it leads to finding more things). Simply put, a candidate is popular if it is found many times in the pattern. A candidate is productive if it often helps find other related items. These two features help verify that a candidate appears both near the group name and near other group members. We created two methods that start with a group name and one example, then automatically make a ranked list of new examples. We tested this on four groups and consistently got high accuracy. We introduce a method that improves itself using a special pattern (called DAP) and is guided by graph ranking.

The Complexity of Phrase Alignment Problems Many phrase alignment models work within the complex area of matching phrases one-to-one. We prove that finding the best way to match these phrases is extremely difficult, while calculating the expected outcomes of these matches is also very hard. On the other hand, we show that finding the best match can be turned into a type of math problem called an integer linear program, which offers a straightforward method to make predictions about phrase matches that works quite well in practice.

Enforcing Transitivity in Coreference Resolution A good feature of a coreference resolution system, which links different words that refer to the same thing, is its ability to handle transitivity constraints. This means that if it thinks one mention (a word or phrase) is likely the same as two other mentions, it should also check if those two mentions are likely the same when deciding. This type of rule is what integer linear programming (ILP), a mathematical method, is great for, but surprisingly, past research using ILP for coreference resolution hasn't included this rule. We train a system that compares pairs of mentions and show how to add this rule to the probabilities from our system to find the most likely correct matches. We provide results from two common datasets showing that applying this rule consistently improves accuracy, with improvements up to 3.6% using a b3 scorer, a method for evaluating, and up to 16.5% using a cluster f-measure, another evaluation method. We introduce a supervised system, which is a system that learns from labeled examples, using ILP to improve the predictions from our mention comparison system.

Self-Training for Biomedical Parsing Parser self-training is a method where we use an existing tool (parser) to analyze more data, and then use that additional data to train a new version of the tool. Here, we use this method to improve parser performance. Specifically, we retrain the common Charniak/Johnson Penn-Treebank parser with biomedical abstracts that haven't been labeled. This method reached an accuracy score of 84.3% on a common test set of biomedical summaries from the Genia corpus. This is a 20% improvement in reducing mistakes compared to the best previous result on similar biomedical data, which was 80.2% on the same test set.

Reinforcement Learning for Mapping Instructions to Actions In this paper, we present a method using reinforcement learning to turn natural language instructions into a series of actions that can be performed. We assume there is a reward system that tells us how good the actions are. While learning, the system repeatedly creates action sequences for some documents, performs those actions, and sees the rewards it gets. We use a method called policy gradient to figure out the details of a model that helps choose actions. We test our method on two types of instructions: guides for fixing Windows problems and game tutorials. Our results show that this method can compete with traditional learning methods that use lots of labeled examples, but it needs very few or no labeled examples. We demonstrate that using policy-gradient with this approach is similar to training a fully supervised method that improves the chance of predicting the right actions.

Learning Semantic Correspondences with Less Supervision A main challenge in learning language connected to real-world situations is figuring out how words and phrases relate to the actual things they talk about. To handle the high level of confusion in this scenario, we introduce a model that breaks down sentences into parts and matches each part to its meaning based on the real-world context. We demonstrate that our model works well in three areas of growing complexity: Robocup sportscasting, weather forecasts (a new area), and NFL game summaries. We suggest a method that uses probability to align natural language (NL) with meaning representations (MRs). Our model follows a structured approach, first deciding what information to talk about and then creating sentences using the main actions and details of those facts.

Efficient Minimum Error Rate Training and Minimum Bayes-Risk Decoding for Translation Hypergraphs and Lattices Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used in most top-level Statistical Machine Translation (SMT) systems today. These methods were first made to work with N-best lists, which are lists of the best translation options, and were later expanded to use lattices, which contain many more possible translations than N-best lists. In this paper, we further develop these methods to work with hypergraphs, which are structures that hold a huge number of translations created by machine translation systems using Synchronous Context Free Grammars. These new methods are more effective than the earlier lattice-based versions. We explain how MERT can be used to fine-tune settings for MBR decoding. Our tests show that using MERT and MBR speeds up the process and improves the performance of MBR decoding across different language pairs. We also explain a quick method for estimating n-gram posterior probabilities, which are chances of certain word sequences appearing in translations.

Recognizing Stances in Online Debates This paper introduces a method for analyzing opinions without needing pre-labeled examples to figure out which side of an argument someone is taking in an online debate. To deal with the challenges of this type of discussion, we search the internet to find connections that suggest what opinion someone has in debates. We use this information along with how people express themselves and set up the task of deciding debate sides as a math problem that finds the best solution from a set of possibilities. Our results demonstrate that our method is much better than difficult standard methods. We describe stance as the overall viewpoint or opinion a person has toward a subject, idea, or proposal.

Co-Training for Cross-Lingual Sentiment Classification The lack of Chinese sentiment corpora (collections of text) limits research progress on understanding Chinese emotions in text. However, there are many freely available English collections on the Web. This paper focuses on the problem of cross-lingual sentiment classification, which means using an available English text collection to help with Chinese sentiment classification by treating the English text as training data. Machine translation services help bridge the language gap between the training set and test set, and English features and Chinese features are seen as two separate parts of the classification problem. We propose a co-training approach to use unlabeled (not marked with sentiment) Chinese data. Experiments show that our method works well, better than standard methods that only learn from labeled data and those that learn from the test data directly. The proposed co-regression (a method that predicts values) algorithm can fully use both the features in the source language (English) and the features in the target language (Chinese) in a combined system. We suggest using an ensemble method (a technique that combines several models) to train a better Chinese sentiment classification model using English labeled data and their Chinese translations. We used the available English text for Chinese sentiment classification by applying the co-training approach to fully use both English and Chinese features in a combined system.

Concise Integer Linear Programming Formulations for Dependency Parsing We describe the problem of non-projective dependency parsing (a type of language analysis) as a mathematical model that uses whole numbers and linear equations, but is not too large. Our method can efficiently consider complex relationships in the data; it works with strict rules already known and can also learn flexible rules from data. Specifically, our model can understand relationships between connected parts (like siblings and grandparents), how often words connect to others (word valency), and preferences for almost-linear structures. The model learns parameters using an advanced method called max-margin with a simplified approach. We test how well our parser works in different languages and find it performs better than the best current methods. We introduce a method called multicommodity flow formulation.

Non-Projective Dependency Parsing in Expected Linear Time We introduce a new method for dependency parsing, which connects words next to each other but can handle complex structures by changing the order of words. Adding this change makes the worst-case processing time longer, from fast to slower, but data suggests that on average, it remains fast for the types of data we have. Testing on five languages shows top-level accuracy, particularly for correctly matching labeled connections. We introduce the projective Stack algorithm.

Dependency Grammar Induction via Bitext Projection Constraints Broad-coverage annotated treebanks, which are detailed language maps needed to teach language analyzers, are not available for many less-studied languages. The large amount of translated text and good English language analyzers now make it possible to learn grammar by partially transferring knowledge across translated text (bitext). We explore models that help computers learn sentence structure by using word matches between languages and an English language analyzer to narrow down the possible sentence structures in the new language. Unlike past methods, our system doesn't need complete sentence structures to be copied over; it allows partial and estimated transfers using simple rules to guide the process. We test different rules, from general sentence structure preservation to specific rules for analyzing helping verbs in a language. We tested our method on Bulgarian and Spanish sentence data and found it works better than methods without guidance and can even beat methods that use some pre-existing knowledge when there's limited data to learn from. We use a method called posterior regularization (PR), where an English language analyzer creates rules based on translated text, which are then used to guide the learning process in the new language.

Minimized Models for Unsupervised Part-of-Speech Tagging We explain a new way to do unsupervised POS (Part-of-Speech) tagging, which is labeling words in a sentence with their parts of speech like nouns or verbs, without using pre-labeled examples. This method uses a mathematical approach called integer programming to find the smallest model that can explain the data, and then uses EM (Expectation Maximization, a way to find the best guesses for certain numbers) to set the values needed for the model. We test our method on a standard set of texts with different sets of tags (one with 45 tags and another smaller one with 17 tags) and show that our method works better than the best current systems for both sets. We get the best score (92.3% accuracy in labeling words) by using a method called Minimum Description Length (a way to find the simplest explanation), with an integer program that finds a small bigram grammar (rules for word pairs) that follows the tag dictionary's rules and fits the observed data. We suggest a strict way to keep the tagging rules small by reducing the number of transition types, which are the ways one tag can change into another. Instead of removing tags manually from the tag dictionary, we suggest that tags with low chances of being correct can be automatically removed using a model that minimizes size and is applied to the text, keeping the full set of tags in mind.

An Error-Driven Word-Character Hybrid Model for Joint Chinese Word Segmentation and POS Tagging In this paper, we introduce a smart model that combines words and characters for both splitting Chinese words and identifying their parts of speech (POS). Our model works very well because it can deal with both familiar words and new words. We explain our methods that find a good balance for learning about familiar and new words. We suggest a policy that focuses on fixing mistakes, which helps us learn about new words by looking at specific errors in a training set. We describe an efficient system for training our model using a method called the Margin Infused Relaxed Algorithm (MIRA), test our method on a well-known Chinese language dataset, and show that it performs better than the best methods currently available. We handle familiar and new words separately and use a set of tags to show how characters are split into words.

Unsupervised Learning of Narrative Schemas and their Participants. We describe a system that automatically learns narrative schemas, which are logical sequences or groups of events (like arrested by POLICE, SUSPECT and convicted by JUDGE, SUSPECT) where the roles are filled with words with specific meanings (JUDGE includes words like judge, jury, court; POLICE includes police, agent, authorities). Unlike most previous work on understanding event structure or the meaning of roles, our system does not rely on pre-labeled data, manually created information, or predefined sets of events or roles. Our automatic learning method uses repeated references in chains of action words to learn both detailed event structures and the roles involved. By handling both tasks together, we improve on past efforts in learning about narratives and frames, and we identify detailed roles specific to each frame. We describe a way to create a partly ordered list of events that relate to a main character by using an automatic method based on patterns in data to learn how events are related when they share common references, followed by classifying them in time to create a partial sequence.

Conundrums in Noun Phrase Coreference Resolution: Making Sense of the State-of-the-Art We aim to clarify the current best methods in NP (noun phrase) coreference resolution by breaking down the differences in the MUC and ACE task definitions, looking at the assumptions made during evaluation methods, and the natural differences in text collections. First, we look at three smaller problems that are important in coreference resolution: recognizing named entities (identifying names of people, places, etc.), determining anaphoricity (deciding if a word refers back to another word), and detecting coreference elements (finding words that refer to the same thing). We study how each of these smaller problems affects coreference resolution and confirm that certain assumptions about these problems in evaluation methods can make the overall task much easier. Second, we evaluate how well a top-performing coreference resolver (a tool that finds related words in text) works with different types of anaphora (words that refer back to something) and use these findings to create a way to estimate how well coreference resolution will perform on new sets of data. We demonstrate that the coreference resolution problem can be divided into different parts based on the type of mention (the way something is referred to).

Automatic sense prediction for implicit discourse relations in text We present a set of tests to automatically figure out the meaning of hidden (implicit) connections in text, which don't use obvious linking words like "but" or "because". We use a collection of these hidden connections found in newspaper articles and share results from a test set that represents how these connections naturally appear. We apply several language-based features, such as positive or negative word labels (polarity tags), types of verbs (Levin verb classes), how long verb phrases are, possibility markers (modality), surrounding text (context), and word features. Additionally, we look back at older methods that used word pairs from text without annotations as features, point out some issues with them, and suggest changes. Our best mix of features does better than the standard method that relies on a lot of data by 4% for comparison types and 16% for cause-and-effect (contingency). Even though the improvement for comparison is small (+1.07%), our top two systems both showed about 10% better performance in measuring accuracy (f score) compared to a leading system from previous research (Pitler et al, 2009a). Our study shows that the most important word pairs (ranked by how much information they add) all include common, everyday words, instead of the idea-related words we expected.

A Gibbs Sampler for Phrasal Synchronous Grammar Induction We introduce a model for understanding how phrases in different languages are equivalent in meaning. Unlike earlier methods, we don't use guesswork or rules based on aligning individual words. Instead, we create a grammar directly from texts that have been translated sentence by sentence. We use a method that prefers simpler grammars with smaller translation parts, guided by a Bayesian prior which is a statistical way to incorporate prior knowledge. We solve this using a new technique called a Gibbs sampler, which is a way to efficiently explore possible translations. This method avoids the complexity of older models, making it faster and able to handle larger sets of translations. We use Gibbs sampler for learning the SCFG (Synchronous Context-Free Grammar) by exploring different ways to translate (Blunsom et al, 2009). We explain how to keep track of translation choices without needing to record every decision. We also use several processors to make the Gibbs sampling faster, showing that this approach works just as well as the traditional, exact method.

Application-driven Statistical Paraphrase Generation Paraphrase generation (PG) is important in many natural language processing (NLP) applications. However, research on PG is still lacking. In this paper, we introduce a new method for statistical paraphrase generation (SPG) that can (1) be used for different tasks using a single statistical model, and (2) easily combine various sources of information to improve how well PG works. In our tests, we use this method to create paraphrases for three different tasks. The results show that the method can be easily adapted from one task to another and can produce useful and interesting paraphrases. We demonstrate a sentence rewriting method that can be adjusted for different needs, including a way to shorten sentences.

Better Word Alignments with Supervised ITG Models This work looks into supervised word alignment methods that use inversion transduction grammar (ITG) rules. We explore goals like maximum margin (finding the best separation between data groups) and conditional likelihood (probability of data given some condition), and introduce a new way to standardize grammar for simplifying derivations (steps of breaking down sentences). Even for non-ITG sentence pairs, we demonstrate that it's possible to learn ITG alignment models by making simple adjustments to structured learning objectives (goals for learning patterns). For efficiency, we explain a set of pruning techniques (cutting down unnecessary parts) that together allow us to align sentences 100 times faster than basic bitext CKY parsing (a method for grammar analysis). Finally, we add many-to-one block alignment features, which greatly improve our ITG models. In total, our method achieves the best reported AER (alignment error rate) numbers for Chinese-English and improves performance by 1.1 BLEU (a score for translation quality) over GIZA++ alignments. We describe a pruning heuristic (rule of thumb) that results in an average case runtime of O(n^3) (a measure of how processing time increases with sentence length).

Distant supervision for relation extraction without labeled data Modern models of relation extraction (finding connections between things) for tasks like ACE (a specific task) use supervised learning, which means learning from small, manually labeled collections of text. We explore a different method that doesn't need labeled text collections, avoiding the specific focus of ACE-style methods, and allowing the use of text collections of any size. Our experiments use Freebase, a large database with many types of connections, to provide distant supervision (a way to learn without direct labels). For each pair of things that show a connection in Freebase, we find all sentences with those things in a large unlabeled text collection and collect text features to train a relation classifier (a tool that identifies connections). Our approach mixes the benefits of supervised learning (using 400,000 noisy pattern features in a probabilistic classifier) and unsupervised learning (finding many connections from large text collections of any type). Our model can find 10,000 examples of 102 connections with an accuracy of 67.6%. We also look at how features perform, showing that features based on sentence structure are especially useful for connections that are unclear or expressed in different words. Additionally, researchers have tried to automatically gather examples for supervised learning from resources like Wikipedia and databases, or tried open information extraction (IE) to find all possible connections. Distant supervision (DS) can automatically collect labeled data by smartly matching things in text with those in a knowledge base (a large collection of known information).

Phrase Clustering for Discriminative Learning We introduce an easy and expandable method for grouping tens of millions of phrases and use these groups as tools in detailed learning systems. To show how powerful and flexible this method is, we use it in two very different cases: recognizing named entities (like names of people or places) and classifying search queries. Our findings reveal that phrase groups are much better than just word groups. Our system for recognizing named entities performs the best on the well-known CoNLL standard test. Our search query classifier matches the top system in the KDDCUP 2005 competition without needing a lot of manual knowledge work. We look into a two-step approach: first, grouping phrases, then using a guided learning system to find useful groups and give correct importance to these group features. We introduce a K-means-like (a common method for grouping) non-hierarchical (not ranked or layered) grouping method for phrases, which uses MapReduce (a way of processing large amounts of data).

Using Syntax to Disambiguate Explicit Discourse Connectives in Text Discourse connectives are words or phrases like "once," "since," and "on the contrary" that clearly show a connection or relationship between parts of a text. There are two types of confusion that need to be cleared up when understanding these connections. First, a word can be unclear about whether it's showing a connection or not. For example, "once" can mean either a time-related connection or just mean "formerly." Secondly, some connectives are unclear about what kind of connection they show. For example, "since" can mean either a time-related or cause-and-effect connection. We show that using sentence structure helps improve figuring out these confusions. We report top-level results for telling apart when a word shows a connection or not, and achieving human-like accuracy in understanding the type of connection. We show that when there are clear markers, the type of connection can be figured out with scores over 90%. However, predicting the type of connection when it's not clearly marked is much harder.

Bayesian Learning of a Tree Substitution Grammar Tree substitution grammars (TSGs), which are a way to structure sentences, offer many advantages over context-free grammars (CFGs), but are hard to learn. Past approaches have resorted to heuristics, which are simple rules or shortcuts. In this paper, we learn a TSG using Gibbs sampling, a method for statistical analysis, with a nonparametric prior, a flexible model, to control subtree size. The learned grammars perform significantly better than those extracted using simple rules on parsing accuracy, which means they understand sentence structures more accurately.

Efficient Third-Order Dependency Parsers We present methods for more advanced dependency parsing, meaning they can look at parts of sentences with three connections at once, and they are "efficient" because they only take O(n4) time, which is a technical way to say they work relatively fast. Importantly, our new parsers can use interactions between siblings (words at the same level) and grandchildren (words connected through another word). We test our parsers on two well-known language databases, the Penn Treebank and Prague Dependency Treebank, and get scores of 93.04% and 87.38%, showing how accurately they connect words without looking at the word labels. We make the set of possible word connections smaller using results from a simpler first-order parser, which was improved using a method called exponentiated gradient descent, a fancy way to say it learns and gets better over time.

Word Representations: A Simple and General Method for Semi-Supervised Learning If we take an existing supervised NLP (Natural Language Processing) system, a simple and general way to improve accuracy is to use unsupervised word representations (ways to understand word meanings without labeled data) as extra word features. We evaluate Brown clusters (a way to group similar words), Collobert and Weston (2008) embeddings (a method to represent words as numbers), and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER (Named Entity Recognition) and chunking (grouping text into parts). We use nearly the best supervised baselines (standard methods), and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for easy use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/ We show that monolingual word clusters (word groups in one language) are broadly useful as features in monolingual models for predicting language structure.

A Latent Dirichlet Allocation Method for Selectional Preferences The calculation of selectional preferences, which are the acceptable argument values for a relationship, is an important task in natural language processing (NLP) with many uses. We introduce LDA-SP, a method that uses LinkLDA (a model from Erosheva et al., 2004) to understand selectional preferences. By figuring out hidden topics and how they relate to different relationships at the same time, LDA-SP combines the advantages of older methods: like traditional class-based methods, it creates easy-to-understand groups that explain each relationship's preferences, but it is also as strong as newer methods that don't use classes in predicting results. We compare LDA-SP to several advanced methods and achieve an 85% improvement in recall, which is the ability to find relevant information, at 0.9 precision, better than using mutual information (a technique from Erk, 2007). We also test LDA-SP's ability to filter out incorrect uses of inference rules and show significant improvements over Pantel et al.'s system (from Pantel et al., 2007). We concentrate on discovering hidden topics and their distributions across various arguments and relationships, like the subject and direct object of a verb.

Practical Very Large Scale CRFs Conditional Random Fields (CRFs) are a popular method for supervised sequence labeling, mainly because they can manage large amounts of data and connect the relationships between labels. Even for a simple linear model, considering the structure requires many settings and calculations that increase significantly with more labels. In this paper, we tackle the challenge of training very large CRFs, which may include hundreds of output labels and billions of features. The efficiency comes from using a special technique called a lscript1 penalty term that keeps things simple. Using our own setup, we compare three new ways to apply this method. Our tests show that very large CRFs can be trained effectively and these large models can improve accuracy while keeping the settings compact.

Dynamic Programming for Linear-Time Incremental Parsing Incremental parsing techniques like shift-reduce have become popular because they are fast, but there is a big issue: the process is greedy, meaning it only looks at a small part of all the possibilities (even with beam search, which tries to consider more options) unlike dynamic programming, which examines more options. We demonstrate that, surprisingly, dynamic programming can actually work for many shift-reduce parsers by combining "equivalent" stacks based on shared features or characteristics. Based on our tests, our method works up to five times faster compared to a top shift-reduce dependency parser without losing any accuracy. This improved search method also improves learning, and our final parser performs better than all previously known dependency parsers for English and Chinese, while still being much faster. In this paper, we build on the research started by Huang and Sagae (2010) and use dynamic programming for (projective) transition-based dependency parsing (Nivre, 2008). The main concept in our method is to assign the task of scoring certain transitions to specific rules in the calculation process (Viterbi score).

Supervised Noun Phrase Coreference Research: The First Fifteen Years The research focus of computational coreference resolution, which is a computer method for finding out if different phrases refer to the same thing, has changed from using simple rules to using machine learning methods in the past ten years. This paper reviews the important achievements in supervised coreference research, which is a method involving training computers with examples, since it started fifteen years ago. We believe that the mention-pair model, which is a specific way to identify coreferences, has several design problems because it only looks at things in a narrow way.

Learning to Translate with Source and Target Syntax Statistical translation models that try to understand the repeated patterns in language have become popular in recent years. These models use different amounts of information from language theory: some use none, some focus on the grammar rules of the language being translated into (target language), and some focus on the grammar of the original language (source language). However, it's been harder to create translation models that can understand how the grammar of both languages relate to each other. We talk about why this is difficult, look at past attempts, and show how combining some old and new ideas can create a simple method that uses the grammar rules from both languages to make translation more accurate. We improve translation by using grammar structures (parse trees) from both the original and target languages to create flexible (not exact) rules that allow for mismatched substitutions. We find that using grammar information from both languages can actually lower translation quality because the systems become too strict.

Intelligent Selection of Language Model Training Data We tackle the issue of choosing general language data, not specific to any field, to create extra language models for tasks like translating languages. Our method involves comparing how well sentences fit into models specific to a field and those not specific, by measuring how unexpected or surprising a sentence is, called cross-entropy, in the text used to make the general model. We demonstrate that this results in better language models, using less data, compared to picking data randomly or using two other older methods. In choosing based on cross-entropy difference, a sentence is scored by subtracting the general surprise measure from the specific field surprise measure. This method has been applied to enhance European parliamentary texts (48 million words) with news articles (3.4 billion words).

:cdec: A Decoder Alignment and Learning Framework for Finite-State and Context-Free Translation Models We present cdec, a free-to-use framework for decoding, aligning, and training different types of machine translation models, like word-based, phrase-based, and those using structured grammar rules. It uses a single way of representing translation options, keeping the specific translation rules separate from general processes like scoring, reducing unnecessary options, and making decisions. From this representation, the decoder can find not just the best translations, but also match them to a reference, or get the information needed for training using methods that adjust based on feedback or without feedback. Its efficient C++ coding means it uses less memory and runs faster than similar tools. We present cdec, a fast version of a top-level translation system that uses complex phrase-based models. Our cdec decoder learns how to break down words from plain text without needing prior examples.

Target-dependent Twitter Sentiment Classification Sentiment analysis on Twitter data has gained a lot of interest recently. In this paper, we focus on classifying sentiments in tweets based on a specific topic or "target." This means we look at whether tweets express positive, negative, or neutral feelings about a given topic or query. The query is the main focus of the sentiments. The best current methods usually don't consider the specific target and might incorrectly assign feelings not related to it. They only look at the tweet itself and ignore other related tweets, which can be important since tweets are often short and unclear. In this paper, we suggest improving this by 1) including features that depend on the target and 2) considering related tweets. Our experiments show that this approach significantly enhances the accuracy of classifying sentiments based on targets. We use target-specific details and related tweets through a method that optimizes connections like a network or "graph." We combine general features (like tweet content and words used) with specific features (rules from analyzing how words in a sentence relate) to better determine if tweets are subjective (opinion-based) or what their sentiment is (positive, negative, neutral).

A New Dataset and Method for Automatically Grading ESOL Texts We show how supervised machine learning (a method where the computer learns from examples) can help grade 'English as a Second or Other Language' (ESOL) exam papers automatically. Specifically, we use a technique called rank preference learning to clearly understand how different grades relate to each other. We look at various features or characteristics of the texts and use ablation tests (which remove features one by one) to see which ones are most important for good performance. We compare two types of models, regression and rank preference, and find that our method is strong. Tests on the first dataset available to the public show our system works almost as well as human examiners, who sometimes don't fully agree with each other. Finally, we use some "outlier" texts, which are unusual cases, to check if our model is accurate and to find where it disagrees with a human examiner. We are also releasing 1,244 ESOL texts from the FCE exam to the public.

Collecting Highly Parallel Data for Paraphrase Evaluation A lack of standard datasets and evaluation metrics has prevented the field of paraphrasing from making the kind of rapid progress enjoyed by the machine translation community over the last 15 years. We address both problems by presenting a new way to collect data that produces very similar text data cheaply and in large amounts. The very similar nature of this data allows us to use simple word pattern comparisons to measure both the meaning and word differences of paraphrase options. In addition to being simple and efficient to compute, experiments show that these measurements match well with human opinions. Our dataset consists of 1,500 pairs of short video descriptions collected using crowd sourcing, which means getting a large group of people to help online.

Lexical Normalisation of Short Text Messages: Makn Sens a #twitter Twitter provides access to large amounts of data quickly, but it is often messy, making it hard to use for natural language processing (NLP). In this paper, we focus on words not typically found in dictionaries in short text messages and suggest a way to find and fix these misspelled words. Our method uses a computer program to spot misspelled words and suggests possible corrections based on how the words look and sound. We then use both word similarity and the surrounding text to choose the most likely correct word. The suggested method doesn't need any special labels or notes and works very well on a collection of text messages and a new set of data from Twitter. We use a computer program to find misspelled words and then suggest corrections based on how the words look and sound.

Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations Information extraction (IE) aims to create a large set of knowledge from text on the internet. Knowledge-based weak supervision, which uses existing data to loosely label a training set, helps achieve this by automatically learning many ways to extract relationships from text. Recently, scientists have created learning methods to handle messy training data that can result from these loose labels, but these methods assume that relationships do not overlap — for example, they cannot extract both Founded(Jobs, Apple) and CEO-of(Jobs, Apple). This paper introduces a new method for learning with overlapping relationships that combines a model for extracting information from individual sentences with a straightforward method for combining these facts from the whole text. We use our model to learn how to extract information from NY Times articles using guidance from Freebase, a large database. Tests show that this method works fast and improves accuracy more than expected, both overall and in individual sentences. We use a straightforward decision-making process called a greedy inference algorithm. We apply multiple logical rules, called deterministic-OR constraints, to train a system that finds relationships within sentences.

Learning Dependency-Based Compositional Semantics Compositional question answering starts by turning questions into logical forms, but training a semantic parser for this usually needs expensive labeling of the target logical forms. In this paper, we learn to connect questions to answers through hidden logical forms, which are automatically created from question-answer pairs. To solve this difficult learning problem, we introduce a new way to represent meaning that shows a similarity between dependency syntax (how words are connected) and the efficient assessment of logical forms. On two common tests for semantic parsing (GEO and JOBS), our system achieves the highest reported accuracy, even though it doesn't need any labeled logical forms. We use DCS for dependency-based compositional semantics, which shows a semantic parse as a tree with points representing database parts and actions, and connections representing relational joins. Dependency-based Compositional Semantics (DCS) offers an easy way to understand the meaning of questions by using simple tree-like structures.

Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections We describe a new way to develop part-of-speech taggers, which identify parts of speech like nouns and verbs, for languages that don't have labeled training data but have texts translated from a language with lots of resources. Our method doesn't require any prior knowledge about the language being studied, meaning it can be used for many languages that lack resources. We use a technique called graph-based label propagation to share knowledge across languages and use these shared labels as features in an unsupervised model (Berg-Kirkpatrick et al., 2010). For eight European languages, our method improves accuracy by 10.4% compared to a leading existing method, and by 16.7% compared to a basic model using the Expectation Maximization algorithm. We create a dictionary for a specific language by transferring labeled data from a language with many resources, using word alignments in texts that are available in both languages.

Template-Based Information Extraction without the Templates Standard algorithms for template-based information extraction (IE) usually need predefined templates, which are like outlines, and often need labeled data to learn how to fill in the blanks (e.g., an embassy is the Target of a Bombing template). This paper talks about a method that can do this without needing to know the outline beforehand. Our method instead learns the outline automatically from plain text by figuring out template outlines as sets of connected events (e.g., bombings include actions like detonate, set off, and destroy) linked to roles in the event. We also complete the usual IE task by using the learned patterns to find role fillers, or details, from specific documents. We test on the MUC-4 terrorism dataset and show that we create templates very similar to manually made ones, and we fill in the roles with an F1 score of .40, which is nearly as good as methods that need full template knowledge. We find event words from an outside source, group these words to form event scenarios, and organize patterns for different roles in events.

Local and Global Algorithms for Disambiguation to Wikipedia Disambiguating concepts and entities, which means figuring out what words or phrases mean in their specific context, is an important problem in understanding language. The thoroughness of Wikipedia has made it a popular choice for solving these meaning-related issues. Solving these issues using Wikipedia is similar to traditional tasks where we try to determine the right meaning of a word, but it's different because Wikipedia's link structure offers extra clues about which meanings fit together. In this work, we look at methods that use this extra information to find sets of meanings that make sense for a document (we call these “global” methods) and compare them to older methods that focus on individual words or phrases (local approaches). We show that while we can make global methods better, the local methods are still a very strong standard that is tough to surpass.

Part-of-Speech Tagging for Twitter: Annotation Features and Experiments We tackle the challenge of identifying parts of speech, like nouns and verbs, for English text from Twitter, a popular social media platform. We create a set of tags, label data, develop characteristics for analysis, and achieve tagging results close to 90% accuracy. The data and tools are shared with researchers to help better analyze text from Twitter and similar social media. Understanding current systems' limits, we build a special part-of-speech tagger for Twitter by making a training set and a tag set that includes online-specific language, like emoticons (smiley faces). We provide a Twitter dataset with about 26,000 words from 1,827 tweets. This dataset uses a specific set of 25 tags for Twitter.

Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability In statistical machine translation, a researcher wants to find out if a new idea (like a new feature, model, or method) makes translation better compared to a basic system. To figure this out, the researcher tests both systems using separate data. In this paper, we look at ways to make these tests more reliable. We analyze how changes in the optimizer (a tool that adjusts settings to get the best results) can affect the test results and suggest ways to report results more accurately. We use a special test method that considers multiple tuning attempts to make the results fairer.

Transition-based Dependency Parsing with Rich Non-local Features Transition-based dependency parsers usually use rule-based methods for decision-making, but they can handle very detailed information. In this paper, we show that we can make these parsers more accurate by using even more detailed information than what was used before. In the usual Penn Treebank setup, our new features improve the accuracy of linking words from 91.4% to 92.9%, giving the best results so far for this method of parsing and competing with the best results overall. For the Chinese Treebank, they provide a big improvement on the best current methods. A free version of our parser is available for anyone to use. We create the feature pattern for the arc-eager model, which is a specific way of organizing the parsing process.

Improving Word Representations via Global Context and Multiple Word Prototypes Unsupervised word representations are very useful in NLP (Natural Language Processing) tasks both as inputs to learning algorithms (methods for teaching computers) and as extra word features in NLP systems. However, most of these models are built with only local context (immediate surroundings of a word) and one representation per word. This is problematic because words often have multiple meanings (polysemous), and global context (overall meaning in a larger text) can also provide useful information for understanding word meanings. We present a new neural network architecture which 1) learns word embeddings (word meanings in a digital form) that better capture the semantics (meanings) of words by incorporating both local and global document context, and 2) accounts for homonymy (same spelling but different meanings) and polysemy (multiple meanings) by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context (within sentences), and evaluate our model on it, showing that our model outperforms competitive baselines (previous top methods) and other neural language models. Our representation is designed to capture word sense disambiguation (figuring out which meaning of a word is correct in a given context).

Parsing with Compositional Vector Grammars Natural language parsing, which is about breaking down sentences to understand their structure, usually uses a few simple categories like NP (noun phrase) and VP (verb phrase). But this doesn't fully capture the complex meanings and structures of language. Trying to improve this by adding more details or splitting categories only partly solves the problem and makes it more complicated. Instead, we suggest using Compositional Vector Grammar (CVG) that combines a method called PCFGs with a type of artificial intelligence (AI) called a recursive neural network, which helps understand both the structure and meaning of sentences. The CVG improves the performance of the Stanford Parser by 3.8%, achieving a score of 90.4% accuracy. It is quick to train and works about 20% faster than the existing Stanford parser. The CVG also learns which words are most important and helps with understanding complex parts of sentences that need extra meaning, like prepositional phrase (PP) attachments. Recursive neural networks, which can create a tree-like structure for sentences, have been used in understanding language. We improved them to recursive neural tensor networks to better explore how meanings are built up (Socher et al, 2013).

Providing A Unified Account Of Definite Noun Phrases In Discourse Linguistic theories usually place language features into groups like syntax (sentence structure), semantics (meaning), or pragmatics (context and use), as if each group is separate from the others. But, many language features in conversation don't fit neatly into just one of these groups. This paper looks at specific features like how definite noun phrases (specific words to refer to things) and pronouns (he, she, it) are used, and studies how they help make conversations clear and connected. Even a quick look at studies on definite descriptions shows not only mistakes in individual theories by experts from different fields, but also big misunderstandings about how syntax, semantics, and pragmatics work together to explain these features. Our research tries to clear up some of these misunderstandings and build a theory that explains various conversation features by showing how all three language aspects work together. Our main idea is that to truly understand these features, we can't just focus on syntax, semantics, or pragmatics alone. The next part of this paper explains two levels of conversation clarity and how they help explain the use of specific noun phrases. To show how these language aspects work together to explain referring expressions, their use on one level, called the local level, is discussed in Sections 3 and 4. This explanation involves introducing the idea of the "centers" of a sentence in a conversation, an idea that can't be explained just by looking at syntax, semantics, or pragmatics alone. In Section 5, how these two levels interact with these language aspects and their impact on using referring expressions in conversation are explored. To solve how referring expressions are used, we develop "centering theory."

Deterministic Parsing Of Syntactic Non-Fluencies It is often noted that when people use language naturally, it tends to break grammar rules. Everyday talking includes false starts, pauses, and self-corrections that interrupt the proper structure of sentences. It's surprising that even with these grammar mistakes, people usually have no trouble understanding these imperfect conversations, which are common in daily life. It's even more surprising that children can learn the grammar of a language from a mix of correct and incorrect sentences. We tackle the issue of fixing self-corrections by adding rules to a deterministic parser (a tool that analyzes sentence structure) to remove unnecessary words. We categorize different types of repairs and how to fix them by updating a deterministic parser.

D-Theory: Talking About Talking About Trees Linguists, including those who study language using computers, like to discuss the idea of trees in language. In this paper, we describe a theory about how we talk about these language trees, which we call Description theory (D-theory). Although there are important details to work out before we fully understand D-theory (and before we can create computer programs that use it), we think this theory will eventually help explain how natural language works in a way that computers can understand. This paper will mainly focus on one reason for this theory, which is dealing with certain language rules using a method called deterministic parsing. Our D-theory model is strong because it allows the last part of a tree (right-most daughter) to be moved under a nearby part (sibling node).

Parsing As Deduction By looking at how parsing (breaking down sentences) and deduction (logical reasoning) are related, a new and broader way to understand chart parsing is found. This method includes parsing for grammar rules that use unification (making things agree), and it forms the basis of the Earley Deduction process for definite clauses (clear statements). The effectiveness of this method for a certain type of grammar is discussed. We expand on Earley deduction, so it can be used for both breaking down and creating sentences by changing a few settings. We show versions of Earley's method for unification grammars, where unification is the only action used to assign values to attributes (characteristics or details).

Features And Values The paper talks about the language-related parts of a new tool for working with features in computing. This program was created for a class I taught at the University of Texas in the fall of 1983. It is a bigger and improved version of a system that Stuart Shieber first made for the PATR-II project at SRI in the spring of 1983, with later updates by Fernando Pereira and me. Like the earlier versions, the new Texas version of the "DG {directed graph}" package is mainly for showing how words are formed and how sentences are structured, but it might also be very helpful for showing meanings too. We give examples of feature structures where a negation operator, which is used to reverse meanings, might be useful.

Functional Unification Grammar: A Formalism For Machine Translation Functional Unification Grammar allows us to include different parts of machine translation systems, like understanding the input, changing it to another language, and creating the output, all in one system instead of handling them separately. One of the benefits of this approach is that it is consistent, meaning data structures can expand in various ways as different possible solutions are explored, but they are never changed once created. An important characteristic of this system is its reversibility, meaning if it can translate from language a to language b, it can also translate back from b to a.

The Design Of A Computer Language For Linguistic Information A large amount of knowledge about creating languages to talk to computers comes from areas like programming language design and meaning. The goal of the PART group at SRI is to use some of this knowledge to create tools that help communicate language information to computers. The PATR-II formalism is our current computer language for encoding linguistic information. This paper gives a short overview of that formalism and explains our design choices based on a set of features that good computer languages should have. PATR-II is a simple rule-based system that builds on basic grammar rules.

A Syntactic Approach To Discourse Semantics A correct structural analysis of a discourse (a structured conversation or text) is necessary for understanding it. This paper outlines a discourse grammar that recognizes different levels of structure. This grammar, called the "Dynamic Discourse Model", uses a special method (Augmented Transition Network parsing) to gradually build a representation of the meaning (semantics) of a discourse, moving from left to right, based on the meanings of the individual parts (clauses) that make up the discourse. The intermediate stages of this method represent the intermediate stages of the social situation that creates the discourse. The paper tries to show that a discourse can be seen as built through sequences and repeated patterns (recursive nesting) of its parts (constituents). It provides detailed examples of discourse structures at different levels and explains how these structures are described in the proposed framework.

Ontological Promiscuity To make it easier to understand how sentences work, the way we logically represent English sentences should be simple and similar to English. In this paper, I suggest a simple way to write logic that uses basic rules and isn't overly complicated, and where understanding meanings can be straightforward. The main idea is to allow more types of things in our way of thinking, instead of making the logic or sentence structure more complex. Three classic issues - tricky adverbs, types of belief statements, and identity problems in complex situations - are looked at to see how they challenge this logic method, and it's shown that these challenges can be solved. The paper ends by explaining the view of meaning that is assumed with this method. We support using a flexible approach that includes many different types of things. We suggest that understanding natural language doesn't need to be overly complex because this flexible approach can solve the problem.

Some Computational Properties Of Tree Adjoining Grammers Tree Adjoining Grammar (TAG) is a system used to understand the structure of natural languages. Some of the basic ideas of TAGs were introduced in [Joshi, Levy, and Takakashi 1975] and by [Joshi, 1983]. A detailed study of how TAGs relate to language has been done in [Kroch and Joshi, 1985]. In this paper, we will explain some new findings for TAGs, especially in these areas: (1) how complex it is to analyze TAGs, (2) some results about how TAGs can be combined or changed, and (3) how TAGs relate to Head grammars. We provide methods to analyze TAGs that could be used for a basic form of a synchronized TAG. We present the first TAG analyzer using a method similar to CYK, which is a known algorithm for parsing.

Using Restriction To Extend Parsing Algorithms For Complex-Feature-Based Formalisms Grammar systems that use complex details to describe grammar rules are popular in language studies and computer programs that deal with natural language. These systems can be compared to simple grammar systems, but instead of using a limited set of basic symbols, they use more complex and potentially endless structures like detailed graphs. Unfortunately, this complexity means standard methods for analyzing sentences (parsing) might not work because they become very slow or even get stuck. In this paper, we talk about how to improve parsing methods for these complex systems, using a method we call restriction to solve the problem. As an example, we show how to improve a well-known parsing method, Earley's algorithm, to work correctly and efficiently by using restriction to narrow down possibilities from the start. Our version of the algorithm proves to cut down unnecessary processes significantly. Lastly, we explain how this method can also be used for other grammar systems and parsing methods, like LR methods (another parsing approach) and models that predict sentence structure. We suggest a new version of the Earley-parser that uses this focused prediction from the start.

Recovering Implicit Information This paper describes the SDC PUNDIT, (Prolog UNDerstands Integrated Text), system for dealing with natural language messages. PUNDIT, written in Prolog, is a flexible system with separate parts for syntax (sentence structure), semantics (meaning), and pragmatics (context). Each part uses different data sets, such as a dictionary, a comprehensive English grammar, ways to break down verbs, rules for connecting sentence structure to meaning, and a model of the subject area. This paper talks about how these parts communicate to make hidden language information clear. The main idea is to let the system recognize missing parts of language as hidden, so they can be identified, and a process called reference resolution can find what these parts refer to. This makes the job of revealing hidden language information part of the reference resolution tasks. This method works well if missing parts of a sentence are marked as omitted and missing roles are marked as ESSENTIAL, helping reference resolution know when to find what they point to. We suggest a first try at automatically marking hidden roles in meaning. We also make an early attempt to automatically find arguments that are not in the sentence.

A Property-Sharing Constraint In Centering A rule is suggested in the Centering method to figure out which pronoun refers to what in a conversation. This "property-sharing" rule says that two pronouns that keep the same topic (called Cb) in nearby sentences must share a specific grammatical feature. This feature is about being the SUBJECT in both Japanese and English conversations, where different pronouns are used to represent the Cb. In Japanese, it's an invisible pronoun, and in English, it's a clear but not emphasized pronoun. This new rule helps fix issues in the original Centering by explaining where it seems wrong and helps understand sentences with many pronouns. It also offers another way to explain how pronouns are understood when sentences look similar in structure. This shows a big benefit of combining centering (focusing on a topic) and parallelism (similar sentence structure). I will then add a new feature called "speaker identification" to this rule to deal with special cases in Japanese conversations. It shows a strong link between centering and the speaker's point of view, and it helps explain how people's reports of what they see affect understanding pronouns. These findings, by using examples from two very different languages, show that the centering idea can work across different languages.

Characterizing Structural Descriptions Produced By Various Grammatical Formalisms We look at the structures created by different grammar rules, focusing on how complicated the paths are and how these paths relate to each other in the structures each system makes. When comparing these grammar rules, we find it helpful to ignore the specific details and instead look at the general way they create structures, shown by their tree-like diagrams. We notice that many of these grammar rules are quite similar because they create tree structures just like those made by simple grammar rules known as Context-Free Grammar. Based on this, we talk about a group of grammar rules called Linear Context-Free Rewriting Systems, which can be identified quickly and only produce simple languages. We introduce Linear Context-Free Rewriting System (LCFRS), a type of grammar rule that allows the creation of groups of separate strings, meaning it can handle phrases that are not continuous.

A Centering Approach To Pronouns In this paper, we explain a way to use the centering approach to understand and follow topics in conversations. We use this to create a step-by-step method to track conversation flow and link pronouns to the right nouns. According to [GJW86], focusing attention on things in a conversation creates states of continuing, retaining, and shifting between sentences. We suggest adding to these states to handle more cases where pronouns are unclear. The method is used in an HPSG (a type of language system) that helps interact with a database query tool. Our method expands on the idea of centering transition relations, which are connections between nearby sentences, to tell different types of shifts apart. The most common changes between states are expected to be less and less clear in the order of CONTINUE, RETAIN, SMOOTH-SHIFT, and ROUGH-SHIFT. The method M.BFP uses a specific way of ordering to figure out the change in state. Strict centering approaches only focus on the last sentence.

A Unification Method For Disjunctive Feature Descriptions Although using "or" choices, known as disjunction, has been applied in several grammar systems that rely on combining parts, current methods have not worked well for descriptions with many "or" choices because they take an extremely long time to process. This paper explains a method of combining by making gradual improvements, leading to better average results. The overall problem of combining two structures with many "or" choices is very complex and grows rapidly with the number of choices. We introduce a method that, for every set of n connected "or" choices, first checks if single options fit with the definite part of the description, then checks pairs of options, and continues up to grouped sets of options to ensure everything fits together.

Interpretation As Abduction An approach to making educated guesses (abductive inference) developed in the TACITUS project has greatly simplified how we think about understanding texts. This method helps solve local language understanding issues like figuring out what words refer to, understanding complex noun phrases, resolving confusion in sentence structure, and interpreting indirect expressions (metonymy). It also offers a smooth and complete combination of sentence structure (syntax), word meanings (semantics), and language use (pragmatics).

Cues And Control In Expert-Client Dialogues We did a study to look at how control is related to the way conversations are structured. We used criteria for control on four conversations and found 3 levels of structure in these talks. We looked at how control changes between these structures and discovered that the type of sentence used, not just specific words, indicated shifts in control. People used certain signals when conversation goals were going well but interrupted when they weren't. We say that initiative is with the person who is leading the conversation at any given moment. We suggest rules for keeping track of who has initiative based on the types of sentences: for example, making statements, offering proposals, and asking questions show initiative, while giving answers and acknowledging do not.

Planning Coherent Multisentential Text Though most text generators can put together more than one sentence, they can't figure out the best order to make a clear and logical paragraph. A paragraph is clear when the information in each sentence makes sense and is easy to follow for the reader. To make these connections, speakers often use certain ways to link sentences together. Mann and Thompson's Rhetorical Structure Theory suggests 20 common ways people usually connect ideas in English. This paper explains how to formalize these connections and use them in a test version of a text planner that organizes information into clear paragraphs. We use planning steps to create clear and logical text.

A Semantic-Head-Driven Generation Algorithm For Unification-Based Formalisms We present a method for creating strings (sequences of characters) from logical form encodings that is better than previous methods because it works with more types of grammar rules. Unlike an earlier method called Earley deduction generator, it can use grammars where meanings can change, and unlike top-down methods, it can also handle rules that refer back to themselves (left-recursion). The key design feature of this method is that it goes through the analysis tree (a structure showing how a sentence is understood) in a way that focuses on the main idea or "head" of the meaning. We mention a problem with ending the process when rules refer back to themselves.

Cooking Up Referring Expressions This paper explains how the EPICURE computer program creates descriptions in natural language for cooking recipes. Key features of the system include: a basic framework (ontology) that allows for representing groups of things; a method to decide which details to use in a description based on their ability to distinguish; and a grammar system similar to PATR for creating sentences. We aim to create descriptions using the fewest details necessary, even though this is a complex task. Our method tries to make a simple unique description by picking the most distinguishing feature each time. We define "minimal" as how often the system makes very short descriptions. We offer a way to make sure descriptions bring up something already mentioned in a conversation.

Word Association Norms Mutual Information And Lexicography The term word association is used in a very particular sense in the psycholinguistic literature. (Basically, people react faster to the word "nurse" if it comes after a word like "doctor" that is closely related). We will expand the term to form a statistical way of describing various interesting language features, ranging from meaning connections like doctor/nurse (both are main words) to rules about which verbs go with which prepositions (main word/helper word). This paper will suggest a new way to measure word connections using the idea of mutual information, to estimate word association patterns from large collections of text that computers can read. (The usual method of finding word association patterns by testing many people with a few words is expensive and not very reliable). The suggested measure, called the association ratio, calculates word association patterns directly from large text collections, allowing us to estimate patterns for tens of thousands of words.

Evaluating Discourse Processing Algorithms In order to develop a method for evaluating Natural Language systems, we did a detailed study. We tried to assess two different methods for understanding anaphors, which are words like pronouns that refer back to something mentioned earlier, by comparing how accurate and thorough two known algorithms are at identifying what pronouns refer to in real texts and conversations. We show the numerical results of manually testing these algorithms, but this analysis also leads to a quality assessment and suggestions for doing such evaluations in general. We show common challenges faced with numerical evaluation. These issues include: (a) accounting for basic assumptions, (b) deciding how to deal with incomplete specifications, and (c) assessing the effect of incorrect matches and mistakes that compound. We describe error chaining as a situation where a pronoun x correctly links to another pronoun y, but the system incorrectly identifies what y refers to.

Structural Disambiguation With Constraint Propagation We introduce a new way of understanding grammar called Constraint Dependency Grammar (CDG), where each grammar rule is a restriction on how words change each other. CDG parsing is set up as a problem of meeting constraints within a limited set, allowing for the use of effective methods to clarify sentence structure without creating separate sentence diagrams. We also talk about how much language CDG parsing can handle and how complex it is to compute. Our constraint dependency grammar connects to the methods used in solving constraint problems.

Mixed Initiative In Dialogue: An Investigation Into Discourse Segmentation Conversation between two people often involves both taking turns to lead, with the lead changing from one person to another. We use a set of guidelines for how to change the lead in 4 sets of conversations with a total of 1862 exchanges. Using these guidelines, we can identify general conversation patterns. These patterns show that taking the lead helps shape the conversation. To understand how taking the lead affects conversation processes like focusing on a topic, we look at how four types of referring back to something in the conversation are spread out in two sets of data. This spread shows that some parts of the conversation are more important than others. The study shows that people in a conversation often agree together on when to change the subject. We also looked at conversations aimed at completing tasks and those giving advice, finding that the way people take and give up the lead is very different in these types. These differences can be explained by how people work together in planning. We find that as people take turns leading, control over the conversation also shifts between speakers. We create rules based on what is said to decide who leads.

Automatically Extracting And Representing Collocations For Language Generation Collocational knowledge, which means knowing how words naturally go together, is important for creating language. The challenge is that these word combinations, called collocations, come in many different forms. They can have two, three, or more words, and these words can belong to different parts of speech, like nouns, verbs, or adjectives, and can be connected in different ways. This causes two main problems: first, we need to learn about these word combinations, and second, we need to keep this information in a way that is flexible and useful for creating language. In this paper, we tackle both issues, especially focusing on how to learn about these word combinations. We talk about a program called Xtract, which automatically finds different word combinations from large collections of text, and we explain how these can be stored in a flexible dictionary using a method that allows them to be easily combined.

Noun Classification From Predicate-Argument Structures A method to find out how similar nouns are by measuring how they appear with subjects, verbs, and objects in a large collection of texts is explained. This way of grouping nouns by meaning shows that words used in similar ways tend to have similar meanings, which could be useful for tasks like automatic sorting, understanding compound words, and figuring out how words change in meaning. We use a method called mutual-information, which is a way to measure how related nouns are. We use the idea that words with similar meanings will appear in similar situations.

Two Languages Are More Informative Than One This paper introduces a new method for solving word meaning confusion (lexical ambiguities) in one language by using statistical information about word relationships (lexical relations) in another language. This method takes advantage of how words are connected to meanings differently in various languages. We focus on choosing the right word in translation (machine translation), where this method can be used directly, and use a statistical model to help choose the correct word. The model was tested with examples from Hebrew and German and was found to be very helpful for clarifying meanings (disambiguation).

Aligning Sentences In Parallel Corpora In this paper we describe a method that uses statistics to match sentences with their translations in two sets of related texts. Besides certain key points in our data, the only thing we use to match sentences is the number of words they have. Because we don't use the specific words in the sentence, the process is quick and practical for very large text collections. We used this method to match several million sentences in the English-French Hansard texts, achieving over 99% accuracy in a random set of 1000 sentence pairs we checked manually. We show that even without key points, the match between the lengths of sentences is strong enough to expect 96% to 97% accuracy. Thus, this method can potentially be used for more types of texts than we have tested. We achieve these results while completely ignoring the actual words in the sentences.

A Program For Aligning Sentences In Bilingual Corpora Researchers in both machine translation (the process of using computers to translate text between languages) and bilingual lexicography (the study of dictionaries in two languages) have recently become interested in studying parallel texts, which are documents like the Canadian Hansards (records of parliamentary meetings) available in multiple languages (French and English). This paper describes a method for matching sentences in these parallel texts, based on a simple statistical model of character lengths. The method was developed and tested on a small three-language sample of Swiss economic reports. A much larger sample of 90 million words from the Canadian Hansards has been matched and given to the ACL/DCI. We pull out pairs of key words such as numbers, proper nouns (like names of organizations, people, titles), dates, and money information. We find that the ratio of sentence lengths in bytes between the target and source sentences follows a normal distribution (a common pattern in statistics). We show how well a global alignment dynamic program algorithm works, where the basic similarity score is based on the difference in sentence lengths, measured in characters.

Automatic Acquisition Of Subcategorization Frames From Untagged Text This paper talks about a computer program that takes plain text as input (without any special dictionary) and creates a partial list of verbs found in the text and the structures (subcategorization frames, or SFs) in which they appear. Verbs are found using a new method based on a linguistic rule called the Case Filter by Rouvret and Vergnaud (1980). The list gets more complete as the verbs appear more often in the text. Mistakes in identifying the verbs happen in one to three percent of the cases. Currently, five types of structures are identified, and more will be added. In the future, the goal is to make a large dictionary of these structures for the language processing community and to create dictionaries for specific groups of texts.

Structural Ambiguity And Lexical Relations We suggest that unclear prepositional phrase placement can be figured out by how strongly the preposition connects with a noun or a verb, which we estimate by looking at how words are spread out in a large collection of texts. This study shows that using this distributional method can help solve sentence structure problems that seem to need complicated thinking. We discovered that people deciding on sentence structure agree more often when they see the whole sentence instead of just the four specific words involved.

Word-Sense Disambiguation Using Statistical Methods We explain a statistical method for figuring out the meanings of words. A word is given a meaning by looking at the situation or context around it. We create a question that strongly relates to how that word translates into another language. When we used this method in our translation system, the number of mistakes went down by 13%. We suggest a way to clear up confusion about English translations of French words by focusing on the most helpful context clue. We match words without prior labeling and choose the best translation for a word by examining the situation around it.

Monotonic Semantic Interpretation Aspects of understanding meaning, like figuring out what parts of a sentence refer to and how they relate, often involve complex processes that can lose information and change the meaning in a non-linear way. The paper explains how these can be done in a straightforward way using an updated representation called Quasi Logical Form (QLF). The meanings for QLF are shown where the meanings of expressions grow consistently as QLF expressions are worked out. We use Quasi Logical Form, a straightforward way to represent how meanings are built up. A quasi logical form allows leaving some details open to interpretation, like references to earlier parts of the text, missing words, and connections in meaning.

Integrating Multiple Knowledge Sources For Detection And Correction Of Repairs In Human-Computer Dialog We have studied 607 sentences from real conversations between people and computers that had mistakes, from a larger set of 10,718 sentences. We show ways to automatically find where a mistake is and fix it. This involves using different kinds of knowledge: looking for patterns, checking grammar and meaning, and listening to sounds. We successfully found 309 out of 406 sentences with significant mistakes, but 191 smooth sentences were wrongly marked as having mistakes. We think sound information might help avoid wrongly marking sentences as having mistakes. We find that knowing where parts of words are cut off is very helpful in finding and fixing speech errors.

Inside-Outside Reestimation From Partially Bracketed Corpora The inside-outside algorithm, which is a method for finding the rules of a random grammar system, is improved to use information about sentence parts (constituent bracketing) in a partially analyzed collection of text. Tests on both formal language and natural language collections show that the new method works faster and better at understanding sentence structure than the old one. Specifically, over 90% accuracy was achieved in identifying sentence parts for grammars learned by our method using a training set of sentences that were manually analyzed into parts of speech, from the Air Travel Information System spoken language collection. Finally, the improved method is quicker than the original when there is enough information about sentence parts. We modify the inside-outside algorithm to work with partly analyzed data taken from the Penn TreeBank, which is a large collection of text used for research.

Estimating Upper And Lower Bounds On The Performance Of Word-Sense Disambiguation Programs We have recently reported on two new word-sense disambiguation systems, one trained using materials in two languages (the Canadian Hansards) and the other trained using materials in one language (Roget's Thesaurus and Grolier's Encyclopedia). After using both the one-language and two-language classifiers for a few months, we are convinced that the performance is very good. However, we want to make a stronger claim, so we decided to develop some more objective ways to evaluate them. Although there is a fair amount of research on sense-disambiguation, it doesn't give much advice on how to determine if a proposed solution, like our two systems, is successful. Many papers avoid giving numerical evaluations because it is hard to come up with believable performance estimates. This paper will try to set upper and lower limits on the performance level expected in an evaluation. We estimate the lower limit of 75% (average for unclear types) by checking how well a basic system performs that ignores context and always picks the most common meaning. We estimate the upper limit by assuming our performance measurement is mostly limited by how well we can get consistent judgments from people. Not surprisingly, the upper limit depends a lot on the instructions given to the judges. Jorgensen, for example, thought that dictionary writers rely too much on one person's judgment and noticed a lot of differences in judgments (only 68% agreement), as she suspected. In our own experiments, we looked for word-sense disambiguation tasks where judges often agree so we could show they did better than the basic system. Under very different conditions, we found 96.8% agreement among judges. We argue that any WSD program that covers a wide range must perform much better than the most-common-meaning classifier to be seriously considered.

Char Align: A Program For Aligning Parallel Texts At The Character Level There have been several recent studies on matching up parallel texts at the sentence level, for example, by Brown et al (1991), Gale and Church (upcoming), Isabelle (1992), Kay and R/Ssenschein (upcoming), Simard et al (1992), Warwick-Armstrong and Russell (1990). These methods work well on clean inputs, like the Canadian Hansards, achieving at least 96% accuracy in matching sentences. However, if the input has errors (due to OCR, which is optical character recognition, and/or unfamiliar formatting), these methods often fail because the errors make it hard to identify where paragraphs or sentences start and end. This paper introduces a new program called charalign, which matches texts at the character level instead of the sentence or paragraph level, using a method suggested by Simard et al. We demonstrate that aligning parts of the text can be done cheaply by using similar-looking words from both languages. Char_align is intended for languages that use the same alphabet.

Aligning Sentences In Bilingual Corpora Using Lexical Information In this paper, we explain a quick method for matching sentences with their translations in a set of texts in two languages. Existing fast methods don't look at specific words and just consider sentence length. Our method builds a simple model that matches words between languages as it aligns sentences. We aim to find the best match that increases the chance of correctly creating the text with this model. We have reached a low error rate of about 0.4% on Canadian Hansard data, which is much better than before. The method works for any language. We notice that methods based only on sentence length struggle when the texts are short or when the languages don't have many similar words. We see that using a flexible method is likely to miss parts that are only in one language. We train our matching models using sentences that have been manually matched.

An Algorithm For Finding Noun Phrase Correspondences In Bilingual Corpora The paper describes a method that uses English and French text tools to link noun phrases in a bilingual text collection that is lined up. These tools give word type categories, which are used by simple pattern matchers to pick out basic noun phrases for both languages. Noun phrases are then connected to each other using a repeated updating method similar to the Baum-Welch method, which helps train the text tools. This method offers another way to find word connections, with the benefit of including language structure. Improvements to the basic method are explained, allowing for context to be considered when creating the noun phrase links. We try to find noun phrase connections in matched text collections using word type tagging and noun phrase finding techniques.

Towards History-Based Grammars: Using Richer Models For Probabilistic Parsing We explain a model for understanding languages called HBG, which uses detailed language information to clear up confusion. HBG uses information about words, sentence structure, meaning, and how sentences are built, in a new way to make sentences clearer. We use a collection of sentences with marked structures, called a Treebank, along with decision trees to identify important parts of a sentence's structure that help figure out the correct way to understand it. This is different from the usual method of trying to adjust grammar rules based on language analysis to get the right sentence structure. In direct comparisons with one of the best existing models for understanding language called P-CFG, the HBG model performs much better, improving the accuracy from 60% to 75%, reducing mistakes by 37%.

GEMINI: A Natural Language System For Spoken-Language Understanding We report a syntactic and semantic coverage of 86% for the DARPA Airline reservation corpus. Gemini is a system that uses a special way of writing rules for language. We present Gemini natural language parser/generator, which tries to understand and create language from the speech recognition output.

Principle-Based Parsing Without Overgeneration Overgeneration, or creating too many possibilities, is the main reason why earlier principle-based parsers, which are programs that analyze sentences based on rules, were complex to use. This paper introduces a message passing method, which is a way for parts of the program to communicate, to analyze sentences based on principles without creating too many options. This method was programmed in C++ and was tested successfully using sample sentences from a study by van Riemsdijk and Williams in 1986. Our parser, or sentence analyzer, identifies how parts of a sentence function together, like how the subject or object relates to the verb. In our structure diagrams, called dependency trees, points represent parts of the text and lines show the grammatical connections between them.

Intention-Based Segmentation: Human Reliability And Correlation With Linguistic Cues Certain parts of speech, called segments, in a conversation are believed to form meaningful groups. The way these segments are structured is thought to affect and be affected by many factors. However, there isn't a strong agreement on what exactly segments are or how to identify or create them. We show detailed results from a study using a collection of spontaneous, storytelling speeches. The first part checks how consistently people can identify segments in our collection, using the speaker's purpose as the guide. We then use the participants' segmentations to see how well dividing the conversation aligns with three language clues (naming words, signal words, and pauses), using methods to measure information. We combine different manual divisions into one, only keeping the breaks that most people agree on. We use a simple model of dividing topics for our best example, based on the purpose of the conversation segments.

Contextual Word Similarity And Estimation From Sparse Data In recent years, there is a lot of interest in how often words appear together, like in phrases (n-grams), pairs like verb-object, or when they appear close to each other in a small text. This paper talks about how to guess the chance of word pairings that don't show up in the training data. We introduce a method that uses local comparisons between unseen word pairings and other pairings with similar words, using a suitable way to measure word similarity. Our evaluation shows that this method works better than current smoothing methods and might be a good alternative to class-based models. We claim that using only a few groups to model word similarity can lead to losing a lot of information. Groups of similar words are checked by how well they can identify data pieces that are temporarily removed from the input text collection one at a time.

Towards The Automatic Identification Of Adjectival Scales: Clustering Adjectives According To Meaning In this paper, we present a way to group adjectives based on their meaning, as a first step towards automatically recognizing scales of adjectives. We talk about the features of adjective scales and groups of adjectives that are related in meaning, and how they suggest sources of language knowledge in text collections. We explain how our system uses this language knowledge to figure out how similar two adjectives are, using statistical methods without needing any detailed meaning information about the adjectives. We also demonstrate how a clustering method can use these similarities to create groups of adjectives, and we show results from our system for a sample set of adjectives. We finish by showing methods to evaluate this task and examining the importance of the results we got. We learn features by grouping adjectives that show values of the same feature.

Distributional Clustering Of English Words We describe and test a method for grouping words based on how they are used in certain sentence structures. Words are shown by how often they appear in different situations, and we use a method called relative entropy to measure how similar these word situations are for grouping them. Groups are shown by the average situations of the words in them, based on how likely the words are to be in the group. Often, these groups can show broad meanings of the words. We use a method called deterministic annealing, where initially stable groups can split into more detailed groups as the process goes on, creating a layered and flexible grouping of the words. These groups help create models to understand how words appear together, and we check how well these models work using separate test data. We also use deterministic annealing to group pairs of verbs and their related nouns into categories.

Automatic Acquisition Of A Large Sub Categorization Dictionary From Corpora This paper introduces a new way to create a dictionary that shows how different verbs and nouns work together in sentences, using large collections of text that haven't been manually labeled. It demonstrates that by using a statistical method to clean and improve the results from a computer program that identifies sentence structures, we can get very good results, even if the program makes some mistakes. Additionally, it claims that this technique can help identify all types of verb and noun patterns, unlike older methods which can't solve the problem completely. We used a collection of 4 million words from the New York Times and focused on parts of sentences that have helping verbs, then analyzed them automatically with a program that identifies sentence structures.

Automatic Grammar Induction And Parsing Free Text: A Transformation-Based Approach In this paper, we describe a new method for analyzing free text: a transformational grammar, which is a set of rules, is automatically learned to accurately break down text into binary-branching syntactic trees (a structure that shows how words group together), with the internal nodes (nonterminals) not labeled. The algorithm starts with very basic knowledge about sentence structure. By repeatedly comparing the current way of grouping words with the correct way given in the training data, the system learns a set of simple changes to improve accuracy. After explaining the algorithm, we show the results and compare them to other recent methods in automatic grammar learning. Our method requires a manually tagged text for training.

Text Segmentation Based On Similarity Between Words This paper introduces a new way to understand text structure, called the lexical cohesion profile (LCP), which helps find where different parts of a text begin and end. A text segment is like a scene where words are connected to each other through relationships of meaning. LCP keeps track of how similar words are to each other in a stretch of text. The similarity of words, showing how well they go together, is calculated using a network that understands word meanings. When compared to parts of the text marked by different people, LCP matches well with what people think. LCP can be useful for dealing with language shortcuts like anaphora (referring back to something already mentioned) and ellipsis (leaving out parts of a sentence). We discovered that using general knowledge to divide text into parts doesn't always work better than only looking at how words are spread out in the text.

Multi-Paragraph Segmentation Of Expository Text This paper describes TextTiling, a method for dividing informative texts into clear sections that show the smaller topics within the texts. The method uses general word frequency and spread to identify how different themes interact at the same time. Two complete versions of the method are explained and shown to divide texts in a way that matches well with human opinions on where the main topic changes are in thirteen long texts. We calculate likenesses between text parts based on how similar their word arrangements are. TextTiling can split messages into sections with more than one paragraph with an overall accuracy of 83% and an ability to find relevant sections of 78%.

Aligning A Parallel English-Chinese Corpus Statistically With Lexical Criteria We describe our experience with automatically matching sentences in English-Chinese texts. Our report covers three related topics: (1) progress on creating the HKUST English-Chinese Parallel Bilingual Corpus; (2) experiments testing if Gale and Church's (1991) method, which uses sentence length to align, works with a language that's not from the Indo-European family; and (3) a better method that uses specific words related to the topic to help with alignment. We find that English and Chinese texts don't match up in length as well as French and English do, leading to lower success rates for methods that rely on sentence length.

Decision Lists For Lexical Ambiguity Resolution: Application To Accent Restoration In Spanish And French This paper introduces a method using statistics for solving problems when words have more than one meaning. The method looks at nearby word patterns and also word combinations that are farther apart, creating a clear and effective way to decide which meaning is correct. It focuses on finding the best clue in the surrounding words to avoid complicated math models. While it can be used for many types of word meaning problems, it is specifically tested on the task of putting missing accent marks back in Spanish and French text. It works with over 99% accuracy for the whole task and over 90% even for the hardest problems. We note that grouping different forms of a word helps make the data easier to manage and apply. The approach we use to understand word relationships is to look at pairs and triplets of words that appear together to describe nearby settings.

Verb Semantics And Lexical Selection This paper will focus on how verbs are understood in computer systems and how this affects word choice problems in machine translation (MT). Two groups of verbs, one in English and one in Chinese, are studied to show that choosing the right words must consider both the meaning of the sentence and any limits placed on the verb's other words. A new way to represent this is suggested and compared with older methods used in translation that rely on these limits. Our method is similar to knowledge-based machine translation (KBMT) and can be added to current systems. Examples and test results will demonstrate that, using this method, even imperfect matches can lead to correct word choices. We design our measure so that nodes that are higher up in the WordNet hierarchy are considered less similar than those deeper down. Our measure is only based on the arrangement of concepts, and doesn't need any data from language examples. Our similarity measure evaluates how deep two concepts are in the WordNet system and how deep their closest shared concept is, then combines these numbers into a similarity score.

Word-Sense Disambiguation Using Decomposable Models Most probabilistic classifiers used for word-sense disambiguation (figuring out the meaning of a word based on context) have either relied on just one nearby word or used a model that guesses how multiple nearby words relate to each other. In this paper, we introduce a new way to create a probabilistic model, along with a study showing how well these models work to clarify the meaning of the noun "interest." We explain a method for creating probabilistic models that use several nearby words to determine word meanings, without needing unverified guesses about how the model should look. With this method, we describe all the variable interactions in a simple way, which reduces the number of estimates needed, making it faster to compute and easier to understand the data. We manually tag 2,476 instances of the word "interest" with meanings from the Longman Dictionary of Contemporary English.

Corpus Statistics Meet The Noun Compound: Some Empirical Results A variety of statistical methods for analyzing noun compounds (combinations of nouns) are used and compared. The results lead to two main conclusions. First, using conceptual association (connecting ideas) not only covers a wide range but also makes the analysis more accurate. Second, a model based on dependency grammar (how words depend on each other) is much more accurate than one based on deepest constituents (the main parts of a sentence), even though the latter is more common in studies. We suggest a method that doesn't need prior learning to estimate the frequencies of different groupings based on a classification system or a thesaurus (a book of words and their synonyms). We test both models that use closeness and dependency on 244 noun compounds taken from Grolier's encyclopedia, which has 8 million words.

D-Tree Grammars DTG are made to share some of the good points of TAG but fix some of its problems. DTG use two ways to put sentences together called subsertion and sister-adjunction. The main difference of DTG is that, unlike TAG, it treats the two operations in the same way with words: subsertion always means adding a complement and sister-adjunction means adding extra information. Also, DTG, unlike TAG, can explain wh-movement (movement of question words) in both English and Kashmiri, even though in Kashmiri the wh word comes second in the sentence, not first like in English. In trying to model how words depend on each other correctly, we increase the ability to generate sentences but end up with more complex sentence processing.

Unsupervised Word Sense Disambiguation Rivaling Supervised Methods This paper presents an unsupervised learning method for word sense disambiguation, which means figuring out the meaning of words. It can work as well as supervised methods, which usually need a lot of time to prepare by manually labeling data. This method uses two strong ideas: words usually have one meaning in a conversation and one meaning when combined with certain other words. These ideas are used in a step-by-step process that helps the method improve itself. The tested accuracy is over 96%. We introduce the idea of keeping word meanings consistent and apply it to related documents. We suggest self-training, a method that partly learns on its own, which we use for figuring out word meanings.

Two-Level Many-Paths Generation Large-scale natural language generation, which is creating text automatically, needs a lot of knowledge: words and their meanings (lexical), rules for sentence structure (grammatical), and understanding of ideas (conceptual). A strong text generator should work well even when some knowledge is missing. It should also handle incomplete or wrong information effectively. To solve these issues, we have created a mixed-type generator that uses statistics to fill in missing knowledge. We explain how this works with algorithms and show test results. We also talk about how this model can make current text generators simpler and easier to use in different situations, even when complete knowledge is technically possible. We use a method where we pick sentences from a structured set based on grammar and rank them by how well they achieve the communication goal.

Statistical Decision-Tree Models For Parsing Syntactic natural language parsers, which help computers understand sentences, struggle with complicated texts like the Wall Street Journal. They don't work well because they can't handle a lot of vocabulary and confusing grammar. That's why people are moving away from using them for text processing. In this paper, I talk about SPATTER, a new type of parser that uses decision-tree learning, which is a method to make decisions based on questions. It can understand every part of a sentence much better than other methods. This work is based on three ideas: (1) creating grammar rules for computers is too complicated to do by hand for interesting topics; (2) to understand sentences correctly, parsers need to pay close attention to the words and the context around them; and (3) current methods using n-gram, which predict words, are not good enough for parsers. In tests, SPATTER did much better than IBM's parser, which uses grammar rules. When checking SPATTER's performance with the Penn Treebank Wall Street Journal collection, using a method called PARSEVAL, SPATTER got 86% precision (correctness), 86% recall (finding all the right parts), and 1.3 mistakes per sentence for sentences with 40 words or less. For shorter sentences of 10 to 20 words, it got 91% precision, 90% recall, and 0.5 mistakes. We create FTB-UC-DEP, which is a collection of sentence structures derived from FTB-UC using a method called head propagation rules. We find that adding specific word information, known as lexicalization, greatly improves performance compared to a basic model that doesn't use this information, like a probabilistic context-free grammar.

Identifying Word Translations In Non-Parallel Texts Common algorithms for sentence and word-alignment, which are methods for matching sentences and words, allow the automatic identification of word translations from parallel texts, which are texts in two languages that line up perfectly. This study suggests that identifying word translations should also be possible with non-parallel, meaning not perfectly aligned, and even unrelated texts. The method proposed is based on the assumption that there is a correlation, or connection, between the patterns of word co-occurrences, which means how often words appear together, in texts of different languages. We propose a computationally demanding, which means it requires a lot of computer power, matrix permutation method, which is a way to rearrange a grid of numbers, that maximizes a similarity between co-occurrence matrices, or charts showing word pairings, in two languages. An underlying assumption in our work is that translations of words that are related, or connected, in one language are also related in the other language.

Integrating Multiple Knowledge Sources To Disambiguate Word Sense: An Exemplar-Based Approach In this paper, we introduce a new method to make word meanings clearer, called word sense disambiguation (WSD), using a learning method that focuses on examples. This method combines different types of information to clarify word meanings, such as the type of nearby words (like nouns or verbs), word structure, the group of words around it, common word pairings, and the relationship between verbs and objects. We tested our WSD program, called LEXAS, on both a standard set of data used in past research and on a large collection of text where every word's meaning was labeled. LEXAS shows better accuracy on the standard data set and outperforms the most common method for guessing meanings on very confusing words in the large collection labeled with detailed meanings from WoRDNET. We achieve an overall accuracy of 87% for the noun "interest" and find that if we only use features that look at word pairings, the accuracy only falls to 80%. We conclude that common word pairings are more important than sentence structure for understanding word meanings. Our DSO collection focuses on 191 common words with multiple meanings (both nouns and verbs) and includes about 1,000 sentences for each word.

A Fully Statistical Approach To Natural Language Interfaces We present a natural language interface system which is based entirely on trained statistical models. The system consists of three stages of processing: parsing (analyzing sentence structure), semantic interpretation (understanding meaning), and discourse (context in conversation). Each of these stages is modeled as a statistical process (using math to predict outcomes). The models are fully integrated, resulting in an end-to-end system that maps input utterances (spoken or written words) into meaning representation frames. Our approach is fully supervised (trained with examples) and produces a final meaning representation in SQL (a language for managing data). We compute the probability that a constituent such as Atlanta filled a semantic slot such as Destination in a semantic frame for air travel (we calculate how likely it is that a word like Atlanta fits into a category like Destination for flight-related information).

Efficient Normal-Form Parsing For Combinatory Categorial Grammar Under categorial grammars that have powerful rules like composition, a simple n-word sentence can have exponentially many parses (many ways to break down and understand the sentence). Generating all parses is inefficient (takes too much time and effort) and obscures (hides) whatever true semantic ambiguities (real meaning differences) are in the input. This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar, by means of an efficient (quick and effective), correct, and easy to implement normal-form parsing technique (a method to break down sentences into a standard form). The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses; that is, spurious ambiguity (unnecessary confusion) is shown to be both safely and completely eliminated. We provide a safe and complete parsing algorithm (a step-by-step method) which can return non-NF derivations (non-standard breakdowns) when necessary to preserve an interpretation if composition is bounded (limited) or the grammar is restricted in other ways.

A Polynomial-Time Algorithm For Statistical Machine Translation We introduce a fast algorithm for statistical machine translation. This new method can replace the costly and slow search methods currently used in translation systems. It uses a model called stochastic bracketing transduction grammar (SBTG), which we recently introduced to improve upon older methods of matching words between languages, while still using a bigram language model that looks at pairs of words. In our experience, this new algorithm is much quicker and does not lose accuracy. We tested our algorithm on translating Chinese to English.

Parsing Algorithms And Metrics Many different ways exist for evaluating parsing results, including methods like Viterbi, Crossing Brackets Rate, Zero Crossing Brackets Rate, and several others. However, most parsing methods, including the Viterbi method, try to improve the same measure, which is the chance of getting the right labeled tree. By selecting a parsing method that fits the evaluation measure, better results can be achieved. We introduce two new methods: the "Labelled Recall Algorithm," which increases the expected Labelled Recall Rate, and the "Bracketed Recall Algorithm," which increases the Bracketed Recall Rate. Test results show that the two new methods perform better than the Viterbi method on many criteria, especially those they are designed to improve. We notice that the Viterbi parse is generally not the best parse for evaluation measures like f-score that are based on the number of correct parts in a parse.

A New Statistical Parser Based On Bigram Lexical Dependencies This paper describes a new statistical parser, which is a tool that analyzes sentences based on the likelihood of connections between main words in a sentence structure. Usual methods for estimating bigram probabilities, which deal with pairs of words, are expanded to figure out the likelihood of connections between word pairs. Tests using Wall Street Journal data show that this method works as well as SPATTER (Magerman 95; Jelinek et al. 94), which is known to have the best results for this type of tool. The approach is simple, allowing the tool to learn from 40,000 sentences in less than 15 minutes. By using a beam search strategy, which is a method to quickly find the best option, the speed at which sentences are analyzed can increase to over 200 per minute without much loss in accuracy. We use a backed-off smoothing technique, a method to handle problems when there is limited data.

Chart Generation Charts make a simple and consistent system for understanding and creating language if we replace tracking string position with a more logical approach and take steps to avoid creating paths with phrases that don't fully make sense. We suggest lessening the number of parts created by only combining parts that have meanings that don't overlap and have matching indices. We suggest a chart-based process that uses grouped data as input to create all different ways to say something without first breaking it down into a complicated standard form.

A Prosodic Analysis Of Discourse Segments In Direction-Giving Monologues This paper examines how changes in voice pitch and rhythm relate to the structure of speech. We look at how different speaking styles (reading from text versus speaking naturally) and methods of dividing the speech into parts (using only text versus using both text and speech) affect this relationship. We also compare the sound features of the beginning, middle, and end parts of speech segments. We discover that using speech helps people agree more when dividing speech into parts. We introduce the Boston Directions Corpus, a publicly available collection of recorded speech with detailed notes, meant for testing automatic voice pattern labeling.

An Empirical Study Of Smoothing Techniques For Language Modeling We present a detailed practical comparison of several smoothing methods in language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We explore for the first time how factors such as the size of training data, type of text collection (e.g., Brown versus Wall Street Journal), and n-gram order (pairs of words versus groups of three words) impact the performance of these methods, which we measure by checking the cross-entropy (a way to evaluate how well the model predicts) of test data. Additionally, we introduce two new smoothing techniques, one is a modified version of Jelinek-Mercer smoothing and the other is a very simple method that combines predictions linearly, both of which perform better than existing methods. Our smoothing technique can combine the predictions of single words, word pairs, word triples, and possibly longer sequences to get accurate probability estimates even when data is limited.

Minimizing Manual Annotation Cost In Supervised Training From Corpora Corpus-based methods for natural language processing often use supervised training, which needs costly manual labeling of training materials. This paper looks into ways to lower labeling costs by choosing specific examples to label. In this method, while training, the learning program reviews many unlabeled examples and chooses only the most useful ones for labeling at each stage. This stops unnecessary labeling of examples that don't add much new information. This paper builds on our earlier work on selecting samples using a group of models for probabilistic classifiers. We explain a group of methods for this type of sample selection and share test results for the task of randomly tagging parts of speech. We find that all methods significantly cut down on labeling costs, although they vary in speed. Notably, the simplest method, which doesn't need any adjustments, works very well. We also show that choosing samples lessens the amount of information the tagging model needs. We use Hidden Markov Models (HMMs) for part-of-speech tagging and find that picking certain sentences can greatly reduce the number of examples needed to reach desired tagging accuracy. We use the "vote entropy" measure, which is the uncertainty in the labels given to an example by a group of classifiers, to gauge the disagreement within the group.

Three Generative Lexicalized Models For Statistical Parsing In this paper, we first suggest a new way to analyze sentences using statistics, which involves a method to create sentences based on a type of grammar that includes words (lexicalized context-free grammar). We then improve this method to better handle certain sentence structures and question words (subcategorisation and wh-movement). Tests on Wall Street Journal articles show that our sentence analyzer correctly identifies parts of sentences with 88.1% accuracy and 87.5% recall, which is on average 2.3% better than an earlier model by Collins from 1996. We also offer a large collection of 29 million words from Wall Street Journal articles that have been analyzed using our method.

Automatic Detection Of Text Genre As the text collections available to users get larger and more varied, the type or style of text (genre) becomes more important for computer language studies, in addition to sorting by topic or structure. We suggest a theory that sees genres as groups of features that are linked with obvious clues in the text, and we claim that identifying genre using these clues is as effective as using complex structural traits. We think that analyzing sentence structure (parsing) and clarifying word meanings (word-sense disambiguation) can also improve with genre classification. We avoid using structured markers that need text to be tagged or analyzed and instead use simple signs like the number of punctuation marks or calculated markers like ratios and variation measures based on word and character signs.

Using Syntactic Dependency As Local Context To Resolve Word Sense Ambiguity Most past methods that use large collections of text figure out the meaning of a word by training a program to recognize how the word was used before. Different programs need to be trained for each word. We introduce a method that uses the same information to figure out the meaning of different words. This method doesn't need a special collection of text where the meanings are already labeled. It works by noticing that two different words probably mean similar things if they appear in the same nearby words or sentences. We say two things are similar if the shared information between them is high compared to the information in their descriptions.

The Rhetorical Parsing Of Unrestricted Natural Language Texts We figure out the structure of texts using two new methods that look at the surface form of the text: one method finds how cue phrases (words or phrases signaling the structure of the text) are used in speech and breaks sentences into parts, and the other method creates valid structure maps for any natural language text. The methods use information gathered from studying a large collection of text to understand cue phrases. We explain a way to summarize text by focusing on the most important parts and keeping key pieces of the structure.

Machine Transliteration It is hard to translate names and technical terms between languages with different alphabets and sounds. These words are often transliterated, meaning they are changed to similar sounding words. For example, "computer" in English becomes "konpyuutaa" in Japanese. Translating these words from Japanese back to English is even harder and important, because many phrases in texts are not in bilingual dictionaries. We explain and assess a method for doing transliterations backwards using machines. This method uses a model that includes several different steps in the transliteration process. We suggest using a set of tools called weighted finite state transducers to solve the problem of changing words from Japanese Katakana back to English.

Predicting The Semantic Orientation Of Adjectives We find and check, using a large collection of text, rules from joining words that help show if the adjectives joined together are positive or negative. A statistical model uses these rules to guess if the joined adjectives share the same positive or negative meaning, reaching 82% accuracy when looking at each joining word on its own. By combining rules across many adjectives, a grouping method sorts the adjectives into positive or negative groups, and then they are labeled as positive or negative. Tests on real data and simulated tests show high performance: the accuracy is over 90% for adjectives that appear in a decent number of joining word examples in the text collection. We group adjectives into positive and negative sets based on how they are joined together, using methods like similarity graphs, cutting techniques, guided learning, and grouping.

PARADISE: A Framework For Evaluating Spoken Dialogue Agents This paper introduces PARADISE, a general system for assessing spoken dialogue agents. The system separates the task needs from how the agent talks, allows for comparison of different talking methods, lets us measure performance in parts and as a whole, explains how different things contribute to success, and lets us compare agents doing different jobs by adjusting for how hard the task is. We point out three things that affect how well these systems work: agent factors (mainly about the dialogue and the system itself), task factors (related to how well the system understands its job), and environmental factors (like noise and connection quality). We want to judge dialogue agent methods by linking overall user happiness to other measures such as task success, speed, and quality.

A Trainable Rule-Based Algorithm For Word Segmentation This paper presents a trainable rule-based method for dividing text into words. The method offers an easy and language-agnostic (works for any language) alternative to complex systems needing a lot of detailed language data. By itself, our method shows high accuracy in splitting Chinese text into words. Additionally, it successfully enhances the results of other existing word-splitting methods in three languages. Our Chinese word separator uses only a hand-divided sample text without using a dictionary.

A Word-To-Word Model Of Translational Equivalence Many applications that use multiple languages need to translate words between languages but can't afford to use a full translation system because it's too expensive in terms of computer power. For these situations, we created a quick method to estimate a simpler translation system that only focuses on matching words directly. You can control how accurate or complete the model is with a single setting. This makes the model better for tasks that aren't entirely based on statistics. You can easily adjust the model using extra information like grammar, dictionaries, or word order. Our model can connect words in texts that are translated side by side and can work with other translation systems. Unlike others, it can create large translation word lists automatically, with over 99% accuracy. We suggest using the Competitive Linking Algorithm to match word pairs and a technique that finds the best match between word pairs by gradually improving the connection. A challenge in matching word for word is if e1 is the translation of f1 and f2 is closely linked to f1 in the same language, then e1 and f2 will also seem to be strongly connected.

A Memory-Based Approach to Learning Shallow Natural Language Patterns Recognizing simple language structures, like basic relationships between words, is a common task in language and text processing. Usually, this task requires manually defining possible pattern structures, often using regular expressions (patterns to search text) or finite automata (simple computing models). This paper introduces a new memory-based learning method that finds simple patterns in new text using a marked-up training set. The training data is stored unchanged in an efficient structure called a suffix tree. When recognizing patterns, comparisons are made between parts of the new text and examples (evidence) from the training data. This method ensures no information is lost from the training data, unlike other systems that create a single simplified model during training. The paper shows test results for finding simple structures like noun phrases, subject-verb, and verb-object patterns in English. Because this learning method can easily be used in new areas, we plan to apply it to language patterns in other languages and to patterns for extracting information. We break down the sequence of part-of-speech (POS) tags (grammatical markers) of a multi-word phrase into small POS units, count how often these units appear in both new and known words in the training data, and detect new words using these counts.

Entity-Based Cross-Document Coreferencing Using the Vector Space Model Cross-document coreference happens when the same person, place, event, or idea is mentioned in more than one text source. Computer recognition of this is important because it helps connect information from different texts, letting a user see details about a particular topic from multiple places at once. In this paper, we explain a method for solving cross-document coreference that uses the Vector Space Model to clear up confusion when people have the same name. We also explain a scoring method to evaluate the links between documents made by our system, and we compare our method to the scoring method used in the MUC-6 task, which deals with coreference within a single document. We suggested using entity-based cross-document co-referencing that uses links between references in each document to create a summary. This summary, rather than the whole article, helps in choosing important words to represent the document.

The Berkeley FrameNet Project FrameNet is a three-year project funded by NSF (National Science Foundation) focused on studying language using real-world text examples, now in its second year. The project's main features are (a) using real examples from text to understand meaning and grammar rules, and (b) showing how words (mainly nouns, adjectives, and verbs) are used in different contexts called "frames." The database will include (a) explanations of the frames that give words their meanings, and (b) how thousands of words and phrases are used in a sentence, each with (c) a collection of examples from texts, showing how "frame elements" (parts of the frame) connect to their use in sentences (like their role in a sentence, type of phrase, and other sentence features). This report will explain the project's goals and process, and provide details about the computer tools developed or adjusted for this work. We present the FrameNet project where we are creating a dictionary based on frame semantics for important English words.

Classifier Combination for Improved Lexical Disambiguation One of the most exciting recent directions in machine learning is the discovery that using multiple classifiers together often gives much better results than using just one classifier. In this paper, we first show that the mistakes made by three top-level part of speech taggers (tools that label words as nouns, verbs, etc.) are very different from each other. Next, we show how we can use these differences to our benefit. By using clues from the surrounding text to help combine these taggers, we create a new tagger that works much better than any of the single ones alone. We explain how to measure the complementarity, or how well two learners work together by looking at how often one is right when the other is wrong, to estimate the best possible accuracy when they are combined.

Error-Driven Pruning of Treebank Grammars for Base Noun Phrase Identification Finding simple, short, basic noun phrases (groups of words that work together as a noun) is an important part of many natural language processing tasks. While earlier methods for finding these basic noun phrases were quite complicated, this paper suggests a much simpler method that suits the straightforward nature of the task. Specifically, we use a method that relies on a large collection of text to find basic noun phrases by matching sequences of part-of-speech (POS) tags, which are labels that describe the function of each word in a sentence, like noun or verb. The training part of the method uses two successful strategies: first, the rules for finding noun phrases are learned from a "treebank" corpus, which is a structured collection of text; then, these rules are improved by picking the ones that score high in "benefit," meaning they are effective in identifying noun phrases. By using this simple method with a basic rule-matching approach, we achieve surprisingly good results when tested on the Penn Treebank Wall Street Journal, a well-known collection of sentences. We keep sequences of POS tags that form complete noun phrases and use these as rules to identify new, unseen data.

Exploiting Syntactic Structure for Language Modeling The paper introduces a language model that builds sentence structures to get useful information from past words, allowing it to understand long-range word connections. The model gives a likelihood to each group of words with a simple tree structure and key word labels, working from left to right, making it useful for speech recognition. The model, its probability setup, and a series of tests to check its prediction ability are discussed; it performs better than the usual three-word sequence model. We select the main words from the two previous parts as found by a type of sentence analyzer and discover it is more effective than a three-word model. We focus on important words by assigning partial sentence structures to past words and moving key words up in the structure.

Investigating Regular Sense Extensions based on Intersective Levin Classes In this paper we specifically explore questions about polysemy, which means a single word having multiple meanings, focusing on verbs and how these meanings can regularly change through adding specific sentence structures. We believe that verb groups are essential for understanding how meanings can regularly change. Current methods for classifying English verbs, like Levin classes and WordNet, have limitations that make them less useful as general classification systems. We introduce an improved version of Levin classes called intersective sets, which offer a more detailed classification with more consistent sentence patterns and related meanings. We have early signs that our intersective sets will match better with WordNet compared to the original Levin classes. We have also started looking at similar verb groups in Portuguese and found that these verbs show consistent sentence patterns and meanings too. We demonstrate that multiple meanings can sometimes be seen as regular changes in meaning and define intersective Levin classes as more detailed and consistent improvements of basic Levin classes. We suggest that using sentence patterns and verb groups can make it easier to define different verb meanings.

An IR Approach for Translating New Words from Nonparallel Comparable Texts We show that the connections between a word and the words around it are kept in similar texts from different languages. We suggest using a method to represent the surroundings of a word or phrase with a set of numbers, where each number matches one word in those surroundings.

Improving Data Driven Wordclass Tagging by System Combination In this paper we look at how using different methods to train systems for the same natural language processing (NLP) task can help achieve better accuracy than using just the best single system. We do this through an experiment focused on a task called morpho-syntactic wordclass tagging, which involves identifying parts of speech or word classes in text. We trained four popular tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules, and Maximum Entropy) using the same set of text data. After comparing their results, we combined their outputs using different voting methods and additional classification techniques. All combined systems performed better than the best individual system, with the top combination reducing errors by 19.1% compared to the best single system. We propose three voting strategies: equal vote, where each system's vote has the same weight; overall accuracy, where the vote's weight is based on how accurate a system generally is; and pair-wise voting.

Pseudo-Projectivity A Polynomially Parsable Non-Projective Dependency Grammar The pseudo-projective grammar we suggest can be analyzed quickly (in polynomial time) and can handle complex connections that are not next to each other (non-local dependencies) using a method called gap-threading. However, the structures that this grammar creates are strictly orderly (projective).

Role of Verbs in Document Analysis We show results from two ways of analyzing news articles based on the type of verbs used. What makes this research special is that it looks at the role of verbs instead of nouns. Two methods are tested, and one can successfully sort documents by type and meaning, or the "event profile." The first method, using WordNet (a tool that helps understand word meanings), had trouble correctly classifying articles because verbs have many different meanings. Our second method, using English Verb Classes and Alternations (EVCA), which categorizes verbs clearly, helped us separate documents better. For instance, articles with many communication verbs are often opinion pieces, while those with many agreement verbs are often about business mergers or legal issues. We checked the results using a method called Kendall's tau. We provide strong proof that using verb types can help classify documents. We show that the type of document is linked to many verbs from a certain EVCA class.

Automatic Retrieval and Clustering of Similar Words Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words (how words are used together). The similarity measure allows us to construct a thesaurus (a book of synonyms and related words) using a parsed corpus (a large collection of texts that have been analyzed for grammar and structure). We then present a new evaluation methodology (a way to test and measure) for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet (a large database of words) than Roget Thesaurus is. We use dependency relation (how words depend on each other in a sentence) as word features to compute word similarities from large corpora (collections of written or spoken language).

Robust Pronoun Resolution with Limited Knowledge Most traditional ways to solve anaphora (referring back to something mentioned earlier) depend a lot on language and subject knowledge. One downside of creating a system based on knowledge is that it's very hard work and takes a lot of time. This paper shows a strong, simple method for figuring out pronouns (like he, she, it) in technical manuals, using text that has been pre-processed by a tool that identifies parts of speech (like nouns, verbs). The input is checked to see if it matches and if it has certain clues pointing to what the pronoun could refer to. Each clue gives possible answers a score, and the one with the highest score is chosen. Tests show a success rate of 89.7%, which is better than the methods we compared it with, using the same data. Also, early tests indicate that this method can be easily adjusted for other languages without much change. We first apply some rules to remove grammatically unsuitable options and then rank the rest based on how noticeable they are. We find that current evaluations of anaphora resolution systems lack a common basis for comparison because they use different data and tools to process the text before analysis.

Multilingual Authoring using Feedback Texts There are clear reasons for trying to automate the creation of documents in multiple languages, especially for routine topics in specific areas (like technical instructions). Two methods have been used: Machine Translation (MT) which translates a text from one language to another, and Multilingual Natural Language Generation (M-NLG) which creates text in different languages using a database of information. For MT, extracting information is a big challenge, because the meaning has to be figured out from the original text; M-NLG skips this problem but seems to need a costly step of organizing the information. We introduce a new method that uses M-NLG during the step of editing information. A 'feedback text', created from possibly incomplete information, explains in everyday language what is known so far and how it can be expanded. This approach lets anyone who speaks one of the supported languages write texts in all of them, needing only to know about the subject, not how to organize the information. We suggest WYSIWYM (What You See Is What You Mean) as a way to write meaningful information by directly working with structures shown in everyday language text. In this system, logical forms are entered interactively and the related language expressions are created in several languages.

Statistical Models for Unsupervised Prepositional Phrase Attachment We introduce several models that don't need prior examples (unsupervised) to decide where prepositional phrases (like "in the park") should be attached in a sentence. These models almost match the accuracy of the best models that do use prior examples (supervised). Our method uses a simple rule (heuristic) based on how close words are to each other and learns from text that only has basic word information like parts of speech (nouns, verbs, etc.) and root word forms, without needing specific attachment details. This makes it less demanding on resources and easier to use in different situations compared to older methods that relied heavily on large collections of text (corpus-based). We show results for both English and Spanish. We first assume that all "of" phrases are connected to nouns, and then we use our methods to clarify where all other phrases should be attached.

MindNet: Acquiring and Structuring Semantic Information from Text As a word-based knowledge collection created automatically from the definitions and example sentences in two digital dictionaries (MRDs), MindNet has several features that make it different from earlier work with these dictionaries. It is, however, more than just a fixed resource. MindNet is a general method for gathering, organizing, accessing, and using meaning-based information from regular language text. MindNet is both a method for extracting information and a word-based structure, distinct from a word net because it was automatically created from a dictionary and its structure is based on such sources.

Noun-Phrase Co-occurrence Statistics for Semi-Automatic Semantic Lexicon Construction Generating semantic lexicons semi-automatically could save a lot of time compared to making them by hand. In this paper, we introduce a method for finding possible entries for a category from online text collections, using a small set of examples. Our method finds more correct terms and fewer wrong ones than previous methods. Also, the entries created might cover more of the category than if one person did it by hand. Our method finds many terms not in WordNet (many more than previous methods) and can be seen as a way to improve existing large resources. We use statistics on how often words appear together in nearby contexts to find related terms. We tested our method using the MUC-4 and Wall Street Journal text collections (about 30 million words). To choose starting words, we list all the main nouns in the training text by how often they appear and pick the first 10 nouns that clearly fit each category. We find that 3 out of every 5 words our system learns are not in WordNet.

Never Look Back: An Alternative to Centering I suggest a model to understand what the listener is focusing on, which relies only on a list of important conversation topics (S-list). The order of items in the S-list also serves the purpose of the backward-looking center from the centering model. The way we rank the S-list depends on whether topics are already known or new to the listener, and it considers preferences for references within and between sentences. The model supports a step-by-step process, analyzing one word at a time. We believe the importance of earlier mentioned information matters more than its grammatical role. We test our model with data that has been manually checked. We limit our model to focus on the current and previous sentence.

Measures Of Distributional Similarity We examine ways to measure how similar distributions are to help predict the chance of events we haven't seen yet. We look at the relationship between verbs and objects, whether the verb is doing the action or having the action done to it. We discover that our method, called asymmetric skew divergence, which is an advanced version of Kullback-Leibler divergence (a mathematical way to measure differences), works best for predicting chances of word pairings we haven't seen before.

Finding Parts In Very Large Corpora We present a way to identify parts of objects from the whole object (like finding "speedometer" from "car"). Using a very large collection of text, our method identifies part words with 55% accuracy for the top 50 words as ranked by the system. The list of parts can be reviewed by users and added to an existing knowledge database (like WordNet), or used as part of a basic word meaning dictionary. To exclude words that describe qualities (such as driving ability) instead of parts (like steering wheels), we remove words ending with -ness, -ing, and -ity.

Inducing A Semantically Annotated Lexicon Via EM-Based Clustering We present a method for automatically creating slot labels for subcategorization frames, using a system that finds hidden groups with a statistical method called EM (Expectation-Maximization). The models are tested using a general decision-making test. Creating slot labels for subcategorization frames is done by applying EM again, tested on data from analyzing large collections of text. We explain how the learned information can be seen as detailed language dictionary entries. We test 3000 random pairs of verbs and nouns, ensuring the verbs and nouns appear between 30 and 3000 times during training. We use a technique called soft clustering to create groups for general use and do not rely on any manually created resources in our approach to understanding how words prefer to combine.

Automatic Construction Of A Hypernym-Labeled Noun Hierarchy From Text Previous work has shown that automatic methods can be used in building dictionaries that explain word meanings. This work goes a step further by automatically creating not just groups of related words, but a hierarchy (like a family tree) of nouns and their hypernyms (general terms), similar to the manually created hierarchy in WordNet (a large database of words). We had three judges evaluate ten main points in the more specific term hierarchy that had at least twenty related words.

Development And Use Of A Gold-Standard Data Set For Subjectivity Classifications This paper shows a study about how to make sure different people agree when labeling parts of text by using statistical methods. Corrected labels are created and used to update the guidelines for tagging and to create a program that can automatically label text. We use a simple computer program, called Naive Bayes classifier, that looks at sentences based on certain features like the presence or absence of certain word types (like pronouns, adjectives, number words, helping verbs, and adverbs), punctuation, and where the sentence appears. We describe subjective sentences as those that express personal thoughts, feelings, or guesses.

Automatic Identification Of Non-Compositional Phrases Non-compositional expressions, which are phrases that don't mean what their individual words suggest, are hard for computer language tools to handle. We introduce a way to automatically find these tricky phrases by looking at their statistical patterns in a large collection of text. Our approach is based on the idea that if a phrase is non-compositional, the mutual information (a measure of how words relate to each other) changes a lot when you swap one word in the phrase with another similar word. We use a technique called LSA (Latent Semantic Analysis, a method to understand word meanings) to tell apart normal phrases from non-compositional ones, like verb-particle combinations (e.g., "give up") and noun-noun pairs (e.g., "coffee cup"). We set up a rule to decide if a phrase is non-compositional by checking how much the mutual information changes when we replace one word with a similar word using a computer-generated thesaurus (a list of words with similar meanings).

Deep Read: A Reading Comprehension System This paper talks about the early stages of Deep Read, a computer program that helps understand reading by taking any story and answering questions about it. We gathered a collection of 60 practice stories and 60 test stories for kids in 3rd to 6th grade; each story has short-answer questions, along with an answer guide. We used these stories to build and test a basic system that finds patterns in words (bag-of-words) and improves it with extra language processing like breaking words down to their root form (stemming), recognizing names, figuring out word types (semantic class identification), and figuring out what pronouns refer to (pronoun resolution). This simple system finds the sentence with the answer 30-40% of the time. We use a word-count method (bag-of-words), comparing the question to the sentence in the story that has the most similar words.

Corpus-Based Identification Of Non-Anaphoric Noun Phrases Coreference resolution is about finding the first mention (antecedents) for words or phrases that refer back to something mentioned earlier (anaphoric discourse entities), like certain nouns. However, many nouns don't refer back to something and make sense on their own because we understand them through common knowledge (like "the White House" or "the news media"). We created a method using a collection of texts (corpus-based algorithm) to automatically find these non-referring nouns, which could make systems that track these references work better and faster. Our method makes lists of these non-referring nouns and patterns from a set of training texts and uses them to spot similar nouns in new texts. By using 1600 news articles about terrorism (MUC-4) as practice material, our method was able to correctly identify these nouns 78% of the time and was accurate 87% of the time in 50 test documents. We built a system to identify new noun phrases (DDs) in writing, using not only rules based on sentence structure (syntax-based heuristics) to spot known nouns but also new techniques to find unfamiliar names and phrases from text collections (corpora), including proper names and meaningful phrases. We also developed a method that learns on its own (unsupervised learning algorithm) to recognize universally understood nouns without changing their structure.

Efficient Parsing For Bilexical Context-Free Grammars And Head Automaton Grammars Several recent statistical parsers (a tool for analyzing sentences) use bilexical grammars, where each word has specific preferences for certain complements (additional words) with specific main words. We provide O(n4) parsing methods for two bilexical systems, which is faster than the previous limit of O(n5). For a common special case known to allow O(n3) parsing (Eisner, 1997), we offer an O(n3) method with a better grammar constant (meaning it works more efficiently). We demonstrate that the dynamic programming techniques for lexicalized PCFGs (a type of grammar that includes word information) need O(m3) states (steps or conditions).

A Statistical Parser For Czech This paper looks at statistical parsing, which is a method of analyzing sentences, for the Czech language. Czech is very different from English in two main ways: (1) it uses many word endings to show the role of words in a sentence (highly inflected), and (2) the order of words in sentences can change a lot (relatively free word order). These differences may cause new challenges for methods that were originally designed for English. We talk about our experience in using the parsing model created by Collins in 1997. Our final results show 80% accuracy in understanding word relationships, which is good progress towards the 91% accuracy achieved with English text (specifically from the Wall Street Journal). We use a changed version of a collection of sentence structures (transformed tree bank) from the Prague Dependency Treebank to help analyze Czech sentences.

Automatic Identification Of Word Translations From Unrelated English And German Corpora Algorithms for matching words in translated texts are well known. However, recently, new methods have been suggested to find word translations from texts that are not directly related or translated. This is harder because most helpful hints used in directly translated texts don't work for unrelated ones. For directly translated texts, studies have shown up to 99% accuracy in matching words, but for unrelated texts, it has been about 30% until now. This study, which is based on the idea that there's a connection between how words appear together in different languages, improves accuracy to about 72% for correctly identifying word translations. We create groups of words called "context vectors" around words in both languages and then match the original language's words to English using a small existing translation dictionary. We remove word pairs that don't show up often enough (less than 100 times). We demonstrate that accurate translations can be found for 100 German nouns that aren't in the initial translation dictionary.

Mining The Web For Bilingual Text STRAND (Resnik, 1998) is a system that works with any language to automatically find texts that are translated in parallel on the internet. This paper improves the early STRAND results by adding features that automatically detect languages, making the system much bigger and testing how well it works in a structured way. The latest result is a collection of 2491 pairs of English and French documents, with about 1.5 million words in each language. We use information from the page layout, not the actual text, to try to match them.

Estimators For Stochastic Unification-Based Grammars Log-linear models offer a reliable statistical method for handling Stochastic "Unification-Based" Grammars (SUBGs) and other grammar types that include random elements. We explain two practical methods for figuring out the rules of these grammars using a set of example sentences with their structures, and then use these methods to create a random version of Lexical-Functional Grammar. We add broad language rules into a log-linear model. We use sentence structures made by a Lexical-Functional Grammar (LFG) parser as input for a Markov Random Field (MRF) method.

Information Fusion In The Context Of Multi-Document Summarization We introduce a way to automatically create a short summary by finding and combining similar points from several related documents. Our method is special because it uses language generation, which means it changes the wording to create the summary. We notice that when summarizing news articles from multiple documents, just pulling out pieces of text (extraction) may not work well because it can make summaries too long or unfairly favor certain sources.

SemEval-2010 Task 13: TempEval-2 TempEval-2 includes tasks for understanding time-related words, events, and how they relate over time. It was divided into four smaller tasks to make it easier to prepare data and figure out these time relationships. Data with human-made notes were given for six languages: Chinese, English, French, Italian, Korean, and Spanish. One task in this workshop is to find out how an event and a time-related word in the same sentence are connected.

SemEval-2010 Task 14: Word Sense Induction &#x26;Disambiguation This paper explains the setup and evaluation process of the SemEval-2010 task on Word Sense Induction & Disambiguation, and shows the results from 26 systems that took part. In this task, participants had to figure out the meanings (senses) of 100 target words using a training set, and then identify the correct meanings in new examples of those words. System answers were evaluated in two ways: (1) without supervision, by using two ways to measure how well items are grouped together (clustering evaluation), and (2) with supervision, in a Word Sense Disambiguation (WSD) task. To build the dataset, we used WordNet (a large database of words) to randomly select one meaning of a word, and then created a group of words related to that chosen meaning (synset).

Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content Synchronization This paper introduces the first part of a challenge called Cross-lingual Textual Entailment for Content Synchronization, which is part of SemEval-2012. The challenge was made to encourage study on understanding meaning (semantic inference) in texts written in different languages, focusing on a real-world use case. Participants were given data sets for different language combinations, where they had to identify how texts relate to each other (do they imply or infer the same meaning: in one direction, both directions, or not at all). We describe the training and test data used for judging, how they were created, the teams that took part (10 teams, 92 attempts), the methods they used, and the results they got.

Centroid-Based Summarization Of Multiple Documents: Sentence Extraction Utility-Based Evaluation And User Studies We present a tool called MEAD that creates summaries of several documents by using cluster centers (centroids) identified by a system that detects and tracks topics. We also explain two new methods, based on how useful a sentence is and the idea of subsumption (the idea that one idea can include another), which we used to evaluate summaries of both single and multiple documents. Finally, we describe two studies with users that test how our models summarize multiple documents. Our summarizer, which looks at main points of a text, rates sentences using features within the sentence and between sentences to see how well they work as part of a summary.

Knowledge-Free Induction Of Morphology Using Latent Semantic Analysis Morphology induction is about figuring out the structure of words and is important for tasks like creating computer-friendly dictionaries and understanding grammar rules automatically. Previous methods for figuring out word structure depended only on counting how often certain word parts like stems (main part of a word) and affixes (word endings or beginnings) appeared together to decide which affixes are valid. This focus on counting rather than understanding word meanings led to problems, such as using correct affixes incorrectly (like turning "ally" into "all"). We present a new method based on word meanings to learn word structures, suggesting word parts only when the main part and the whole word with the affix have similar meanings. We use a method called Latent Semantic Analysis to show that focusing only on word meanings can produce results as good as the best current system for figuring out word structures. We make a list of possible word endings and use it to find words that share the same main part. We try to group related words by starting with a simple search, which involves setting limits on the shortest possible main part and the maximum possible word parts, controlled by how similar the words are in meaning when looked at in a complex word meaning space.

Inducing Syntactic Categories By Context Distribution Clustering This paper deals with the automatic creation of syntactic categories (like nouns, verbs, etc.) from large collections of text that haven't been labeled or annotated. Past methods work well, but struggle with words that have multiple meanings or are not used often. A new method, called context distribution clustering (CDC), is introduced that can naturally handle these challenges. We use syntactic grouping and reduce complexity without needing prior knowledge to create meaningful groups. In our step-by-step method, we first group words that are most clearly related by their usage in sentences and then slowly add words that are similarly used to these groups.

Introduction To The CoNLL-2000 Shared Task: Chunking We provide background information about the data sets, give a general summary of the systems that participated in the shared task, and briefly talk about how well they performed. The dataset is taken from the WSJ Penn Treebank, which is a large collection of language data, and includes 211,727 examples for training and 47,377 examples for testing.

Use Of Support Vector Learning For Chunk Identification Support Vector Learning is a method from computer science used to help computers recognize patterns. In this context, "chunk identification" refers to the process of finding and labeling parts of a sentence that belong together, like phrases. This technique is useful for understanding language better and can be applied in areas like speech recognition or text analysis. The support vector method involves creating a model that can make decisions about which words or groups of words form a "chunk" based on examples it has learned from. This approach can improve how well a computer understands and organizes information in a text.

Two Statistical Parsing Models Applied To The Chinese Treebank This paper shows the first results of using statistical parsing models, which are computer methods to understand sentence structure, on the newly-available Chinese Treebank, a collection of Chinese sentences. We used two models: one taken and changed from BBN's SIFT System and another based on TAG (a type of grammar). For sentences with less than 40 words, the first model correctly identifies 69% of the sentence parts (precision) and finds 75% of all parts it should find (recall). The second model correctly identifies 77% and finds 78%. Our parser works at the word-level, meaning it looks at individual words, assuming the sentences are already divided into words.

Japanese Dependency Structure Analysis Based On Support Vector Machines This paper introduces a method for understanding Japanese sentence structure using Support Vector Machines (SVMs). Traditional methods for analyzing language using computers, like Decision Trees and Maximum Entropy Models, struggle to pick out useful information and combine it effectively. However, SVMs are known for their ability to work well even with very complex data. Additionally, SVMs use a technique called the Kernel principle, which allows them to handle complex calculations more easily and efficiently. We use SVMs to solve the problem of identifying how parts of Japanese sentences depend on each other. Tests conducted with data from Kyoto University show that our system is 89.09% accurate, even with a small amount of training data (7958 sentences). We also introduce a new kind of information called dynamic features, which are created during the analysis process.

Enriching The Knowledge Sources Used In A Maximum Entropy Part-Of-Speech Tagger This paper shows results for a maximum-entropy-based part of speech tagger, which performs better mainly by adding more information for tagging. We improve results by adding these features: (i) better handling of capital letters in unknown words; (ii) features to help decide verb tense; (iii) features to tell apart particles from prepositions and adverbs. The best accuracy for the tagger on the Penn Treebank is 96.86% overall, and 86.91% on new words. We achieve 96.9% on known words and 86.9% on new words with a MEMM (Maximum Entropy Markov Model).

Evaluation Metrics For Generation Certain generation applications may benefit from using random (stochastic) methods. When creating these random methods, it's important to quickly compare the strengths of different approaches or models. In this paper, we show several types of internal system measures (intrinsic metrics) that we have used for basic numerical assessment. This numerical assessment should then be expanded to a more complete evaluation that looks at quality aspects. To do this, we describe an experiment that checks the connection between the numerical measures and human judgment of quality. The experiment confirms that internal measures cannot replace human evaluation, but some match well (correlate significantly) with human opinions on quality and understandability and can be useful for checking during development. We suggest using Simple String Accuracy as a basic measure for evaluating how well computer systems generate natural language.

Robust Applied Morphological Generation In practical systems that create natural language, it's often helpful to have a separate part that focuses only on handling the forms of words (morphological processing). We introduce such a part: a quick and strong tool that creates different forms of English words. It uses simple, step-by-step methods (finite-state techniques) to make a word form when given the base form of a word (lemma), its role in a sentence (part-of-speech), and the type of word change needed (inflection). We explain how this tool is used in an early version of a system that automatically makes English newspaper text easier to read, and talk about practical problems with word forms and spelling (orthographic issues) we faced when creating text without limits in this system.

Limitations Of Co-Training For Natural Language Learning From Large Datasets Co-Training is a type of learning where two computer programs, called classifiers, learn using different parts of the same data. This helps start learning from a small amount of labeled data (data with answers) by using a large amount of unlabeled data (data without answers). This study looks at how well co-training works for tasks that usually need a lot of examples to work well. When we use it for a specific task, like finding base noun phrases in text, we see that co-training can reduce errors by 36% compared to when using only labeled data. However, the quality of the new data created during co-training becomes a problem because it can be less accurate. To fix this, we suggest a version of co-training where a person checks and fixes mistakes made by the computer. Our study shows that this corrected method and others like it could help co-training work better for big language tasks. We demonstrate that it is very important for the data labeled by the computer to be accurate because too many mistakes stop the learning process from being successful.

Classifying The Semantic Relations In Noun Compounds Via A Domain-Specific Lexical Hierarchy We are developing techniques based on collections of text to identify how words relate to each other. These techniques are more detailed than simple grammar rules but not as complex as advanced knowledge systems. In this paper, we explain a method for finding relationships between two-word noun combinations. We discovered that a straightforward approach using a computer algorithm and a specialized word hierarchy works well. It can apply learned patterns to new words better than just using the words themselves in training. We categorize noun combinations in the medical field into 13 types based on how the main noun and the describing noun relate. We use a type of decision-making tool to assign 18 types of relationships to noun pairs from medical writings and reach 60% accuracy.

Is Knowledge-Free Induction Of Multiword Unit Dictionary Headwords A Solved Problem? We are looking for a way to find groups of words that go together (multiword units) from large text collections without using prior knowledge, to use them as dictionary entries that computers can read. We review and test nine existing tools that find these word groups and show that they still need improvement. We use a method called Latent Semantic Analysis (a technique to understand relationships between words) to make some progress, but we also highlight the big challenges we face with this method. We demonstrate that WordNet (a large database of words and meanings) is as useful for testing methods to find multiword expressions as the internet, even though WordNet is smaller and doesn't change over time. We compare the meaning or context of a phrase and its individual words in two ways: one way uses the phrase's context to create the meaning for the parts, and the other way does not.

Latent Semantic Analysis For Text Segmentation This paper explains a way to divide text into parts that is as good or better than the best current methods. We measure how similar sentences are using latent semantic analysis (LSA), a technique that finds hidden meanings and relationships. We find where to divide the text using a method called divisive clustering, which breaks text into smaller groups. Tests show that LSA is a better way to measure similarity. We use all the words in the text to create simple, smaller versions called low-dimensional document vectors.

Corpus Variation And Parser Performance Most work in statistical parsing, which is about analyzing sentences using statistics, has focused on just one set of text: the Wall Street Journal part of the Penn Treebank. While this has made it easier to compare different parsing methods, it doesn't answer how other kinds of text might change how well parsers work, or how easily parsing models can be used with different text collections. We look into these questions by comparing results from the Brown and WSJ text collections, and also check which parts of the parser's probability model are specifically adjusted for the text collection it was trained on. This leads us to a method for cutting down unnecessary details to make the parsing model smaller. We show that the accuracy of parsers trained on the Penn Treebank becomes less precise when used on different types of writings and subject areas. We present results on sentences with 40 or fewer words across all sections of the Brown text collection, where we get 80.3%/81.0% recall/precision when training only on data from the WSJ collection, and 83.9%/84.8% when training on data from both the WSJ collection and all sections of the Brown collection.

Assigning Time-Stamps To Event-Clauses We explain a method for organizing the events in news stories into a timeline to show how a situation unfolds. We explain the parts of the system that focus on 1. splitting sentences into event-clauses (parts of sentences that describe events) and 2. figuring out both clear and hidden time references. Evaluations show a success rate of 52%, compared to humans. We guess the time based on the most recently assigned date or the date of the article.

Building A Discourse-Tagged Corpus In The Framework Of Rhetorical Structure Theory We share our experience in creating a collection of texts that are marked for their structure and meaning for others to use. Using Rhetorical Structure Theory, a method to analyze how parts of a text connect, we made a large collection with very consistent results by following a clear method and plan. This collection is available to everyone through the Linguistic Data Consortium so researchers can build applications that are based on real examples and focused on how texts are structured. In our Discourse Tree Bank, only 26% of Contrast (differences) relations are shown by signal words or phrases, while in NTC-7, about 70% of Contrast relations are shown this way. Our collection includes 385 Wall Street Journal articles marked according to Rhetorical Structure Theory.

NLTK: The Natural Language Toolkit NLTK, the Natural Language Toolkit, is a collection of free software tools, guides, and practice exercises, offering easy-to-use resources for studying how computers understand human language. NLTK includes methods for both rule-based and data-driven language processing and connects to databases of language examples. Students enhance and modify existing tools, learn organized coding by example, and work with complex models from the start. NLTK, the Natural Language Toolkit, is a set of Python tools providing various language data types, processing tasks, sample texts, and reading materials, along with step-by-step examples, guides, and practice exercises.

Tuning Support Vector Machines For Biomedical Named Entity Recognition We explore using Support Vector Machines (SVMs) to identify names in biomedical texts. To handle the large amount of data in the GENIA corpus (a collection of biomedical texts), we suggest breaking down the non-entity class (words that are not names) into smaller groups using part-of-speech (POS) information, which tells us the role of a word in a sentence. We also try new tools like a word cache (a storage for frequently used words) and states from a Hidden Markov Model (HMM), which is a statistical model learned without direct supervision. Tests on the GENIA corpus show that our method of splitting classes not only makes it possible to train with the GENIA corpus but also makes the results more accurate. The new tools we use also help improve accuracy. We compare our SVM system to another system that uses a method called Maximum Entropy tagging. For identifying protein names, we achieve scores of 0.492 for precision (correctness), 0.664 for recall (completeness), and 0.565 for f-score (a balance between precision and recall). We use a set of features that include lexical information (word meanings), POS tags, affixes (word parts like prefixes or suffixes), and their combinations to recognize and classify terms into general biological categories used in the GENIA project (GENIA, 2003).

Machine Transliteration Of Names In Arabic Texts We present a method for changing names from one language to another using sound and spelling rules with a simple machine process. This method can be learned using relatively small lists of names. We introduce a new spelling-focused method that is more accurate than the latest models that focus on sounds and can be trained with data that is easier to obtain. We use our method to change names from Arabic to English. We measure how accurate our method is by checking if it matches exactly and by asking people for their opinions. We also compare how accurate our method is with how accurate human translators are. We change names in Arabic text to English by combining methods that focus on sounds and spellings, and by re-ranking options using web data, name references, and context. We show that using extra language resources, like web data counts, can greatly improve accuracy. Our spelling-focused method directly changes English letters to Arabic letters and uses probability, which is learned from a small list of English and Arabic names, without needing to know how words are pronounced in English.

Unsupervised Discovery Of Morphemes We introduce two ways to automatically break down words into smaller parts called morpheme-like units without needing pre-labeled data. The approach is especially good for languages with complex word structures, like Finnish. The first way uses a principle called Minimum Description Length (MDL) and works as it goes. The second way uses a technique called Maximum Likelihood (ML) optimization. We check how good the word breakdowns are by comparing them to existing word structure analyses. Tests on Finnish and English text collections show that our methods work well compared to a leading current system. Our approach involves reducing both the list of word parts and the size of all word forms using the MDL cost function, which helps in finding the simplest explanation for the word structures.

Building A Sense Tagged Corpus With Open Mind Word Expert Open Mind Word Expert is a system that uses active learning, which means it learns by asking questions, to collect word meanings from people on the internet. It can be found at http://teach-computers.org. We hope this system will gather a large amount of good quality training data for understanding word meanings at a much cheaper price than the usual way of paying experts called lexicographers. Therefore, we suggest an activity called Senseval-3, where we gather training data using Open Mind Word Expert. If it works well, we can use this process to create a complete collection of word meanings. Lastly, similar to how Wikipedia gathers information, we use the Open Mind Word Expert system to collect word meaning details from volunteers online. We introduced another interesting idea that asks internet users to help create collections of words with their meanings.

Learning A Translation Lexicon From Monolingual Corpora This paper discusses how to create a dictionary for translating words using only separate collections of texts in one language. We use different hints like words that look similar (cognates), words used in similar situations, keeping word similarities, and how often words appear. We tested this by creating a list of German-English nouns. We achieved a 39% accuracy in translating nouns when checked against a matching test set. We automatically create the first basic dictionary using features like words with the same spelling (cognates) and similar word usage.

Improvements In Automatic Thesaurus Extraction The use of semantic resources (tools that understand word meanings) is common in modern NLP (Natural Language Processing) systems, but methods to extract lexical semantics (word meanings) have only recently begun to perform well enough for practical use. We evaluate existing and new similarity metrics (ways to measure how alike something is) for thesaurus extraction, and experiment with the trade-off between extraction performance and efficiency (how well it works versus how fast it works). We propose an approximation algorithm (a simplified method), based on canonical attributes (standard features) and coarse and fine-grained matching (broad and detailed matching), that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty (a slight decrease in how well it works). We show that synonymy extraction (finding words with similar meanings) for lexical semantic resources using distributional similarity (comparing how words are used in context) produces continuing gains in accuracy as the volume of input data increases. We demonstrate that dramatically increasing the quantity of text used to extract contexts significantly improves synonym quality (better matching words with similar meanings). We find the JACCARD measure and the TTEST weight to have the best performance in our comparison of distance measures (ways to measure similarity).

Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms We explain new methods for teaching tagging models, which are another option instead of maximum-entropy models or conditional random fields (CRFs). These methods use Viterbi decoding, a way to look at training examples, together with simple step-by-step updates. We explain the reasoning behind the methods by changing the proof that shows how the perceptron algorithm (a type of learning algorithm) works for sorting problems. We show test results on labeling parts of speech and identifying base noun phrases, and in both cases, our methods work better than those for a maximum-entropy tagger. We explain how the voted perceptron can be used to teach maximum-entropy style taggers and discuss the ideas behind using the perceptron algorithm for ranking tasks. Voted perceptron training tries to reduce the gap between the overall feature list for a training example and the same list for the best possible label for that example according to the current model.

An Empirical Evaluation Of Knowledge Sources And Learning Algorithms For Word Sense Disambiguation In this paper, we assess different information sources and teaching methods used by computers to understand the meaning of words in context, using data from SENSEVAL-2 and SENSEVAL-1 competitions. Our information sources include the type of words next to the target word (part-of-speech), individual nearby words, common word pairings, and the grammatical relationships between words. The computer learning methods we tested include Support Vector Machines (SVM), Naive Bayes, AdaBoost, and decision tree techniques. We share results showing how each information source and learning method contributes to understanding word meanings. Notably, using all information sources together with SVM (a type of computer learning method) leads to better accuracy than the best official results from both SENSEVAL-2 and SENSEVAL-1 data. Our features, or pieces of information we use, include: small groups of nearby words (local context), all words in the context (global context), groups of word types (parts-of-speech), and grammatical details from sentence analysis.

Thumbs Up? Sentiment Classification Using Machine Learning Techniques We look at the challenge of sorting documents by their overall feeling or mood, like deciding if a review is good or bad, instead of by topic. When we use movie reviews as examples, we discover that typical computer methods for learning (machine learning) do a much better job than the basic methods made by people. However, the three computer methods we used (which are Naive Bayes, maximum entropy classification, and support vector machines) are not as good at identifying feelings as they are at sorting topics. We finish by looking into reasons why identifying feelings is harder. We gather movie reviews and label them as good, bad, or neutral based on the reviewer's guidance. We propose that models focusing on individual words do better than those that count word appearances.

A Phrase-Based Joint Probability Model For Statistical Machine Translation We introduce a joint probability model for statistical machine translation that automatically figures out word and phrase matches from bilingual text collections. Translations made using parameters from this joint model are more precise than those using IBM Model 4. Our joint probability model explores the phrase alignment space, simultaneously learning translation dictionaries for words and phrases without worrying about possibly less-than-perfect word matches and shortcuts for phrase extraction.

Generation Of Word Graphs In Statistical Machine Translation The paper talks about a smart system that helps people in Trondheim, Norway, find bus routes using natural language. You can use this system on the Internet, and it has been on the bus company's website since 1999. The system works in two languages and uses a logic system that doesn't depend on any specific language. Between the user asking a question and getting an answer, the system goes through steps like understanding words, sentence structure, meaning, practical reasoning, and searching a database. It can be said that the needed information can be obtained by asking the customer four things: where they are leaving from, where they are going, the earliest they can leave, and/or the latest they can arrive. We create word graphs to help search from the bottom up, using IBM's rules. A word graph is a special kind of diagram where each point represents a part of the possible translation, and each line has a word from the sentence we are translating into, and the lines are given weights or scores by the model.

A Bootstrapping Method For Learning Semantic Lexicons Using Extraction Pattern Contexts This paper explains a step-by-step method called Basilisk that helps create high-quality lists of words with similar meanings (semantic lexicons) for different groups. Basilisk starts with a large collection of texts that are not labeled and a few example words for each group, which it uses to find new words for each group. Basilisk guesses the meaning group of a word by looking at a lot of information from different ways the word is used in sentences. We test Basilisk on six groups of similar-meaning words. The word lists created by Basilisk are more accurate than those made by older methods, with some groups showing a big improvement. We learn many groups of similar-meaning words at once, based on the idea that a word can't belong to more than one group.

Phrasal Cohesion And Statistical Machine Translation There has been a lot of interest in using the way phrases move to make statistical machine translation better. We look into how well phrases stick together across two languages, specifically English and French, and check the specific situations where they don't. We show that even though sometimes phrases don't stick together well, there are many patterns that a statistical machine translation system can use. We also compare three different grammar structures to see which one keeps phrases together the best. We measure how well phrases stick together in the most accurate alignments by counting how often they cross over each other. We compare analyses from a type of grammar that uses tree-like structures, a version with simplified verb phrases (VPs), and structures that show how words depend on each other.

Efficient Deep Processing Of Japanese We present a wide-ranging Japanese grammar written in the HPSG formalism (a framework for understanding sentence structure) with MRS semantics (a way to represent meanings). The grammar is made for real-world use, so being strong and fast is important. It is linked to a tool that tags parts of speech (like nouns and verbs) and splits words. This grammar is being developed to work with multiple languages, needing MRS structures that can be easily compared between languages. Our carefully crafted Japanese HPSG grammar, JACY, gives meaning information and analyzes complex sentence structures in a way that makes sense linguistically.

The Grammar Matrix: An Open-Source Starter-Kit For The Rapid Development Of Cross-Linguistically Consistent Broad-Coverage Precision Grammars The grammar matrix is a free-to-use kit that helps create detailed language rules using HPSGs (Head-driven Phrase Structure Grammars). It uses a type hierarchy (a system to organize language features) to show common patterns across different languages and works with other free tools for building, testing, understanding, and creating language rules. This helps users start quickly and expand to cover more language details needed for strong language processing and understanding. Our LinGO Grammar Matrix project is a collection of reusable language knowledge and a way to give users this knowledge in a form that can be expanded and used to create precise language rules.

The Parallel Grammar Project We discuss the Parallel Grammar (ParGram) project that uses a tool called the XLE parser and grammar development platform to work with six languages: English, French, German, Japanese, Norwegian, and Urdu. The ParGram English LFG is a detailed set of language rules created manually using the XLE platform.

Japanese Dependency Analysis Using Cascaded Chunking In this paper, we introduce a new way to analyze Japanese sentence structure using a step-by-step grouping technique. Traditional methods for analyzing Japanese sentences rely on guessing models, which are not always fast or able to handle large amounts of data. We suggest a new approach that is simple and effective because it processes a sentence by only deciding if the current part connects to the part directly next to it on the right. Tests using a set of sentences from Kyoto University show that our method works better than older systems and makes understanding and learning sentences faster. Our step-by-step grouping model doesn't need guessing the connections between words and processes a sentence in a straightforward manner.

A Comparison Of Algorithms For Maximum Entropy Parameter Estimation Conditional maximum entropy (ME) models are a general machine learning tool that has been used successfully in different areas like computer vision and econometrics, and for many types of sorting tasks in language processing. However, ME models' flexibility comes with a cost. Although estimating parameters (key numbers) for ME models is simple in theory, in real-world language tasks, ME models are usually very large and may contain thousands of adjustable parameters. In this paper, we look at several methods for estimating these parameters, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Surprisingly, the commonly used iterative scaling methods work poorly compared to others, and in all tests, a limited-memory variable metric method did better than the rest. We present the open-source Toolkit for Advanced Discriminative Model, which uses this efficient limited-memory variable metric method.

Introduction To The CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition We explain the CoNLL-2002 shared task, which is about recognizing named entities (like names of people, places, or organizations) in a way that works for any language. We provide some background on the data used and how we measure success, give a general look at the different systems that participated, and talk about how well they did. We specifically focus on recognizing named entities in Spanish and Dutch.

Inducing Translation Lexicons Via Diverse Similarity Measures And Bridge Languages This paper introduces a way to create translation dictionaries for two very different languages without needing paired language texts or an initial bilingual dictionary. The method effectively uses patterns of word appearance over time in news articles, compares word usage in different languages both broadly and in specific contexts, measures differences in spelling using a technique called weighted Levenshtein distance, and looks at how often and suddenly words appear. These techniques are combined using a bridge language, which is a language related to the ones being translated, and a strong method of combining different classification approaches for both Slavic and Northern Indian languages. We build translation dictionaries for languages that don't share common texts by using a bridge language related to the target languages. We create collections of words' contexts around both the starting and target language words and then convert the starting language's word context into the target language's context using a small existing translation dictionary.

An Evaluation Exercise For Word Alignment This paper explains the task, materials, systems that took part, and the comparison results for the shared task on word alignment, which was organized as part of the HLT/NAACL 2003 Workshop on Building and Using Parallel Texts. The shared task included sub-tasks for Romanian-English and English-French and had seven teams from around the world join in. We provide a small set of 447 pairs of sentences that do not overlap, which can be used to check how well word-alignment systems work.

Learning Subjective Nouns Using Extraction Pattern Bootstrapping We explore the idea of creating a tool that can tell if a sentence expresses personal opinions (subjective) or facts (objective) by using lists of opinion-based words (subjective nouns) learned through a process called bootstrapping. Our research goal is to build a system that can tell the difference between sentences based on opinions and sentences based on facts. First, we use two special methods (bootstrapping algorithms) that use patterns to find and learn lists of opinion-based words. Then, we train a simple computer program (Naive Bayes classifier) using these lists of words, conversation features, and hints about opinions found in earlier studies. The special methods found over 1000 opinion-based words, and the tool worked well, correctly identifying 77% of opinion sentences and doing so accurately 81% of the time. We use hand-made pattern guides to find opinion-based words with bootstrapping. We search for opinion-based words in texts that haven't been labeled with two bootstrapping methods that use patterns involving word choice and order and starting points that we choose by hand.

Unsupervised Personal Name Disambiguation This paper presents a set of methods to tell apart people's names that refer to more than one person in text, with little or no guidance. The method uses a grouping technique that doesn't need prior labeling, based on detailed information about people, which is automatically gathered using a process that doesn't depend on the language. The grouped names are then divided and connected to the right people using the gathered information. We check how well this works using both a set of names that have been manually labeled and fake names created automatically. We gather details like birth date or place, job, and family members to help clear up confusion about people's names.

Bootstrapping POS-Taggers Using Unlabelled Data This paper looks into improving part-of-speech taggers (tools that label words in a sentence as nouns, verbs, etc.) by using co-training. In co-training, two taggers (labeling tools) are retrained using each other's results over several rounds. Since the taggers' results can be messy, we need to decide which new examples to include for learning. We explore choosing examples by increasing how much the taggers agree with each other on data that hasn't been labeled yet. This approach has support from both theory and previous studies. Our findings show that when taggers agree more with each other, it can greatly enhance their performance, especially when starting with a small amount of labeled data. More findings reveal that this method is much better than self-training, where a tagger improves using only its own previous results. However, we also find that just retraining with all the new labeled data can sometimes give similar results as the more complex agreement method, but with much less computer work. We see good results with little labeled data but not so good when we have more labeled data. Self-training is defined here as retraining a tagger with its own labeled examples at each step.

Introduction To The CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition We explain the CoNLL-2003 shared task, which is about recognizing names, places, or organizations in text without being limited to a specific language. We provide background information on the data sets used (English and German) and describe how the systems were tested, give a general overview of the systems that participated in the task and talk about how well they performed.

Language Independent NER Using A Maximum Entropy Tagger Named Entity Recognition (NER) systems need to combine a lot of different information to work well. This paper shows that a maximum entropy tagger, which is a type of software, can effectively use this information to find named entities (like names of people or places) with high accuracy. The tagger uses features that can be found in many languages and works well not just for English, but also for languages like German and Dutch. We decide the label (or name) of a word based on the label of the last time we saw that word in a previous sentence in the same document. Our named entity recognizer is used on documents that are already analyzed for parts of speech and sentence structure to find and pick out named entities as possible topics.

Named Entity Recognition Through Classifier Combination This paper presents a method to recognize named entities (like names of people, places, or organizations) by combining the results of four different types of computer programs (classifiers). These classifiers are a strong linear classifier, maximum entropy, transformation-based learning, and a hidden Markov model. When we don't use extra resources like gazetteers (which are lists of names, places, etc.) or other training tools, the combined system achieves a performance score of 91.6F on the English test data. However, when we include these lists and use more general data to train the system, we can reduce the error rate by 15 to 21% on the English data. We tried different ways to combine the results from the four systems and found that a method called robust risk minimization gives the best results.

Named Entity Recognition With Character-Level Models We discuss two named-entity recognition models which use characters and character n-grams (small groups of characters) either exclusively or as an important part of their data representation. The first model is a character-level Hidden Markov Model (HMM) with minimal context information, and the second model is a maximum-entropy conditional Markov model with much richer context features. Our best model achieves an overall F1 score of 86.07% on the English test data (92.31% on the development data). This number represents a 25% error reduction over the same model without word-internal (substring) features. We find that the introduction of character n-gram features improved the overall F1 score by over 20%.

Early Results For Named Entity Recognition With Conditional Random Fields Feature Induction And Web-Enhanced Lexicons Named Entity Recognition (NER) is a way to identify important information like names of people, places, or organizations in text. The study uses a method called Conditional Random Fields (CRF), which is a type of machine learning that helps in finding patterns in data. Feature induction in CRF is about automatically finding the best characteristics or features in the data to improve accuracy. The study also uses web-enhanced lexicons, which are lists of words or phrases collected from the internet, to help the system recognize more entities accurately.

Hedge Trimmer: A Parse-And-Trim Approach To Headline Generation This paper introduces Hedge Trimmer, a system for making newspaper headlines using smart language-based rules to pick a possible headline. We show tests that prove our method works by choosing words straight from a story to make a headline. We also share test results showing our smart language method works better than an HMM-based model (a different method), using both opinions from people and automatic tools to compare them. Our method focuses on picking one or two important sentences from the document and changing them using language rules to make the summary shorter.

Use Of Deep Linguistic Features For The Recognition And Labeling Of Semantic Arguments We use deep linguistic features, which are detailed language characteristics, to predict roles in sentences, and these work much better than simpler features. We also show that using a simple parser that finds these detailed features works almost as well as using a full parser that only finds basic features. We believe that detailed language features from FrameNet, a database of word meanings, help to successfully assign PropBank roles, which are specific roles in sentences, to parts of sentences. We use a specific method of breaking down sentence structures for Semantic Role Labeling (SRL), which is identifying what parts of a sentence do. Instead of using the usual sentence structure features in SRL, we use the path in a basic tree from the main action to the part of the sentence it affects.

Identifying Semantic Roles Using Combinatory Categorial Grammar We introduce a system that automatically identifies PropBank-style semantic roles, which are different parts played by words in a sentence, using results from a statistical parser (a tool that breaks down sentences into parts) for Combinatory Categorial Grammar (a type of grammar). This system works as well as one based on a traditional Treebank parser (another type of sentence analyzing tool) and does even better with main parts of a sentence. We discovered that using details from a Combinatory Categorial Grammar setup makes recognizing key parts of sentences more accurate.

A General Framework For Distributional Similarity We present a general framework for distributional similarity based on the concepts of precision (how accurate the results are) and recall (how many relevant results are found). Different settings within this framework can mimic existing ways to measure similarity, as well as create new ways that haven't been tried yet. We demonstrate that the best settings perform better than two current top methods for measuring similarity in tests using common and rare nouns. We propose a general framework for distributional similarity that consists of notions of precision and recall.

Learning Extraction Patterns For Subjective Expressions This paper introduces a step-by-step process that teaches how to find detailed patterns for opinion-based expressions. Accurate tools label unmarked data to automatically create a large set of examples, which is then used by a learning algorithm to find these patterns. The patterns found are then used to spot more opinion-based sentences. This step-by-step process learns many opinion-based patterns and finds more of them while staying very accurate. We create a very accurate tool for continuous sentences by counting strong and weak opinion words in current and nearby sentences. We present a method to learn opinion-based patterns that fit certain sentence structures using a very accurate tool for sentence-level opinion detection and a large set of unmarked text.

Towards Answering Opinion Questions: Separating Facts From Opinions And Identifying The Polarity Of Opinion Sentences Opinion question answering is a difficult task for computers that understand human language. In this paper, we talk about an important part of a system that answers opinion questions: telling the difference between opinions (what people think or feel) and facts (what is true), both for entire documents and individual sentences. We introduce a method called a Bayesian classifier to separate documents that mostly contain opinions, like opinion pieces, from regular news articles. We also explain three methods that don't need labeled data to solve the tougher task of finding opinions in sentences. Additionally, we introduce our first model to decide if opinion sentences are positive (good) or negative (bad) based on the main viewpoint being shared in the opinion. We share results from many news articles and a review by people of 400 sentences, showing that we are very accurate in classifying documents (over 97% accuracy) and quite good at finding and classifying opinions in sentences as positive, negative, or neutral (up to 91% accuracy). At the sentence level, we aim to sort opinion sentences as positive or negative based on the main viewpoint in those sentences.

Improved Automatic Keyword Extraction Given More Linguistic Knowledge In this paper, experiments on automatic keyword extraction from summaries using a machine learning method that learns from examples are explained. The main idea is that by adding language understanding (like grammar details), instead of just using numbers (like how often a word appears and word groups), we get better results compared to keywords chosen by experts. Specifically, using noun phrases (NP-chunks) is more accurate than using simple word groups (n-grams), and by including the part of speech (POS) tags for words, we see a big improvement in results, no matter how the words are chosen. We suggest a system for pulling out keywords from summaries that uses learning from examples with word and grammar details, which has shown to be much better than past methods.

Transliteration Of Proper Names In Cross-Lingual Information Retrieval We deal with the challenge of converting English names into Chinese writing to help with understanding speech and text in different languages. We show how using statistical machine translation methods can "translate" how an English name sounds into a series of sounds (called initials and finals) used in Chinese pronunciation. After that, we use another statistical translation model to turn these sounds into Chinese characters. We also test how well this method works by retrieving Mandarin spoken documents using English text searches. We use a method called the noisy channel model, which is a way to handle uncertain information.

The First International Chinese Word Segmentation Bakeoff This paper shares the outcomes from the First International Chinese Word Segmentation Bakeoff, a competition sponsored by ACL-SIGHAN, held in 2003 and announced at the Second SIGHAN Workshop on Chinese Language Processing in Sapporo, Japan. We explain why there was a need for an international segmentation contest (since there had been two contests only within China before), and we provide the results of this first international contest, examine these results, and offer some suggestions for the future.

Chinese Word Segmentation As LMR Tagging In this paper, we present methods for dividing Chinese text into words using something called LMR tagging. Our LMR taggers use a model called the Maximum Entropy Markov Model, and we combine the results with a method called Transformation-Based Learning to improve accuracy. Our system correctly segments words 95.9% of the time on a test from Academia Sinica and 91.6% on a test from Hong Kong City University. We also introduce a new method for Chinese word segmentation that classifies each character by giving it a tag to show its position in a word.

HHMM-Based Chinese Lexical Analyzer ICTCLAS This document shares the results from the Institute of Computing Technology, Chinese Academy of Sciences (CAS) in the First International Chinese Word Segmentation competition organized by ACL SIGHAN. The authors introduce the HHMM-based (a type of statistical model) framework of our Chinese word analyzer ICTCLAS and explain how it works across six different tests. Then they show the evaluation results and provide more analysis. Testing on ICTCLAS shows that it works well compared to others. Compared with other systems, ICTCLAS was ranked at the top in the CTB and PK closed track tests. In the PK open track test, it got second place. The ICTCLAS BIG5 version, which is a different text format, was created from the GB version in just two days and performed well in two BIG5 closed track tests. Through the first competition, we learned more about the progress in Chinese word separation and are more confident in our HHMM-based method. At the same time, we found some issues during the evaluation. The competition was fun and useful.

A Statistical Approach To The Semantics Of Verb-Particles This paper explains a method that uses patterns in language to understand the meaning of verb-particle combinations (like "put up" or "make off"). We first explain a system for creating and testing these models. Next, we discuss how we use certain methods that apply statistics from large collections of texts to figure out what these verb-particle combinations mean.

Detecting A Continuum Of Compositionality In Phrasal Verbs We explore using a computer-generated thesaurus to measure how much different multiword verbs (like phrasal verbs in English) are made up of their parts. We find these verbs using a reliable computer program. We look at different ways to measure this by checking words that are similar to the phrasal verb. Sometimes we also look at similar words for the simple version of the verb. We find that some of these methods match well with how humans rate these verbs in terms of being made up of their parts. We also find that while these human ratings do match some common statistics used to find multiword phrases, they match even better with the computer-generated thesaurus.

An Empirical Model Of Multiword Expression Decomposability This paper introduces a specific way of understanding how multiword expressions (like phrases made of several words) can be broken down into their parts, using a method called latent semantic analysis (LSA), which helps understand the meaning of words. We use LSA to measure how similar a phrase is to the individual words it contains, suggesting that if they are more similar, the phrase can be broken down more easily. We test this idea on English phrases made of two nouns or a verb and a particle (like "pick up"), and see how well it matches with a tool called WordNet, which groups words by meaning. By looking at average meanings of word groups based on similarity, we show that these similarities match up with the meanings in WordNet. We looked into how to pull out meaning from phrases because we wanted to understand how phrases can be split into their parts. We suggest an LSA-based model to measure how phrases can be broken down by checking how similar they are to their individual words, with more similarity showing they can be broken down more.

Incrementality In Deterministic Dependency Parsing Deterministic dependency parsing is a strong and fast method for analyzing the structure of sentences in any natural language text. In this paper, we look into its ability to process sentences bit by bit as they are read and find that it cannot fully do this in its current form. However, we also demonstrate that we can reduce the times it needs to stop and look at the whole sentence by picking the best parsing method. We back this up with tests showing that the method can handle sentences in parts for 68.9% of the time when we tried it on some Swedish text.

Senseval-3 Task: Automatic Labeling Of Semantic Roles The SENSEVAL-3 task to perform automatic labeling of semantic roles was created to promote research and use of the FrameNet dataset. This task was built on the large increase in FrameNet data since the first study of automatic role labeling by Gildea and Jurafsky. The FrameNet data offer a large, reliable set of information ("gold standard") used in word meaning research and as a base for further use in language processing technology (NLP). Eight teams took part in the task, with a total of 20 attempts. Talks among participants during the task's development and evaluation of their attempts led to a successful task. Participants used many different methods, exploring various parts of the FrameNet data. They achieved results that greatly improved from Gildea and Jurafsky’s original study. Importantly, their work has helped make the complex FrameNet dataset easier to use. They have clearly shown that FrameNet is a significant resource for language study and will allow for much more research and use in NLP applications in the future. We conduct an evaluation exercise in the Senseval-3 workshop.

The Senseval-3 English Lexical Sample Task This paper explains the task details, materials used, teams involved, and comparison of results for the English word meaning task, which was part of the SENSEVAL-3 testing event. The task involved 27 teams from different countries, with 47 systems in total.

The English All-Words Task We talk about our experience in getting the sense-tagged corpus (a collection of texts where words are labeled with their meanings) ready for the English all-words task, and we present the scores in a table.

ROUGE: A Package For Automatic Evaluation Of Summaries ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes ways to automatically check how good a summary is by comparing it to other perfect summaries made by people. These methods count the number of similar parts like short word sequences (n-gram), word order, and word pairs between the summary created by a computer and the perfect summaries made by people. This paper introduces four different ROUGE methods: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their tests. Three of them have been used in the Document Understanding Conference (DUC) 2004, a big event for testing summaries, sponsored by NIST.

Biomedical Named Entity Recognition Using Conditional Random Fields And Rich Feature Sets Biomedical Named Entity Recognition is a process of identifying specific names or terms in the field of medicine and biology. It uses a method called Conditional Random Fields, which is a mathematical model to predict sequences, along with detailed sets of features or characteristics to improve accuracy.

The ICSI Meeting Recorder Dialog Act (MRDA) Corpus We describe a new collection of more than 180,000 hand-labeled dialogue act tags (labels that explain parts of conversation) and related adjacency pair annotations (connections between parts of a conversation) for about 72 hours of talks from 75 real meetings. We provide a short overview of the labeling system and process, statistics on how much different people agree on labels, overall distribution numbers, a description of extra files that come with the collection, and details on how to get the data.

A Linear Programming Formulation For Global Inference In Natural Language Tasks Given a collection of separate random variables that show the outcomes of local predictors in natural language, like named entities (names of people, places, etc.) and relationships (how these entities are connected), we want to find the best overall assignment to these variables while considering general rules or limitations that are not in order. These rules include things like what type of arguments a relationship can have and how different relationships can be active at the same time. We create a method using linear programming (a mathematical way to find the best outcome) to solve this problem and test it while learning about named entities and relationships at the same time. Our method helps us quickly include specific rules for the task or field when making decisions, leading to big improvements in how accurate and human-like the conclusions are. We use ILP (Integer Linear Programming, a specific type of linear programming) to handle the problem of identifying named entities and relationships together. We used an ILP model to assign roles to the entities in a sentence and to recognize the relationships between them at the same time. We explained a framework based on classification (sorting things into categories) that learns to identify named entities and relationships together.

Word Sense Discrimination By Clustering Contexts In Vector And Similarity Spaces This paper carefully compares methods that automatically figure out different meanings of a word by grouping examples of the word from plain text using both vector and similarity spaces. Each example's context is shown as a vector in a space with many features. The grouping is done by directly clustering these context vectors in the vector space and also by finding how similar the vectors are to each other and then clustering them in the similarity space. We use two different ways to show the context where a word appears. First-order context vectors show the context as a group of features that appear together. Second-order context vectors give an indirect view by averaging vectors of words that appear in the context. We test the grouped results by doing experiments with examples of 24 words from SENSEVAL2 and the well-known collections of Line, Hard, and Serve with meanings already tagged.

Memory-Based Dependency Parsing This paper shares the results of tests using memory-based learning to help a simple rule-following parser for any kind of natural language text. Using data from a small collection of Swedish sentences organized in a tree structure, classifiers (tools) that predict the parser's next move are created. The correctness of such a classifier is checked using separate test data from the tree collection, and its effectiveness as a parser helper is tested by analyzing the separate part of the tree collection. The results show that memory-based learning greatly improves over an earlier probability-based model that used complex math, and adding word-related features makes it even more accurate.

Models For The Semantic Classification Of Noun Phrases This paper presents a method for finding meaning connections in noun phrases. A learning method, called semantic scattering, is used to automatically label complex noun phrases, possessive phrases, and adjective noun phrases with the correct meaning connection. We suggest a 35-category system to classify these connections in different phrases. We suggest a method called semantic scattering for understanding noun compounds (NCs).

The NomBank Project: An Interim Report This paper talks about NomBank, a project that will show how common nouns (naming words) are used in sentences in the Penn Treebank II collection of texts. NomBank is part of a bigger plan to add more detailed labels to the Penn Treebank II text collection. The University of Pennsylvania's PropBank, NomBank, and other labeling projects together aim to create better tools for automatically understanding text. This paper explains the NomBank project in detail, including its features and how it was made. We provide simple labels for some of the possessive forms (showing ownership) in the Penn Treebank, but only those that fit their rules.

The Language Of Bioscience: Facts Speculations And Statements In Between We look into how uncertain or speculative language is used in summaries from MEDLINE, a medical research database. Results from a manual labeling test show that identifying speculative sentences can be done accurately by people. Additionally, a test using computer methods also indicates that reliable computer methods might be created. We also present observations on how this language is spread and discuss how a system that can identify speculative language could be used. We focus on explaining the problem, exploring labeling challenges, and describing possible uses rather than the details of the machine learning (ML) approach, and show some results using a manually made substring matching tool and a supervised Support Vector Machine (SVM), which is a type of computer algorithm, on a collection of Medline summaries. We look into challenges with labeling speculative language in biomedical fields and describe possible uses. We present a study on labeling uncertain claims in biomedical documents.

Integrated Annotation For Biomedical Information Extraction We explain a method for getting important information in two areas of medical research: creating new medicines and studying cancer genes. We've made a system that includes different levels of text labeling: a Treebank that shows sentence structure, a Propbank that shows how actions connect with things, and labeling of items and how they relate to each other. A key part of this method is correctly identifying items as parts of relationships, which helps combine item labeling with sentence structure and still allows labeling and finding more complicated events. We are teaching computer programs to recognize patterns using this labeling to help find information and make the labeling better.

Max-Margin Parsing We introduce a new method for sentence structure analysis that is inspired by the idea of creating a clear distinction between categories, similar to how support vector machines work. Our method breaks down the problem in a way similar to traditional methods used for sentence analysis. This means it can quickly learn to identify the entire range of possible sentence structures, not just the best few options. Our models can use any details from input sentences, allowing them to include important word information without making the process more complicated by focusing on specific word roles. We offer a fast method for learning these models and provide evidence that they perform better than a simple model and a more complex word-based model. We proposed a way for analyzing sentence structures that uses a step-by-step approach to solve problems of decoding and estimating values.

:VerbOcean: Mining The Web For Fine-Grained Semantic Verb Relations Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks. We present a semi-automatic method for extracting detailed meaning connections between verbs. We find similarity, strength, opposite meanings, enablement (how one action allows another), and time-order relations between pairs of closely related verbs using word and sentence patterns found on the Web. On a set of 29,165 closely related verb pairs, our extraction method was correct 65.5% of the time. Analysis of mistakes shows that for the relation strength we were correct 75% of the time. We provide the resource, called VERBOCEAN, for download at http://semantics.isi.edu/ocean/. We introduce a 5-class set, designed specifically for describing how verbs relate in meaning. We use patterns to find a set of relations between verbs, such as similarity, strength, and opposite meanings.

Scaling Web-Based Acquisition Of Entailment Relations Paraphrase recognition, which means identifying when different sentences mean the same thing, is an important part of understanding language. Many computer programs that deal with language would benefit from having a large database of different ways to say the same thing. However, the current methods for collecting these paraphrases don't work well when trying to gather a lot of information. We introduce a new automatic method for finding entailment relations, which are a broader version of paraphrases, from the internet. Our focus is on making this method work on a larger scale and in more general situations compared to previous attempts, with the goal of creating a complete database of these relationships. Our system starts with a list of verbs and looks online for patterns that show how these verbs are used in related ways. Tests show that our method works well towards our main goal, handling larger amounts of data better than previous internet-based approaches. We explain the TEASE method, which is our way of finding these related patterns from the web.

Bilingual Parsing With Factored Estimation: Using English To Parse Korean We explain how basic and well-known statistical models, like tools that identify sentence structure (statistical dependency parsers), rules for building sentences (probabilistic context-free grammars), and methods for matching words between languages (word-to-word translation models), can be effectively combined into one bilingual tool that simultaneously finds the best English sentence structure, Korean sentence structure, and word matching between the two languages, where these hidden parts all influence each other. The model used for finding sentence structures is completely divided into the two parsers and the translation model, allowing each part to be adjusted separately. We test our bilingual parser on the Penn Korean Treebank and compare it to several basic systems, showing improvements in understanding Korean with very little labeled data. We suggested combining an English parser, a word matching model, and a Korean sentence rules parser trained from a small number of Korean sentence structures into one unified model that uses a specific method called a log linear model.

Mining Very-Non-Parallel Corpora: Parallel Sentence And Lexicon Extraction Via Bootstrapping And EM We present a method that can find matching sentences from very different collections of texts, called "very-non-parallel corpora," better than older methods called "comparable corpora." We do this by using a process called bootstrapping on top of a system known as IBM Model 4 EM. In the first step, like older methods, we look for similar documents and then find matching sentences and new word translations. But unlike older methods, we add a repeating process based on the idea of "find-one-get-more," which means if one matching sentence is found, more can be in the same document even if it's not very similar. We keep matching documents based on the sentences we find and keep improving the process until it works well. This new "find-one-get-more" idea helps us find more matching sentences from different documents. Tests show our method is almost 50% better than the basic method without repeating. We also show that our method improves the results of the IBM Model 4 EM, which is better than the older Model 1 but struggles with very-different text collections.

Calibrating Features For Semantic Role Labeling This paper examines the elements used in semantic role tagging (a method for understanding sentence roles) and shows that the information from the input, usually a structured sentence diagram called a syntactic parse tree, hasn't been fully used. We suggest more elements, and our tests show that these new elements lead to noticeable improvements in the tasks we did. We also show that different tasks need different elements. Lastly, we demonstrate that by using a Maximum Entropy classifier (a type of machine learning model) with fewer elements, we achieved results similar to the best results previously reported with SVM models (another type of machine learning model). We think this clearly shows that creating elements that capture the right information is important for improving semantic analysis (understanding language meaning).

Unsupervised Semantic Role Labeling We introduce a method that doesn't need pre-labeled data to identify the roles of words connected to verbs in sentences. Our process starts with clear role assignments and keeps improving by updating the probability model, which helps make future decisions. A unique part of our method is using information about verbs, their positions in sentences, and groups of similar nouns to improve the probability model. We successfully reduce errors by 50-65% compared to a baseline that uses some prior knowledge, showing promise for a task that typically needs a lot of manually created training data. We perform this task without supervision by using specially created verb dictionaries instead of relying on pre-labeled data for training.

Monolingual Machine Translation For Paraphrase Generation We use tools from statistical machine translation (SMT) to create new ways to say the same sentences in one language. The system learns from lots of sentence pairs that are automatically taken from grouped news articles found online. We measure Alignment Error Rate (AER) to check how good the collection of sentences is. A simple tool called a monotone phrasal decoder makes changes based on the context. Human evaluation shows that this system works better than basic methods for generating paraphrases and, unlike past efforts, it covers more and can handle larger tasks than the best current methods. We developed a model to generate paraphrases from a set of similar sentences in one language using a statistical machine translation method, where the language model checks if the new sentences are grammatically correct. We introduce a complete paraphrasing system inspired by a method called phrase-based machine translation that can both find new paraphrases and use them to create new sentences.

Applying Conditional Random Fields To Japanese Morphological Analysis This paper discusses how to analyze Japanese words using a method called conditional random fields (CRFs). Previous studies using CRFs assumed that the start and end of words were already known. But in Japanese, it's not always clear where one word ends, and another begins, so we can't use CRFs directly without adjustments. We explain how CRFs can be used even when it's hard to tell where words start and end. CRFs help solve long-standing issues in analyzing Japanese language using large sets of text data or statistics. First, they allow for flexible ways to categorize words in layers. Second, they reduce errors related to labeling and word length. We tested CRFs with a standard set of Japanese text data and compared our results to older methods called HMMs and MEMMs. Our findings show that CRFs not only address old problems but also work better than HMMs and MEMMs. We looked into dividing Japanese text into words and identifying parts of speech using CRFs and rules for dealing with unknown words.

Chinese Part-Of-Speech Tagging: One-At-A-Time Or All-At-Once? Word-Based Or Character-Based? Chinese part-of-speech (POS) tagging is about giving each word in a Chinese sentence a label that tells what part of speech it is, like noun or verb. But since Chinese sentences don't have clear spaces between words, we first need to split the sentence into words, which is called word segmentation. We can do the tagging after this splitting step (one-at-a-time approach), or we can do both splitting and tagging together in one step (all-at-once approach). We can also choose to tag based on whole words by looking at nearby words (word-based), or tag based on individual characters by looking at nearby characters (character-based). This paper explores these different methods of processing and feature use for Chinese POS tagging, using a maximum entropy framework, which is a way to predict outcomes based on known data. We found that doing everything at once with character-based tagging works best, but doing it one step at a time with character-based tagging is almost as good, and it's faster to train and use. In our study, we also created an advanced Chinese word splitter that works better than the top performers from the SIGHAN 2003 competition in most tests.

Adaptation Of Maximum Entropy Capitalizer: Little Data Can Help A Lot A new method for adjusting maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is introduced. This method helps computers automatically capitalize text that is all in lowercase. Automatic capitalization is important because speech recognition systems need to make text look correct with capital letters, and word processors also fix capitalization along with spelling and grammar. Capitalization can also help in finding names and translating languages. A basic capitalizer trained on 20 million words from the Wall Street Journal (WSJ) of 1987 is adjusted for two sets of news text from 1996: one from ABC Primetime Live and the other from NPR/CNN. The WSJ capitalizer works 45% better than a simple method when tested on a 1994 WSJ set. On different news data, it performs 60% better than the simple method, and using just a little bit of matching news text (25-70k words) improves it by 20-25%. An error rate of 1.4% is achieved for news data. Using our method with just 140,000 words of specific training data gives more improvement than using up to 20 million words of general training data. Our method for transferring learning in Maximum Entropy models involves changing certain values in a mathematical formula. We use the settings from an old model as a starting point when training a new model on different data.

A Boosting Algorithm For Classification Of Semi-Structured Text The research in sorting text by category has grown from just figuring out the topic to more difficult tasks like figuring out opinions or the way something is said. Sadly, these harder tasks can't be done well with the old method of just counting words, so we need a better way to represent text. So, we need to create learning programs that can understand the structure of texts. In this paper, we suggest a Boosting algorithm, which is a method to improve prediction, that can find small structures hidden in texts. Our idea includes i) simple decision rules that use parts of sentence trees as features and ii) the Boosting method which uses these tree-based simple rules as basic tools to learn. We also talk about how our method relates to SVMs (Support Vector Machines), a type of machine learning model, with tree kernel, which is a way to measure similarity using trees. Two tests on finding opinions or modality show that using tree parts is important. We use the BACT learning method to effectively learn tree parts that are useful for identifying what comes before something (antecedent) and finding hidden subjects in sentences (zero pronoun detection).

LexPageRank: Prestige In Multi-Document Text Summarization Multidocument extractive summarization uses the idea of sentence centrality, which helps find the most important sentences in a document. Centrality usually means how often important words appear or how similar a sentence is to a main idea sentence. We are now looking at a method to figure out sentence importance using eigenvector centrality (prestige), which we call LexPageRank. In this model, we create a sentence connection chart using cosine similarity, a way to measure how similar sentences are. If the similarity between two sentences is higher than a set level, we connect them in the chart. We tested our method with data from DUC 2004. The results show that our method does better than the centroid-based summarization and is very successful compared to other summarization systems. We propose LexPageRank, a way to measure sentence importance using eigenvector centrality.

Statistical Significance Tests For Machine Translation Evaluation Automatic evaluation methods for Machine Translation (MT) systems, like BLEU, METEOR, and the similar NIST method, are becoming more crucial in MT research and development. This paper shows a comparison of n-gram-based automatic MT evaluation methods using significance tests, which help determine how meaningful the results are. Statistical significance tests use bootstrapping methods (a way to estimate accuracy by repeatedly sampling data) to judge how reliable automatic machine translation evaluations are. Based on this reliability check, we explore the features of different MT evaluation methods and how to build reliable and efficient evaluation groups.

TextRank: Bringing Order Into Texts In this paper, we introduce TextRank - a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications. In particular, we propose two new methods that don't require pre-labeled data for finding important words and sentences, and show that the results are as good or better than results from other well-known tests. We propose TextRank, which is one of the most well-known graph-based methods for extracting key phrases. We propose the TextRank model to rank important words based on how often they appear together with other words.

Sentiment Analysis Using Support Vector Machines With Diverse Information Sources This paper presents a method for understanding opinions or emotions in text using support vector machines (SVMs), which are a type of computer program. It combines different types of useful information, like ratings for phrases and adjectives and, when possible, knowledge about the subject of the text. The new method is combined with a simple word-counting model known as a unigram model, which has worked well before (Pang et al., 2002), and versions of this model that group similar words together. Tests on movie reviews from the Internet Movie Database show that mixing these different approaches using SVMs provides better results than any previous methods. Additional tests on a smaller set of music reviews, which were labeled by hand to show their topics, also show that adding topic information to these models might make them even better.

A Statistical Semantic Parser That Integrates Syntax And Semantics We introduce a learning tool called SCISSOR that converts natural-language sentences into a detailed, formal language that explains their meaning. It first uses a combined statistical parser to create a tree structure that includes both syntax (sentence structure) and semantics (meaning) for each part of the sentence. Then, a step-by-step process is used to turn this tree into a final representation of meaning. We test the system in two areas: a database interface that uses natural language and a program that interprets coaching instructions for robotic soccer. Our experiments show that SCISSOR creates more accurate representations of meaning than several older methods. We introduced a method, SCISSOR, where creating meaning representations is guided by sentence structure.

Generalized Inference With Multiple Semantic Role Labeling Systems We present a way to assign roles to words in a sentence (semantic role labeling or SRL) by using the results from different systems that identify roles, then combining these results into one clear outcome. This is done by solving a best-fit problem (optimization problem). The optimization step uses a method called integer linear programming, which considers both the suggestions from the systems and specific rules that need to be followed. This step helps tidy up the results and make sure the final role assignments make sense. We show that this method greatly improves how well SRL works overall. We used the results from different SRL systems (each working on a single structure of a sentence) and combined them into a clear outcome by solving this best-fit problem.

Syntactic Features For Evaluation Of Machine Translation Automatic evaluation of machine translation, based on computing n-gram similarity (comparing short sequences of words) between system output and human reference translations, has revolutionized the development of MT (Machine Translation) systems. We explore the use of syntactic information (grammar-related details), including constituent labels (parts of sentence like subject or object) and head-modifier dependencies (how words relate to each other), in computing similarity between output and reference. Our results show that adding syntactic information to the evaluation metric (method of measuring) improves both sentence-level and corpus-level (overall text) correlation with human judgments. We measure the syntactic similarity (grammar-based comparison) between MT output and reference translation. We used syntactic structure (sentence grammar layout) and dependency information to go beyond the surface level matching (basic word comparison).

METEOR: An Automatic Metric For MT Evaluation With Improved Correlation With Human Judgments We describe METEOR, a tool that automatically checks how good machine translations are by comparing them to reference translations done by humans. It works by matching single words or parts of words (unigrams) between the machine and human translations, based on how they look, their root forms, and their meanings. METEOR can also be adapted to use more advanced matching techniques. After finding all the matches, METEOR calculates a score using three things: how many unigrams match (unigram-precision), how many of the important ones match (unigram-recall), and how well the order of matched words follows the human translation. We test METEOR by seeing how its scores match up with human opinions of translation quality. We measure the Pearson R correlation, which is a statistical measure, between METEOR's scores and human quality ratings for Arabic-to-English and Chinese-to-English translations from a 2003 dataset. By checking each part of the translation, we find METEOR has a 0.347 correlation for Arabic and 0.331 for Chinese, which is better than just using simple match counts. We also do tests to see how different parts of METEOR contribute to its success.

Measuring The Semantic Similarity Of Texts This paper presents a knowledge-based method for measuring how similar the meanings of texts are. While there is a lot of previous research on finding how similar the meanings of concepts and words are, using these word-focused methods to find text similarity hasn't been fully explored yet. In this paper, we introduce a method that combines measurements of word-to-word similarity into a way to measure text-to-text similarity, and we show that this method works better than the traditional methods that rely on matching exact words. We proposed a mixed method by combining six existing knowledge-based methods.

Better K-Best Parsing We talk about how important k-best parsing is for recent uses in natural language processing (how computers understand human language) and create fast methods for finding the best k results (k-best trees) using hypergraph parsing (a complex structure to analyze language). To prove how fast, scalable (can handle large data), and accurate these methods are, we perform tests using Bikel’s version of Collins’ PCFG model (a type of grammar model), and on Chiang’s CFG-based decoder (a tool for translating languages in a structured way). We specifically show how the better results from our methods can improve the outcomes of parse reranking systems (tools that reorder results) and other uses.

A Classifier-Based Parser With Linear Run-Time Complexity We introduce a tool that breaks down sentences into parts quickly. It uses a simple method to decide how to break down sentences, not by following strict rules, but by using a decision-making tool. This method is similar to an older technique by Nivre and Scholz (2004) but is more advanced. We demonstrate that by choosing the right details to focus on, this straightforward method can be just as accurate as more complicated ones. We tested our tool on a specific section of a well-known collection of language data and found it to be quite accurate, with scores of 87.54% and 87.61%. We suggest this method to figure out the structure of sentences.

Extracting Opinions Opinion Holders And Topics Expressed In Online News Media Text This paper introduces a way to find an opinion, who holds it, and what it's about, using a sentence from online news articles. We explain a method that uses the meaning structure of a sentence, focusing on a verb or adjective that shows opinion. This approach involves identifying roles in the sentence to determine who holds the opinion and what it is about, using a resource called FrameNet. We break down the task into three steps: finding the word that shows opinion, labeling related parts of the sentence, and then identifying who has the opinion and what it's about from these labeled parts. To cover more cases, we also use a grouping method to guess the most likely situation for a word not defined in FrameNet. Our tests show that our system works much better than a simple starting point. We identify who holds opinions and what they target using a method of labeling sentence parts.

Automatic Identification Of Non-Compositional Multi-Word Expressions Using Latent Semantic Analysis Making use of latent semantic analysis (a method to understand the meaning of words using math), we explore the idea that the nearby words in a sentence can help identify groups of words that don't mean what you'd expect from the individual words. We suggest that comparing how often these word groups appear together compared to their separate parts can tell us how much they make sense together. We present experiments that show when this similarity is low, it often means the group of words has a unique meaning. We create a supervised method (a guided approach) where we figure out the meaning of a word group when it's used normally and when it has a special meaning, using example data. We used a supervised learning method to tell apart when a group of words is used in the usual way or in a special way (in German text) by using information about nearby words, understood through Latent Semantic Analysis (LSA) vectors.

SPMT: Statistical Machine Translation With Syntactified Target Language Phrases We introduce SPMT, a new type of statistical Translation Models that use structured (syntactified) phrases in the language being translated to. The SPMT models perform better than the latest standard phrase-based model by 2.64 Bleu points on the NIST 2003 Chinese-English test set and by 0.28 points on a human-based quality measure that rates translations from 1 to 5.

Phrasetable Smoothing For Statistical Machine Translation We talk about different ways to make the phrasetable smoother in Statistical Machine Translation (MT) and share results from various translation situations. We demonstrate that using any smoothing method is better than using relative-frequency estimates, which are commonly used. The best smoothing techniques lead to consistent improvements of about 1% according to the BLEU metric (a way to measure translation quality).

Domain Adaptation With Structural Correspondence Learning Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are from the same type of data set. For many NLP tasks, however, we face new areas where labeled data (data that is already categorized or tagged) is limited or not available. In such cases, we try to adapt existing models from a data-rich area to a data-poor area. We introduce structural correspondence learning to automatically create connections between features (important parts) from different areas. We test our method on part of speech tagging (identifying the role of words, like noun or verb) and show it performs better with different amounts of training data from both areas, as well as improves the accuracy of understanding the target area using our improved tagger. Following (Blitzer et al, 2006), we show how structural correspondence learning can be applied to non-projective dependency parsing (a method to understand sentence structure) (McDonald et al, 2005). Our method is to train a separate parser for data outside the target area and use this to generate extra features for both labeled and unlabeled data within the target area. We introduce SCL, a way to represent features effectively in complex NLP problems, like part-of-speech tagging and sentiment analysis (understanding opinions). We use the multitask algorithm of (Ando and Zhang, 2005) for adapting NLP tasks to new areas. We add predicted pivots (words appearing in both the original and new areas) to the source domain (original area) labeled data to adapt a POS tagger to a new area.

Incremental Integer Linear Programming For Non-Projective Dependency Parsing Integer Linear Programming (ILP), a mathematical approach for decision-making, has recently been used for decoding in many models that predict outcomes, to make sure all rules are followed. However, in specific tasks like non-projective dependency parsing (a type of language analysis) and translating languages, fully setting up the problem using ILP makes it too complicated to solve. We introduce a method that solves the problem step-by-step, avoiding these overly complex ILP issues. This method is used for analyzing Dutch sentence structures, showing that adding rules based on language understanding significantly improves results compared to the best current methods. In analyzing sentence structures (dependency parsing), we explore using ILP to include overall language rules. Our work shows that ILP can efficiently handle very large problems if tackled in steps. We demonstrate that even extremely large problems can be solved efficiently with ILP tools if a Cutting-Plane Algorithm, a step-by-step method from 1954, is used. We approach the Maximum A Posteriori (MAP) problem, which finds the most likely sentence structure, by starting with a simplified version, solving it, and only adding more rules if needed.

Get Out The Vote: Determining Support Or Opposition From Congressional Floor-Debate Transcripts We look into whether we can tell from the records of U.S. Congressional floor debates if the speeches are for or against proposed laws. To tackle this, we use the fact that these speeches are part of a larger conversation. This lets us gather information from how the parts of the discussion relate to each other, like if one person's speech agrees with another's opinion. We find that using this extra information helps a lot more than just looking at each speech alone. We introduce a method using support vector machines (a type of computer program) to decide if the speeches show support or opposition to proposed laws, using the records from these debates.

Fully Automatic Lexicon Expansion For Domain-Oriented Sentiment Analysis This paper suggests a way to automatically create a list of words or phrases that show positive or negative feelings in a specific area, without needing human guidance. These words or phrases are called polar atoms, which are the smallest understandable parts that show if a sentence is positive or negative. To find these polar atoms, we look at context coherency, which is the pattern of similar positive or negative words appearing together in a text. By looking at how often and how accurately these patterns show up in a large collection of text, we can identify the right polar atoms without having to adjust any settings manually. Experiments show that using this automatically created list, we can correctly identify if sentences are positive or negative 94% of the time, and this method works well across different topics and starting points. We confirm that sentences with the same positive or negative feelings often appear together, making the text flow smoothly. We suggest a method to automatically expand an initial list of opinion words based on context coherency, using the pattern of similar feelings appearing together. We use linking rules to handle this issue with large collections of text from specific areas. We incorporate knowledge about the area by picking out emotion-related words from texts specific to that area.

Joint Extraction Of Entities And Relations For Opinion Recognition We present a method to find both entities (like things or people) and their relationships in opinion analysis. We look for two types of opinion-related entities: the opinions themselves and who is expressing them, and how they are connected. Inspired by Roth and Yih (2004), we use a method called integer linear programming (a math-based technique) to handle this task, showing that thinking about the whole picture with rules can greatly improve how well we identify relationships and opinion-related entities. It gets even better when we add a system that labels semantic roles (or the meaning of words in a sentence). Our system reaches scores of 79 and 69 out of 100 for identifying entities and relationships, much better than previous work. We suggest using the ILP method to jointly find who holds opinions, the opinions themselves, and how they are connected, showing this combined approach works well. Others have expanded this approach to also find out who holds opinions at the word level (Choi et al 2006) and to figure out if the opinions are positive or negative and how strong they are (Choi and Cardie, 2010).

Broad-Coverage Sense Disambiguation And Information Extraction With A Supersense Sequence Tagger In this paper we tackle the challenge of understanding word meanings and extracting information from text as a single problem using tagging. The task involves labeling text with a set of 41 categories from Wordnet, which is like a dictionary, for nouns and verbs. Since this set is linked to Wordnet's word meanings, the tagger partly helps figure out word meanings. Also, because the noun labels include standard categories for identifying names of people, places, organizations, time, etc., the tagger also provides extra information about these names. We treat the task of assigning supersense labels as a process of labeling text in order, and we test this approach using a method called a Hidden Markov Model, which learns from examples to make predictions. Tests on the main datasets that have word meanings labeled, like Semcor and Senseval, show significant improvements over the commonly used basic method that guesses the most common meaning first. Our supersense tagger labels text with a set of 46 categories from Wordnet supersense categories (WNSS).

Using WordNet-Based Context Vectors To Estimate The Semantic Relatedness Of Concepts In this paper, we introduce a way to measure how closely related concepts are in meaning using WordNet, a database of English words, by combining its structure and content with information from how words appear together in text. We use this information along with WordNet definitions to create "gloss vectors," which are like maps for each concept in WordNet. We give scores to show how related a pair of concepts are by looking at the angle between their gloss vectors. We show that this method is good at matching human judgments of how related concepts are in meaning, and it works well in algorithms that figure out which meaning of a word is being used based on relatedness. This method is flexible because it can compare any two concepts no matter what part of speech they are (like nouns or verbs). Plus, it can be adapted for different fields because any collection of text can be used to get the word appearance information. We make combined co-occurrence vectors for a WordNet sense by adding up the vectors of the words in its WordNet explanation. We introduce a way to use vectors to determine how closely concepts are related.

Which Side Are You On? Identifying Perspectives At The Document And Sentence Levels In this paper we explore a new problem of figuring out the point of view from which a document is written. By perspective, we mean a point of view, like from Democrats or Republicans. Can computers learn to find out the perspective of a document? Not every sentence clearly shows a perspective. Can computers learn to find which sentences clearly show a particular perspective? We create statistical models to understand how perspectives are shown in both the whole document and individual sentences, and test these models on articles about the Israeli-Palestinian conflict. The results show that the models can successfully learn how perspectives are shown in the words used and can identify the perspective of a document very accurately. We use hierarchical Bayesian modeling, a type of advanced statistical method, for understanding opinions (Lin et al, 2006). Our tests were done on a collection of political debates (Lin et al 2006). We look into connections between sentence-level and document-level classification for a task like predicting stance or viewpoint. We introduce implicit sentiment, which is a topic in computational linguistics (the study of language using computers) under the area of identifying perspective, although similar work started earlier in political science.

CoNLL-X Shared Task On Multilingual Dependency Parsing Each year the Conference on Computational Natural Language Learning (CoNLL) has a shared task where participants train and test their systems using the same data sets to easily compare how well they work. The tenth CoNLL (CoNLL-X) had a shared task on Multilingual Dependency Parsing, which is about understanding sentence structure in different languages. In this paper, we explain how language data for 13 languages was changed to the same format and how we checked the performance. We also provide a summary of the methods participants used and the results they got. Finally, we try to understand what makes a language or its data easier or harder to analyze and which parts are difficult for any system to understand. The CoNLL-X shared tasks focused on multilingual dependency parsing.

Experiments With A Multilanguage Non-Projective Dependency Parser We introduce a Shift/Reduce parser that uses a fixed method to classify words. We create DeSR, a parser that builds sentence structure step-by-step using a fixed method. We suggest a system for changing sentence structure where each step can only handle non-projective dependencies (complex sentence structures) to a certain degree, based on how far apart the related words are in the sentence.

Multilingual Dependency Analysis With A Two-Stage Discriminative Parser We introduce a two-step system to analyze sentence structures in multiple languages and test it on 13 different languages. The first step uses a model for finding the basic structure of sentences, as explained by researchers McDonald and Pereira in 2006, and adds details about word forms for some languages. The second step takes the result from the first step and adds labels to all parts of the sentence structure using a classifier trained to look at the whole structure. We share our findings based on specific data sets and discuss the mistakes found. We adjust the results afterward for certain complex structures and for adding labels. We consider adding labels to sentence parts as a problem of labeling things in a sequence. The model we explore looks at scores over pairs of connections between words, instead of just one at a time, and uses a thorough search for finding basic structures, along with a separate tool to add labels to each connection.

Labeled Pseudo-Projective Dependency Parsing With Support Vector Machines We use SVM (Support Vector Machine) classifiers to predict the next step of a parser (a tool that analyzes sentence structure) that builds labeled projective dependency graphs step-by-step. Non-projective dependencies (complex sentence connections) are handled indirectly by changing the training data for the classifiers to fit a simpler model and then changing it back for the parser's output. We show evaluation results and analyze errors, focusing on Swedish and Turkish languages. Our pseudo-projective method changes complex training trees into simpler ones but keeps the information needed to change them back in the DEPREL (dependency relation), allowing this reverse change on test trees as well (Nivre et al, 2006).

Why Generative Phrase Models Underperform Surface Heuristics We look into why generative models, which create translations based on learned patterns, perform worse than simple rule-based estimates in translating phrases. We start by suggesting a basic model that creates phrases and find that it gives worse results than using straightforward statistics. The main reason for this is adding a hidden segmentation variable, which makes it easier to fit the model too closely to the training data during a method called maximum likelihood training with EM (a way to find best-fit parameters). Specifically, while models that translate individual words greatly improve with repeated estimates, models that translate phrases don't. This is because different ways to connect words can't all be right, but different ways to divide phrases can. These different divisions, rather than connections, compete with each other, leading to less flexible phrase lists, less ability to generalize, and lower BLEU scores (a measure of translation quality). We also show that mixing the two methods can slightly improve BLEU scores. We test another model similar to IBM's word-translation Model 3 and again find the simple model does better. We look at estimating the chance of phrase pairs using a conditional translation model, which is based on an older method of source-channel models. We conclude that using segmentation variables in the generative model leads to fitting the training data too closely, even though it makes the data look more likely than when using rule-based estimates.

Discriminative Reordering Models For Statistical Machine Translation We present discriminative reordering models for phrase-based statistical machine translation. These models help decide the best order of words in translations. The models are trained using the maximum entropy principle, a method that uses probabilities to make decisions. We use several types of features: based on words, based on word classes (groups of similar words), and based on the local context (nearby words). We evaluate how well these reordering models work overall and how each type of feature helps by testing on a word-aligned corpus (a set of matched words in different languages). Additionally, we show improved translation performance using these reordering models compared to a state-of-the-art baseline system, meaning our method is better than the most advanced existing system. Despite their high perplexities (how uncertain or confused the models are), reordered LMs (language models) yield some improvements when integrated into a PSMT (phrase-based statistical machine translation) baseline that already includes a discriminative phrase orientation model (a system that decides the order of phrases). To lexicalize reordering (making the word order more natural), a discriminative reordering model (Zens and Ney, 2006a) is used. We use clustered word classes in a discriminative reordering model and show that they reduce the classification error rate (the number of mistakes in putting things in the correct order).

Manual And Automatic Evaluation Of Machine Translation Between European Languages We checked how well machine translation works for six European language pairs that were part of a shared project: translating texts between French, German, Spanish, and English. We used the BLEU score, an automatic measurement, and also checked by hand for how natural and correct the translations were. The workshop results showed that the BLEU score often underestimated the quality of translations made by systems based on set rules. We discuss and look into several cases where human judges and the BLEU score strongly disagreed on which systems were the best.

Syntax Augmented Machine Translation Via Chart Parsing We show translation results for the task called "Using Parallel Texts for Statistical Machine Translation." This is created by using a special chart parsing decoder that works with phrase tables, which are lists of word groups, that are improved and expanded with grammar rules from the target language. We use a tool called a parser to create tree diagrams for each sentence in the target language part of the training data. These diagrams are matched with word group maps created for the source language sentence. By looking at word groups that match the grammar rules in these tree diagrams, we develop methods to improve (assign a grammar-based category to a pair of words) and expand (create mixed phrases using both basic and complex phrases) the word list into a bilingual grammar system. We show outcomes for translating French to English, which are much better than the basic system used in the workshop. Our translation system is available for free under the GNU General Public License. In our research, we successfully add grammar rules to a complex type of machine translation. We start with a complete list of word groups made by standard methods, then add grammar labels to each word group in the target language based on the grammar tree. We use broken grammar pieces to improve their rules and increase the number of rules; while we learn the best tree pieces changed from original ones using a special method, they just list the pieces from the original trees without a learning process.

A Syntax-Directed Translator With Extended Domain Of Locality A syntax-directed translator first breaks down the input language into a structured format, and then step-by-step changes this structure into a string in the desired language. We demonstrate this change using an advanced method that works with multi-layered structures, giving our system more ability to express and adapt. We also create a straightforward probability model and use a fast algorithm to find the best translation. The model is then expanded to a broader approach to improve results with additional tools like language models that predict word sequences. We create an easy yet effective method to produce multiple top translations without repeats for further evaluation. Initial test results on translating English to Chinese are shared. We explore a model based on tree structures for aligning language elements. We explain the Extended Tree-to-String Transducer.

Seeing Stars When There Aren’t Many Stars: Graph-Based Semi-Supervised Learning For Sentiment Categorization We present a method that uses graphs and a mix of labeled and unlabeled data to help figure out the sentiment or feeling behind a text, like movie reviews, even when we don't have many examples with known ratings, like "4 stars." The goal is to guess the ratings of texts we don't know yet, based on the mood or emotion they show. We focus on situations where we don't have a lot of labeled data, meaning examples where we already know the rating. We show that using both known and unknown ratings together helps make better guesses about the unknown ones. We do this by making a graph, a kind of map, that includes both types of data to help us make educated guesses. Then, we solve a math problem to smooth out the ratings across this map. This approach does a better job at predicting ratings than other methods that don't use the extra data. We use this graph method for figuring out sentiment but we don't use any pre-existing word knowledge or features. We suggest using this method to solve the problem of guessing ratings when we have very few examples to learn from.

Chinese Whispers - An Efficient Graph Clustering Algorithm And Its Application To Natural Language Processing Problems We introduce Chinese Whispers, a graph-clustering algorithm that uses randomness and works quickly as it only needs to consider the connections (edges) once. After we clearly explain how the algorithm works and discuss its good and bad points, we test how well Chinese Whispers performs on different Natural Language Processing (NLP) tasks like separating languages, finding word types based on grammar, and figuring out the meaning of words in context. In doing so, we use the fact that many graphs in NLP have a feature called the small-world property, meaning they are highly connected. We also introduce a method for grouping based on how often things occur together in a graph.

Inversion Transduction Grammar for Joint Phrasal Translation Modeling We introduce a new type of grammar called "phrasal inversion transduction grammar" to use instead of the usual joint phrasal translation models. This type of grammar works like previous models that use a straightforward string of phrases, but it allows us to use faster methods to align words and train the model. We show that the rules that help older models work better also make these new methods 80 times faster. We also prove that the translation tables made by this new grammar are better, improving a quality score called BLEU by up to 2.5 points. Lastly, we look at how this joint phrasal translation model can help us match words correctly for the first time. We use a method called synchronous ITG and some rules to find phrases that don't directly translate word-for-word.

CCG Supertags in Factored Statistical Machine Translation Combinatorial Categorial Grammar (CCG) supertags give phrase-based machine translation a chance to use detailed grammatical information for each word. The challenge is how to use this information in the translation process. Factored translation models let us add supertags as a part of either the original or the translated language. We demonstrate that this improves translation quality and that the main benefit of using these grammatical supertags in basic phrase-based models is mainly due to better local rearrangements of words. We use factored phrase-based translation models to link each word with a supertag, which holds most of the information needed to understand the entire sentence structure.

Mixture-Model Adaptation for SMT We explain a method called mixture-model to adjust a Statistical Machine Translation System for new topics, using weights that depend on how close the text is to different parts of the model. We look into different versions of this method, including changing between topics versus adapting as you go; using straight-line versus log-line combinations; adjusting language and translation models; different ways to assign weights; and how detailed the part of the text being adapted is. The best methods improve by about one BLEU percentage point over a top-level system that hasn’t been adjusted. We find that the best way is to linearly combine submodels of the same kind (like several different Translation Models or several different Language Models), while combining models of different kinds (like a mix of a Translation Model with a Language Model) using log-line methods. We mix the specific topic and general topic phrase tables together, giving either straight-line or log-line weights to the parts in the tables before combining parts that overlap; this is now common practice.

(Meta-) Evaluation of Machine Translation This paper looks at how well machine translation systems can translate between 8 language pairs, specifically French, German, Spanish, and Czech to English and the other way around. We did a detailed evaluation by people, which helped us not only list the different machine translation (MT) systems in order of quality but also allowed us to look deeper into how we evaluate them. We checked the time taken and how much the evaluators agreed with each other, both within the same group (intra-annotator) and between different groups (inter-annotator), for three types of personal judgment. We also compared automatic evaluation tools with human opinions to see how well they match. This deeper examination (meta-evaluation) shows surprising things about the most commonly used methods. We found that organizing sentences in order works better for getting evaluators to agree than just scoring how accurate and smooth the sentences are.

Experiments in Domain Adaptation for Statistical Machine Translation The special challenge of the WMT 2007 shared task was domain adaptation. We used this chance to try different ways of adjusting a statistical machine translation system to a specific topic area (here: news commentary), even though most training information comes from a different area (here: European Parliament speeches). This paper also explains the submission from the University of Edinburgh for the shared task. We used factored translation models, which are a type of model that considers different aspects of language to help with domain adaptation. We use two language models and two translation models: one for the specific topic and one for the other topic, to adjust the system. We find the best combination of these models by using weights for language models trained with both specific topic and other topic data, aiming to make a test set easier to understand and blending the models together.

METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments Meteor is a tool for evaluating machine translations, and it has been shown to closely match human opinions on translation quality, doing better than the commonly used Bleu metric. It's one of several tools tested in this year's shared task at the ACL WMT-07 workshop. This paper reviews the technical details behind the tool and explains recent improvements. The latest version includes better settings and now can evaluate translations in Spanish, French, and German, as well as English. In evaluating machine translations, groups of similar meanings (sense clusters) have been added to METEOR, improving its agreement with human opinions on translation quality in various languages.

The Third PASCAL Recognizing Textual Entailment Challenge This paper introduces the Third PASCAL Recognizing Textual Entailment Challenge (RTE-3), giving a summary of how the dataset was made and the systems that were submitted. For this year's dataset, longer texts were added to make the challenge more like real-life situations. Also, a set of resources was provided so participants could use the same tools. A trial task was set up to tell apart unknown entailments (where it's not clear if one sentence logically follows from another) from contradictions (where one sentence clearly disagrees with another) and to explain the system's overall decisions. 26 participants submitted 44 attempts, using different methods, usually introducing new ways to recognize entailment and achieving better results than in previous challenges. The task of recognizing textual entailment is to decide if the hypothesis sentence (a statement to check) can logically follow from the premise sentence (the given sentence) (Giampiccolo et al, 2007). Textual Entailment (TE) has become an important approach for understanding meaning, meeting the needs of many text understanding applications.

Detection of Grammatical Errors Involving Prepositions This paper talks about ongoing work to find preposition mistakes made by people learning English who aren't native speakers. Since prepositions make up a large part of grammar mistakes by ESL (English as a Second Language) learners, creating a computer program (NLP application) that can spot these mistakes accurately will be a great help for ESL students. To solve this, we use a method called a maximum entropy classifier along with some rules to find preposition mistakes in a collection of student essays. Even though we are just starting, we successfully identify errors with 80% accuracy (precision) but only find 30% of all errors (recall). Chodorow and others (2007) used a similar method to predict the use of 34 prepositions by looking at 25 nearby context features, which include words and grammatical chunks. A context is described using 25 word features and 4 combination features, such as word patterns and parts of speech (POS) around the preposition, as well as the main verb before it (PV), the main noun before it (PN), and the main noun after it (FN), if available.

SemEval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems The goal of this task is to compare different systems that figure out the meaning of words and tell them apart, and also to compare these systems to other systems that are trained using examples or rely on existing knowledge. In total, there were 6 systems taking part. We used a part of the SemEval-2007 English test that deals with understanding word meanings, and set up both a test that groups things without prior training (using Onto Notes word meanings as the standard answer) and a test that uses training (using part of the data to match answers). We provide a comparison to the results of the systems that took part in the word meaning test of task 17. The goal of the word meaning task of SENSEVAL-4 is to group 27,132 examples of 100 different words (35 nouns and 65 verbs) into meanings or categories. Methods using graphs, which are like networks of connected points, have been used to figure out word meanings.

SemEval-2007 Task 07: Coarse-Grained English All-Words Task This paper talks about the coarse-grained (simplified) English all-words task at SemEval-2007. We explain our experience in creating a simpler version of the WordNet sense inventory (a list of meanings) and getting the sense-tagged (labeled with meanings) text ready for the task. We show the results from different systems that took part and talk about future plans. We demonstrate that the performance of Word Sense Disambiguation (WSD, a task to find meanings of words in context) systems clearly shows that WSD is not easy unless one uses a coarse-grained (simplified) approach, and then systems tagging all words only perform slightly better than just guessing the most common meaning (Navigli et al, 2007).

SemEval-2007 Task 10: English Lexical Substitution Task In this paper we describe the English Lexical Substitution task for SemEval. In the task, annotators and systems find another word or phrase to replace a target word in a sentence. The task involves both finding similar words (synonyms) and understanding the sentence context. Participating systems can use any word-related resource. There is a subtask that requires finding cases where the word is part of a phrase and figuring out what that phrase is. In the lexical substitution task, a system tries to create a word (or group of words) to replace a target word, while keeping the sentence's meaning the same. We set a standard for models that compare word meanings based on context.

The SemEval-2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task This paper introduces the task details, available resources, participant involvement, and results comparison for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise. This task involves grouping together a set of documents that mention a person’s name that can refer to multiple people, based on who is actually being referred to. We look at the challenge of figuring out which person a name refers to in a web search situation. The aim of the Web People Search task is to sort web pages into groups, where each group includes all and only the pages about one specific person. Our Web Persona Search (WePS) task has created a standard set of data for comparison.

SemEval-2007 Task 15: TempEval Temporal Relation Identification The TempEval task suggests an easy method to test how well computers can find time-related connections in text. It avoids the difficulties of judging a network of related labels by having three smaller tasks that let us evaluate time-related connections one pair at a time. This task not only makes evaluation simple but also avoids the complicated process of fully understanding all time-related information in text. Understanding time-related information in text is an important part of language processing, and our project, TempEval, has helped improve it. TempEval07 (Verhagen et al, 2007) grouped 14 time link (TLINK) connections into three simple ones: before, after, and.

SemEval-2007 Task-17: English Lexical Sample SRL and All Words This paper describes our experience in preparing the data and evaluating the results for three parts of SemEval-2007 Task-17 - Lexical Sample, Semantic Role Labeling (SRL, which is identifying the roles words play in a sentence) and All-Words. We organize and examine the results of participating systems. Using broad sense groups (Palmer et al, 2007) has significantly improved Word Sense Disambiguation (WSD, which is figuring out the correct meaning of words) performance, with accuracies of about 90%.

SemEval-2007 Task 19: Frame Semantic Structure Extraction This task involves identifying words and phrases that trigger semantic frames, which are patterns of meaning, as defined by the FrameNet project (http://framenet.icsi.berkeley.edu). It also involves identifying their related words, which are often, but not always, their grammatical dependents (like subjects). The training data consisted of sentences that were already labeled with FrameNet information. During testing, participants automatically labeled three new texts to match a perfect standard set by humans, including predicting new patterns and roles that hadn't been seen before. Accuracy and completeness were measured both for matching the labels of frames and Frame Elements (FEs) and for matching the structures that show how words depend on each other based on the labels. Our shared tasks show that identifying frame-semantic structures in text is challenging, partly because texts often include many frames for which there is little or no labeled training data.

On the Complexity of Non-Projective Data-Driven Dependency Parsing In this paper, we look at different algorithms for parsing sentences that don't follow a straight line structure (non-projective parsing). We offer new solutions that work efficiently (in polynomial time) by assuming that each decision about how words are related is made separately, which we call the edge-factored model. We also explore algorithms that consider more complex information and present some difficult problems related to these. This means it's probably not feasible to do exact non-projective parsing for models more complex than the edge-factored model. We argue that the main challenge is that non-projective parsing becomes very complicated (NP-hard) when moving beyond simple models like the arc-factored models.

Further Meta-Evaluation of Machine Translation This paper examines how well machine translation systems translate between 10 language pairs, including Czech, English, French, German, Hungarian, and Spanish. We present the translation quality of over 30 different translation systems based on a detailed manual evaluation that took hundreds of hours. We use human opinions about the systems to examine automatic methods for evaluating translation quality and report how closely these methods match human opinions at both the overall system level and for individual sentences. We confirm the reliability of our manual evaluation method by checking the consistency of ratings from the same person (intra-annotator agreement) and between different people (inter-annotator agreement), along with the time taken for evaluation. The human ratings for the WMT 2008 dataset were collected as simple yes-or-no comparisons, which are much easier to make. Usually, human ratings for machine translation quality are given as absolute scores on a scale of five or seven points, but concerns have been raised due to their low reliability.

Optimizing Chinese Word Segmentation for Machine Translation Performance Previous studies have shown that dividing Chinese text into words is helpful for translating into English, but it's still unclear how different ways of dividing words affect translation. In this paper, we show that just following existing standards for word division doesn't always make translations better. We found that other things, like keeping word division consistent and deciding how detailed the division should be, can be more important for translation. Based on this, we use methods in a tool called a conditional random field segmenter to directly adjust how detailed the word division is to help with translation, which improves the score by 0.73 BLEU (a measure of translation quality). We also show that making word division more uniform by using external dictionaries and recognizing proper names improves the score by 0.32 BLEU. We created the CRF-based Stanford Chinese segmenter, which is trained to follow the Chinese Treebank's word division for uniformity. We improved a CRF's word division tool for translation tasks by adjusting the detail of word division and making it more consistent.

Parallel Implementations of Word Alignment Tool Training word alignment models on large sets of text is a very time-consuming task. This paper describes two parallel versions of GIZA++ that make this word alignment process faster. One version runs on groups of computers working together (computer clusters), and the other runs on a single computer with several processors using multi-threading technology (a way for a computer to do multiple tasks at once). Results show that the process becomes almost proportionally faster with more CPUs (computer processing units) used, and the quality of alignment (matching words correctly) is maintained. We use a version of the GIZA++ tool that can do multiple tasks at once (multi-threaded version). This speeds up the process and fixes a mistake in GIZA++ that can happen with uncommon words.

The Stanford Typed Dependencies Representation This paper looks at the Stanford typed dependencies representation, which was made to give an easy-to-understand description of how parts of a sentence relate to each other, helping anyone who needs to understand text automatically. For this, we believe that these systems should be simple and give meaningful information, as well as have a way to automatically find these relationships. We examine the basic design ideas of the Stanford system from this point of view and compare it to the GR and PARC systems. Lastly, we discuss whether the Stanford system is good for testing sentence analyzers (parsers). Stanford dependencies provide a simple description of relations between pairs of words in a sentence.

TAG Dynamic Programming and the Perceptron for Efficient Feature-Rich Parsing We describe a parsing method that uses the perceptron algorithm (a simple type of machine learning) along with dynamic programming (a method to solve problems step by step) to create complete parse trees (structures showing how sentences are built). This method allows for detailed features of parse trees, like PCFG-based features (probability rules), bigram and trigram dependency features (relationships between two or three words), and surface features (basic sentence parts). A big challenge in using this method for full sentence parsing is making the parsing process fast enough. We show that fast training is possible using a Tree Adjoining Grammar (TAG) based parsing method. A simpler dependency parsing model is used to narrow down the options for the full model, making it faster. Tests on the Penn WSJ treebank (a collection of sentence examples) show that the model works very well, accurately understanding sentence structure and word relationships. Many connections can be ignored early on by using results from a simpler model (Carreras et al2008).

The CoNLL 2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies The Conference on Computational Natural Language Learning has a yearly project aimed at improving tools that help computers understand human language and testing them in a consistent way. In 2008, the project focused on both syntactic and semantic dependencies, which means understanding sentence structure and meaning at the same time. This task brought together ideas from the past four years and added new features: this year, syntactic dependencies included more details like named-entity boundaries (identifying names of people, places, etc.), and semantic dependencies looked at roles of actions and things in sentences. In this paper, we explain what the task was about and how we prepared the data. We also discuss the results and how different systems approached the task. We start by explaining the task of classifying predicates, which is like figuring out the right meaning of a word based on context. We describe in detail how we combined information and changed sentence structures to understand them better.

Dependency-based Syntactic–Semantic Analysis with PropBank and NomBank This paper presents what we contributed to the 2008 CoNLL Shared Task (Surdeanu et al., 2008). To solve the problem of analyzing sentence structure and meaning together, the system uses two parts: one for syntax (sentence structure) and one for semantics (meaning). The syntax part is a step-by-step parser that uses tricks to handle complex sentence structures, and the meaning part uses methods to make decisions based on a sequence of decision-making tools. The final result for both syntax and semantics is chosen from a group of options created by the subsystems. The system got the best score in the challenge: 89.32% accuracy for syntax, 81.65 for semantic F1 score (a measure of a test's accuracy), and 85.49 for macro F1 score. Our system uses 30 different subsystems to deal with action words (verbs) and naming words (nouns). We show how important it is to understand distant connections in sentence parts for analyzing how words relate to each other in a sentence. In our work, we looked at how different ways of representing grammar affect the task of basic semantic parsing (understanding the basic meaning of sentences) and pointed out the issue of having a limited ability to generalize word meanings.

Findings of the 2009 Workshop on Statistical Machine Translation This paper shows the results from the WMT09 shared tasks, which included a translation task, a system combination task, and an evaluation task. We did a large manual review of 87 machine translation systems and 22 combined systems. We ranked these systems to see how closely automatic scoring methods match human opinions on translation quality, using more than 20 methods. We introduce a new evaluation method where system output is edited and checked for accuracy. Our Fr-En 109 collection gathers a large number of matching French-English sentences from the internet. We demonstrate that the performance of machine translation that uses statistical data from texts has reached the level of the traditional rule-based method.

Joshua: An Open Source Toolkit for Parsing-Based Machine Translation We explain Joshua, a free toolkit for statistical machine translation. Joshua uses all the methods needed for special grammar rules: chart-parsing (breaking down sentences into parts), n-gram language model integration (using word patterns), beam- and cube-pruning (simplifying options), and k-best extraction (choosing the best outcomes). The toolkit can also create grammar rules from suffix-arrays (word endings) and uses minimum error rate training (improving accuracy). It uses shared and spread-out computing methods to handle large tasks. We show that the toolkit performs as well as the best in translating French to English in a specific 2009 test. We build the Joshua system, which uses clever methods to quickly calculate the changes needed for improving results.

Domain Adaptation for Statistical Machine Translation with Monolingual Resources Domain adaptation is becoming popular in statistical machine translation (SMT) to handle the drop in performance when the conditions we test under are different from those we trained with. The main idea is to use data from the specific area of interest (in-domain data) to adjust all parts of an existing system. Earlier research showed small improvements by using limited bilingual data from the specific area. Here, we aim for bigger improvements by using a lot of cheap, single-language (monolingual) data from the area of interest, either from the original language or the language it’s being translated into. We suggest creating a bilingual dataset by translating this single-language data into the other language. Studies were done using a top-notch system based on phrases, trained on the Spanish–English part of the UN dataset, and adjusted with the matching Europarl data. Translation, rearranging of words, and language models were estimated after translating the in-domain texts with the original setup. By fine-tuning how we mix these models on a practice set, the BLEU score (a measure of translation quality) improved from 22.60% to 28.10% on a test set. To use single-language data from the original language, we use a method called transductive learning to first translate this data using the best setup (original setup plus in-domain word list and language model) and get the best translation for each sentence. We adjust an SMT system with these automatic translations and train the translation and reordering models on word alignments used by a tool called Moses.

Fluency Adequacy or HTER? Exploring Different Human Judgments with a Tunable MT Metric Automatic Machine Translation (MT) evaluation metrics have traditionally been judged by how well their scores match with human opinions on translation quality. Different types of human opinions, like Fluency (how smooth the translation sounds), Adequacy (how much meaning is conveyed), and HTER (Human Translation Edit Rate), focus on different parts of translation quality that automatic MT metrics try to capture. We look at these differences using a new adjustable MT metric: TER-Plus, which builds on the Translation Edit Rate by adding adjustable settings and considering word forms, synonyms, and rephrased sentences. TER-Plus was found to be one of the best metrics in NIST's Metrics MATR 2008 Challenge, having the highest average score in terms of two statistical measures, Pearson and Spearman correlation, which show how well it matches human judgment. Adjusting TER-Plus to fit different types of human opinions leads to much better matches and important changes in the importance given to different types of changes, showing big differences between types of human judgments. We expand the TER method similarly to create a new evaluation metric, TER plus (TERp), which allows adjusting the cost of changes to get the best match with human opinions.

A Metalearning Approach to Processing the Scope of Negation Finding negation signals and their scope in text is a key task in extracting information. In this paper, we introduce a computer system that uses machine learning to identify the scope of negation in medical texts. The system uses multiple computer programs called classifiers and operates in two steps. To see how strong the approach is, we tested the system on three parts of the BioScope collection, which include different types of texts. It achieves the best results so far for this task, with a 32.07% decrease in errors compared to the best methods available. We explain a way to better determine the scope of negation by combining three techniques: IGTREE, CRF (Conditional Random Fields), and Support Vector Machines (SVM) (Morante and Daelemans, 2009). We were the first to study how to find the scope of negation by treating it as a chunking problem, which means identifying whether words in a sentence are within or outside the range of a negation signal.

Design Challenges and Misconceptions in Named Entity Recognition We examine some basic challenges and misunderstandings in creating a strong and effective NER (Named Entity Recognition) system. Specifically, we look at problems like how to represent parts of text, how to combine small NER decisions into a bigger picture, where to get helpful background information, and how to use it in an NER system. While comparing different ways to solve these problems, we find some unexpected results and build an NER system that scores 90.8 F1 on the CoNLL-2003 NER task, the best result for this data. We use IOBES notation, which is a way to mark parts of text, treating NER as a task where each word is labeled in a specific way. We explore the challenges in designing NER and show that how we label outputs and use information from outside sources is more crucial than the learning model itself. In the CoNLL-2003 task, we demonstrate that a simple method called Greedy decoding, which is a fast way of making decisions, works nearly as well as the popular Viterbi algorithm but is over 100 times faster.

Learning the Scope of Hedge Cues in Biomedical Texts Identifying hedged information in biomedical literature is an important task in gathering information because it can be misleading to treat uncertain information as if it were certain. In this paper, we introduce a computer learning system that identifies where uncertain phrases begin and end in biomedical texts. The system is based on a similar system that identifies where negation (saying "not") occurs. We show that this method of finding where phrases begin and end works for both negation and hedging (uncertain statements). To test how well the method works, the system is used on three parts of the BioScope collection that have different types of text. We create a tool to detect the beginning and end of phrases using a supervised (guided by examples) approach. We present a system that learns from itself to find where uncertain phrases begin and end in biomedical texts. We use simple sentence structure features.

Overview of BioNLP&rsquo;09 Shared Task on Event Extraction Even though a lot of focus has been on understanding the meaning of words and making large lists that categorize words by positive or negative feelings, emotion analysis research has had to work with small and limited emotion lists. In this paper, we explain how we create a high-quality, medium-sized list of emotions using Mechanical Turk, an online platform for tasks. We also show how adding a question about word choice can prevent bad data input, identify when someone might not know the word well (so we can ignore such inputs), and help us get more accurate emotion tags at the specific meaning level of the word rather than just the word itself. We do a detailed study of these tags to understand which emotions are linked to words of different types of speech. We find out which emotions often occur together with the same word and show that some emotions are indeed closely related. The BioNLP 09 Shared Task on Event Extraction, the first large-scale test of systems for identifying biomedical events, attracted 24 groups and set up a standard method for representing events and provided datasets. The BioNLP 09 Shared Task is the first event that offered a consistent dataset and evaluation tools for finding such biological connections.

Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon Even though a lot of attention has been given to understanding the positive or negative meaning of words and creating large lists of such words, research in studying emotions has had to rely on small lists of emotions. In this paper, we show how we create a good-quality, medium-sized list of emotions using Mechanical Turk, a platform for gathering data from online workers. Besides asking questions about the feelings words bring up, we show how adding a word choice question can help prevent wrong data, identify when someone doesn't know the word (so we can ignore their input), and get more detailed emotion data. We do a detailed analysis of the data to better understand how different types of words cause different feelings. We find out which feelings often come together with the same word and show that some feelings indeed appear together. We focus on feelings caused by everyday words and phrases. We explore using Mechanical Turk to create this list based on people's opinions. We build a publicly gathered list of word-feeling connections with over 10,000 word-meaning pairs associated with eight basic emotions: joy, sadness, anger, fear, trust, disgust, surprise, and anticipation, which are considered the main typical emotions.

Creating Speech and Language Data With Amazon’s Mechanical Turk In this paper, we introduce how to use Amazon’s Mechanical Turk, an online platform where lots of people can help with tasks, to collect information for language technology. We look at papers from the NAACL2010 Workshop. In this workshop, 24 researchers joined a challenge to create speech and language data with just $100. We test using Amazon Mechanical Turk (AMT) to make and check human language information (Callison-Burch and Dredze, 2010). We give a summary of different tasks where MTurk has been used and suggest some best practices to make sure the data is of good quality.

Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation This paper presents the results of the WMT10 and Metrics MATR10 shared tasks, which included a translation task (translating text from one language to another), a system combination task (combining different translation systems), and an evaluation task (checking how good the translations are). We conducted a large-scale manual evaluation of 104 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics (computer-based measurements) correlate with human judgments of translation quality for 26 metrics. This year we also investigated increasing the number of human judgments by hiring non-expert annotators (people who mark or comment on data) through Amazon's Mechanical Turk (an online platform for hiring people to do tasks). We release the News test set in the 2010 Workshop on Statistical Machine Translation.

A Regression Model of Adjective-Noun Compositionality in Distributional Semantics In this paper, we explore how to use computers to understand how adjectives and nouns combine in language. We focus on how these words pair up in English, using examples from a large collection of British texts. We create a system that organizes words into a space where common adjective-noun pairs are grouped together. We then test three ways to understand how words combine: a simple addition method, a method that multiplies values, and a more complex method called Partial Least Squares Regression (PLSR). We suggest two ways to test these methods. Our study finds that the regression methods, which involve predicting outcomes based on patterns, work better than the simple add or multiply methods and have benefits that are promising for future research. Our method of combining adjectives with nouns is inspired by older theories where adjectives are seen as adding extra meaning to nouns, and we treat adjectives as sets of numbers that change noun meanings in a predictable way. Our main new idea is to use data from real texts to teach a computer model how these words work together. We analyze groups of words from the text to automatically understand how they combine.

Driving Semantic Parsing from the World&rsquo;s Response Current methods for semantic parsing, which means turning text into a structured meaning, depend on training data that pairs sentences with their logical meanings. Getting this training data is a big problem when trying to improve semantic parsers. This paper introduces a new way of learning to reduce the need for this detailed training data. We created two new learning methods that can predict complicated structures using only simple yes/no feedback based on the surrounding environment. Additionally, we change the way we approach semantic parsing to make our model less dependent on sentence structures, which helps it improve with less training data. Surprisingly, our findings reveal that even without detailed examples, learning with simple feedback can create a parser that works almost as well as one trained with full examples. We teach systems using question and answer pairs by automatically figuring out the meanings of the questions that lead to the right answers.

The CoNLL-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text The CoNLL-2010 Shared Task was about finding words or phrases that show uncertainty and understanding their context in regular language writing. The reason for this task was that telling apart facts from unsure information in texts is very important for gathering information. The CoNLL-2010 shared task, which looked for signs of uncertainty in texts, focused on these clues to see if sentences had unclear information. The goal of the CoNLL 2010 Shared Task is also to create tools that can identify the context of these uncertain phrases.

Sentiment Analysis of Twitter Data We look at how to analyze feelings or opinions in Twitter data. The main points of this paper are: (1) We present POS-specific prior polarity features, which are tools to help determine the emotion or attitude in a sentence based on parts of speech like nouns or verbs. (2) We investigate using a tree kernel, a method that makes it easier to analyze data without needing to do a lot of manual work to create features. The new features, along with previously suggested ones, and the tree kernel work about the same, both doing better than the current best method. In our work, we carried out a study on a smaller set of tweets that were labeled by hand.

Overview of BioNLP Shared Task 2011 The BioNLP Shared Task 2011, an event focused on pulling out specific information from texts, took place over 6 months, ending in March 2011. It had a lot of people taking part, with 46 final entries from 24 teams. There were five main tasks and three extra tasks. The outcomes showed progress in detailed information extraction in the biomedical field (related to biology and medicine) and proved that these methods work well in different areas. The 2011 series expanded by including more kinds of texts, fields, and target events.

Overview of Genia Event Task in BioNLP Shared Task 2011 The Genia event task, which is about finding and understanding events in biological texts, was one of the main challenges in the BioNLP Shared Task 2011. This was the second time this challenge was held to see how much progress had been made since 2009, and to check if the technology could work well with complete scientific papers. After working on their systems for 3 months, 15 teams sent in their test results. These results show that the community has greatly improved both in how well their systems perform and in how well these systems can handle different types of texts.

CoNLL-2011 Shared Task: Modeling Unrestricted Coreference in OntoNotes The CoNLL-2011 shared task involved predicting how things mentioned in a text are related using OntoNotes data. Resources in this field have tended to focus only on linking nouns to each other, often on a limited set of things, such as ACE entities. OntoNotes provides a large collection of data that covers general linking of words and phrases, not limited to nouns or specific types of things. OntoNotes also offers extra layers of connected information, capturing additional basic meaning structures. This paper briefly explains the OntoNotes data labeling (linking words and other layers) and then describes the shared task details including how it was set up, any preparation details, and how success was measured, and presents and discusses the results achieved by the participating systems. Having a standard test set and evaluation rules, all based on a new resource that provides multiple connected information layers (like sentence structures, roles words play in a sentence, meanings, named entities, and linking words) that could support combined models, should help to boost ongoing research in the task of linking people, places, or things and events in text. An overview of all systems participating in the CONLL-2011 shared task and their results is provided here.

Stanford’s Multi-Pass Sieve Coreference Resolution System at the CoNLL-2011 Shared Task This paper explains the coreference resolution system Stanford submitted for the CoNLL-2011 shared task. Our system is a group of step-by-step models that solve coreference, which is identifying when different words refer to the same thing in a text, by using word choice, sentence structure, meaning, and how it fits in the larger conversation. All these models look at the whole document by sharing details like gender and number among the same groups of words or names. We competed in both the open and closed categories and provided results using both predicted and correct mentions. Our system achieved first place in both categories, scoring 57.8 in the closed track and 58.3 in the open track. The Stanford coreference resolver, which won the CoNLL-2011 shared task, uses a once-popular method based on rules to resolve pronouns, which are words like "he," "she," or "it," by applying traditional language rules like ensuring pronouns match in gender and number. Although these tricky pronouns don't occur often in standard test collections, they are still very important.

Findings of the 2011 Workshop on Statistical Machine Translation This paper shows the results from the WMT11 shared tasks, which included a translation task, a system combination task (where different systems work together), and a task to evaluate machine translation quality. We did a large-scale manual review of 148 machine translation systems and 41 combined systems. We looked at the rankings of these systems to see how well automatic scoring methods matched human opinions of translation quality for 21 different scoring methods. This year, there was a task to translate SMS messages from Haitian Creole to English, which were sent to emergency services after the Haitian earthquake. We also tested a new 'tunable metrics' task to see if adjusting a single system to different scoring methods would lead to noticeably different translation results. Even without using special language tools and only using the training data provided by the workshop, a detailed manual review showed that the translations were much better than both statistical systems (using math-based methods) and rule-based systems (using language rules) that used special language tools.

Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems This paper talks about Meteor 1.3, which we submitted to the 2011 EMNLP Workshop where they focus on evaluating machine translation systems. The new features include better ways to standardize text, more accurate matching of similar phrases, and the ability to tell the difference between important words and less important ones. We also have different versions of the metric: one for ranking translations and one for checking how good they are, both closely matching human opinions on translation quality. There's also a balanced version for fine-tuning, which works better than BLEU, a common evaluation method, especially when training a phrase-based system for translating Urdu to English.

KenLM: Faster and Smaller Language Model Queries We present KenLM, a library that uses two smart ways to handle language model queries efficiently, making them faster and use less memory. The PROBING method uses a type of fast lookup table designed for speed. Compared to the popular SRILM, our PROBING model is 2.4 times faster and uses 57% less memory. The TRIE method is a structure that organizes data efficiently using less memory with special tricks like bit-level packing and other techniques to save space. TRIE uses less memory than the smallest similar option and less processing power than the fastest option. Our code is open-source (free for anyone to use and modify), safe for multiple users at the same time, and works with the Moses, cdec, and Joshua translation systems. This paper describes the different ways we boost performance and compares our methods to other options. We describe a library for language modeling.

Findings of the 2012 Workshop on Statistical Machine Translation This paper shows the results of the WMT12 shared tasks, which included a translation task, a task to evaluate how well machine translation works, and a task to estimate translation quality quickly. We performed a large manual evaluation of 103 machine translation systems submitted by 34 teams. We used the ranking of these systems to see how well automatic measurements matched human opinions of translation quality for 12 evaluation metrics (ways to measure performance). We introduced a new task to estimate quality this year and looked at the submissions from 11 teams. We provide information for several automatic metrics on the entire WMT12 English-to-Czech dataset.

Robust Bilingual Word Alignment For Machine Aided Translation We have developed a new program called word_align for matching parallel text, like the Canadian Hansards, which are available in two or more languages. The program uses the result from char_align (Church, 1993), a strong alternative to programs that match sentences, and applies word-level limits using a version of Brown et al.'s Model 2 (Brown et al., 1993), which is changed and improved to handle consistency issues. Word_align was tested on a part of the Canadian Hansards provided by Simard (Simard et al., 1992). The combination of word_align and char_align decreases the variation (average square error) by 5 times compared to using char_align alone. More importantly, because word_align and char_align were made to work well on smaller and messier texts than the Hansards, they have been successfully used at AT&T Language Line Services, a business translation service, to help them with hard words. We show that knowing the length of the target text is not essential for the model's effectiveness.

Has A Consensus NL Generation Architecture Appeared And Is It Psycholinguistically Plausible? I review some recent systems for generating natural language (NL) that focus on practical applications. I claim that even though these systems come from different theoretical backgrounds, they have a surprisingly similar structure. This structure involves dividing the process into parts (modules), what tasks these parts do, and how they work together. I also compare this shared structure among applied NL generation systems with knowledge about how humans naturally speak (psycholinguistics), and argue that some parts of this shared structure match what is known about how humans produce language, even though making it similar to human language wasn't a main goal for the developers of these systems. Most systems make decisions in stages about meaning (pragmatic and semantic), word choice (lexical), and sentence structure (syntactic). We show that both understanding human language and engineering solutions often create systems that are similar in important ways.

Unsupervised Learning Of Disambiguation Rules For Part Of Speech Tagging In this paper we describe a method, called an unsupervised learning algorithm, that automatically teaches a computer program to identify parts of speech (like nouns, verbs, etc.) in sentences without needing examples that have already been labeled by humans. We compare this method to the Baum-Welch algorithm, which is another method used to teach computers how to identify parts of speech without pre-labeled examples. Then, we show a way to combine both unsupervised methods (without pre-labeled examples) and supervised methods (with some pre-labeled examples) to build a very accurate program using only a small amount of pre-labeled sentences. We introduce a method that creates rules for identifying parts of speech from scratch. We suggest a way to gather rules that depend on the surrounding words to figure out the correct part of speech, and we made an accurate program using even a tiny amount of labeled sentences by mixing both learning approaches.

Prepositional Phrase Attachment Through A Backed-Off Model Recent studies have looked at using collections of text or statistical methods to solve the problem of unclear prepositional phrase attachment, which means deciding which part of a sentence a phrase belongs to. Usually, confusing verb phrases like v rip1 p rip2 are understood by a model that looks at the four main words involved (v, nl, p, and 77,2). This paper shows that this problem is similar to how n-gram language models work in speech recognition, and that a common method in language modeling, called the backed-off estimate, can be used here. Using this method on data from the Wall Street Journal achieved 84.5% accuracy. We use a Back-Off model, which allows them to consider rare occurrences effectively on the Ratnaparkhi dataset (with good results). We make changes to the Ratnaparkhi et al (1994) dataset to address the issue of not having enough data and use this updated version to train their backed-off model.

A Bayesian Hybrid Method For Context-Sensitive Spelling Correction Two types of methods are useful for solving word confusion. The first looks at specific words near the confusing word; the second examines word patterns and grammar around the word. These methods cover different areas: the first captures the general topic or feel (like what the discussion is about or the tense), while the second focuses on sentence structure. Yarowsky combined these methods using decision lists, which are like a set of rules to decide the best solution, by using the strongest evidence from either method. This paper uses Yarowsky's idea to fix spelling mistakes that depend on context. Decision lists usually work better than using each method alone. However, better results are possible by considering all available evidence, not just the strongest one. A new combined method, using Bayesian classifiers (a type of statistical tool), is introduced to do this, showing improved results. We create a tool that uses a detailed set of context clues.

Disambiguating Noun Groupings With Respect To Wordnet Senses Word groupings useful for language processing tasks are increasingly available, as thesauri (word lists that show words with similar meanings) appear online, and as methods for grouping words based on how they are used improve. However, for many tasks, people are interested in relationships among meanings of words, not just the words themselves. This paper presents a method for automatically figuring out which meaning of a noun is intended when it appears with related nouns, like in online thesauri or as a result of grouping words based on usage. This process is done using WordNet senses, which are detailed meanings; however, the method also allows using broader WordNet categories instead of specific meanings. The method is mainly shown using examples, but results from more detailed testing are also included. In this work, using a dictionary database to measure how similar words are in meaning is seen as providing important clues for grouping words. We define the meaning similarity between two words as the amount of shared information of the most informative idea that includes both words in a structured word list. We try to combine two strategies: one that looks at the role of words in language and one that looks at how words are used together.

Text Chunking Using Transformation-Based Learning. Eric Brill introduced a learning method that can identify parts of speech with quite good accuracy. This method can also be used to find groups of words, called chunks, in sentences. These chunks include simple "baseNP" chunks, which don't have sub-chunks inside them. To do this, chunking is seen as a tagging task where each word in a sentence gets a new label that shows its chunk structure. In tests using data from a known source, this method accurately identified baseNP chunks 92% of the time and slightly more complex chunks 88% of the time. Some useful changes to this learning method are also suggested by this task. We explain chunking as a task where each word is marked as the beginning, inside, or outside of a chunk. We are among the first to use machine learning techniques for chunking.

Automatic Evaluation And Uniform Filter Cascades For Inducing N-Best Translation Lexicons This paper explains how to create a list of the best translations from a collection of bilingual texts by using statistics (math-based information) along with four extra sources of knowledge. These sources of knowledge are used like filters, so any combination of them can be applied in a consistent way. A new way to objectively measure and compare the quality of these translation lists is introduced, showing that using the best combination of filters can improve translation quality by up to 137% compared to just using basic statistical methods, and can even get close to how well humans translate. Cutting down the size of the text collection used for training has a much smaller negative effect on the translation quality when these knowledge sources are applied. This is helpful for training with small, manually created text collections when large ones are not available for certain language pairs. Additionally, three out of the four filters are still helpful even when using large text collections for training. We use a method called the Longest Common Subsequence Ratio (LCSR) to measure how similar things are.

MBT: A Memory-Based Part Of Speech Tagger-Generator. We introduce a way to identify parts of speech (like nouns or verbs) using memory. This method is called memory-based learning, where the system learns by comparing similar past examples. To find the part of speech for a word, it looks at similar examples stored in memory. This method works well when we have a set of tagged examples to learn from. Using these examples, the system can automatically create a tool to tag new text, saving time. Memory-based tagging is similar to other smart learning methods but has extra benefits: (i) it needs only a small set of examples, (ii) it can learn bit by bit, (iii) it can explain its choices, (iv) it can easily use different kinds of information, (v) it doesn’t rely on fixed rules, (vi) it handles unknown words well, and (vii) it learns and tags quickly. We show that using this method on a large scale works well. It matches the accuracy of other known methods and is efficient in terms of space and time when using IGTree, a system for organizing and searching large amounts of information. IGTree also helps in deciding the best context size for understanding words. Our tool uses a very detailed set of tags.

Comparative Experiments On Disambiguating Word Senses: An Illustration Of The Role Of Bias In Machine Learning This paper compares seven different learning methods to figure out how to determine the meaning of a word based on its context. The tested methods include statistical approaches, neural networks (a type of computer system modeled after the human brain), decision trees (a decision support tool that uses a tree-like model), rule-based methods, and case-based classification techniques (which use past cases to solve new problems). The specific challenge was clarifying six meanings of the word "line" using words from the current and previous sentences for context. Statistical methods and neural networks performed the best on this task, and we explore why this might be the case. We also talk about how bias (a tendency to favor certain outcomes) in machine learning affects performance on specific tasks. We suggest that Naive Bayes classification and perceptron classifiers (types of algorithms used for sorting data into categories) are particularly good for problems involving word meanings because they consider all features rather than just a few, which helps in making decisions. Using sets of individual words (unigrams) has been successful in text classification and figuring out word meanings, and even though it can create a lot of unnecessary information (noise), it still provides helpful data for making distinctions.

A Maximum Entropy Model For Part-Of-Speech Tagging This paper introduces a statistical model that learns from a text dataset marked with Part-Of-Speech tags and applies these tags to new text with very high accuracy (96.6%). The model, known as a Maximum Entropy model, uses various context clues (features) to predict the correct POS tag. Additionally, the paper shows how to use special features to handle tricky tagging choices, talks about problems with consistency in the dataset found while using these features, and suggests a training method to reduce these issues. We assume the tag of a word does not depend on the tags of all earlier words, just on the tags of the two words before it. We provide a publicly available maximum entropy tagger.

Efficient Algorithms For Parsing The DOP Model Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993c). Unfortunately, existing methods take a lot of computing power and are hard to carry out. Previous methods are costly because they need a huge number of rules and rely on a complex random process called Monte Carlo parsing. In this paper, we solve the first problem by simplifying the DOP model into a smaller, similar grammar that's easier to handle. We solve the second problem by using a straightforward method that aims to get the most correct parts, instead of focusing on the chance of getting the whole structure right. With these improvements, tests show a 97% rate of matching brackets and 88% rate with no mismatched brackets. This is very different from Bod's results and similar to another experiment by Pereira and Schabes (1992) on the same data. We show that Bod's results were partly due to lucky test choices and cleaner data than others used. We provide a way to change a DOP model into a similar structure in a reasonable time, where its size matches the size of the training data.

Corpus Based PP Attachment Ambiguity Resolution With A Semantic Dictionary This paper addresses two important issues in understanding language: where prepositional phrases (like "on the table") attach in a sentence, and the different meanings a word can have. We suggest a new supervised learning approach for deciding where prepositional phrases should attach, using a database of sentences where meanings are marked. Since there isn't a large enough database with marked meanings, we also suggest a new unsupervised method (which works on its own without guidance) to figure out word meanings based on the sentence context, which helps improve the training database for figuring out prepositional phrase attachment. We show the results of our method and check how well it figures out prepositional phrase attachment compared to other techniques. We created a specific, clear method to decide word meanings as part of their decision-making system.

Finding Terminology Translations From Non-Parallel Corpora We introduce a statistical tool called the Word Relation Matrix, which helps find translated word pairs and terms from collections of texts that are not directly aligned, across different languages. Online dictionary entries are used as starting points to create Word Relation Matrices for unknown words based on how they relate to each other. These matrices are then compared across the texts to find translation pairs. The accuracy of translations is about 30% when only the top choice is considered. However, when considering the top 20 choices, the accuracy improves by an average of 50.9% compared to human translators. In our study, when using a translation model for two unrelated languages (English and Japanese) with randomly chosen test words, including many multi-word terms, the accuracy is around 30% when only the top choice is suggested.

Selectional Preference And Sense Disambiguation The lack of training data is a big issue for methods that rely on large text collections (corpus) to figure out word meanings, and it's a problem that won't be fixed soon. Selectional preference, which is usually linked with confusion in word meanings, is explored in this paper to show how a statistical model, which does not need manual labeling or guided training, can help in figuring out word meanings. We describe selectional preference as the information a verb gives about the types of meanings its related words can have. We introduce a way to gather groups of related meanings for words, using selectional preferences, based on the idea that some language structures limit how words can be understood into specific groups. To decide on selectional preferences, we evenly spread out the recorded usage counts of a word over all its possible meanings.

A Linear Observed Time Statistical Parser Based On Maximum Entropy Models This paper introduces a statistical parser for natural language, which is a tool that breaks down sentences to understand their structure. It achieves high accuracy, about 87% for precision (correct parts identified) and 86% for recall (total parts identified), better than previous results on the Wall Street Journal texts. The parser needs little human help because it uses simple, clear information to make decisions, and it combines this information automatically using a method called maximum entropy, which balances different factors to make the best prediction. The time it takes for the parser to analyze a sentence is directly related to the sentence's length, meaning it is efficient. Also, the parser gives several options for how a sentence can be structured, and this paper shows that choosing the best from the top 20 options can greatly improve accuracy to 93% for both precision and recall. The paper introduces a concept called oracle re-ranking, which imagines a perfect way to choose the best sentence structure with the highest quality score from the top choices. The method uses a step-by-step approach where decision-makers are trained on specific parts instead of the overall quality, and these parts are put together to form the complete structure.

Global Thresholding And Multiple-Pass Parsing We introduce a new version of beam thresholding methods that is up to ten times faster than the old method, while still working just as well. We also introduce a new method called global thresholding, which when used with the new beam thresholding, makes things twice as fast. Additionally, we have a new method called multiple pass parsing that, when combined with the others, makes things 50% faster. We use a new search method to fine-tune the settings of these different techniques all at once. We explain a way to create a simple but rough version of a standard context-free grammar, which is a basic set of language rules.

Automatic Discovery Of Non-Compositional Compounds In Parallel Data Automatic splitting of text into the smallest meaningful parts is still a challenge, even for languages like English. Spaces between words give a simple starting point, but this is not enough for machine translation (MT), where many groups of words are not translated directly word-for-word. This paper shows an effective automatic way to find groups of words that are translated together as one unit. The method works by comparing pairs of statistical translation models created from texts that are in two different languages. It can find hundreds of non-compositional compounds (phrases that don't translate directly) in each round and builds longer phrases from shorter ones. Testing on a basic machine translation task has shown that this approach can help improve the quality of MT results. The method doesn't rely much on specific data types, so it can be used with other types of paired information, like word spellings and pronunciations. We suggest a way to identify multi-word compounds (phrases made of multiple words) in bilingual texts that relies on the predictive ability of a translation model. We explore techniques for finding non-compositional compounds in English-French text pairs and highlight that translation models considering these compounds are more precise.

A Corpus-Based Approach For Building Semantic Lexicons Semantic knowledge (understanding the meaning of words and sentences) can be very helpful for systems that process human language, but it is often manually created for each use. While some information about meaning is available in general databases like WordNet and Cyc, many systems need special dictionaries (lexicons) for specific topics. In this paper, we introduce a method that uses a collection of texts (corpus) to create dictionaries for specific topics. The system starts with a few example words (seed words) for a topic and a collection of texts. It produces a list of words ranked by how closely they relate to the topic. A person then checks the top words and decides which ones should be part of the dictionary. In tests with five topics, people usually found around 60 words for each topic in 10-15 minutes to create a basic dictionary. We notice that nouns (naming words) in pairs or groups often have related meanings. We suggest grouping nouns together by looking at these pairs or groups; we do this by checking the nearest noun phrases (NP) around a particular noun phrase. We also give importance to words linked with, but not part of, a specific category.

Distinguishing Word Senses In Untagged Text This paper explains a test comparing three automatic learning methods that figure out the meaning of a confusing word in text without extra information. The methods explained are McQuitty's similarity analysis, Ward's minimum-variance method, and the EM algorithm. These methods use certain noticeable parts of the text to decide which meaning of a confusing word is correct. These methods work better for figuring out the meanings of nouns rather than adjectives or verbs. Overall, McQuitty's similarity analysis works the best when combined with a detailed set of features. We suggest a method that looks at how similar or different each pair of examples of the target word is.

Using Lexical Chains For Text Summarization We explore a method to create a summary of a text without needing to fully understand its meaning, instead using a model of how topics progress in the text based on lexical chains (links between related words). We introduce a new process to calculate these lexical chains by combining several reliable tools: the WordNet thesaurus (a database of words with similar meanings), a tool that identifies parts of speech (words like nouns or verbs), and a basic parser for identifying groups of nouns, along with a text-splitting method from Hearst, 1994. Summarization happens in three steps: first, the text is divided into sections, then lexical chains are created, strong chains are identified, and important sentences are picked out from the text. We show in this paper the practical results of finding strong chains and important sentences. We discover that using a part-of-speech tagger can prevent incorrect word inclusions, like the word "read," which can be a noun or a verb in WordNet. In automatic text summarization, similar words are used to recognize repeated information to avoid unnecessary repetition in a summary. Cohesion (connection) is achieved by using related terms, references, ellipses (omitting words), and conjunctions (connecting words) in the text.

From Discourse Structures To Text Summaries We describe tests that show how ideas of rhetorical analysis (how arguments are made) and nuclearity (identifying core parts) can be used to find the most important parts in a text. We explain how to apply these ideas and discuss the results we got with a program that summarizes text based on its structure.

GermaNet - A Lexical-Semantic Net For German We introduce "GermaNet," a network that connects meanings and concepts for German words, combining knowledge about word meanings and categories. It's similar to the Princeton WordNet but includes specific changes to how it's built and organized, and how words and concepts relate to each other. GermaNet handles cases where words have multiple related meanings, made-up ideas, and verbs with added particles in a new way. It also includes information on how words are classified and basic sentence structure, making it a useful tool for studying how word order and meaning work together. Creating such a large resource is crucial because, until now, German hasn't had essential online tools to analyze huge collections of text for meanings. GermaNet is a big database of words, where words are linked with parts of speech (types of words like nouns or verbs) and types of meanings, arranged in a detailed order.

CogNIAC: High Precision Coreference With Limited Knowledge And Linguistic Resources This paper introduces a system that can accurately figure out which noun a pronoun refers to, achieving over 90% accuracy and more than 60% effectiveness in finding pronouns. The system focuses on resolving certain pronouns that don't need broad knowledge of the world or complex language skills to understand. It does this by being very careful with uncertainty and only making decisions when it is very sure. The system can detect uncertainty by ensuring there is a clear previous noun (antecedent) in a list of important words, where multiple nouns can be equally important. The rules used by the system are likely suitable for many different topics and mimic how people generally understand language. The system has been tested in two separate studies that confirm its effectiveness. Our approach, CogNiac, uses a simple method that relies on a set of reliable rules applied one after another to the pronoun being analyzed.

Indexing With WordNet Synsets Can Improve Text Retrieval The traditional method for finding text by using a vector space model works better (up to 29% better in our tests) when WordNet synsets (groups of related words) are used for organizing information instead of individual words. This result was achieved using a test set that was manually cleared of confusion (using SEMCOR, a set of text with meanings labeled). We also looked at how mistakes in automatically figuring out meanings affect performance when organizing documents. Lastly, if the search terms (queries) are not clarified, using synsets to organize information works only as well as the usual method of using words. We also mention some weaknesses of using WordNet for finding information, such as the lack of specific field information and the fact that WordNet separates word meanings too finely for this task.

An Empirical Approach To Conceptual Case Frame Acquisition Conceptual natural language processing systems usually depend on using case frames to understand events and the roles of objects in text. But creating a good set of these case frames for a specific topic is time-consuming, boring, and mistakes can easily happen. We have created a method using a large collection of texts to automatically develop conceptual case frames from text that hasn't been specially marked or annotated. Our method builds on earlier research that used large text collections to find patterns and create lists of word meanings for specific topics. With patterns and a list of word meanings for a topic, our method learns what types of words fit each pattern and combines patterns that fit together grammatically to create case frames with specific rules. These case frames provide more consistent results and make fewer mistakes than the original patterns. Our system only needs some pre-labeled texts and a few hours of manual checking to refine the word lists, showing that conceptual case frames can be made from unmarked text without special training materials. In our Conceptual Case Frame Acquisition project, patterns, a list of word meanings for the topic, and a list of roles and related word categories for the topic are used to create multi-part case frames with specific rules.

Edge-Based Best-First Chart Parsing Best-first probabilistic chart parsing tries to work efficiently by focusing on the most promising parts of a sentence (called edges) based on a probability score. Recent methods use probabilistic context-free grammars (PCFGs), which assign probabilities to different parts of a sentence, using these probabilities to decide which parts to focus on first. This paper improves this method by using a probability score to evaluate parts of sentences that are not complete yet, allowing for more precise control over how parsing is done. We demonstrate a simple method to achieve this by using a common technique called binarizing the PCFG, which breaks down complex structures into simpler ones. The results are about twenty times better than previous methods, meaning our parser can achieve the same results with only one-twentieth of the work. Additionally, we show that this improvement comes with greater accuracy and recall compared to methods that check every possible interpretation. We also introduce a method called overparsing, which improves accuracy by continuing to analyze the sentence even after finding the first complete interpretation.

Exploiting Diverse Knowledge Sources Via Maximum Entropy In Named Entity Recognition This paper explains a new type of system for identifying named entities (like "proper names") using a method called maximum entropy. This system uses a flexible design to take in lots of different types of information when deciding how to label words. These types of information include whether a word is capitalized, the type of words used, where the text appears (like in a headline or the main part), and lists of words or phrases. The system relies entirely on statistical methods without using any manually created rules, and it works as well as the best systems that also use statistics. But when this system is used together with systems that do have manually created rules, it performs even better than any other systems published so far.

A Statistical Approach To Anaphora Resolution This paper introduces a method for identifying pronouns and their references, and includes two experiments using this method. We include several factors for resolving pronouns into a statistical system—such as the distance between the pronoun and its possible reference, whether the reference matches in gender/number/is a living thing, key word information, and repetition of noun phrases. We merge these into one probability to help identify what the pronoun refers to. Our first test shows how much each piece of information helps and reports an 82.9% success rate when using all sources together. The second test explores a method for learning gender/number/living thing information without human guidance. We show some tests demonstrating the method's accuracy and note that with this additional information, our pronoun identification method reaches 84.2% accuracy. We add labels to the references of specific pronouns in a database. We implement a feature called Hobbs distance, which assigns a rank to a possible pronoun reference based on Hobbs's important rule-based method from 1978. We count how often a reference has been mentioned already in the conversation. Our probability-based method combines three factors (besides the agreement check): the result from the Hobbs method, how often something is mentioned depending on where the sentence is in the article, and the likelihood of the reference appearing near the pronoun. We explain a pronoun identification method using complete sentence structure information.

Experiments Using Stochastic Search For Text Planning Marcu highlighted a tough problem in arranging information in writing: how to best organize facts and use linking ideas to create the most effective text. We explain tests with different shortcut search methods to tackle this issue. We explore how to create a conversation structure from basic speech parts that are somewhat limited by linking ideas. We suggest using genetic algorithms, which mimic natural selection, instead of trying every possible way to order descriptions of museum objects.

WordNet 2 - A Morphologically And Semantically Enhanced Resource This paper describes an ongoing project to improve WordNet by focusing on word structure (morphology) and meaning (semantics). The reason for this work comes from the current weaknesses of WordNet when used as a language knowledge base. We imagine a software tool that automatically analyzes the explanations of concepts, assigning grammatical categories and grouping phrases. The nouns, verbs, adjectives, and adverbs from each definition are then clarified and connected to their corresponding groups of synonyms (synsets). This boosts the links between synsets, allowing the retrieval of related concepts. Additionally, the tool changes the explanations, first into logical forms and then into meaning-based forms. By using word formation rules, new connections are added between the synsets. We suggest a method for adding meaning tags to key parts of sentences while changing WordNet explanations into a logical form. The eXtended WordNet is a publicly available version of WordNet where each term in a WordNet explanation (except for those in example phrases) is simplified to its base form and linked to the synset it belongs to.

Improved Alignment Models For Statistical Machine Translation In this paper, we describe better methods for matching parts in statistical machine translation. This approach uses two types of information: a translation model and a language model. The language model is a bigram or general m-gram model, which means it looks at pairs or small groups of words. The translation model is broken down into a part that deals with words (lexical) and a part that deals with matching words and phrases (alignment). We explain two different methods for statistical translation and show test results. The first method looks at how single words depend on each other; the second method considers simple phrase structures by using two levels of alignment: one for phrases and one for individual words. We show results using the Verbmobil task, which is a specific spoken language task between German and English with a 6000-word vocabulary. The tests were done on both written text and spoken language recognized by a computer. To get the best single alignment, we use a method to combine alignments from different directions. We suggest a rule-based method where all matched phrase pairs (x?, a?, y?) that meet these criteria are chosen: (1) x? and y? are groups of consecutive words from x and y, and both are no longer than k words, (2) a? is the match between words of x? and y? created by a, (3) a? has at least one link, and (4) there are no links in a that have just one part in x? or y?.

Noun Phrase Coreference As Clustering This paper introduces a new, unsupervised (not guided by human input) method for solving noun phrase coreference (identifying when different phrases refer to the same thing). Instead of using traditional methods, it treats this as a grouping task. In tests using the MUC-6 dataset, it gets a score of 53.6%, which is better than the worst system (40%) but not as good as the best one (65%). More importantly, this method does better than the only MUC-6 system that uses learning to solve coreference. The grouping method seems to offer a flexible way to apply rules that don't change with context and those that do, to correctly group noun phrases that refer to the same thing. We use WordNet (a database of words) along with lists of proper names to find out if noun phrases can refer to the same thing in the grouping method. Methods that only use grouping can easily make sure that all related phrases are connected. We use distances between noun phrases to group mentions in a document. Our system uses the distance between nodes in WordNet (up to 4 steps apart) as one part of the measure for guiding the grouping method. Solving coreference is done in two steps: first, checking how likely it is that each pair of noun phrases refers to the same thing; then, forming groups of phrases that refer to the same thing, aiming to make the best overall grouping.

Language Independent Named Entity Recognition Combining Morphological And Contextual Evidence Identifying and classifying names of people, places, organizations, or other things in a text is important for many uses. This paper explains and tests a method that works for any language. It uses a step-by-step learning process to find patterns in words and their surroundings. The method learns from text that hasn't been specially prepared and works well even with a very short list of labeled names and no need for specific language tools or information. We look at the idea that terms usually have the same meaning within one document. We examine named entity recognition (NER) at the level of individual characters, using the beginnings and endings of words. The learning stage uses the initial or current guesses about entities to estimate how likely different classes are for both the entities and their contexts along their paths, and then updates these guesses for the contexts or possible entities they are connected to, repeating this until all possible points are considered.

Unsupervised Models For Named Entity Classification This paper talks about using examples that aren't labeled to figure out what type of named entities (like names of people, places, or organizations) are. Usually, you need lots of rules and labeled examples to teach a computer program to do this. But we show that using examples without labels can make it easier and you only need 7 basic "seed" rules to start with. This method works well because, in many cases, both the way a name is spelled and the words around it can tell you what type of thing it is. We introduce two methods. The first one is similar to a method from a 1995 study by Yarowsky, with some changes inspired by a 1998 study by Blum and Mitchell. The second method builds on ideas from boosting algorithms, which are usually used for tasks where you know the answers (supervised learning), and adapts them to the approach suggested by Blum and Mitchell in 1998. We improve the use of programs that help each other by adding rules to a boosting method called AdaBoost, making them agree with each other.

Exploiting Diversity In Natural Language Processing: Combining Parsers Three advanced computer programs that analyze sentence structure (parsers) are combined to create more precise sentence structures and define new limits on accuracy. Two main methods are introduced, and each method has two ways of combining parsers. Both models that use strict rules (parametric) and those that don't (non-parametric) are examined. The new parsers achieve better results than the best previous results for analyzing sentences in the Penn Treebank, which is a large language database. We improve the best parser's accuracy score, known as F-measure, from 89.7 to 91.3 by using a simple voting method called naive Bayes on sentence parts in the Penn TreeBank (a 16% reduction in mistakes). In our study on combining systems, we suggest two ways to combine parsers: one method chooses an entire tree from one parser, and the other method creates a new tree by picking parts from the original trees. We choose the best sentence structure by trying to get the highest possible accuracy from the combined sentence structures.

Detecting Text Similarity Over Short Passages: Exploring Linguistic Feature Combinations Via Machine Learning We introduce a new way to measure how similar pieces of text are by combining information from different language features to see how much meaning differs between small text parts. We look at several possible features and use machine learning to find the best mix. We talk about a stricter definition of similarity than the usual methods that focus on larger text or finding information and explain why this is important for summarizing information from multiple documents. Our system's results are compared to standard methods, showing that our new approach is better at finding closely related text pieces. For short texts or sentences, we use more than just N-grams (sequences of words), incorporating WordNet synonyms (words with similar meanings), and considering the order and spacing between shared words.

Cascaded Grammatical Relation Assignment In this paper, we talk about a step-by-step method using memory-based techniques to assign grammatical roles in sentences. In the first steps, we identify groups of words (called chunks) like noun phrases (NP), verb phrases (VP), adjective phrases (ADJP), adverb phrases (ADVP), and prepositional phrases (PP), and give them a role like indicating time or place. In the final step, we connect these word groups to show their grammatical relationships. We examined the impact of adding more steps to this method and found that even less accurate word group detectors improved the process of finding relationships. We achieved a 71.2 F-score, which is a measure of accuracy, for identifying grammatical relationships in text that has been automatically labeled and grouped, after training on about 40,000 sentences from the Wall Street Journal.