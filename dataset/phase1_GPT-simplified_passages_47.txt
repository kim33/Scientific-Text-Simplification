Efficient Parsing Of Highly Ambiguous Context-Free Grammars With Bit Vectors An efficient bit-vector-based CKY-style parser for context-free parsing is presented. Recent advancements in [machine learning] have led to significant improvements in [natural language processing (NLP)]. In this study, researchers introduce a novel [deep learning architecture] that integrates [convolutional neural networks] with [recurrent neural networks] to enhance text understanding. Experimental results on [benchmark datasets] show that the proposed model achieves a 15 % improvement in accuracy over traditional methods. Additionally, the model demonstrates robust performance in handling [noisy data] and [long-range dependencies] within text. In this study, researchers present a new [deep learning model] that combines [convolutional and recurrent neural networks] to better understand text.  The authors highlight potential uses in [sentiment analysis], [machine translation], and [information extraction], and they plan to continue improving the model’s efficiency and scalability. The parser uses bit-vector operations, which are a type of computer operation, to do multiple parsing tasks at the same time. This parser is especially useful when every possible analysis is needed, not just the most likely one. We use the Viterbi algorithm, which is good at handling situations with many possible grammar interpretations.

A Monolingual Tree-based Translation Model for Sentence Simplification In this paper, we look at sentence simplification as a type of translation where the complicated sentence is changed into a simple sentence. We suggest a Tree-based Simplification Model (TSM), which we believe is the first statistical model that combines splitting sentences, removing unnecessary parts, changing the order of words, and replacing words all together. We also explain an effective way to train our model using a large set of matching data from Wikipedia and Simple Wikipedia. The testing shows that our model makes sentences easier to read compared to other basic systems. We use a tree-based simplification model which applies methods from statistical machine translation (SMT) with this data. We explore using paired articles from English Wikipedia and Simple Wikipedia to guide the sentence simplification process with data. We suggest that simplifying sentences can be seen as a task of translating within the same language, aiming for an output that is easier to read but has the same meaning as the input.

Robust Sentiment Detection on Twitter from Biased and Noisy Data In this paper, we suggest a way to automatically find feelings in Twitter messages (tweets) by looking at how tweets are written and extra information about the words in these messages. We also use imperfect data from some websites that judge emotions on Twitter as our training data. In our tests, we demonstrate that because our method captures a more general idea of tweets, it works better than older methods and handles imperfect and biased data better, which is the type of data from these sources. We introduce a two-step method to sort the feelings in tweets using SVM classifiers (a type of machine learning model) with general features.

Enhanced Sentiment Learning Using Twitter Hashtags and Smileys Automated identification of different types of feelings can be helpful for many language processing systems like those that summarize reviews or analyze public media. In some of these systems, there is an option to give a feeling value to a single sentence or a very short text. In this paper, we suggest a supervised way to classify feelings using data from Twitter, a widely used microblogging platform. By using 50 Twitter tags and 15 smileys (faces showing emotions) as labels for feelings, this method avoids the need for time-consuming manual labeling, allowing it to identify and classify different feeling types in short texts. We check how different features help in classifying feelings and show that our method can successfully identify feelings in sentences that aren't tagged. The accuracy of identifying feelings was also confirmed by human judges. We also look at the connections and similarities between different feeling types shown by smileys and Twitter hashtags. We used 50 hashtags and 15 emoticons (simple pictures expressing emotions) as rough labels to create a dataset for classifying feelings on Twitter.

D-PATR: A Development Environment For Unification-Based Grammars We explain systems where Feature Structures (FSs) can be changed by default rules so that this characteristic does not always apply automatically.

Categorial Unification Grammars Categorial unification grammars (CUGs) combine key features of both unification and categorial grammar systems. They offer an effective and consistent way to express language knowledge using familiar and widely accepted methods, making them useful for computer applications and language research. This paper will introduce the basic ideas of CUGs and give simple examples of how they are used. It will suggest that the methods and possibilities of CUGs make them worth further study in the larger field of research on unification grammars. The paper will also talk about how CUGs can handle certain language topics like long-distance dependencies (connections between parts of a sentence that are far apart), adjuncts (extra information in a sentence), word order (the arrangement of words in a sentence), and extraposition (moving parts of a sentence to a different place).

A Statistical Approach To Language Translation An approach to automatic translation is explained that uses methods to gather information from large databases. The method relies on having large pairs of texts that are translations of each other. In our case, the texts are in English and French. Key to the technique is a detailed list (glossary) of matching phrases. The steps in the translation process are: (1) Break down the original text into set phrases. (2) Use the glossary and surrounding information to choose the matching set phrases to form the final sentence. (3) Arrange the words of the chosen phrases to form the final sentence. We have created statistical methods to help both make the glossary automatically and carry out these three translation steps, using aligned matching sentences in the two texts. Although we cannot yet show examples of French/English translation, we present promising initial results about creating the glossary and arranging word sequences. We develop certain models, using basic actions like moving, copying, and translating that work on individual words in the original sentence. Generally, a statistical machine translation system has three parts: a language model, a translation model, and a decoder. The language model shows how likely a sentence is in the original language, the translation model shows how likely a target sentence is a translation of the original, and the decoder is what actually translates the original sentence.

Synchronous Tree-Adjoining Grammars The special features of Tree-adjoining grammars (TAG) make it difficult to use them outside of their typical use in sentence structure, like for understanding meaning or translating languages automatically. We introduce a version of TAGs called synchronous TAGs that show connections between languages. This method is meant to link words and sentences in one language to their meanings in a logical form or to their translations in another language, allowing TAGs to be used for more than just sentence structure. We talk about using synchronous TAGs with real-life examples and briefly mention some computer-related challenges that come up when interpreting them. Synchronous Tree Adjoining Grammars is mainly introduced for understanding meaning but will also be suggested for translation later. A synchronous process for creating sentence structures in both languages shows how similar the sentence structures (trees) are between the two languages.

Parsing Strategies With 'Lexicalized' Grammars: Application To Tree Adjoining Grammars. In this paper, we introduce a general method for breaking down sentences (parsing) that came from creating a parsing algorithm similar to Earley's for Tree Adjoining Grammars (TAGs) and from recent studies on TAGs. In our method, basic sentence structures are linked with their main words (lexical heads). These structures cover larger areas than traditional grammars, allowing us to set rules about them. These rules either apply to the structure itself or explain how it can connect with other structures. We explain when grammars based on simple rules can be turned into 'lexicalized' ones without changing the original sentence structures. We claim that even if you expand simple grammars to include whole trees, just using substitution doesn't let you choose the main word of each structure freely. We demonstrate how using 'adjunction' (adding extra elements) allows us to easily create 'lexicalized' grammars from simple ones. We then explain how 'lexicalized' grammars come naturally from the larger areas covered by TAGs and discuss some language benefits of our method. We talk about a new general method for parsing 'lexicalized' grammars. First, the parser creates structures corresponding to the sentence, then it analyzes the sentence based on these structures. This method works with any language theory or grammar system, but we focus on TAGs. Since the number of trees needed for parsing a sentence is limited, any search method can be used. Particularly, a top-down method can be used as it avoids problems with repeating patterns. The parser can also use information from outside the immediate area to help with the search. We then describe how the Earley-type parser for TAGs can be changed to use this new method. Lexicalized grammars provide major benefits because the number of steps needed to break down a sentence is clearly limited by the sentence's length.

A Uniform Architecture For Parsing And Generation The idea of using one set of language rules (grammar) for both understanding (parsing) and creating (generation) sentences is attractive and has been considered by many researchers. In this paper, we explore a more daring idea: not only can one grammar be used by different processes working in different ways, but the same system for understanding and creating language can be used to handle the grammar in different modes. Specifically, understanding and creating language can be seen as two tasks performed by a single flexible system that figures out the logical meaning of the structure. We explain our current version of this system, which is designed to work for either task with language rules written in a specific format called PATR. Additionally, this system can be adjusted to match different methods of processing, including models for understanding language that are designed to imitate human language understanding. These adjustments allow the system to work as efficiently as previous systems focused only on understanding, but with much more ability to handle other tasks. We mention that to ensure all possibilities are covered when using a pre-prepared entry in the system's record, the entry must cover the formula being created from the top down.

Feature Structures Based Tree Adjoining Grammars We have combined Tree Adjoining Grammars (TAG), a type of grammar used in language processing, with a system that uses feature structures to unify information. This new system, called Feature Structure based Tree Adjoining Grammars (FTAG), simplifies the way dependencies and repetitive patterns are handled, which is a key aspect of TAGs. We demonstrate that FTAG can describe things more effectively than the original TAG system. We also explore simpler versions of this system and some possible language rules that can be applied. We provide a brief overview of a method to represent the structures used by this system, building upon previous work by Rounds and Kasper, which involved the logical setup of feature structures. A Feature-based TAG is made up of basic tree structures (either auxiliary or initial) and involves two main ways to combine these trees: substitution, which means replacing one part with another, and adjunction, which means adding extra parts.

Word Sense Disambiguation With Very Large Neural Networks Extracted From Machine Readable Dictionaries In this paper, we explain a method to automatically create very large neural networks (VLNNs) from the text of definitions in digital dictionaries and show how these networks help in figuring out the correct meaning of words. We apply conventional spreading activation approaches to word sense disambiguation.:::  Word Sense Disambiguation With Very Large Neural Networks Extracted From Machine Readable Dictionaries In this paper, we describe a means for automatically building very large neural networks (VLNNs) from definition texts in machine-readable dictionaries, and demonstrate the use of these networks for word sense [disambiguation.]:::  Our method brings together two earlier, independent approaches to word sense [disambiguation]: the use of machine-readable dictionaries and spreading and activation models. Building these VLNNs automatically allows us to run experiments with real-sized neural networks for understanding human language, which helps us understand how they work and how we might make them better. We use traditional spreading activation methods to help determine word meanings.

Constraint Grammar As A Framework For Parsing Running Text Grammars used in parsers (tools that analyze sentence structure) are often taken from grammar theories and practices not specifically designed for parsing. Parsers have been created for English using theories like Government and Binding Theory, Generalized Phrase Structure Grammar, and Lexical-Functional Grammar. We introduce a system for parsing that makes grammar rules closer to real sentences and addresses common problems like ambiguity (when a sentence can have more than one meaning). This system is based on language rules. It indirectly uses probabilities (how likely something is). Probabilities aren't directly included in the rules. The rules (called constraints) don't just define what a 'correct sentence' is. They are more flexible and focus on word structure, helping with the main job of parsing. Parsing here means figuring out the structure of a sentence from individual words and phrases. Constraints are based on extensive study of language use. They can represent strict rules or tendencies that involve some level of risk. Strict rules are preferred. The second central idea is to treat morphological disambiguation and syntactic labelling by the same mechanism of discarding improper alternatives.:::  We see this task as one of inferring surface structure from [a stream of concrete tokens] in a basically bottom-up mode.:::  The ensemble of constraints for language L [constitute] a Constraint Grammar (CG) for L.:::  One central idea is to maximize the use of morphological information for parsing purposes.:::  Our input tokens to CGP are [morphologically analyzed word-forms.] A CG is used by the Constraint Grammar Parser (CGP), which is a program written in Lisp (a programming language). The input for CGP is words that have been analyzed for their structure. One key idea is to use as much word structure information as possible for parsing. The structure is assigned directly through a dictionary, word structure analysis, and simple connections from word structure to sentence structure. The goal of the constraints is to eliminate as many wrong options as possible, ideally leaving a sentence with only one clear meaning. Another key idea is to handle both word structure and sentence labeling by removing incorrect options. A good parsing system should meet several needs: constraints should be rules, not step-by-step instructions, they should handle any real-world text (not just examples made by linguists), they should be separate from the program code they run on, the system should work for any language, it should be easy to set up (ideally as simple automated processes), and it should work efficiently. The CG system meets these needs. We suggest using the Constraint Grammar framework.

Toward Memory-Based Translation An essential problem of example-based translation is how to use more than one translation example for translating one original sentence. This paper proposes a method to solve this problem. We introduce a tool, called matching expression, which shows how parts of translation examples fit together. The translation process has three steps: (1) Create the source matching expression from the original sentence. (2) Change the source matching expression into the target matching expression. (3) Build the target sentence from the target matching expression. This system creates some translation options. To choose the best translation from these options, we define a score for each translation. We combine a way to measure how similar the structure is with a way to measure the difference in words to get an overall distance measure used for matching.

Typed Unification Grammars We introduce TFS, a computer system that is part of logic systems and includes a strong way to categorize information. Its main data structures are called typed feature structures, which are like organized templates. The type system supports an object-oriented method (a way of organizing information like objects) for describing language by providing a way to share properties [(multiple inheritance)] and a way to define relations between different language levels, treated as classes of objects. We explain this method starting from a simple DCG (a basic grammar model), and show how to use the type system to apply general rules and divide language descriptions into parts. We also explain how more general ideas lead to a grammar similar to HPSG (a complex grammar model). This approach can make the data structure a bit more complicated.

Automatic Processing Of Large Corpora For The Resolution Of Anaphora References Manual acquisition of semantic constraints in broad domains is very expensive. This paper presents an automatic method for collecting data on patterns that frequently occur together in a large collection of texts. To a large extent, these data show limitations based on meaning, and thus are used to clarify unclear references and sentence structure. The method was carried out by collecting data from the results of other language tools. An experiment was conducted to resolve the references of the pronoun "it" in sentences that were randomly chosen from the collection. Title results of the experiment show that in most cases, the frequently occurring data indeed reflect the limitations based on meaning and thus provide a basis for a useful tool to clarify confusion. We use the distribution of a pronoun's context to determine which possible previous words can fit the context. We present one of the earliest methods for using how often certain words appear together in resolving pronouns.

Word Identification For Mandarin Chinese Sentences Chinese sentences are made up of a series of characters without spaces to separate words. However, the basic unit needed to break down and understand a sentence is a word. So, the first step in processing Chinese sentences is to find the words. The challenges in finding words include (1) identifying complex words, like those that combine a word and a measure word, repeated words, words made from other words, etc., (2) recognizing proper names, (3) solving unclear separations between words. In this paper, we suggest possible solutions for these challenges. We use a matching method with 6 different practical rules to solve the unclear separations and achieve a 99.77% success rate. The data shows that the best matching method is the most effective approach. We suggest using the forward maximum matching method.

Two-Level Morphology With Composition We understand that using a series of combined FSTs (Finite State Transducers, which are tools that map between different sets of symbols) could achieve the two-level model. We notice that the sets of rules can be combined with the dictionary transducers efficiently, and the combined transducer ended up being about the same size as the dictionary transducer itself.

A Stochastic Japanese Morphological Analyzer Using A Forward-DP Backward-A* N-Best Search Algorithm We introduce a new way to break down a sentence into words and label the words with their parts of speech (like nouns or verbs). It uses a statistical model of language and a fast two-step search method to find the best word options. This method doesn't need spaces between words, making it perfect for written Japanese. Our Japanese word analyzer correctly found 95.1% of the words it looked for and was accurate 94.6% of the time when tested on the ATR Corpus, a collection of texts. We suggest a way to find the top N best groups of words.

PRINCIPAR - An Efficient Broad-Coverage Principle-Based Parser We introduce an efficient English language parser that uses broad rules to understand sentences. The parser is built using C++ and works on SUN Sparcstations with a graphical interface called X-windows. It includes a word list (lexicon) with over 90,000 entries, created automatically by using rules to extract and convert data from digital dictionaries. We also offer MiniPar, a quick and reliable tool for analyzing grammatical connections between words.

A TAG-Based Noisy-Channel Model Of Speech Repairs This paper talks about a system that can find and fix mistakes in spoken words written down. It uses a tool that checks sentence structure as the main model and a new kind of machine (TAG-based transducer) as the second model. This method uses the idea that the mistake is a "rough copy" of the correction. The system is tested with a special set of conversations that have mistakes marked in them. These models are good at finding mistakes. Even though the basic model works well, using a special tool called a log linear re-ranker can make it even better. Our TAG system is very good at fixing errors because it keeps track of words that overlap between mistakes and corrections.

Dependency Tree Kernels For Relation Extraction We build on earlier work to measure how similar the sentence structures (dependency trees) are. Using this method with a machine learning tool called Support Vector Machine, we identify and categorize connections between pieces of information (entities) in a set of news stories (ACE corpus). We test different features like Wordnet hypernyms (a tool to find word meanings), parts of speech (like nouns, verbs), and types of entities, and discover that our method improves accuracy by 20% compared to just counting word occurrences. To compare connections in two sentences, we suggest looking at the smaller tree structures formed by the relation parts, which means calculating similarities between the shared points (lowest common ancestors) in the sentence structure. We also apply this method to identify types of important names (Named Entity classes) in everyday language.

Comlex Syntax: Building A Computational Lexicon We explain how we created Complex Syntax, a computer-based dictionary that gives detailed grammar information for about 38,000 main English words. We look at the kinds of mistakes that can happen when making this dictionary, and how we can find and fix these mistakes. Our COMLEX syntax dictionary includes details about how verbs are used and different ways to say things in sentences, but because they are organized by words, it is not easy to use them for creating new sentences directly.

Recognizing Text Genres With Simple Metrics Using Discriminant Analysis A simple method for sorting texts into set categories based on their type or style is shown using a statistical method called discriminant analysis, applied to a collection of texts known as the Brown corpus. Discriminant analysis helps to take many different features that might be unique to a specific collection of texts or information and turn them into a few important calculations, giving more importance to features that are better at identifying text types. This method is also talked about in relation to finding information. We use word length as a sign of how formal something is for tasks like identifying text types.

Discovering Relations Among Named Entities From Large Corpora Discovering important connections hidden in documents would be very helpful not just for finding information but also for answering questions and creating summaries. Previous methods for finding these connections needed large collections of labeled documents, which took a lot of time and effort. We suggest a method that doesn't require labeled data (unsupervised method) to find connections in large collections of text. The main idea is to group pairs of named entities (like people, places, organizations) based on the similarity of the words found between them. Our tests using a year's worth of newspaper articles show that not only can we find connections between named entities accurately, but we can also automatically assign suitable labels to these connections. We introduce a fully unsupervised system called Open IE, which clusters pairs of entities that share the same connections. We use large text collections and a tool called an Extended Named Entity tagger to discover new types of connections and their participants.

Collective Information Extraction With Relational Markov Networks Most information extraction (IE) systems treat each piece of information they find as separate and not connected to others. However, in many cases, thinking about how different pieces of information affect each other could make the system more accurate. Statistical methods using undirected graphical models, like conditional random fields (CRFs), have been shown to work well for creating accurate IE systems. We introduce a new IE method that uses Relational Markov Networks (an advanced form of CRFs), which can show any kind of connection between pieces of information. This allows for "collective information extraction" that takes advantage of the way possible pieces of information can influence each other. Tests on learning to find protein names from scientific text show the benefits of this method. We present AImed, a corpus for the evaluation of [PPI] extraction systems

A High-Performance Semi-Supervised Learning Method For Text Chunking In machine learning, a key question is whether we can make a more accurate system for classifying data by using data that isn't labeled (this is called semi-supervised learning). Although many semi-supervised methods have been suggested, how well they work for natural language processing (NLP) tasks isn't always certain. This paper introduces a new semi-supervised method using a learning approach we call structural learning. The idea is to understand "what makes a good classifier" by studying thousands of automatically made extra classification problems using unlabeled data. By doing this, the common patterns that help predict outcomes in these problems can be identified and used to perform better on the main problem we want to solve. This method achieves better results than the previous best methods on CoNLL’00 syntactic chunking and CoNLL’03 named entity chunking (in English and German). We use a system that learns from multiple tasks within our semi-supervised algorithm to find features that are helpful for many related tasks. Our structural learning method applies a technique called alternating structural optimization (ASO). For reasons related to both computing speed and statistical analysis, we calculate a simplified version of the space where important predictors are found.

A Hierarchical Phrase-Based Model For Statistical Machine Translation We present a statistical phrase-based translation model that uses hierarchical phrases - phrases that contain smaller phrases within them. The model is technically a [synchronous] context-free grammar, but it is learned from a pair of aligned texts (bitext) without using any grammar rules.:::  We use the [k-best parsing method] in a grammar-based model to find the best feature weights that improve BLEU scores. This means it shifts to using formal grammar rules like syntax-based translation systems but without relying on language rules. In our tests, using a scoring method called BLEU, the hierarchical phrase-based model shows a 7.5% improvement compared to Pharaoh, which is a top-notch phrase-based system. We use the k-best parsing method in a grammar-based model to find the best feature weights that improve BLEU scores. We note that when combining two parts of a translation, we need to check how smooth they sound together by including the score of any language model features that affect the overall flow of the sentence. To better use grammar rules but still allow for translations that don't follow those rules, we keep a count for each proposed translation and add it whenever it fits perfectly with the grammar rules of the original text. Our hierarchical phrase models for machine translation build on from the traditional word-based models (Brown et al, 1993) and phrase-based models (Koehn et al, 2003a).

Reading Level Assessment Using Support Vector Machines And Statistical Language Models Reading skill is an essential part of being good at a language. However, it's hard for teachers to find suitable texts for learners of foreign or second languages. This problem can be solved using technology that helps understand language to check the reading level. Current ways to measure reading levels are not very effective for this job, but past research and our own small tests show that using statistical language models (ways to predict text patterns) can help. In this paper, we also use support vector machines (a type of computer algorithm) to combine different features from traditional reading level methods, statistical language models, and other language tools to create a better way to evaluate reading levels. We create a system using support vector machines that combines a text classifier based on trigram language models (which looks at three-word combinations for each difficulty level), some features like average tree height from parsing (breaking down sentences into parts), and variables usually used to determine readability. We use sentence structure features, such as the height of parse trees (a visual breakdown of sentence structure) or the number of passive sentences, to estimate reading grade levels.

Probabilistic CFG With Latent Annotations This paper describes a method for creating parse trees, which are structures used in understanding sentences, called PCFG-LA. This method improves upon a basic version known as PCFG by adding hidden elements to the symbols used in the trees. Detailed rules for constructing these trees are automatically learned from a collection of example sentences by training the PCFG-LA model using a technique called the EM-algorithm. Since finding an exact solution with PCFG-LA is very complex and difficult (NP-hard), the paper discusses several simplified methods and compares them through experiments. In tests with a well-known set of sentences (Penn WSJ corpus), our model, which was trained automatically, performed quite well with an accuracy of 86.6% for sentences up to 40 words long, similar to another model that required a lot of manual adjustment. We use a special method (markovized grammar) to get better results when interpreting sentence structures but do not apply this method to the training examples. We also change the structure of example sentence data to make it simpler, using only two types of constructions (unary and binary).

Dependency Treelet Translation: Syntactically Informed Phrasal SMT We explain a new way to use computers to translate languages by mixing grammar details from the original language with modern methods of translating phrases. This technique needs a tool that breaks down the sentence structure of the original language, splits words in the target language, and finds word connections without human help. We match sentences that say the same thing in two languages, apply the original language's grammar structure to the target language sentence, pull out small parts of sentences that match, and teach a model to arrange these parts correctly. We talk about a fast program that uses these small sentence parts together with usual translation models to create a strong method that mixes phrase translation with the broad understanding of language from grammar tools. Our system is trained using about 4.6 million pairs of sentences from different places like books in two languages, dictionaries, and online articles. We expand from paths to treelets, which are any connected parts of sentence structures, and suggest a model based on these pairs. We show how using small parts of the target language's grammar, called treelets, helps make phrase translation better.

Semantic Role Labeling Using Different Syntactic Views Semantic role labeling is the task of marking parts of a sentence with labels that show their meaning and role. In this paper, we introduce a top-level system for semantic role labeling using Support Vector Machine classifiers. We enhance this system by: i) adding new details, including information from dependency parses (which show how words in a sentence are related), ii) choosing and adjusting the details carefully, and iii) merging results from different methods of analyzing sentence structure. An error check of the basic system showed that about half of the mistakes in identifying roles came from errors in sentence structure analysis. To fix this, we combined different sentence structure analyses from Minipar and a simplified method with our original system based on Charniak's analysis. All these methods led to better results. We mix systems based on different ways to analyze sentence structure, like phrase-structure, dependency, and shallow parsing. We use features related to parts of the sentence and their roles to improve performance. We combine results from multiple analyzers to gather accurate sentence structure information, which is used in a machine learning test to assign meaning roles.

Paraphrasing With Bilingual Parallel Corpora Previous work has used collections of similar texts in the same language to find and create paraphrases, which are different ways of saying the same thing.:::  We define a way to measure how likely something is a paraphrase, which helps organize paraphrases found in two-language text collections using how likely they are to translate, and show how to improve this by considering surrounding words.:::  By using methods from computer programs that translate languages, we show how you can find paraphrases in one language by using a phrase in another language as a middle step. We show that this can be done using collections of texts in two different languages, a more common resource. By using methods from computer programs that translate languages, we show how you can find paraphrases in one language by using a phrase in another language as a middle step. We define a way to measure how likely something is a paraphrase, which helps organize paraphrases found in two-language text collections using how likely they are to translate, and show how to improve this by considering surrounding words. We test our methods for finding and organizing paraphrases using a set of carefully matched words, and compare the quality with paraphrases found using automatic matching. We define how likely two phrases are paraphrases based on their chance of being translated through all possible middle phrases.

Using Emoticons To Reduce Dependency In Machine Learning Techniques For Sentiment Classification Sentiment Classification tries to figure out if a text shows a positive or negative feeling from the writer towards a topic. Traditional machine learning methods have been used for this and have worked fairly well, but they only work well if the training data (examples used to teach the computer) and test data (new examples to check the computer) are similar in topic. This paper shows that having similar data in terms of subject area and timing is also important. It shares early experiments using training data with emoticons (like smiley faces or frowns), which might not depend on subject, topic, or time. In sentiment analysis research, we use emoticons in online group posts to find examples useful for training emotion detectors. We notice that when people use emoticons in messages, they are actually adding emotional signals to their text.

Minimum Cut Model For Spoken Lecture Segmentation We are working on dividing lectures into sections without any prior training. We treat this task like solving a puzzle by cutting a "graph" into parts in the best way possible. Our method looks at the big picture instead of just small parts and considers how well everything sticks together over long stretches. Our findings show that this broader view helps us cut the lecture more accurately, even when there are mistakes in the speech-to-text process. Our method looks at the big picture instead of just small parts and considers how well everything sticks together over long stretches.:::  We focus on finding the best way to cut based on how similar the sentences are to each other, using a method similar to cosine similarity (a way to measure how alike two things are).:::  Our findings show that this broader view helps us cut the lecture more accurately, even when there are mistakes in the speech-to-text process. Our goal is to identify where new topics start in the written version of the lectures. We collected a set of lectures that were divided into sections by four different people, each doing it in their own detailed way.

Joint Learning Improves Semantic Role Labeling Despite much recent progress in accurately identifying and labeling the roles words play in sentences (semantic role labeling), previous methods mostly used separate tools that worked independently, sometimes combining these with other models that sequence labels using a method called Viterbi decoding. This approach is very different from the linguistic idea that the main parts of a sentence (core argument frame) are connected and depend on each other. We demonstrate how to create a combined model that takes into account these connections, using new features that capture these interactions in advanced log-linear models (a type of mathematical model). This system reduces errors by 22% for all arguments and 32% for core arguments compared to a top independent classifier when using perfect sentence structures from PropBank (a resource for linguistic analysis). We present a combined approach for semantic role labeling (SRL) and show that a model assessing the complete structure of a sentence can significantly reduce errors compared to using separate classifiers for each part of the sentence. To make training efficient, we use a method called decomposition, which means we train the models using only a part of the training examples, specifically those that have a case marker (a word indicating the grammatical function of a noun).

Randomized Algorithms And NLP: Using Locality Sensitive Hash Functions For High Speed Noun Clustering In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data. We apply these algorithms to generate noun similarity lists from 70 million pages. We reduce the running time from quadratic (which grows quickly as data increases) to practically linear (grows slowly with more data) in the number of elements to be computed. We reduce the running time from quadratic (which grows quickly as data increases) to practically linear (grows slowly with more data) in the number of elements to be computed.:::  We show that by using the LSH (Locality Sensitive Hashing, a method to find similar items) nearest neighbors calculation can be done in O(nd) time (a way to describe how fast an algorithm runs). Our method can produce over 70% accuracy in extracting synonyms (words with similar meanings).

Multi-Engine Machine Translation Guided By Explicit Word Matching We describe a new way to combine the outputs of different Machine Translation (MT) systems working on the same text. The aim is to create a combined translation that is better than all the original translations. Our method treats each MT system as a separate unit and doesn't need them to work together. A decoding process uses specific word matches, along with confidence levels for each system and a three-word language model, to evaluate and rank different sentence options that mix words from the original systems. The best sentence option is chosen as the final translation. Tests with several Arabic-to-English systems of similar quality show a big improvement in translation quality. We suggest a method based on rules that allows flexible word matching between the different sentence options.

Bootstrapping Path-Based Pronoun Resolution We present a method for figuring out what pronouns refer to using sentence structure paths. Through a simple learning process, we determine how likely it is that a pronoun refers to a specific noun based on the path between them in a sentence structure diagram. This path information helps us solve difficult pronoun references and also effectively deals with traditional sentence structure rules for pronouns. Highly connected paths also allow us to gather detailed information about gender and number. We mix statistical knowledge with well-known features in a machine learning tool for resolving pronouns. We see major improvements in performance across several data sets. Using an automatically processed text collection, we take from each sentence structure a path showing relationships, represented as a series of points and labels linking a pronoun and a possible noun it refers to, and gather statistical data from these paths to determine how likely they are linked. We show that learning about gender is the most important part of their pronoun resolution systems. We reach top-level performance in identifying noun gender, and we share the database of the noun genders we found online. We create a statistical model from paths that include the base form of middle words, but replace the end points with noun, pronoun, or a special pronoun type for nouns, pronouns, and reflexive pronouns, respectively.

Discriminative Word Alignment With Conditional Random Fields In this paper, we introduce a new way to match words from sentences in two different languages. We use a Conditional Random Field (CRF), which is a special method trained with a small set of examples. The CRF looks at both the original and translated texts, allowing it to use different and overlapping characteristics from these texts. Additionally, the CRF is quick and effective in learning and finding the best solutions. We use this model to match words in both French-English and Romanian-English language pairs. We show that many useful characteristics can easily be added to the CRF, and that even with just a few hundred training sentences, our model is better than the current best methods, with error rates of 5.29 and 25.8 for the two tasks, respectively.

Extracting Parallel Sub-Sentential Fragments From Non-Parallel Corpora We introduce a new way to find matching parts of sentences in texts that are similar but not exactly the same in two different languages. By looking at sentence pairs that might be similar using a method inspired by signal processing, we figure out which parts of the original sentence match parts in the translated sentence and which do not. This allows us to gather useful data for training translation machines, even from texts that don't have any matching sentences. We test the quality of this data by showing it makes a modern translation system work better. We start by using a tool called GIZA++ (with a specific method called grow-diag-final-and) to match words between the original and translated texts, then measure how strongly these words are connected. We first find possible matching sentences from the similar texts and then accurately pick out matching parts of sentences using a special bilingual dictionary. We use standard methods to search for information along with simple word-based translation to find information across languages (CLIR), and we identify phrases from the search results using a clear bilingual dictionary and a method to smooth out the data. We identify phrases by combining clean word matching dictionaries for initial signals with strategies to refine the matches for final extraction of sentence parts.

Reranking And Self-Training For Parser Adaptation Statistical parsers, which are computer programs that analyze sentence structure, have gotten much better over the last 10 years when they are trained and tested using the Penn Wall Street Journal (WSJ) language data set. This improvement is mostly because they are learning from more and more details in the WSJ data. However, there is a worry that these programs might work too well only on this specific data and not as well on other types of writing. These concerns are valid. The commonly used "Charniak parser" scores 89.7% accuracy in understanding sentences in the WSJ test but drops to 82.9% in a different set of sentences from the Brown data set. This paper aims to ease these worries. It shows that by using a technique called reranking, which was explained by Charniak and Johnson in 2005, the parser's performance on the Brown data improves to 85.2%. Additionally, using another method called self-training, mentioned in a 2006 study by McClosky and others, raises the accuracy to 87.8%, which means 28% fewer mistakes, and this is done without using labeled Brown data, which are pre-identified examples. We used self-training, a method that learns from unlabeled or raw data, to improve sentence understanding and saw impressive results when adapting this technique to new types of writing.

Espresso: Leveraging Generic Patterns For Automatically Harvesting Semantic Relations In this paper, we introduce Espresso, a computer program that is not heavily supervised, is versatile, and accurately finds connections between meanings. Our main contributions are: i) a way to use broad patterns by removing wrong examples with help from the Internet; and ii) a careful method to judge how trustworthy patterns and examples are, which helps in filtering. We compare Espresso with other top systems on various texts of different sizes and types, to find different general and specific connections. Tests show that using broad patterns greatly improves the system's ability to find connections without much loss in accuracy. In the pattern creation step, our system calculates a trust score for each possible pattern using a method called weighted pointwise mutual information, PMI, which looks at how often the pattern appears with all examples found so far. We create specific trustworthy patterns step-by-step for finding connections between entities. Our program, which doesn't need much [supervision], starts with a single set that mixes different types of examples, like leader-panel and oxygen-water, which relate to member-of and part-of relationships as described by Keet and Artale in 2008.

Efficient Unsupervised Discovery Of Word Categories Using Symmetric Patterns And High Frequency Words We introduce a new way to find groups of words that share similar meanings. We use common patterns of frequently used words and important words to find potential patterns. Then, we identify balanced patterns using methods based on graphs, and word groups are formed based on connected sets in these graphs. Our method is the first to use patterns without needing any pre-marked text or starting words. We test our method on large text collections in two languages, using both human feedback and checks against WordNet, a language database. Our method, which doesn't need any pre-labeled text, works better than past methods that used labeled texts and is much faster for large text collections. We demonstrate that words that often appear together in balanced patterns usually belong to the same group, meaning they have similar meanings.

Named Entity Transliteration With Comparable Corpora In this paper, we explore how to change Chinese names into English using collections of texts in both languages that are on similar topics but are not direct translations of each other. We introduce two different methods for changing names: one uses sound-based [transliteration] and the other looks at how often the name pairs appear together over time. Both methods work well, but combining them gives even better results. We then suggest a new way to improve these results further by using a method that spreads scores based on how often name pairs appear together in related documents. This new method improves the results even more. We compare names from similar and current English and Chinese texts by using a learning program that checks how the sounds of the names match and also considers how often the names appear over time.

Meaningful Clustering Of Senses Helps Boost Word Sense Disambiguation Performance Fine-grained sense distinctions, which means very detailed differences between word meanings, are one of the main challenges in successfully telling apart different meanings of a word. In this paper, we present a method for making the WordNet sense inventory, a collection of word meanings, less detailed by connecting it to a manually created dictionary that organizes meanings, specifically the Oxford Dictionary of English. We check how good this connection is and how well the groupings work, and we test how well systems that handle less detailed word meanings perform in the Senseval-3 English all-words task. In our task, we first grouped the sense inventory into clusters, or groups, semi-automatically, with each group representing a set of similar meanings. We present an automatic way to connect different collections of word meanings; here, we use similarities in word explanations and the organized relationships between the two collections of meanings to connect WordNet meanings with the broader categories in the Oxford English Dictionary. We believe that automatically creating new connections is hard because of word ambiguities, which means words can have multiple meanings, different levels of detail in meanings, or ideas that are specific to a language.

Correcting ESL Errors Using Phrasal SMT Techniques This paper presents an initial study on using phrasal Statistical Machine Translation (SMT) methods to find and fix writing mistakes made by people learning English as a Second Language (ESL). By using examples of errors with mass nouns (words like "information" that don't usually have a plural form) found in the Chinese Learner Error Corpus (CLEC), we created a special training set. We show that using the SMT approach can find mistakes that common [proofreading tools] for native speakers often miss. Our system was able to fix 61.81% of errors in real-life examples of mass noun mistakes found online, indicating that collecting examples of before and after edits of ESL writings can help develop SMT-based writing tools. These tools can fix many complicated grammar and word choice problems in ESL writing. We use phrasal SMT methods to correct ESL writing errors and show that this approach, which needs a lot of data, is very promising. However, we also note that the SMT approach depends on having a large amount of training data.

