[{"user_id":351,"passage_id":501,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 2","understanding_comment":"Much less special lingo in 2","naturalness_comment":"Same reasoning as above","simplicity_comment":"Shorter sentences, easily understandable","id":105,"complex_a":"A TAG-Based Noisy-Channel Model Of Speech Repairs This paper describes a noisy channel model of speech repairs, which can identify and correct repairs in speech transcripts.:::\n A [syntactic parser] is used as the [source model], and a novel type of [TAG-based transducer] is the [channel model].:::\n The use of TAG is motivated by the intuition that the [reparandum] is a \"rough copy\" of the repair.:::\n The model is trained and tested on the [Switchboard disfluency-annotated corpus].:::\n [Noisy channel models] do well on the [disfluency detection task].:::\n Although the standard noisy channel model performs well, a [log linear re-ranker] can be used to increase performance.:::\n Our TAG system achieves a high [EDIT-F score], largely as a result of its explicit tracking of overlapping words between [reparanda and alterations].","complex_b":"N/A"}, 
 {"user_id":354,"passage_id":511,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"No Difference","understanding_comment":"better grammar and use of synonyms makes it easier to read","naturalness_comment":"Passage 1 reads like a collection of sentences, Passage 2 like a text","simplicity_comment":"Passage 1 has shorter sentences, Passage 2 ist better understandable","id":106,"complex_a":"Discovering Relations Among Named Entities From Large Corpora Discovering the significant relations embedded in documents would be very useful not only for information retrieval but also for question answering and summarization.","complex_b":"Discovering Relations Among Named Entities From [Large Corpora Discovering] the significant relations embedded in documents would be very useful not only for information retrieval but also for question answering and summarization"}, 
 {"user_id":354,"passage_id":512,"understanding":"Passage 1","naturalness":"No Difference","simplicity":"No Difference","understanding_comment":"the added clarifications in brackets","naturalness_comment":"the differ mostly in a few synonyms","simplicity_comment":"the differ mostly in a few synonyms","id":107,"complex_a":"N/A","complex_b":"We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a [bag-of-words kernel]. "}, 
 {"user_id":354,"passage_id":513,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"No Difference","understanding_comment":"simpler word choices","naturalness_comment":"Passage 1 is overloaded with scientific terms","simplicity_comment":"the passages differ mostly in word choice","id":108,"complex_a":"We present AImed, a corpus for the evaluation of [PPI] extraction systems","complex_b":"N/A"}, 
 {"user_id":358,"passage_id":521,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 2","understanding_comment":"Passage 2 uses simpler words.","naturalness_comment":"N/A","simplicity_comment":"Passage 1 has more conjuctions while Passage 2 has shorter simple sentances.","id":109,"complex_a":"By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve performance on the target problem.","complex_b":"N/A"}, 
 {"user_id":358,"passage_id":522,"understanding":"Passage 1","naturalness":"Passage 2","simplicity":"Passage 2","understanding_comment":"Passage 1 explains the technical words better.","naturalness_comment":"No excess explaination.","simplicity_comment":"Sentances are very simple.","id":110,"complex_a":"In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of [86.6% (F1, sentences <= 40 words)], which is comparable to that of an unlexicalized [PCFG parse]r created using extensive manual feature selection.","complex_b":"N/A"}, 
 {"user_id":360,"passage_id":531,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 2","understanding_comment":"The words are easier, especially for non native speakers and those not used to talking about computer learning usw. Passage 2 also seems to explain things better, even though it  conveys the same information.","naturalness_comment":"Closer to spoken language.","simplicity_comment":"While the sentences are about the same lenght, the ones in Passage 2 feel more fluent and connected through the usage of conjunctions.","id":111,"complex_a":"The model is formally a [synchronous] context-free grammar but is learned from a bitext without any syntactic information.:::\n We note that whenever we combine two dynamic programming items, we need to score the [fluency of their concatentation] by incorporating the score of any language model features which cross the [target side boundaries of the two concatenated items].:::\n We use the [k-best parsing algorithm in a CFG-based log-linear translation model] in order to learn feature weights which maximize BLEU.","complex_b":"The model is technically a [synchronous] context-free grammar, but it is learned from a pair of aligned texts (bitext) without using any grammar rules.:::\n We use the [k-best parsing method] in a grammar-based model to find the best feature weights that improve BLEU scores."}, 
 {"user_id":360,"passage_id":532,"understanding":"Passage 1","naturalness":"Passage 1","simplicity":"Passage 1","understanding_comment":"easier words","naturalness_comment":"sentences are structured less complex","simplicity_comment":"sentences are linked better","id":112,"complex_a":"N/A","complex_b":"This method requires a source language dependency [parser], target language word segmentation and an unsupervised word alignment component.:::\n We align a [parallel corpus], project the [source dependency parse] onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model.:::\n We describe an efficient decoder and show that using these tree-based models in combination with [conventional SMT models] provides a promising approach that incorporates the power of phrasal SMT with the linguistic [generality] available in a [parser]."}, 
 {"user_id":364,"passage_id":541,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 2","understanding_comment":"Explains context for readers without a technical background","naturalness_comment":"Passage 2 uses more everyday phrasing (an essential part of being good at a language)vs. Passage 1's academic expressions (a fundamental component of language competency.)","simplicity_comment":"Passage 2 breaks down sentences and makes the content more digestible","id":114,"complex_a":"We develop a SVM categoriser combining a classifier based on trigram language models (one for each level of difficulty), some parsing features such as average tree height, and variables traditionally used in readability.:::\n Reading Level Assessment Using Support Vector Machines And Statistical Language Models Reading proficiency is a fundamental component of language competency.","complex_b":"n/a"}, 
 {"user_id":364,"passage_id":542,"understanding":"Passage 1","naturalness":"Passage 1","simplicity":"Passage 1","understanding_comment":"phrases like “break down the sentence” and “rearranging the words\" in Passage 1 are clearer than technical phrases like “parse the source language string” or “surface string\" from Passage 2","naturalness_comment":"Passage 1 is less formal","simplicity_comment":"Passage 1 breaks ideas into short, direct sentences. Passage 2 describes complex ideas with longer, technical sentences","id":115,"complex_a":"Clause Restructuring For Statistical Machine Translation We describe a method for incorporating syntactic information in statistical machine translation systems.:::\n The first step of the method is to parse the source language string that is being translated.:::\n The second step is to apply a series of transformations to the parse tree, effectively reordering the surface string on the source language side of the translation system.:::\n The reordering approach is applied as a pre-processing step in both the training and decoding phases of a phrase-based statistical MT system.","complex_b":"We point out it's unsure if the conditions needed for another method, called bootstrap resampling, apply to Bleu scores, so we suggest using the sign test."}, 
 {"user_id":364,"passage_id":548,"understanding":"Passage 1","naturalness":"Passage 1","simplicity":"Passage 2","understanding_comment":"explaining definitions","naturalness_comment":"more approachable","simplicity_comment":"less clear but passage 2 looks cleaner without parenthesis/added context","id":124,"complex_a":"Randomized Algorithms And NLP: Using Locality Sensitive Hash Functions For High Speed Noun Clustering In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data.","complex_b":"We reduce the running time from quadratic (which grows quickly as data increases) to practically linear (grows slowly with more data) in the number of elements to be computed.:::\n We show that by using the LSH (Locality Sensitive Hashing, a method to find similar items) nearest neighbors calculation can be done in O(nd) time (a way to describe how fast an algorithm runs)."}, 
 {"user_id":364,"passage_id":543,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 2","understanding_comment":"Passage 2 explains unfamiliar technical terms ","naturalness_comment":"Passage 2 uses clearer everyday language","simplicity_comment":"Passage 1 feels packed with long and formal technical sentences","id":116,"complex_a":"In this paper, we present a syntax-based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar.:::\n Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees.:::\n We first introduce our approach to inducing such a grammar from parallel corpora.:::\n Second, we describe the graphical model for the machine translation task, which can also be viewed as a stochastic tree-to-tree transducer.:::\n We introduce a polynomial time decoding algorithm for the model.:::\n We present a translation model based on Synchronous Dependency Insertion Grammar (SDIG), which handles some of the non-isomorphism but requires both source and target dependency structures.","complex_b":"Next, we describe our method for translation using a visual model that acts like a tree-to-tree converter, changing one language structure into another using probability.:::\n In this paper, we introduce a system that uses these statistical methods with a specific type of grammar called probabilistic synchronous dependency insertion grammar."}, 
 {"user_id":364,"passage_id":544,"understanding":"Passage 1","naturalness":"Passage 1","simplicity":"Passage 1","understanding_comment":"Simpler words","naturalness_comment":"More direct","simplicity_comment":"Too many jargons in Passage 2","id":117,"complex_a":"n/a","complex_b":"Arabic Tokenization Part-Of-Speech Tagging And Morphological Disambiguation In One Fell Swoop We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including part-of-speech tagging) Arabic words in one process.:::\n We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer."}, 
 {"user_id":364,"passage_id":545,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 2","understanding_comment":"explains definition of unfamiliar terms","naturalness_comment":"less academic","simplicity_comment":"passage 1 sentences are dense and nested","id":120,"complex_a":"Semantic Role Labeling Using Different Syntactic Views Semantic role labeling is the process of annotating the predicate-argument structure in text with semantic labels.:::\n In this paper we present a state-of-the-art baseline semantic role labeling system based on Support Vector Machine classifiers.:::\n In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses.:::\n We use the Constituent, Predicate, and Predicate-Constituent related features for the kernel, resulting in the best performance.:::\n We combine the outputs of multiple parsers to extract reliable syntactic information, which is translated into features for a machine learning experiment in assigning semantic roles.","complex_b":"To fix this, we combined different sentence structure analyses from Minipar and a simplified method with our original system based on Charniak's analysis."}, 
 {"user_id":364,"passage_id":546,"understanding":"Passage 1","naturalness":"Passage 1","simplicity":"Passage 1","understanding_comment":"easier words, definition provided. (it's still a bit long though)","naturalness_comment":"more conversational","simplicity_comment":"sentences too complex in passage 2","id":121,"complex_a":"Joint Learning Improves Semantic Role Labeling Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding.:::\n This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between arguments.:::\n We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative log-linear models.:::\n This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a state-of-the art independent classifier for gold-standard parse trees on PropBank.:::\n We introduce a joint approach for SRL and demonstrate that a model that scores the full predicate argument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation.:::\n We employ decomposition for efficiency in training: that is, the decomposition allows us to train the classification models on a subset of training examples consisting only of those phrases that have a case marker.","complex_b":"Joint Learning Improves Semantic Role Labeling Despite much recent progress in accurately identifying and labeling the roles words play in sentences (semantic role labeling), previous methods mostly used separate tools that worked independently, sometimes combining these with other models that sequence labels using a method called Viterbi decoding."}, 
 {"user_id":364,"passage_id":547,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 2","understanding_comment":"easier words","naturalness_comment":"more natural","simplicity_comment":"more direct sentences but still long","id":122,"complex_a":"We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account.:::\n We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments.:::\n We define a paraphrasing probability between two phrases based on their translation probability through all possible pivot phrases.:::\n Paraphrasing With Bilingual Parallel Corpora Previous work has used monolingual parallel corpora to extract and generate paraphrases.","complex_b":"Paraphrasing With Bilingual Parallel Corpora Previous work has used collections of similar texts in the same language to find and create paraphrases, which are different ways of saying the same thing.:::\n We define a way to measure how likely something is a paraphrase, which helps organize paraphrases found in two-language text collections using how likely they are to translate, and show how to improve this by considering surrounding words.:::\n By using methods from computer programs that translate languages, we show how you can find paraphrases in one language by using a phrase in another language as a middle step."}, 
 {"user_id":364,"passage_id":549,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 2","understanding_comment":"easier words","naturalness_comment":"more natural expressions","simplicity_comment":"some words didn't necessarily need added explainers in passage 2, but still easier to read","id":125,"complex_a":"This paper demonstrates that match with respect to domain and time is also important, and presents preliminary experiments with training data labeled with emoticons, which has the potential of being independent of domain, topic and time.:::\n Traditional machine learning techniques have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the training and test data with respect to topic.","complex_b":"n/a"}, 
 {"user_id":364,"passage_id":550,"understanding":"Passage 1","naturalness":"Passage 1","simplicity":"Passage 1","understanding_comment":"easier words","naturalness_comment":"more natural expressions","simplicity_comment":"less technical","id":130,"complex_a":"n/a","complex_b":"Multi-Engine Machine Translation Guided By Explicit Word Matching We describe a new approach for synthetically combining the output of several different Machine Translation (MT) engines operating on the same input.:::\n The goal is to produce a synthetic combination that surpasses all of the original systems in translation quality.:::\n Our approach uses the individual MT engines as \"black boxes\" and does not require any explicit cooperation from the original MT systems.:::\n A decoding algorithm uses explicit word matches, in conjunction with confidence estimates for the various engines and a trigram language model in order to score and rank a collection of sentence hypotheses that are synthetic combinations of words from the various original engines."}, 
 {"user_id":365,"passage_id":551,"understanding":"Passage 2","naturalness":"No Difference","simplicity":"Passage 2","understanding_comment":"passage 2 uses easier language","naturalness_comment":"Both passages are grammatically correct","simplicity_comment":"simple sentence structures","id":131,"complex_a":"We create a [corpus] of course lectures segmented by four annotators, noting that the annotators operated at different levels of [granularity].:::\n Our results demonstrate that global analysis improves the [segmentation accuracy] and is robust in the presence of speech recognition errors.","complex_b":"We focus on finding the best way to cut based on how similar the sentences are to each other, using a method similar to [cosine similarity] (a way to measure how alike two things are)."}, 
 {"user_id":365,"passage_id":552,"understanding":"Passage 1","naturalness":"Passage 2","simplicity":"Passage 1","understanding_comment":"Simple sentence structure","naturalness_comment":"Sentences are longer but make sense more.","simplicity_comment":"Simple sentence structure","id":132,"complex_a":"N/A","complex_b":"Highly [coreferent paths] also allow mining of precise probabilistic gender/number information.:::\n Given an automatically parsed corpus, we extract from each parse tree a dependency path, which is represented as a sequence of nodes and dependency labels connecting a pronoun and a candidate [antecedent], and collect statistical information from these paths to determine the likelihood that a pronoun and a candidate antecedent connected by a given path are coreferent.:::\n We build a statistical model from paths that include [the lemma of the intermediate tokens], but replace the end nodes with noun, pronoun, or pronoun-self for nouns, pronouns, and reflexive pronouns, respectively."}, 
 {"user_id":365,"passage_id":553,"understanding":"Passage 2","naturalness":"No Difference","simplicity":"Passage 2","understanding_comment":"Easier vocabs","naturalness_comment":"Both sound natural in English","simplicity_comment":"simple sentence structure","id":133,"complex_a":"Moreover, the CRF has efficient training and decoding processes which both find globally [optimal] solutions.:::\n We show how a large number of highly predictive features can be easily incorporated into the CRF, and [demonstrate] that even with only a few hundred word-aligned training sentences, our model improves over the current [state-of-the-art] with [alignment] error rates of 5.29 and 25.8 for the two tasks respectively.","complex_b":"N/A"}, 
 {"user_id":365,"passage_id":554,"understanding":"Passage 1","naturalness":"No Difference","simplicity":"Passage 1","understanding_comment":"easier terms","naturalness_comment":"Both speaks good english","simplicity_comment":"simpler sentence structure","id":134,"complex_a":"We introduce two different methods for changing names: one uses sound-based [transliteration] and the other looks at how often the name pairs appear together over time.","complex_b":"We present two [distinct] methods for transliteration, one approach using [phonetic transliteration], and the second using the temporal distribution of candidate pairs.:::\n We then propose [a novel score propagation method] that utilizes the co-occurrence of transliteration pairs within document pairs.:::\n This [propagation] method achieves further improvement over the best results from the previous step."}, 
 {"user_id":365,"passage_id":555,"understanding":"Passage 2","naturalness":"No Difference","simplicity":"Passage 2","understanding_comment":"less technical terms are used","naturalness_comment":"Both passages sound natural in english","simplicity_comment":"passage 2 sounds simpler","id":135,"complex_a":"This method enables us to extract useful machine translation training data even from very [non-parallel corpora], which contain no parallel sentence pairs.:::\n We first use the GI ZA++ [(with grow-diag-final-and heuristic)] to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words.:::\n We first extract the candidate parallel sentences from the comparable corpora and further extract the accurate [sub-sentential] bilingual fragments from the candidate parallel sentences using the in-domain probabilistic [bilingual lexicon.]:::\n We perform phrase extraction by combining clean [alignment lexica] for initial signals with heuristics to smooth alignments for final fragment extraction.","complex_b":"N/A"}, 
 {"user_id":366,"passage_id":551,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 1","understanding_comment":"states the goal explicitly ","naturalness_comment":"less repetitive, no notable grammatical errors when Passage 1 has one (a normalized minimum-cut criteria-> \"a normalized minimum-cut criterion\" OR \"normalized minimum-cut criteria\")","simplicity_comment":"shorter sentences","id":136,"complex_a":"Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of [speech recognition errors].","complex_b":"Our method looks at the big picture instead of just small parts and considers how well everything sticks together over long stretches.:::\n We focus on finding the best way to cut based on how similar the sentences are to each other, using a method similar to cosine similarity (a way to measure how alike two things are).:::\n Our findings show that this broader view helps us cut the lecture more accurately, even when there are mistakes in the speech-to-text process."}, 
 {"user_id":15,"passage_id":41,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 2","understanding_comment":"Passage 2 is easier to grasp because it replaces dense terms (“novel deep-learning architecture”) with simpler ones (“new deep-learning model”), breaks long sentences into shorter, clearer statements, and avoids parenthetical clauses such as “while also outlining,” so the paper’s objective emerges with less ambiguity.","naturalness_comment":"Passage 2 sounds more natural.\nIts sentences are shorter and more direct, it uses everyday verbs such as “present” and “combines” instead of heavier phrases like “introduce a novel architecture,” and it avoids stacked modifiers (e.g., “robust performance in handling noisy data”) that make Passage 1 feel denser. Both passages are grammatically correct, but Passage 2’s simpler word choices and cleaner sentence structure give it a smoother, more fluent flow.","simplicity_comment":"Passage 2 again comes out ahead.\nEvery sentence is short enough to fit on one-to-two text lines, avoids subordinate clauses (“although,” “while,” “whereas”), and sticks to a clear subject-verb-object pattern, so a reader can skim a single sentence and immediately grasp a key point (e.g., “Tests on standard datasets show that this model improves accuracy by 15%…”). Passage 1, by contrast, strings together multiple ideas in longer sentences that often spill beyond two rows, making it harder to spot the main content at a glance.","id":137,"complex_a":"Recent advancements in [machine learning] have led to significant improvements in [natural language processing (NLP)].\nIn this study, researchers introduce a novel [deep learning architecture] that integrates [convolutional neural networks] with [recurrent neural networks] to enhance text understanding.\nExperimental results on [benchmark datasets] show that the proposed model achieves a 15 % improvement in accuracy over traditional methods.\nAdditionally, the model demonstrates robust performance in handling [noisy data] and [long-range dependencies] within text.\nThe authors discuss potential applications in [sentiment analysis], [machine translation], and [information extraction], while also outlining future work to further optimize the model’s efficiency and scalability.","complex_b":"In this study, researchers present a new [deep learning model] that combines [convolutional and recurrent neural networks] to better understand text.\n\nThe authors highlight potential uses in [sentiment analysis], [machine translation], and [information extraction], and they plan to continue improving the model’s efficiency and scalability."}, 
 {"user_id":15,"passage_id":42,"understanding":"Passage 1","naturalness":"Passage 2","simplicity":"Passage 1","understanding_comment":"Passage 1 is easier to grasp.\nIt replaces technical terms such as “log-linear model,” “derivation space,” and “normal-form derivation model” with everyday phrases like “mathematical method” and “label-assigning process,” breaks ideas into shorter, plainly worded sentences, and relies on clear cause-and-effect statements (“The supertagger makes the process more efficient by reducing the number of possible options…”). Passage 2, by contrast, packs dense jargon and longer noun phrases into single sentences, so the paper’s objective is harder to spot without prior knowledge of parsing theory.","naturalness_comment":"Passage 2 sounds more natural.\nIts sentences follow standard academic‐English patterns (“reduces the derivation space over which model estimation is performed”), keep subjects and verbs close together, and avoid informal or awkward phrases such as “label assigning process,” “grammar created by a machine,” or “you can achieve much faster speeds,” which make Passage 1 feel less polished. Despite the heavier jargon, Passage 2’s syntax, word choice, and punctuation read smoothly and contain no grammatical slips, giving it a more fluent, professional tone.\n\n\n\n\n\n\n\n\n\n","simplicity_comment":"Passage 1.\nMost of its points are broken into shorter, plain-language sentences without stacked clauses, so readers can process each idea quickly. Passage 2 often compresses multiple technical phrases into longer sentences, making its structure comparatively more complex.","id":138,"complex_a":"This paper explains the role of [supertagging] in a [CCG parser] that uses a [mathematical method] to choose the best analysis.\n\nThe [supertagger] makes the process more efficient by reducing the number of possible options the model needs to evaluate, which also makes training faster.\n\nCCG parsing has two steps: first, the [supertagger] assigns likely [categories] to each word, and then a few rules—along with [type-changing] and [punctuation rules]—help build a [structured representation] using a method called the [CKY algorithm].\n\nWe suggest a way to combine the [supertagger] with the [parser]: at first, only a few categories are given to each word, and more are added if the parser can’t find a complete analysis.","complex_b":"This paper describes the role of [supertagging] in a [wide-coverage CCG parser] which uses a [log-linear model] to select an analysis.\n\nWe show that large increases in speed can be obtained by tightly integrating the [supertagger] with the [CCG grammar] and parser.\n\nThis is the first work we are aware of to successfully integrate a [supertagger] with a [full parser] which uses an [automatically extracted grammar].\n\nThe result is an accurate [wide-coverage CCG parser] which is an [order of magnitude] faster than comparable systems for other [linguistically motivated formalisms].\n\nThe [CCG parsing] consists of two phases: first the [supertagger] assigns the most probable [categories] to each word, and then the small number of [combinatory rules], plus the [type-changing] and [punctuation rules], are used with the [CKY algorithm] to build a [packed chart].\n\nWe propose a method for integrating the [supertagger] with the parser: initially a small number of [categories] is assigned to each word, and more categories are requested if the parser can not find a [spanning analysis]."}, 
 {"user_id":15,"passage_id":43,"understanding":"Passage 2","naturalness":"Passage 1","simplicity":"Passage 1","understanding_comment":"Passage 2 is easier to follow.\nIt replaces less common terms like “notion” and “arbitrary threshold” with plainer phrases such as “idea” and “chosen limit,” and it explicitly spells out abbreviations—adding “(Machine Translation)” after “MT” and “(Quality Estimation)” after “QE.” Those small tweaks reduce jargon and ambiguity, so the paper’s objective—evaluating ways to judge MT quality at sentence and word levels—comes across more clearly.\n\n\n\n\n\n\n\n\n\n","naturalness_comment":"Passage 1 sounds a little more natural overall.\nIts word choices (“arbitrary threshold,” “notion of correctness,” “sentence-level QE system”) match standard academic English and read smoothly, whereas some substitutions in Passage 2—such as “idea of correctness is not easy to understand” and “chosen limit”—feel slightly informal or awkward. Both passages are grammatically correct, but Passage 1’s vocabulary and phrasing align better with conventional, fluent scholarly style.\n\n\n\n\n\n\n\n\n\n","simplicity_comment":"Passage 1—its sentences are a little shorter and have fewer parenthetical insertions or stacked phrases, so each idea is delivered in a cleaner, more easily digestible structure.","id":139,"complex_a":"Various methods for determining whether [MT] output is correct are investigated, for both [whole sentences] and [words].\n\nSince the [notion of correctness] is not intuitively clear in this context, different ways of defining it are proposed.\n\nWe introduce a [sentence-level QE system] where an [arbitrary threshold] is used to classify the [MT output] as good or bad.","complex_b":"Since the [idea of correctness] is not easy to understand in this context, different ways of defining it are proposed.\nWe introduce a [sentence-level QE (Quality Estimation) system] where a chosen limit is used to classify the [MT output] as good or bad."}, 
 {"user_id":365,"passage_id":560,"understanding":"Passage 1","naturalness":"No Difference","simplicity":"Passage 1","understanding_comment":"better explanation for technical terms","naturalness_comment":"both sound natural in English","simplicity_comment":"shorter and simpler sentence structure","id":151,"complex_a":"N/A","complex_b":"Much of this improvement, however, is based upon an [ever-increasing] number of features to be trained on (typically) the WSJ treebank data.:::\n This paper should allay these fears.:::\n We successfully applied self-training to parsing by [exploiting] available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation."}, 
 {"user_id":15,"passage_id":44,"understanding":"Passage 1","naturalness":"Passage 2","simplicity":"Passage 1","understanding_comment":"Passage 1 is easier to grasp.\nIt replaces dense technical phrases (“monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles”) with plainer language (“sentences with the same meaning but different words from news articles grouped by time and topic”), breaks long ideas into shorter statements, and offers quick glosses for jargon (e.g., explaining edit distance as “counting changes needed to make them the same”). Those choices reduce ambiguity and let the paper’s objective—building a large paraphrase corpus using two unsupervised methods—stand out clearly.","naturalness_comment":"Passage 2 reads more naturally in standard academic English.\nIts sentences follow familiar scholarly patterns (“We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases …”), avoid contractions such as “don’t,” maintain consistent terminology, and keep subjects and verbs close together, giving the prose a smooth, formal flow. Passage 1, by contrast, mixes conversational wording (“look into methods that don’t need human guidance,” “rule of thumb”) with some awkward constructions (“a scoring method from translating languages”), which makes it feel less polished even though both passages are largely grammatical.\n\n\n\n\n\n\n\n\n\n","simplicity_comment":"Passage 1 uses simpler sentence structure.\nMost ideas are broken into shorter, standalone sentences with basic subject-verb-object patterns and few embedded clauses, making each point easy to process quickly. Passage 2, although well-written, frequently packs multiple technical phrases into longer sentences and uses more subordinate clauses (e.g., “collected from thousands of web-based news sources”), which increases structural complexity.","id":140,"complex_a":"We look into methods that don't need human guidance for finding [paraphrases] from a [collection of news articles] grouped by time and topic from many online news sources.\n\nWe use two methods: (1) [simple string edit distance], and (2) a [heuristic strategy] that pairs initial sentences from different news stories in the same group.\n\nResults show that the [edit-distance data] is cleaner and easier to match, with an [alignment error rate (AER)] of 11.58 % on a similar test set.\n\nWe introduce the [Microsoft Research Paraphrase Corpus (MSRPC)], which leverages [web-aggregated news stories] to learn both sentence- and word-level alignments.","complex_b":"We investigate [unsupervised techniques] for acquiring [monolingual sentence-level paraphrases] from a corpus of [temporally and topically clustered news articles] collected from thousands of [web-based news sources].\n\nTwo techniques are employed: (1) [simple string edit distance], and (2) a [heuristic strategy] that pairs initial (presumably summary) sentences from different news stories in the same cluster.\n\nResults show that edit-distance data is cleaner and more easily aligned than the heuristic data, with an overall [alignment error rate (AER)] of 11.58 % on a similarly extracted test set.\n\nOn test data extracted by the [heuristic strategy], however, performance of the two training sets is similar, with [AERs] of 13.2 % and 14.7 % respectively.\n\nAnalysis of 100 pairs of sentences from each set reveals that the edit-distance data lacks many of the complex [lexical and syntactic alternations] that characterize [monolingual paraphrase].\n\nThe summary sentences, while less readily alignable, retain more of the [non-trivial alternations] that are of greatest interest when learning [paraphrase relationships]."}, 
 {"user_id":15,"passage_id":45,"understanding":"Passage 2","naturalness":"Passage 1","simplicity":"Passage 2","understanding_comment":"Passage 2 is easier to grasp.\nIt swaps specialized terms like “unsupervised language-model adaptation techniques” and “interpolated with a general background model” for plainer phrases (“adjust language models … without human guidance,” “combined with a general model”), breaks dense ideas into shorter sentences, and repeats the core workflow in straightforward language (“turn results into search requests,” “find similar sentences,” “create specific language models”). Those choices eliminate jargon and reduce sentence complexity, so the paper’s main objective—using MT output as queries to build better language models that improve translation—emerges with the least ambiguity.\n\n\n\n\n\n\n\n\n\n","naturalness_comment":"Passage 1 reads more naturally in polished academic English.\nIts sentences employ standard technical terms (“unsupervised language-model adaptation,” “interpolated with a general background model”) and vary rhythm without awkward repetition. Passage 2, while perfectly grammatical, relies on informal or slightly clumsy substitutes (“search requests,” “big improvements,” “adjust language models … without human guidance”) and repeats the same ideas almost verbatim in successive sentences, which makes the prose feel less smooth and professional.\n\n\n\n\n\n\n\n\n\n","simplicity_comment":"Passage 2.\nIts points are divided into shorter, straightforward sentences with few embedded clauses, making each step of the method easy to follow, whereas Passage 1 often compresses several technical details into longer, multi-clause statements.","id":141,"complex_a":"The hypotheses from the [machine translation output] are converted into [queries at different levels of representation power] and used to extract similar sentences from a [very large monolingual text collection].\n\nWe apply a slightly different [sentence-level strategy] to language-model adaptation, first generating an [nbest list] with a [baseline system], then finding similar sentences in a [monolingual target-language corpus].\n\nWe construct [specific language models] by using [machine translation output] as queries to extract similar sentences from [large monolingual corpora].","complex_b":"The results from the translation process are turned into [search requests] at [different levels of detail] and used to find similar sentences from a [huge collection of text] in one language.\n\nWe use a slightly different method by first creating a list of possible translations, then finding similar sentences in a [text collection] of the [target language].\n\nWe make specific language models by using [machine translation results] as [search requests] to find similar sentences in [large text collections].\n\nWe change initial [machine translation guesses] into [search requests] and find similar sentences from a [large collection of text] in one language."}, 
 {"user_id":15,"passage_id":46,"understanding":"Passage 1","naturalness":"Passage 2","simplicity":"Passage 1","understanding_comment":"Passage 1 is easier to grasp.\nIt swaps technical phrases like “corpus level,” “product‐moment,” and “higher-ordered n-grams” for ordinary words (“large set of texts,” “two-word matches”), breaks ideas into brisk, declarative sentences, and consistently explains jargon in plain language (“accuracy and smoothness,” “human opinions can be unreliable and costly”). That straightforward wording and sentence structure make the paper’s aim—offering ORANGE as a way to test MT-evaluation metrics without extra human effort—immediately clear and unambiguous.","naturalness_comment":"Passage 2 sounds more natural and polished.\nIts sentences follow standard academic phrasing (“are usually conducted on corpus level,” “we introduce a new evaluation method”), it uses precise connectors (“however,” “such comparisons rely on”), and it avoids conversational fillers like “looking at things like” found in Passage 1. The terminology is consistent and domain-appropriate, and punctuation is handled cleanly, giving the prose a smoother, more professional flow with no noticeable grammatical slips.\n\n\n\n\n\n\n\n\n\n","simplicity_comment":"Passage 1.\nMost of its ideas are delivered in short, stand-alone sentences with basic subject-verb-object patterns and few embedded clauses, so the structure is quick to parse. Passage 2 often stacks multiple technical phrases in one sentence (e.g., the long opening sentence describing correlation statistics), which makes its syntax denser and harder to scan at a glance.","id":142,"complex_a":"Comparisons of [automatic evaluation measures] for [machine translation] are usually done on a large set of texts using [statistical methods] like [Pearson’s correlation coefficient] or [Spearman’s rank correlation coefficient].\n\nHowever, these comparisons depend on [human opinions] about how good the translation is, looking at things like [how accurate and smooth] it is.\n\nIn this paper, we introduce a new method called [ORANGE], which allows us to evaluate [automatic translation evaluation tools] without needing [extra human input], except for using a set of [reference translations].\n\nBLEU, a common evaluation tool, is adjusted (Lin and Och, 2004b) and it only looks at [two-word matches] because this aligns better with [human opinions] than using [longer word sequences].\n\n\n\n\n\n\n\n\n\n","complex_b":"Comparisons of [automatic evaluation metrics] for [machine translation] are usually conducted on [corpus level] using [correlation statistics] such as [Pearson’s product-moment correlation coefficient] or [Spearman’s rank-order correlation coefficient] between [human scores] and [automatic scores].\n\nIn this paper, we introduce a new evaluation method, [ORANGE], for evaluating [automatic machine-translation evaluation metrics] automatically without [extra human involvement] other than using a set of [reference translations].\n\n[BLEU] is [smoothed] (Lin and Och, 2004b), and it considers only matching up to [bigrams] because this has higher correlations with [human judgments] than when [higher-ordered n-grams] are included.\n\n\n\n\n\n\n\n\n\n"}, 
 {"user_id":15,"passage_id":47,"understanding":"Passage 2","naturalness":"Passage 1","simplicity":"Passage 2","understanding_comment":"Passage 2 is easier to understand.\nIt substitutes technical terms like “clump-based,” “linguistic phrases,” and “broad-coverage rule-based parsers” with everyday wording (“group-based,” “language phrases,” “tools that analyze sentence structure”), explains jargon in parentheses (e.g., defining parsers and the BLEU metric), and breaks dense ideas into shorter, direct sentences—so the paper’s aim of learning rewrite patterns to fix word-order problems in machine translation is immediately clear and unambiguous.\n\n\n\n\n\n\n\n\n\n","naturalness_comment":"Passage 1 reads more naturally in polished academic English.\nIts terminology (“clump-based statistical MT systems,” “broad-coverage rule-based parsers”) and sentence rhythm match the conventions of research writing, it avoids contractions, and it stays concise without parenthetical asides that break flow. Passage 2, though grammatical, introduces informal touches (“don’t,” “tools that analyze sentence structure”), repeats ideas almost verbatim, and uses slightly awkward substitutions (“group-based,” “while running”), so the prose feels less smooth and professional.","simplicity_comment":"Passage 2.\nIts ideas are broken into shorter, plain sentences—often one clause each—and it adds clarifying parentheses instead of embedding long, technical phrases inside single statements, so readers can process each point quickly. Passage 1 bundles more information into longer, multi-clause sentences and uses denser technical wording, making its structure comparatively more complex.","id":143,"complex_a":"Current [clump-based statistical MT systems] have two limitations with respect to word ordering: first, they lack a [mechanism for expressing and using generalization] that accounts for reorderings of [linguistic phrases].\n\nTo address these limitations, we propose to use [automatically learned rewrite patterns] to preprocess the source sentences so that they have a word order similar to that of the [target language].\n\nThe basic model is statistical, but we use [broad-coverage rule-based parsers] in two ways—during [training] for learning rewrite patterns and at [runtime] for reordering the source sentences.\n\nOur [reordering rules] are automatically learned from aligning [parse trees] for both the source and target sentences.","complex_b":"Current [group-based statistical machine translation (MT) systems] have two problems with word order: first, they don’t have a way to handle changes in word order that cover different language phrases.\n\nTo fix these problems, we suggest using [automatically learned rewrite patterns] to rearrange the words in the original sentences so they match the order of the target language.\n\nThe main model is statistical, but we also use [rule-based parsers] in two ways—one during [training] to learn rewrite patterns and the other while [running] to rearrange the original sentences."}, 
 {"user_id":365,"passage_id":556,"understanding":"Passage 1","naturalness":"No Difference","simplicity":"Passage 1","understanding_comment":"easier terms, less technical vocabs","naturalness_comment":"both sounds natural in english","simplicity_comment":"shorter sentences with explanations","id":144,"complex_a":"N/A","complex_b":"In this paper, we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense [hierarchies], namely the Oxford Dictionary of English.:::\n In our [coarse-grained task], the sense inventory was first clustered semi-automatically with each cluster representing an equivalence class over senses."}, 
 {"user_id":25,"passage_id":83,"understanding":"Passage 2","naturalness":"No Difference","simplicity":"Passage 2","understanding_comment":"extra explanation","naturalness_comment":"both sound natural","simplicity_comment":"simpler sentences","id":212,"complex_a":"A subset of the [acquisition algorithm] is implemented and the results are used to [attgment] and critique the structure of [a large hand-built thesaurus.]:::\n We identify a set of [lexico-syntactic patterns] that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest.","complex_b":"n/a"}, 
 {"user_id":15,"passage_id":48,"understanding":"Passage 1","naturalness":"Passage 2","simplicity":"Passage 1","understanding_comment":"Passage 1 is easier to grasp.\nIt replaces dense academic phrases such as “lexicon,” “bi-tag HMMs,” and “non-training intensive framework” with plainer words like “dictionary,” “simpler method,” and “doesn’t require extensive training,” and it briefly defines technical ideas (e.g., “a tool that labels words in a sentence”). Sentences are shorter, connectors are straightforward, and there are fewer embedded clauses, so the paper’s main objective—building and evaluating a context-aware HMM tagger whose accuracy hinges on dictionary quality—emerges clearly and without ambiguity.\n\n\n\n\n\n\n\n\n\n","naturalness_comment":"Passage 2 sounds more natural and polished.\nIts sentences follow standard academic style (“we present,” “evaluate it in both the unsupervised and supervised case”), employ precise vocabulary (“lexicon,” “state-of-the-art,” “framework”), and avoid informal wording or minor slips such as “with help (supervised)” found in Passage 1. Passage 2 also keeps grammatical structures consistent and uses smoother connectors (“Finally,” “Observing that”), giving the prose a fluent, professional flow without awkward pauses or redundancies.","simplicity_comment":"Passage 1.\nIts ideas are mostly delivered in short, single-clause sentences (often under 20 words) with straightforward subject-verb-object patterns and minimal parenthetical insertions, so each point can be digested quickly. Passage 2, by contrast, often packs several technical phrases into longer, multi-clause sentences—especially in the opening and closing lines—which makes its structure denser and slightly harder to scan at a glance.","id":145,"complex_a":"We also provide the first detailed comparison of [unsupervised methods] for labeling [parts of speech], pointing out that previous results have not been consistent across different sets of texts or dictionaries.\n\nWe find that the dictionary's quality greatly affects how accurate the labeling is, so we offer a way to train [HMMs] that makes the labeling more accurate when the dictionary is unstable.\n\nWhile repeating previous tests, we find that success relies on cleaning up the [tag dictionaries] with helpful data from the text.\n\nWe show that the [expectation maximization algorithm], a method for improving accuracy in tagging, works well with [HMMs] when we have a dictionary and certain favorable conditions.\n\nWe notice that past high results in [unsupervised HMM-EM] tests were due to using improved dictionaries, where only common word analyses were included.\n\n\n\n\n\n\n\n\n\n","complex_b":"We present a new [HMM tagger] that exploits context on both sides of a word to be tagged, and evaluate it in both the [unsupervised] and [supervised] case.\n\nAlong the way, we present the first comprehensive comparison of [unsupervised methods] for [part-of-speech tagging], noting that published results to date have not been comparable across [corpora] or [lexicons].\n\nObserving that the quality of the [lexicon] greatly impacts the accuracy that can be achieved by the algorithms, we present a method of [HMM training] that improves accuracy when training of [lexical probabilities] is unstable.\n\nWhile replicating earlier experiments, we discover that performance was highly dependent on cleaning [tag dictionaries] using statistics gleaned from the tokens.\n\nWe show that the [expectation maximization algorithm] for [bi-tag HMMs] is efficient and quite effective for acquiring accurate POS taggers given only a [lexicon] ([tag dictionary]) and certain favorable conditions.\n\nWe observe that earlier [unsupervised HMM-EM] results were artificially high due to use of [Optimized Lexicons], in which only frequent-enough analyses of each word were kept."}, 
 {"user_id":15,"passage_id":49,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 2","understanding_comment":"Passage 2 is easier to follow.\nIt replaces technical jargon like “linear-chain conditional random fields (CRFs) to perform robust and accurate Chinese word segmentation by providing a principled framework” with plain, explanatory phrases (“a method called linear-chain conditional random fields … can effectively handle Chinese word segmentation”), defines the task in everyday terms (“breaking down sentences into words”), and explains CRFs as “a statistical tool for analyzing sequences.” Sentences are shorter, definitions are embedded where needed, and the workflow—segmenting characters and detecting new words—is described in simple yes-or-no decisions, so the paper’s objective comes across with minimal ambiguity.","naturalness_comment":"Passage 2 sounds more natural overall.\nIts sentences flow smoothly, avoid duplication, and contain no obvious grammatical slips, whereas Passage 1 repeats ideas almost verbatim, contains awkward phrasing (“was also demonstrated in word segmentation)”), and has mismatched punctuation. Passage 2 keeps terminology consistent, explains concepts clearly, and maintains a steady, readable rhythm, giving it the more fluent, polished feel.","simplicity_comment":"Passage 2.\nMost ideas are delivered in short, single-clause sentences (e.g., “We also introduce a method for finding new words, which helps improve accuracy.”), and technical concepts are broken into step-by-step descriptions—so readers can grasp each point quickly. Passage 1, by contrast, packs multiple technical phrases into longer, multi-clause sentences and repeats similar statements, making its structure denser and harder to scan at a glance.","id":146,"complex_a":"This paper demonstrates the ability of [linear-chain conditional random fields (CRFs)] to perform robust and accurate Chinese word segmentation by providing a [principled framework] that easily supports the integration of domain knowledge via [multiple lexicons of characters and words].\n\nCRF is a [statistical sequence-modeling framework] introduced by [Lafferty et al. (2001)], and we use it for the Chinese word-segmentation task by treating word segmentation as a [binary decision task].\n\nWe first use this framework for Chinese word segmentation by treating it as a [binary decision task], such that each character is labeled either as the [beginning of a word] or the [continuation of one].\n\nWe define the [word-segmentation problem] as labeling each character according to whether the [previous character boundary] of the current character is a [word boundary].\n\n\n\n\n\n\n\n\n\n","complex_b":"This paper shows how a method called [linear-chain conditional random fields (CRFs)] can effectively handle Chinese word segmentation by providing a [structured approach] that easily incorporates [specialized knowledge] using [various lists of characters and words].\n\nCRF is a [statistical tool for analyzing sequences], introduced by Lafferty et al. (2001), and we use it for Chinese word segmentation by [treating it as a task] where you make [yes-or-no decisions].\n\nWe apply this method by [labeling each character as either the start of a new word or a continuation of the current word].\n\nWe define the word-segmentation problem by [deciding if each character is the start of a new word or continues from the previous character]."}, 
 {"user_id":365,"passage_id":557,"understanding":"Passage 1","naturalness":"No Difference","simplicity":"Passage 1","understanding_comment":"easier terms and shorter sentences","naturalness_comment":"both sounds natural in English","simplicity_comment":"shorter sentences","id":147,"complex_a":"We present an empirical comparison of Espresso with various state of the art systems, on different size and [genre corpora], on extracting various general and specific relations.:::\n Experimental results show that our [exploitation] of generic patterns substantially increases system recall with small effect on overall [precision.]:::\n In the pattern [induction step], our system computes a [reliability score] for each candidate pattern based on the weighted pointwise mutual information, PMI, of the pattern with all instances extracted so far.","complex_b":"Our program, which doesn't need much [supervision], starts with a single set that mixes different types of examples, like leader-panel and oxygen-water, which relate to member-of and part-of relationships as described by Keet and Artale in 2008."}, 
 {"user_id":15,"passage_id":50,"understanding":"Passage 1","naturalness":"Passage 2","simplicity":"Passage 1","understanding_comment":"Passage 1 is easier to understand.\nIt translates the technical workflow into everyday language—e.g., “identifying the main parts of sentences and the meaning behind them” rather than “identifying predicate-argument structures,” and “make informed guesses” instead of “perform structured probabilistic inference.” Jargon such as “semantic representations,” “domain and scenario model,” or “scalable and expressive representation” is either simplified or omitted. Sentences are generally shorter, with clear step-by-step explanations, so the paper’s goal—using semantic structures plus probabilistic reasoning to answer tough natural-language questions—emerges with little ambiguity.\n\n\n\n\n\n\n\n\n\n","naturalness_comment":"Passage 2 reads more naturally in polished academic English.\nIts sentences follow standard scholarly phrasing (“complex questions posed in Natural Language,” “performing structured probabilistic inference”), use precise connectors (“In this paper we describe…,” “The results indicate…”), and avoid colloquial expressions like “make informed guesses” that appear in Passage 1. Terminology is used consistently, clauses are balanced, and punctuation is smooth, giving Passage 2 a fluent, professional cadence with no noticeable grammatical slips.","simplicity_comment":"Passage 1.\nMost ideas are broken into shorter, single-clause sentences with everyday wording (“make informed guesses,” “using the relationships we extract”) and only occasional parenthetical phrases, so each step is quick to parse. Passage 2 often combines several technical phrases in longer, multi-clause sentences—especially when describing semantic representations and inference—making its structure denser and harder to scan at a glance.","id":148,"complex_a":"The ability to answer hard questions written in everyday language depends on (1) how detailed the [semantic information] is and (2) the [reasoning methods] that this information supports.\n\nIn this paper, we describe a [question-answering (QA) system] where questions are examined and possible answers are generated by 1) identifying the [main parts of sentences] and the [meaning] behind them from the input, and 2) using these [extracted relationships] to make [informed guesses] within a [specific topic or situation].\n\nA new feature of our system is a flexible and clear way of showing actions and events using [Coordinated Probabilistic Relational Models (CPRM)], which is a method to predict outcomes based on relationships.\n\nOur [QA system] uses [PropBank/FrameNet labels] as input, which help identify what actions are being described and with what details, and then it gives an answer by using [models that predict actions based on chance and reasoning].","complex_b":"The ability to answer complex questions posed in [Natural Language] depends on (1) the depth of the available [semantic representations] and (2) the [inferential mechanisms] they support.\n\nIn this paper we describe a [QA architecture] where questions are analyzed and candidate answers generated by 1) identifying [predicate argument structures] and [semantic frames] from the input and 2) performing [structured probabilistic inference] using the extracted relations in the context of a [domain and scenario model].\n\nA novel aspect of our system is a scalable and expressive representation of actions and events based on [Coordinated Probabilistic Relational Models (CPRM)].\n\nIn this paper we report on the ability of the implemented system to perform several forms of [probabilistic and temporal inferences] to extract answers to complex questions.\n\nOur [question answering system] takes [PropBank/FrameNet annotations] as input, uses the [PropBank targets] to indicate which actions are being described with which arguments, and produces an answer using [probabilistic models] of actions as the tools of inference."}, 
 {"user_id":365,"passage_id":558,"understanding":"Passage 1","naturalness":"No Difference","simplicity":"Passage 1","understanding_comment":"easier terms","naturalness_comment":"both sound natural","simplicity_comment":"simpler and shorter length","id":149,"complex_a":"We show that using the SMT approach can find mistakes that common [proofreading tools] for native speakers often miss.","complex_b":"Our system was able to correct 61.81% of mistakes in a set of naturally-occurring examples of mass noun errors found on the World Wide Web, suggesting that efforts to collect [alignable corpora] of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the [complex syntactic and lexical problems] found in the writing of ESL learners.:::\n We utilize phrasal Statistical Machine Translation (SMT) techniques to correct ESL writing errors and demonstrate that this [data-intensive SMT approach] is very promising, but we also point out SMT approach relies on the availability of large amount of training data."}, 
 {"user_id":365,"passage_id":559,"understanding":"Passage 2","naturalness":"No Difference","simplicity":"Passage 2","understanding_comment":"easier terms and less technical vocabs","naturalness_comment":"both sound natural","simplicity_comment":"shorter and simpler sentences","id":150,"complex_a":"We [utilize meta-patterns] of high-frequency words and content words in order to discover pattern candidates.:::\n Symmetric patterns are then identified using graph-based measures, and word categories are created based on [graph clique sets].","complex_b":"N/A"}, 
 {"user_id":25,"passage_id":84,"understanding":"Passage 1","naturalness":"No Difference","simplicity":"Passage 1","understanding_comment":"easier to understand","naturalness_comment":"both natural in English","simplicity_comment":"simpler sentences","id":213,"complex_a":"n/a","complex_b":"We show that [conventional context-free parsing techniques] can be used in creating a parse forest for a sentence in DOP1."}, 
 {"user_id":16,"passage_id":51,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 2","understanding_comment":"For people who has no idea what the topic is, passage 2 make it easier for readers to understand ","naturalness_comment":"Simply because it is easier to understand ","simplicity_comment":"Compared to passage 1 it is much easier to understand ","id":152,"complex_a":"We propose, in the scenario of extracting is-a relations, one pattern-based approach and compared it with a baseline syntactic distributional similarity method (called syntactic co-occurrence in their paper).:::\n We propose a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performance and efficiency.","complex_b":"N/A"}, 
 {"user_id":16,"passage_id":52,"understanding":"Passage 1","naturalness":"Passage 1","simplicity":"Passage 1","understanding_comment":"Concise and use simple words","naturalness_comment":"Complexity in passage 2","simplicity_comment":"Simpler vocabulary ","id":153,"complex_a":"N/A","complex_b":"Abstracting from results for concrete test sets, we try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends."}, 
 {"user_id":16,"passage_id":53,"understanding":"Passage 1","naturalness":"Passage 1","simplicity":"Passage 1","understanding_comment":"Clearer phrases0","naturalness_comment":"Natural and easy to understand ","simplicity_comment":"Easily understandable ","id":154,"complex_a":"N/A","complex_b":"Unlike the basic connection structures (dependency structures) the parser gives, these can be used directly to understand meaning (semantic interpretation)."}, 
 {"user_id":16,"passage_id":54,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 2","understanding_comment":"Easier words ","naturalness_comment":"Natural","simplicity_comment":"Easily understandable ","id":155,"complex_a":"The system is tested on data from a specific competition in 2004 about identifying word roles and performs very well.","complex_b":"N/A"}, 
 {"user_id":19,"passage_id":61,"understanding":"Passage 1","naturalness":"Passage 1","simplicity":"Passage 1","understanding_comment":"Easy","naturalness_comment":"Simple","simplicity_comment":"Easily understandable ","id":158,"complex_a":"N/A","complex_b":"A Uniform Approach to Analogies Synonyms Antonyms and Associations Recognizing analogies (like comparisons), synonyms (words with similar meanings), antonyms (opposite words), and associations (related words) seem like four different jobs, needing different language processing methods."}, 
 {"user_id":19,"passage_id":64,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 2","understanding_comment":"Easier","naturalness_comment":"Simpler ","simplicity_comment":"Understandable ","id":161,"complex_a":"Robust Sentiment Detection on Twitter from Biased and Noisy Data In this paper, we suggest a way to automatically find feelings in Twitter messages (tweets) by looking at how tweets are written and extra information about the words in these messages.","complex_b":"N/A"}, 
 {"user_id":19,"passage_id":65,"understanding":"Passage 1","naturalness":"Passage 1","simplicity":"Passage 1","understanding_comment":"Easier ","naturalness_comment":"Simple","simplicity_comment":"Understandable ","id":162,"complex_a":"N/A","complex_b":"By using 50 Twitter tags and 15 smileys (faces showing emotions) as labels for feelings, this method avoids the need for time-consuming manual labeling, allowing it to identify and classify different feeling types in short texts."}, 
 {"user_id":19,"passage_id":69,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 2","understanding_comment":"Easy","naturalness_comment":"Simple","simplicity_comment":"Understandable ","id":166,"complex_a":"Since the set of trees needed to parse an input sentence is supposed to be finite, the parser can use in principle any search strategy.","complex_b":"N/A"}, 
 {"user_id":19,"passage_id":70,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 2","understanding_comment":"Easier ","naturalness_comment":"Simpler ","simplicity_comment":"Understandable ","id":167,"complex_a":"In this paper, we explore a more daring idea: not only can one grammar be used by different processes working in different ways, but the same system for understanding and creating language can be used to handle the grammar in different modes.","complex_b":"N/A"}, 
 {"user_id":372,"passage_id":571,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"No Difference","understanding_comment":"Easy verb to read","naturalness_comment":"More fluent sentence connection","simplicity_comment":"No specific diff","id":176,"complex_a":"Finally, we show that a taxonomy built using our algorithm shows a 23% relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs.","complex_b":"We successfully add 10,000 new word groups to WordNet 2.1 with high accuracy, making 70% fewer mistakes compared to older methods."}, 
 {"user_id":375,"passage_id":581,"understanding":"No Difference","naturalness":"No Difference","simplicity":"Passage 1","understanding_comment":"no ambiguity","naturalness_comment":"there is not much verb or noun difference","simplicity_comment":"passage1 seems use more conjunction","id":177,"complex_a":"We offer an alternative approach to resolve such irregularities by normalizing SMS texts before MT.","complex_b":"We offer a different solution to fix these differences by adjusting SMS texts before using MT."}, 
 {"user_id":375,"passage_id":582,"understanding":"Passage 2","naturalness":"No Difference","simplicity":"Passage 1","understanding_comment":"passage2 seems like deductive paper than passage 1","naturalness_comment":"there is no big difference of grammatical error and fluency.","simplicity_comment":"passage1 is more shorter sentence to read ","id":178,"complex_a":"We show that a parser can be accurate and fast without needing large, manually-made treebanks from specific fields, making it easier to use in tools that need to understand sentence structures.","complex_b":"Evaluating The Accuracy Of An Unlexicalized Statistical Parser On The PARC DepBank We evaluate the accuracy of an unlexicalized statistical parser, trained on 4K treebanked sentences from balanced data and tested on the PARC DepBank."}, 
 {"user_id":21,"passage_id":61,"understanding":"Passage 2","naturalness":"Passage 1","simplicity":"Passage 2","understanding_comment":"easier words","naturalness_comment":"natural with lack in details","simplicity_comment":"much more understandable","id":179,"complex_a":"N/A","complex_b":"M/A"}, 
 {"user_id":21,"passage_id":62,"understanding":"Passage 1","naturalness":"Passage 1","simplicity":"Passage 2","understanding_comment":"clear descriptions for first-readers","naturalness_comment":"easier to read","simplicity_comment":"shorter than 1","id":180,"complex_a":"N/A","complex_b":"We are convinced that the Hash Kernel and the parallelization can be applied successful to other NLP applications as well such as transition based dependency parsers, phrase structrue parsers, and machine translation."}, 
 {"user_id":21,"passage_id":63,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 2","understanding_comment":"no ambiguity","naturalness_comment":"natural","simplicity_comment":"easier words","id":181,"complex_a":"N/A","complex_b":"N/A"}, 
 {"user_id":21,"passage_id":64,"understanding":"Passage 1","naturalness":"Passage 2","simplicity":"Passage 1","understanding_comment":"easier words and phrases","naturalness_comment":"natural","simplicity_comment":"easy understanding","id":182,"complex_a":"In our tests, we demonstrate that because our method captures a more general idea of tweets, it works better than older methods and handles imperfect and biased data better, which is the type of data from these sources.","complex_b":"In our experiments, we show that since our features are able to capture a more abstract representation of tweets, our solution is more effective than previous ones and also more robust regarding biased and noisy data, which is the kind of data provided by these sources."}, 
 {"user_id":21,"passage_id":65,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 1","understanding_comment":"easier words","naturalness_comment":"natural","simplicity_comment":"shorter","id":183,"complex_a":"N/A","complex_b":"N/A"}, 
 {"user_id":21,"passage_id":66,"understanding":"Passage 1","naturalness":"Passage 1","simplicity":"Passage 1","understanding_comment":"clear","naturalness_comment":"natural","simplicity_comment":"easily understandable","id":184,"complex_a":"N/A","complex_b":"N/A"}, 
 {"user_id":21,"passage_id":67,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 2","understanding_comment":"easier","naturalness_comment":"natural","simplicity_comment":"understandable","id":185,"complex_a":"n/a","complex_b":"n/a"}, 
 {"user_id":21,"passage_id":68,"understanding":"Passage 2","naturalness":"Passage 1","simplicity":"Passage 1","understanding_comment":"clear phrases","naturalness_comment":"easy words used","simplicity_comment":"shorter","id":186,"complex_a":"n/a","complex_b":"The language model tells how probable a given sentence is in the source language, the translation model indicates how likely it is that a particular target sentence is a translation of a given source sentence, and the decoder is what actually takes a source sentence as input and produces its translation as output."}, 
 {"user_id":21,"passage_id":69,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 2","understanding_comment":"easier","naturalness_comment":"easier and natural expressions","simplicity_comment":"easy understanding","id":187,"complex_a":"n/a","complex_b":"n/a"}, 
 {"user_id":21,"passage_id":70,"understanding":"Passage 2","naturalness":"Passage 2","simplicity":"Passage 1","understanding_comment":"clear","naturalness_comment":"natural","simplicity_comment":"shorter","id":188,"complex_a":"n/a","complex_b":"n/a"}, 
 {"user_id":22,"passage_id":71,"understanding":"Passage 1","naturalness":"Passage 1","simplicity":"Passage 1","understanding_comment":"Shorter sentences.","naturalness_comment":"Shorter sentences. Straight to the point.","simplicity_comment":"Same reason","id":189,"complex_a":"N/A","complex_b":"N/A"}, 
 {"user_id":23,"passage_id":71,"understanding":"Passage 2","naturalness":"No Difference","simplicity":"Passage 2","understanding_comment":"Easier words","naturalness_comment":"Both sounds natural in english","simplicity_comment":"Simple, shorter sentences","id":190,"complex_a":"The resulting system, Feature Structure based Tree Adjoining Grammars (FTAG), captures the principle of factoring dependencies and recursion, fundamental to TAG's.:::\n A Feature-based TAG consists of a set of (auxiliary or initial) elementary trees and of two tree-composition operations: substitution and adjunction.","complex_b":"N/A"}, 
 {"user_id":23,"passage_id":72,"understanding":"Passage 1","naturalness":"No Difference","simplicity":"Passage 1","understanding_comment":"Easier words","naturalness_comment":"Both sounds natural in English","simplicity_comment":"shorter sentences","id":191,"complex_a":"Our method combines two earlier, separate techniques for understanding word meanings: using digital dictionaries and spreading activation models [(a way to mimic how the brain processes information)].","complex_b":"We apply conventional spreading activation approaches to word sense disambiguation.:::\n Word Sense Disambiguation With Very Large Neural Networks Extracted From Machine Readable Dictionaries In this paper, we describe a means for automatically building very large neural networks (VLNNs) from definition texts in machine-readable dictionaries, and demonstrate the use of these networks for word sense [disambiguation.]:::\n Our method brings together two earlier, independent approaches to word sense [disambiguation]: the use of machine-readable dictionaries and spreading and activation models."}, 
 {"user_id":23,"passage_id":73,"understanding":"Passage 2","naturalness":"No Difference","simplicity":"Passage 2","understanding_comment":"Easy to understand","naturalness_comment":"Both sounds natural","simplicity_comment":"shorter sentences","id":192,"complex_a":"The second central idea is to treat morphological disambiguation and syntactic labelling by the same mechanism of discarding improper alternatives.:::\n We see this task as one of inferring surface structure from [a stream of concrete tokens] in a basically bottom-up mode.:::\n The ensemble of constraints for language L [constitute] a Constraint Grammar (CG) for L.:::\n One central idea is to maximize the use of morphological information for parsing purposes.:::\n Our input tokens to CGP are [morphologically analyzed word-forms.]","complex_b":"N/A"}, 
 {"user_id":23,"passage_id":74,"understanding":"Passage 1","naturalness":"No Difference","simplicity":"Passage 1","understanding_comment":"Easier words","naturalness_comment":"Both sounds natural","simplicity_comment":"Simple, shorter sentences","id":193,"complex_a":"N/A","complex_b":"N/A"}, 
 {"user_id":23,"passage_id":75,"understanding":"Passage 2","naturalness":"No Difference","simplicity":"Passage 2","understanding_comment":"Easier words","naturalness_comment":"Both sounds natural","simplicity_comment":"shorter words","id":194,"complex_a":"A synchronous derivation process for the two syntactic structures of both languages suggests the level of cross-lingual isomorphism between the two trees (e.g.:::\n The formalism's intended usage is to relate expressions of natural languages [to their associated semantics represented in a logical form language], or to their translates in another natural language; in summary, we intend it to allow TAGs to be used beyond their role in syntax proper.","complex_b":"N/A"}, 
 {"user_id":23,"passage_id":76,"understanding":"No Difference","naturalness":"No Difference","simplicity":"No Difference","understanding_comment":"Both easy to undersatnd","naturalness_comment":"Both sound natural","simplicity_comment":"Both are simple","id":195,"complex_a":"The type system supports an object-oriented method (a way of organizing information like objects) for describing language by providing a way to share properties [(multiple inheritance)] and a way to define relations between different language levels, treated as classes of objects.","complex_b":"na"}, 
 {"user_id":23,"passage_id":77,"understanding":"No Difference","naturalness":"No Difference","simplicity":"No Difference","understanding_comment":"Both easy to understand","naturalness_comment":"Both sound natural","simplicity_comment":"No difference","id":196,"complex_a":"This paper presents an automatic scheme for collecting statistics on cooccurrence patterns in a large corpus.","complex_b":"na"}, 
 {"user_id":23,"passage_id":78,"understanding":"Passage 1","naturalness":"No Difference","simplicity":"No Difference","understanding_comment":"easy to understand","naturalness_comment":"both sound natural","simplicity_comment":"both are simple","id":197,"complex_a":"na","complex_b":"The difficulties of identifying words include (l) the identification of complex words, such as Determinative-Measure, reduplications, derived words etc., (2) the identification of proper names,(3) resolving the [ambiguous segmentations.]:::\n We adopt a matching algorithm with 6 different [heuristic rules] to resolve the ambiguities and achieve a 99.77% of the success rate."}, 
 {"user_id":23,"passage_id":79,"understanding":"No Difference","naturalness":"No Difference","simplicity":"No Difference","understanding_comment":"no difference","naturalness_comment":"no difference","simplicity_comment":"no difference","id":198,"complex_a":"n/a","complex_b":"n/a"}, 
 {"user_id":23,"passage_id":80,"understanding":"Passage 1","naturalness":"No Difference","simplicity":"Passage 1","understanding_comment":"easier words","naturalness_comment":"Both sound natural","simplicity_comment":"simpler sentences","id":199,"complex_a":"n/a","complex_b":"We apply generalizations about the salience of properties of objects and conventions about what words make base level attributions to incrementally select words for inclusion in a description."}, 
 {"user_id":376,"passage_id":581,"understanding":"No Difference","naturalness":"No Difference","simplicity":"No Difference","understanding_comment":"no difference","naturalness_comment":"both sound natural","simplicity_comment":"no difference","id":200,"complex_a":"n/a","complex_b":"n/a"}, 
 {"user_id":376,"passage_id":582,"understanding":"Passage 1","naturalness":"No Difference","simplicity":"Passage 1","understanding_comment":"easier to understand","naturalness_comment":"both sound natural","simplicity_comment":"simpler sentences","id":201,"complex_a":"n/a","complex_b":"We demonstrate that a parser which is competitive in accuracy (without sacrificing processing speed) can be quickly tuned without reliance on large [in-domain manually-constructed treebanks.]:::\n We show that the system has equivalent accuracy to the PARC XLE parser when the [morphosyntactic features] in the original DepBank gold standard are taken into account."}, 
 {"user_id":376,"passage_id":583,"understanding":"Passage 2","naturalness":"No Difference","simplicity":"Passage 2","understanding_comment":"provides extra explanation","naturalness_comment":"both sound natural","simplicity_comment":"simpler sentences","id":202,"complex_a":"We use a publicly available structured output SVM to create a [max-margin syntactic aligner ]with [a soft cohesion constraint].","complex_b":"This aligner is the first, as far as we know, to use a learning method that focuses on differences to train an ITG bitext parser (a tool for analyzing bilingual text).:::\n We use dependency structures as flexible rules to improve word pairings in an ITG framework (a method for managing translations)."}, 
 {"user_id":376,"passage_id":584,"understanding":"Passage 1","naturalness":"No Difference","simplicity":"Passage 1","understanding_comment":"easier","naturalness_comment":"both sound natural","simplicity_comment":"shorter sentences","id":203,"complex_a":"n/a","complex_b":"In dependency-based parsing, several constraints have been proposed that restrict the class of permissible structures, such as [projectivity, planarity, multi-planarity, well-nestedness, gap degree, and edge degree].:::\n [In this paper, we review and compare the different constraints theoretically, and provide an experimental evaluation using data from two treebanks, investigating how large a proportion of the structures found in the treebanks are permitted under different constraints.]"}, 
 {"user_id":376,"passage_id":585,"understanding":"Passage 2","naturalness":"No Difference","simplicity":"No Difference","understanding_comment":"easy to understand","naturalness_comment":"both sound natural","simplicity_comment":"shorter, simpler","id":204,"complex_a":"Given a user's query, the system will automatically create patterns to extract [salient relations] in the text of the topic, and build tables from the extracted information using paraphrase discovery technology.","complex_b":"n/a"}, 
 {"user_id":376,"passage_id":586,"understanding":"Passage 1","naturalness":"No Difference","simplicity":"Passage 1","understanding_comment":"shorter, easier","naturalness_comment":"both sound natural","simplicity_comment":"easier","id":205,"complex_a":"n/a","complex_b":"Since the error surface for many natural language problems is piecewise constant and riddled with local minima, many systems instead optimize log-likelihood, which is conveniently differentiable and convex."}, 
 {"user_id":376,"passage_id":587,"understanding":"Passage 2","naturalness":"No Difference","simplicity":"No Difference","understanding_comment":"easier words","naturalness_comment":"both sound natural","simplicity_comment":"both are simple","id":206,"complex_a":"We [conceptualize] a network of words that capture [the word co-occurrence patterns].","complex_b":"n/a"}, 
 {"user_id":376,"passage_id":588,"understanding":"Passage 1","naturalness":"No Difference","simplicity":"Passage 1","understanding_comment":"easier","naturalness_comment":"both natural","simplicity_comment":"shorter, simpler","id":207,"complex_a":"n/a","complex_b":"The new version includes a revised and more [semantically-motivated output representation], an enhanced grammar and [part-of-speech tagger lexicon], and a more flexible and semi-supervised training method for the structural parse ranking model."}, 
 {"user_id":376,"passage_id":589,"understanding":"Passage 2","naturalness":"No Difference","simplicity":"Passage 2","understanding_comment":"easier to understand","naturalness_comment":"both sound natural","simplicity_comment":"simpler, shorter","id":208,"complex_a":"We propose a novel model for unsupervised word alignment which explicitly takes into account [target language constituent structure], while [retaining the robustness and efficiency of the HMM alignment model].:::\n Tailoring Word Alignments to Syntactic Machine Translation Extracting tree transducer rules for syntactic MT systems can be hindered by word alignment errors that [violate syntactic correspondences].","complex_b":"n/a"}, 
 {"user_id":376,"passage_id":590,"understanding":"No Difference","naturalness":"No Difference","simplicity":"No Difference","understanding_comment":"both were easy to understand","naturalness_comment":"both sound natural in English","simplicity_comment":"both are simple","id":209,"complex_a":"n/a","complex_b":"n/a"}, 
 {"user_id":25,"passage_id":81,"understanding":"Passage 2","naturalness":"No Difference","simplicity":"Passage 2","understanding_comment":"extra explanation","naturalness_comment":"both sound natural","simplicity_comment":"simpler sentences","id":210,"complex_a":"Finally, we should how SLTAG enables to define a [lexicalized version of stochastic context-free grammars ]and we report [preliminary experiments] showing some of the advantages of SLTAG over stochastic context-free grammars.","complex_b":"n/a"}, 
 {"user_id":25,"passage_id":82,"understanding":"Passage 1","naturalness":"No Difference","simplicity":"Passage 1","understanding_comment":"easy to understand","naturalness_comment":"both natural","simplicity_comment":"simpler sentences","id":211,"complex_a":"n/a","complex_b":"The categories listed for a word in Roger's index tend to correspond to sense distinctions; thus selecting the most likely category provides a useful level of sense disambiguation.:::\n Applied to the 10 million word Grolier's Encyclopedia, the system [correctly disambiguated] 92% of the instances of 12 [polysemous words] that have been previously studied in the literature.:::\n From [the perspective of a generative process], neighboring words of a target are generated by the target's underlying sense."}, 
 {"user_id":25,"passage_id":85,"understanding":"Passage 1","naturalness":"No Difference","simplicity":"Passage 2","understanding_comment":"extra explanation","naturalness_comment":"both natural","simplicity_comment":"simpler, shorter sentences","id":214,"complex_a":"We present a surface-syntactic analyser that extracts maximal length noun phrases mainly sequences of determiners, [premodifiers, nominal heads,] and certain kinds of post modifying prepositional phrases and adjectives from French texts for terminology applications.:::\n In the first stage, LEXTER uses a base of rules designed to identify [frontier markers] in view to analysing the texts and extracting maximal-length noun phrases.","complex_b":"n/a"}, 
 {"user_id":25,"passage_id":86,"understanding":"No Difference","naturalness":"No Difference","simplicity":"No Difference","understanding_comment":"no difference","naturalness_comment":"no difference","simplicity_comment":"no difference","id":215,"complex_a":"In this paper, a new part-of-speech tagging method using neural networks (Net-Tagger) is introduced and its performance is compared to that of an HMM-tagger (a previous method from Cutting et al., 1992) and a trigram-based tagger (another method from Kempe, 1993).","complex_b":"n/a"}, 
 {"user_id":25,"passage_id":87,"understanding":"Passage 2","naturalness":"No Difference","simplicity":"Passage 2","understanding_comment":"easier to understand","naturalness_comment":"both sound natural","simplicity_comment":"simpler","id":216,"complex_a":"The proposed Japanese [morphological analyzer] achieved 95.l% recall and 94.6% precision for open text when it was trained and tested on the ATR Corpus.","complex_b":"n/a"}, 
 {"user_id":25,"passage_id":88,"understanding":"No Difference","naturalness":"No Difference","simplicity":"Passage 1","understanding_comment":"both are easy to understand","naturalness_comment":"both sound natural","simplicity_comment":"simpler sentence structure","id":217,"complex_a":"n/a","complex_b":"Comlex Syntax: Building A Computational Lexicon We describe the design of Complex Syntax, a computational lexicon providing detailed syntactic information for approximately 38,000 English headwords."}, 
 {"user_id":25,"passage_id":89,"understanding":"Passage 2","naturalness":"No Difference","simplicity":"No Difference","understanding_comment":"easy to understand","naturalness_comment":"both sound natural","simplicity_comment":"both are simple","id":218,"complex_a":"n/a","complex_b":"n/a"}, 
 {"user_id":25,"passage_id":90,"understanding":"Passage 1","naturalness":"No Difference","simplicity":"Passage 1","understanding_comment":"easier to understand","naturalness_comment":"both sound natural","simplicity_comment":"simpler sentences","id":219,"complex_a":"n/a","complex_b":"[Discriminant analysis makes it possible to use a large number of parameters taht may be specific for a certain corpus or information streatm, and combine them into a small number of function, with the parameters weighted on bais of how useful they are for discriminating text genres.]:::\n We word length [as an indicator of formality] for applications such as genre classification."}]