--
-- PostgreSQL database dump
--

-- Dumped from database version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
-- Dumped by pg_dump version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

SET default_tablespace = '';

SET default_table_access_method = heap;

--
-- Name: passages; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.passages (
    id integer NOT NULL,
    passage_a text,
    passage_b text,
    fake_complex text
);


ALTER TABLE public.passages OWNER TO postgres;

--
-- Name: passages_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.passages_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.passages_id_seq OWNER TO postgres;

--
-- Name: passages_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.passages_id_seq OWNED BY public.passages.id;


--
-- Name: responses; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.responses (
    user_id integer NOT NULL,
    passage_id integer,
    understanding text,
    naturalness text,
    simplicity text,
    understanding_comment text,
    naturalness_comment text,
    simplicity_comment text,
    id integer NOT NULL,
    complex_a text,
    complex_b text
);


ALTER TABLE public.responses OWNER TO postgres;

--
-- Name: responses_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.responses_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.responses_id_seq OWNER TO postgres;

--
-- Name: responses_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.responses_id_seq OWNED BY public.responses.id;


--
-- Name: survey_set_passages; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.survey_set_passages (
    id integer NOT NULL,
    survey_set_id integer,
    passage_id integer
);


ALTER TABLE public.survey_set_passages OWNER TO postgres;

--
-- Name: survey_set_passages_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.survey_set_passages_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.survey_set_passages_id_seq OWNER TO postgres;

--
-- Name: survey_set_passages_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.survey_set_passages_id_seq OWNED BY public.survey_set_passages.id;


--
-- Name: survey_sets; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.survey_sets (
    id integer NOT NULL,
    name text NOT NULL
);


ALTER TABLE public.survey_sets OWNER TO postgres;

--
-- Name: survey_sets_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.survey_sets_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.survey_sets_id_seq OWNER TO postgres;

--
-- Name: survey_sets_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.survey_sets_id_seq OWNED BY public.survey_sets.id;


--
-- Name: updated_passages; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.updated_passages (
    id integer NOT NULL,
    passage_a text,
    passage_b text
);


ALTER TABLE public.updated_passages OWNER TO postgres;

--
-- Name: user_assignments; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.user_assignments (
    user_id integer NOT NULL,
    survey_set_id integer NOT NULL
);


ALTER TABLE public.user_assignments OWNER TO postgres;

--
-- Name: users; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.users (
    user_id integer NOT NULL,
    survey_token text,
    assigned boolean DEFAULT false
);


ALTER TABLE public.users OWNER TO postgres;

--
-- Name: users_user_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.users_user_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.users_user_id_seq OWNER TO postgres;

--
-- Name: users_user_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.users_user_id_seq OWNED BY public.users.user_id;


--
-- Name: passages id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.passages ALTER COLUMN id SET DEFAULT nextval('public.passages_id_seq'::regclass);


--
-- Name: responses id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.responses ALTER COLUMN id SET DEFAULT nextval('public.responses_id_seq'::regclass);


--
-- Name: survey_set_passages id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.survey_set_passages ALTER COLUMN id SET DEFAULT nextval('public.survey_set_passages_id_seq'::regclass);


--
-- Name: survey_sets id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.survey_sets ALTER COLUMN id SET DEFAULT nextval('public.survey_sets_id_seq'::regclass);


--
-- Name: users user_id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.users ALTER COLUMN user_id SET DEFAULT nextval('public.users_user_id_seq'::regclass);


--
-- Data for Name: passages; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.passages (id, passage_a, passage_b, fake_complex) FROM stdin;
1	TnT - A Statistical Part-Of-Speech Tagger Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework. A recent comparison has even shown that TnT performs significantly better for the tested corpora. We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words. Furthermore, we present evaluations on two corpora. We achieve the automated tagging of a syntactic-structure-based set of grammatical function tags including phrase-chunk and syntactic-role modifiers trained in supervised mode from a tree bank of German.	TnT - A Statistical Part-Of-Speech Tagger Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Unlike what others have said in scientific writings, we believe that a tagger using Markov models works just as well as other methods like the Maximum Entropy approach. A recent comparison even showed that TnT works much better for the tested text collections. We explain the basic model of TnT, the methods used for smoothing (making data less rough) and for dealing with unknown words. Additionally, we show results from tests on two text collections. We successfully automate the tagging of grammatical function tags (marking parts of speech) based on sentence structure, including phrase chunks (groups of words) and syntactic roles (word functions), trained with guidance from a German language database.	successfully automate
2	Sentence Reduction For Automatic Text Summarization We present a novel sentence reduction system for automatically removing extraneous phrases from sentences that are extracted from a document for summarization purpose. The system uses multiple sources of knowledge to decide which phrases in an extracted sentence can be removed, including syntactic knowledge, context information, and statistics computed from a corpus which consists of examples written by human professionals. Reduction can significantly improve the conciseness of automatic summaries. We study a new method to remove extraneous phrase from sentences by using multiple source of knowledge to decide which phrase in the sentences can be removed. In our approach, decisions about which material to include/delete in the sentence summaries do not rely on relative frequency information on words, but rather on probability models of subtree deletions that are learned from a corpus of parses for sentences and their summaries.	Sentence Reduction For Automatic Text Summarization We introduce a new system that automatically cuts out unnecessary parts from sentences taken from a document to make summaries. The system uses different types of information to figure out which parts of a sentence can be removed, like sentence structure knowledge, surrounding context, and data from a collection of examples written by experts. Cutting down sentences can greatly improve how clear and brief summaries are. We explore a new way to remove extra parts from sentences by using various types of information to decide which parts can be taken out. In our method, the choices about what to keep or remove in sentence summaries are not based on how often words appear, but instead on models that predict which parts of a sentence can be deleted, based on examples of sentences and their summaries.	surrounding context
3	Advances In Domain Independent Linear Text Segmentation This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering. We design an artificial dataset, built by concatenating short pieces of texts that have been extracted from the Brown corpus.	Advances In Domain Independent Linear Text Segmentation This paper describes a method for breaking down text into parts, which is twice as accurate and over seven times faster than the latest technology (Reynar, 1998). Instead of comparing sentences directly, it uses their position within the nearby text. Boundaries, or breaks in the text, are found using a method that splits the text into groups. We create a fake dataset by joining together short pieces of text taken from the Brown corpus, which is a collection of texts used for research.	research
4	A Simple Approach To Building Ensembles Of Naive Bayesian Classifiers For Word Sense Disambiguation This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. Despite the simplicity of this approach, empirical results disambiguating the widely studied nouns line and interest show that such an ensemble achieves accuracy rivaling the best previously published results. We present an ensemble of eighty-one Naive Bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features.	A Simple Approach To Building Ensembles Of Naive Bayesian Classifiers For Word Sense Disambiguation This paper talks about a method to figure out the meaning of words that uses a group (ensemble) of simple mathematical models called Naive Bayesian classifiers. These models look at words that appear around the target word in different-sized sections of text to help understand its meaning. Even though this method is simple, tests on common words like "line" and "interest" show that it can be just as accurate as the best methods out there. We use a group of eighty-one Naive Bayesian classifiers that examine the words surrounding the target word to help figure out its meaning.	meaning
5	A Maximum-Entropy-Inspired Parser We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5,9,10,15,17] "standard" sections of the Wall Street Journal tree- bank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is the use of a "maximum-entropy-inspired" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head. As an alternative to hard coded heuristics, we proposed to recover the Penn functional tags automatically. Our parser is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents.	A Maximum-Entropy-Inspired Parser We present a new parser, a tool for analyzing sentences, that breaks down text into a structure similar to the Penn Treebank, a well-known database. It achieves 90.1% accuracy for sentences with up to 40 words, and 89.5% accuracy for sentences with up to 100 words when tested on standard parts of the Wall Street Journal database. This is a 13% improvement in reducing errors compared to the best previous single-parser results on this data. The main technical advancement is using a "maximum-entropy-inspired" model, a method that helps in making predictions and handling data smoothly, allowing us to test and combine many different variables. We also show some partial results that demonstrate how different types of information affect the parser, including a surprising 2% improvement from guessing the word type before the main word in a sentence. Instead of using fixed rules, we suggest automatically identifying specific tags in the Penn database. Our parser works in two stages: the first stage is a refined Markov grammar (a model using up to three previous sentence parts as context), and the second stage is a detailed Markov grammar with extra details about the sentence structure.	model using
6	An Unsupervised Method For Detecting Grammatical Errors We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora. The system was developed and tested using essay-length responses to prompts on the Test of English as a Foreign Language (TOEFL). The error-recognition system, ALEK, performs with about 80% precision and 20% recall. We attempt to identify errors on the basis of context -- more specifically a 2 word window around the word of interest, from which we consider function words and POS tags. We use a mutual information measure in addition to raw frequency of n grams. The grammar feature covers errors such as sentence fragments, verb form errors and pronoun errors. We utilize mutual information and chi-square statistics to identify typical contexts for a small set of targeted words from a large well-formed corpus.	An Unsupervised Method For Detecting Grammatical Errors We introduce a method that doesn't need supervision for finding grammar mistakes by figuring out what is wrong from edited collections of texts. The system was created and tested using essay-length answers to questions on the Test of English as a Foreign Language (TOEFL). The error-detecting system, ALEK, works with about 80% accuracy and 20% ability to find errors. We try to find errors based on the context—specifically a 2-word range around the word we are interested in, looking at small words that show relationships and part-of-speech tags, which tell what role words play in a sentence. We use a measure of mutual information (a way to find how much knowing one thing tells us about another) along with the simple count of word combinations. The grammar feature includes errors like incomplete sentences, wrong verb forms, and pronoun mistakes. We use mutual information and a statistical method called chi-square to find common contexts for a small set of targeted words from a large collection of correctly written texts.	mistakes
7	Cut And Paste Based Text Summarization We present a cut and paste based text summarizer, which uses operations derived from an analysis of human written abstracts. The summarizer edits extracted sentences, using reduction to remove inessential phrases and combination to merge resuiting phrases together as coherent sentences. Our work includes a statistically based sentence decomposition program that identifies where the phrases of a summary originate in the original document, producing an aligned corpus of summaries and articles which we used to develop the summarizer. We first extract sentences, then remove redundant phrases, and use (manual) recombination rules to produce coherent output. We manually analyze 30 human-written summaries, and find that 19% of sentences can not be explained by cut-and-paste operations from the source text.	Cut And Paste Based Text Summarization We present a cut and paste based text summarizer, which uses methods learned from studying human-written summaries. The summarizer edits chosen sentences by shortening them to remove unnecessary parts and combining them to form clear sentences. Our work includes a computer program that uses statistics to break down sentences and find out where parts of a summary come from in the original document, creating a matched set of summaries and articles that we used to build the summarizer. We first pick sentences, then take out repeated parts, and use (manual) rules to put them together into clear output. We manually study 30 human-written summaries and find that 19% of the sentences cannot be explained by just cutting and pasting from the original text.	summarizer
8	Trainable Methods For Surface Natural Language Generation We present three systems for surface natural language generation that are trainable from annotated corpora. The first two systems, called NLG1 and NLG2, require a corpus marked only with domain-specific semantic attributes, while the last system, called NLG3, requires a corpus marked with both semantic attributes and syntactic dependency information. All systems attempt to produce a grammatical natural language phrase from a domain-specific semantic representation. NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase. The systems NLG2 and NLG3 learn to determine both the word choice and the word order of the phrase. We present experiments in which we generate phrases to describe flights in the air travel domain. We use maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features. We use a large collection of generation templates for surface realization. We present maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain.	Trainable Methods For Surface Natural Language Generation We introduce three systems for creating natural language sentences from data that can be trained using labeled text samples. The first two systems, NLG1 and NLG2, need text with only specific topic-related meanings, while the third system, NLG3, needs text with both meanings and information on sentence structure. All systems aim to create a correct language sentence from a given topic-specific meaning. NLG1 is a basic system that generates an entire sentence in one go using common phrase patterns, while NLG2 and NLG3 use complex statistical models to generate each word separately. NLG2 and NLG3 learn how to choose words and arrange them in order. We conduct tests to generate sentences about flights in the air travel topic. We use statistical models to guide the sentence creation process, considering meaning features not yet expressed. We use many templates to help form sentences. We present statistical models to learn how to order attributes and choose words for sentence creation from a structured list of feature-value pairs, focused on the air travel topic.	statistical
9	A Novel Use Of Statistical Parsing To Extract Information From Text Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations. Our rule-based methods employ a number of linguistic rules to capture relation patterns. We take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations. We integrate various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. We combine entity recognition, parsing, and relation extraction into a jointly-trained single statistical parsing model that achieves improved performance on all the subtasks. Part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly-labeled data is unavailable.	A Novel Use Of Statistical Parsing To Extract Information From Text Since 1995, a few statistical parsing methods have shown a big improvement in understanding sentence structure, as compared to the UPenn TREEBANK, which is a top standard for checking accuracy. In this paper, we talk about modifying a type of sentence structure analysis called a lexicalized, probabilistic context-free parser to pull out information and test this new method on the MUC-7 template parts and their connections. Our rule-based methods use several language rules to identify patterns in relationships. We believe that finding relationships is just a type of probability-based sentence analysis where sentence diagrams are enhanced to find all connections. We combine different tasks like identifying parts of speech (like nouns and verbs), recognizing names, extracting template parts, and finding relationships, into one model. We merge recognizing names, sentence analysis, and finding relationships into a single, jointly-trained statistical model that performs better on all these small tasks. Part of what we are adding with this work is to show that working on tasks together can be effective even when training them together isn’t possible because there is no data labeled for both.	their connections
10	Assigning Function Tags To Parsed Text It is generally recognized that the common non-terminal labels for syntactic constituents (NP, VP, etc.) do not exhaust the syntactic and semantic information one would like about parts of a syntactic tree. For example, the Penn Treebank gives each constituent zero or more 'function tags' indicating semantic roles and other related information not easily encapsulated in the simple constituent labels. We present a statistical algorithm for assigning these function tags that, on text already parsed to a simple-label level, achieves an F-measure of 87%, which rises to 99% when considering 'no tag' as a valid choice. As an alternative to hard coded heuristics, we propose to recover the Penn functional tags automatically.	Assigning Function Tags To Parsed Text It is generally understood that the usual non-terminal labels for parts of a sentence structure (like NP for noun phrase, VP for verb phrase, etc.) don't cover all the details about the structure and meaning one might want. For example, the Penn Treebank adds extra labels called 'function tags' to show roles and other information that aren't covered by the basic labels. We introduce a method using statistics to add these function tags, which works on text that already has basic labels. It gets an accuracy score of 87%, which goes up to 99% if we count choosing 'no tag' as correct. Instead of using fixed rules, we suggest automatically finding the Penn functional tags.	introduce
11	Using Semantic Preferences To Identify Verbal Participation In Role Switching Alternations We propose a method for identifying diathesis alternations where a particular argument type is seen in slots which have different grammatical roles in the alternating forms. The method uses selectional preferences acquired as probability distributions over WordNet. Preferences for the target slots are compared using a measure of distributional similarity. The method is evaluated on the causative and conative alternations, but is generally applicable and does not require a priori knowledge specific to the alternation. We use skew divergence (a variant of KL divergence proposed by Lee, 1999) to compare the sense profile of one argument of a verb (e.g., the subject position of the intransitive) to another argument of the same verb (e.g., the object position of the transitive), to determine if the verb participates in an argument alternation involving the two positions.	Using Semantic Preferences To Identify Verbal Participation In Role Switching Alternations We suggest a way to spot changes in how verbs are used when a certain type of noun (like a subject or object) appears in different roles in sentences. This method looks at how often certain words are linked together, using information from WordNet, which is like a big dictionary that shows how words are related. We compare these word linkages by seeing how similar they are. We tested this method on specific verb changes, like when a verb can mean causing something or doing something without a direct object, but it can work for other verb changes too and doesn’t need special knowledge beforehand. We use a special method called skew divergence, which is a type of comparison suggested by Lee in 1999, to see if a verb uses different sentence structures by comparing one noun position in a sentence to another, like comparing the subject of a sentence where there’s no direct object to the object position in a sentence with a direct object.	special
12	A Stochastic Parts Program And Noun Phrase Parser For Unrestricted Text Our part of speech tagger can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency. Our part-of-speech tagger performs not only part-of-speech analysis, but it also identities the most simple kinds of noun phrases mostly sequences of determiners, premodifiers and nominal heads by inserting brackets around them.	A Stochastic Parts Program And Noun Phrase Parser For Unrestricted Text Our part of speech tagger can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency. Our part-of-speech tagger performs not only part-of-speech analysis, but it also identifies the most basic types of noun phrases, which are mostly sequences of words like articles, adjectives, and main nouns, by placing brackets around them.	unrestricted
13	Applied Text Generation We divide tasks in the generation process into three stages: the text planner has access only to information about communicative goals, the discourse context, and semantics, and generates a non-linguistic representation of text structure and content. The sentence planner chooses abstract linguistic resources. It passes an abstract lexico-syntactic specification5 to the Realizer, which inflects, adds function words, and linearizes, thus producing the surface string.	Applied Text Generation We break down the task of creating text into three steps: First, the text planner works with information about the purpose of the communication, the context of the conversation, and the meaning, and it creates a basic outline of the text's structure and content. Next, the sentence planner picks basic language tools. It sends a rough outline of words and sentence structure to the Realizer, which then adjusts word forms, adds necessary small words, and organizes the text into a final readable form.	structure
14	A Practical Part-Of-Speech Tagger We present an implementation of a part-of-speech tagger based on a hidden Markov model. The methodology enables robust and accurate tagging with few resource requirements. Only a lexicon and some unlabeled training text are required. Accuracy exceeds 96%. We describe implementation strategies and optimizations which result in high-speed operation. Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment. Our semi-supervised model makes use of both labeled training text and some amount of unlabeled text. We train statistical models using unlabeled data with the expectation maximization algorithm. We report very high results (96% on the Brown corpus) for unsupervised POS tagging using Hidden Markov Models (HMMs) by exploiting hand-built tag dictionaries and equivalence classes.	A Practical Part-Of-Speech Tagger We present a way to create a part-of-speech tagger using a hidden Markov model, which is a mathematical model that helps predict sequences. This method allows for strong and accurate tagging without needing many resources. You only need a word list (lexicon) and some text that hasn't been labeled for training. The accuracy is over 96%. We explain how we set it up and made it run fast. There are three uses for tagging: recognizing phrases, figuring out the meaning of words, and assigning grammatical roles. Our model, which uses both labeled (annotated) and unlabeled texts, learns from both types. We use a statistical approach with the expectation-maximization algorithm, which is a method to find hidden patterns in data. We achieved very high accuracy (96% on the Brown corpus, which is a collection of text) for tagging without supervision by using Hidden Markov Models (HMMs) along with carefully crafted tag dictionaries and grouping similar items (equivalence classes).	grouping similar
15	A Simple Rule-Based Part Of Speech Tagger Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule-based methods. In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, corpus genre or language to another. Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rule-based tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below. Our rule based POS tagging methods extract rules from training corpus and use these rules to tag new sentence. We also show that assigning the most common part of speech for each lexical item gives a baseline of 90% accuracy.	A Simple Rule-Based Part Of Speech Tagger Automatic part of speech tagging is a field in computer language processing where using data analysis methods has been more successful than using fixed rules. In this paper, we introduce a simple rule-based part of speech tagger that automatically creates its own rules and tags with accuracy similar to data-based (stochastic) taggers. The rule-based tagger has several advantages over these taggers, including needing much less stored information, having a clear and small set of useful rules, being easy to improve, and being more flexible to use with different sets of tags, types of texts, or languages. Perhaps the most important contribution of this work is showing that using data analysis is not the only successful way for part of speech tagging. The fact that a simple rule-based tagger that learns its rules on its own can work so well should encourage researchers to explore rule-based tagging further, looking for better and more detailed sets of rule patterns and other variations on the simple but effective approach described below. Our rule-based POS tagging methods create rules from a set of example sentences and use these rules to tag new sentences. We also show that assigning the most common part of speech for each word gives a basic accuracy of 90%.	further
16	Termight: Identifying And Translating Technical Terminology We propose a semi-automatic tool, termight, that helps professional translators and terminologists identify technical terms and their translations. The tool makes use of part-of-speech tagging and word-alignment programs to extract candidate terms and their translations. Although the extraction programs are far from perfect, it isn't too hard for the user to filter out the wheat from the chaff. The extraction algorithms emphasize completeness. Alternative proposals are likely to miss important but infrequent terms/translations. To reduce the burden on the user during the filtering phase, candidates are presented in a convenient order, along with some useful concordance evidence, in an interface that is designed to minimize keystrokes. Termight is currently being used by the translators at AT&T Business Translation Services (formerly AT&T Language Line Services).	Termight: Identifying And Translating Technical Terminology We propose a semi-automatic tool, termight, that helps professional translators and terminologists find technical terms and their translations. The tool uses part-of-speech tagging (a way to identify words as nouns, verbs, etc.) and word-alignment programs to pick out possible terms and their translations. Although these programs aren't perfect, it's not very difficult for the user to separate the valuable information from the irrelevant. The algorithms used for finding terms focus on being thorough. Other methods might miss important but rare terms/translations. To make it easier for the user during the filtering process, the terms are shown in a helpful order, along with useful examples, in a user-friendly interface that requires minimal typing. Termight is currently being used by translators at AT&T Business Translation Services (formerly AT&T Language Line Services).	filtering
17	Does Baum-Welch Re-Estimation Help Taggers? In part of speech tagging by Hidden Markov Model, a statistical model is used to assign grammatical categories to words in a text. Early work in the field relied on a corpus which had been tagged by a human annotator to train the model. More recently, Cutting et al. (1992) suggest that training can be achieved with a minimal lexicon and a limited amount of a priori information about probabilities, by using an Baum-Welch re-estimation to automatically refine the model. In this paper, I report two experiments designed to determine how much manual training information is needed. The first experiment suggests that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy. The second experiment reveals that there are three distinct patterns of Baum-Welch re-estimation. In two of the patterns, the re-estimation ultimately reduces the accuracy of the tagging rather than improving it. The pattern which is applicable can be predicted from the quality of the initial model and the similarity between the tagged training corpus (if any) and the corpus to be tagged. Heuristics for deciding how to use re-estimation in an effective manner are given. The conclusions are broadly in agreement with those of Merialdo (1994), but give greater detail about the contributions of different parts of the model. We report an accuracy of 75.49%, 80.87% and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tag set of 134 tags.	Does Baum-Welch Re-Estimation Help Taggers? In part of speech tagging using a Hidden Markov Model (a statistical method), the goal is to label words with their grammatical roles. Early models were trained using a dataset tagged by humans. Recently, Cutting et al. (1992) proposed that training can be done with a small dictionary and limited initial probability information, by using Baum-Welch re-estimation. This method updates the model automatically. This paper discusses two experiments to find out how much human-provided training data is necessary. The first experiment shows that starting with either word or transition probabilities is crucial for good results. The second experiment finds that Baum-Welch re-estimation can follow three different patterns. In two of these, re-estimation actually lowers accuracy instead of improving it. The applicable pattern depends on the quality of the initial model and how similar the tagged training data is to the data being tagged. Guidelines for effective re-estimation use are provided. The findings align with Merialdo (1994), but offer more detail on different model components' roles. We report accuracy rates of 75.49%, 80.87%, and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, using a set of 134 tags.	training
18	Three Heads Are Better Than One Machine translation (MT) systems do not currently achieve optimal quality translation on free text, whatever translation method they employ. Our hypothesis is that the quality of MT will improve if an MT environment uses output from a variety of MT systems working on the same text. In the latest version of the Pan-gloss MT project, we collect the results of three translation engines -- typically, sub-sentential chunks -- in a chart data structure. Since the individual MT systems operate completely independently, their re- sults may be incomplete, conflicting, or redundant. We use simple scoring heuristics to estimate the quality of each chunk, and find the highest-score sequence of chunks (the "best cover"). This paper describes in detail the combining method, presenting the algorithm and illustrations of its progress on one of many actual translations it has produced. It uses dynamic programming to efficiently compare weighted averages of sets of adjacent scored component translations. The current system operates primarily in a human-aided MT mode. The translation delivery system and its associated post-editing aide are briefly described, as is an initial evaluation of the usefulness of this method. Individual MT engines will be reported separately and are not, therefore, described in detail here. We produce the first MEMT system by combining outputs from three different MT engines based on their knowledge of the inner workings of the engines. We develop a multi-engine MT system, which builds a chart using the translation units inside each input system and then uses a chart walk algorithm to find the best cover of the source sentence.	Three Heads Are Better Than One Machine translation (MT) systems currently can't deliver the best quality translations for free text, no matter which method they use. We believe MT quality will improve if different MT systems work together on the same text. In the latest Pan-gloss MT project, we gather translations from three engines, usually parts of sentences, into a chart. Since each MT system works separately, their results might be missing parts, disagree with each other, or repeat the same things. We use simple rules to judge the quality of each part and pick the best sequence of parts (the "best cover"). This paper explains how we combine these parts, showing the method and examples of its progress on real translations. It uses a method called dynamic programming to quickly compare the average scores of neighboring translation parts. The current system mainly works with help from a human. The translation delivery system and an editing helper are briefly explained, along with an initial look at how useful this method is. Individual MT engines will be discussed in other reports, so they are not explained here. We create the first multi-engine MT (MEMT) system by combining outputs from three different MT engines, using knowledge of how the engines work. We develop a system that builds a chart with translation parts from each input system and uses a method called a chart walk algorithm to find the best translation of the original sentence.	quality
19	A Maximum Entropy Approach To Identifying Sentence Boundaries We present a trainable model for identify- ing sentence boundaries in raw text. Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of . , ?, and / as either a valid or invalid sentence boundary. The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Roman-alphabet language. Performance is compa rable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains. Our statistical system, mxTerminator employs simpler lexical features of the words to the left and right of the candidate period.	A Maximum Entropy Approach To Identifying Sentence Boundaries We introduce a model that can learn to find the end of sentences in plain text. Using a collection of text where the sentence ends are already marked, our model learns to decide if a dot (.), question mark (?), or slash (/) is really the end of a sentence or not. This learning process does not need any special rules, dictionaries, grammar tags, or specific subject area info. This means the model can be easily taught on any kind of English text and likely on any other language that uses the Roman alphabet. Its results are as good as or better than similar systems, and it is easy to adjust for different types of text. Our system, called mxTerminator, uses simple word details from before and after the potential sentence end.	mxTerminator
20	A Non-Projective Dependency Parser We describe a practical parser for unrestricted dependencies. The parser creates links between words and names the links according to their syntactic functions. We first describe the older Constraint Grammar parser where many of the ideas come from. Then we proceed to describe the central ideas of our new parser. Finally, the parser is evaluated.	A Non-Projective Dependency Parser We explain a useful tool for analyzing sentence structure with no limitations. This tool connects words and labels the connections based on their roles in the sentence. We start by discussing the older Constraint Grammar tool, which inspired many of our ideas. Then, we explain the main concepts of our new tool. Lastly, we test how well the tool works.	which inspired
21	An Annotation Scheme For Free Word Order Languages We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme. The resulting scheme reflects a stratificational notion of language, and makes only minimal assumptions about the interrelation of the particular representational strata. We release the NEGRA corpus, a hand parsed corpus of German newspaper text containing approximately 20,000 sentences.	An Annotation Scheme For Free Word Order Languages We explain a method and a tool designed to create linguistically marked collections of texts for languages with flexible word order. Because the needs for this method are different from those for languages with fixed word order, we added several features that affect the design of the method. The final method shows a layered idea of language and makes only basic assumptions about how these layers relate to each other. We share the NEGRA collection, a manually analyzed set of German newspaper texts with around 20,000 sentences.	languages
22	Nymble: A High-Performance Learning Name-Finder This paper presents a statistical, learned approach to finding names and other non-recursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model. We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach. We develop Nymble, an HMM-based name tagging system operating in English and Spanish. Nymble uses statistical learning to acquire a Hidden Markov Model (HMM) that recognises NEs in text.	Nymble: A High-Performance Learning Name-Finder This paper introduces a method based on statistics to find names and other specific items in text, using a version of the hidden Markov model, which is a mathematical model used for prediction. We explain why this problem is important, our method, the model details, and the successful results. We created Nymble, a system that tags names in English and Spanish using a hidden Markov model, which learns from data to identify names in text.	mathematical model
23	Disambiguation Of Proper Names In Text Identifying the occurrences of proper names in text and the entities they refer to can be a difficult task because of the many-to-many mapping between names and their referents. We analyze the types of ambiguity -- structural and semantic -- that make the discovery of proper names difficult in text, and describe the heuristics used to disambiguate names in Nominator, a fully-implemented module for proper name recognition developed at the IBM T.J. Watson Research Center. We use hand-written rules and knowledge bases to classify proper names into broad categories.	Disambiguation Of Proper Names In Text Identifying when proper names appear in text and what they refer to can be tricky because one name can refer to many things, and one thing can have many names. We look into the types of confusion -- related to structure and meaning -- that make it hard to find proper names in text, and we explain the methods used to clear up these confusions in Nominator, a complete tool for recognizing proper names created at the IBM T.J. Watson Research Center. We use specially created rules and collections of knowledge to sort proper names into general groups.	tricky because
24	A Fast And Portable Realizer For Text Generation Systems We release a surface realizer, RealPro, and it is  intended as off-the-shelf plug-in realizer. Our RealPro surface realizer which produces a surface linguistic utterance.	A Fast And Portable Realizer For Text Generation Systems We introduce RealPro, a tool that helps generate text, and it's designed to be easily added to other systems. Our RealPro tool creates sentences or phrases in a language.	Portable Realizer
25	Automatic Extraction Of Subcategorization From Corpora We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount. We use a grammar and a sophisticated parsing tool for argument-adjunct distinction.	Automatic Extraction Of Subcategorization From Corpora We describe a new method and built system to create a subcategorization dictionary from text collections. Each entry in the dictionary shows how often different subcategorization types appear in English. An early test with 14 verbs that can be used in different ways shows that this method is as accurate as older methods, which only cover a small range of subcategorization types. We also show that using a subcategorization dictionary made with this system makes a parser more accurate by a noticeable amount. We use a set of grammar rules and an advanced tool to separate main sentence parts from additional information.	grammar rules
26	Exploiting A Probabilistic Hierarchical Model For Generation Previous stochastic approaches to generation do not include a tree-based representation of syntax. While this may be adequate or even advantageous for some applications, other applications profit from using as much syntactic knowledge as is available, leaving to a stochastic model only those issues that are not determined by the grammar. We present initial results showing that a tree-based model derived from a tree-annotated corpus improves on a tree model derived from an unannotated corpus, and that a tree-based stochastic model with a hand-crafted grammar outperforms both. Our system, FERGUS takes dependency structures as inputs, and produced XTAG derivations by a stochastic tree model automatically acquired from an annotated corpus. The Fergus system employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees.	Exploiting A Probabilistic Hierarchical Model For Generation Previous random-based methods for creating language do not use a tree structure to represent sentence structure. While this might work well for some uses, others benefit from using as much information about sentence structure as possible, leaving only the parts that grammar doesn't decide to a random-based model. We show early results indicating that using a tree model from a collection of texts with sentence structure markers is better than using one without them, and that a random-based tree model with a custom-made grammar performs better than both. Our system, FERGUS, uses dependency structures (relationship between words) as inputs and creates XTAG derivations (detailed sentence structures) using a random-based tree model that is automatically learned from a marked-up text collection. The Fergus system uses a statistical tree model to choose likely trees and a word sequence model to rank the possible sentences made from the best trees.	choose likely
27	Effects Of Adjective Orientation And Gradability On Sentence Subjectivity Subjectivity is a pragmatic, sentence-level feature that has important implications for text processing applications such as information extraction and information retrieval. We study the effects of dynamic adjectives, semantically oriented adjectives, and gradable adjectives on a simple subjectivity classifier, and establish that they are strong predictors of subjectivity. A novel trainable method that statistically combines two indicators of gradability is presented and evaluated, complementing existing automatic techniques for assigning orientation labels. Unlike nouns, many adjectives are inherently subjective, and the number of adjectives in texts correlates with human judgements of their subjectivity. We report a statistical correlation between the number of adjectives in a text and human judgments of subjectivity. We show that automatically detected gradable adjectives are a useful feature for subjectivity classification.	Effects Of Adjective Orientation And Gradability On Sentence Subjectivity Subjectivity is a practical feature at the sentence level that is important for handling text in areas like pulling out information and finding information. We look at how changeable adjectives, adjectives with specific meanings, and adjectives that can vary in degree affect a simple tool that decides how subjective a sentence is, and we find that these adjectives are strong indicators of subjectivity. A new method that can be taught to statistically combine two signs of how much an adjective can vary is introduced and tested, enhancing current automatic methods for giving orientation labels. Unlike nouns, many adjectives are naturally subjective, and the number of adjectives in texts matches up with how people judge their subjectivity. We share a statistical link between the number of adjectives in a text and how people judge its subjectivity. We demonstrate that adjectives that can change in intensity are helpful for determining how subjective a text is.	adjectives
28	The Automated Acquisition Of Topic Signatures For Text Summarization In order to produce a good summary, one has to identify the most relevant portions of a given text. We describe in this paper a method for automatically training topic signatures -- sets of related words, with associated weights, organized around head topics and illustrate with signature we created with 6,194 TREC collection texts over 4 selected topics. We describe the possible integration of topic signatures with ontologies and its evaluaton on an automated text summarization system. We first introduced topic signatures which are topic relevant terms for summarization.	The Automated Acquisition Of Topic Signatures For Text Summarization In order to produce a good summary, one has to find the most important parts of a given text. We explain a method in this paper for automatically creating topic signatures, which are groups of related words with importance levels, centered around main topics, and we show an example with a signature made from 6,194 texts from the TREC collection on 4 chosen topics. We discuss how topic signatures can be combined with ontologies (big systems of knowledge) and tested in an automated text summarization system. We first introduced topic signatures, which are terms related to a topic used for making summaries.	Automated
29	Automatic Acquisition Of Domain Knowledge For Information Extraction In developing an Information Extraction (IE) system for a new class of events or relations, one of the major tasks is identifying the many ways in which these events or relations may be expressed in text. This has generally involved the manual analysis and, in some cases, the annotation of large quantities of text involving these events. This paper presents an alternative approach, based on an automatic discovery procedure, EXDISCO, which identifies a set; of relevant documents and a set of event patterns from un-annotated text, starting from a small set of "seed patterns". We evaluate EXDISCO by comparing the performance of discovered patterns against that of manually constructed systems on actual extraction tasks. We propose an algorithm for learning extraction patterns for a small number of examples which greatly reduced the burden on the application developer and reduced the knowledge acquisition bottleneck. We choose an approach motivated by the assumption that documents containing a large number of patterns already identified as relevant to a particular IE scenario are likely to contain further relevant patterns. ExDisco uses a bootstrapping mechanism to find new extraction patterns using unannotated texts and some seed patterns as the initial input.	Automatic Acquisition Of Domain Knowledge For Information Extraction In creating a system that pulls out specific information (IE) for new types of events or relationships, a major job is figuring out all the different ways these can be written in text. Usually, this involves going through and sometimes labeling large amounts of text where these events are mentioned. This paper suggests a different method using a computer program called EXDISCO, which automatically finds important documents and event patterns from text that hasn't been labeled, starting with a few example patterns. We test EXDISCO by seeing how well the patterns it finds work compared to systems made by people for real extraction tasks. We suggest a method for learning how to find patterns from a few examples, which makes it much easier for developers and speeds up the process of gathering knowledge. We assume that documents with many already known patterns related to a specific IE situation will likely have more useful patterns. ExDisco uses a process called bootstrapping to find new patterns by using texts that haven't been labeled and some starting patterns as initial input.	figuring
30	More Accurate Tests For The Statistical Significance Of Result Differences Statistical significance testing of differences in values of metrics like recall, precision and balanced F-score is a necessary part of empirical natural language processing. Unfortunately, we find in a set of experiments that many commonly used tests often underestimate the significance and so are less likely to detect differences that exist between different techniques. This underestimation comes from an independence assumption that is often violated. We point out some useful tests that do not make this assumption, including computationally-intensive randomization tests. Standard deviations for F scores are estimated with bootstrap resampling.	More Accurate Tests For The Statistical Significance Of Result Differences Statistical significance testing of differences in values of metrics like recall, precision, and balanced F-score is a necessary part of empirical natural language processing. Unfortunately, we find in a set of experiments that many commonly used tests often underestimate the significance and so are less likely to detect differences that exist between different techniques. This underestimation comes from an independence assumption that is often violated. We point out some useful tests that do not make this assumption, including computationally-intensive randomization tests. Standard deviations for F scores are estimated with bootstrap resampling.	necessary
31	A Comparison Of Alignment Models For Statistical Machine Translation In this paper, we present and compare various alignnment models for statistical machine translation. We propose to measure tile quality of an alignment model using the quality of the Viterbi alignment compared to a manually-produced alignment and describe a refined annotation scheme to produce suitable reference alignments. We also comppre the impact of different alignment models on tile translation quality of a statistical machine translation system. In order to improve transition models in the HMM based alignment, we extend the transition models to be word-class dependent.	A Comparison Of Alignment Models For Statistical Machine Translation In this paper, we present and compare different methods for matching words or phrases in statistical machine translation. We suggest evaluating how good a matching method is by comparing its results to a manually created match and explain a better way to make suitable reference matches. We also look at how different matching methods affect the quality of translations in a statistical machine translation system. To improve word sequence models in the Hidden Markov Model (HMM) based matching, we make these models depend on word categories.	important
32	Base Noun Phrase Translation Using Web Data And The EM Algorithm We consider here the problem of Base Noun Phrase translation. We propose a new method to perform the task. For a given Base NP, we first search its translation candidates from the web. We next determine the possible translation(s) from among the candidates using one of the two methods that we have developed. In one method, we employ an ensemble of Naive Bayesian Classifiers constructed with the EM Algorithm. In the other method, we use TF-IDF vectors also constructed with the EM Algorithm. Experimental results indicate that the coverage and accuracy of our method are significantly better than those of the baseline methods relying on existing technologies. In our method, translation candidates of a term are compositionally generated by concatenating the translation of the constituents of the term and are re-ranked by measuring contextual similarity against the source language term.	Base Noun Phrase Translation Using Web Data And The EM Algorithm We consider here the problem of translating base noun phrases, which are simple noun groups like "big dog" or "red apple". We suggest a new way to do this. First, we search for possible translations of these phrases on the web. Then, we choose the best translation from these options using one of two methods we created. In one method, we use a group of simple decision-making tools called Naive Bayesian Classifiers, which are set up with a technique called the EM Algorithm (a way to find patterns in data). In the other method, we use TF-IDF vectors, which help find important words in documents, also set up with the EM Algorithm. Our tests show that our method is much better in terms of finding and correctly translating phrases than older methods that use current technology. In our approach, we create possible translations by putting together the translations of the smaller parts of the phrase and then rank them by how well they match the meaning in the original language.	statistical machine
44	Unsupervised Construction Of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources. Two techniques are employed: (1) simple string edit distance, and (2) a heuristic strategy that pairs initial (presumably summary) sentences from different news stories in the same cluster. We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation. Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set. On test data extracted by the heuristic strategy, however, performance of the two training sets is similar, with AERs of 13.2% and 14.7% respectively. Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase. The summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships. we introduce Microsoft Research Paraphrase Corpus (MSRPC). We use Web-aggregated news stories to learn both sentence-level and word-level alignments.	Unsupervised Construction Of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources We look into methods that don't need human guidance for finding paraphrases, which are sentences with the same meaning but different words, from a collection of news articles grouped by time and topic from many online news sources. We use two methods: (1) simple string edit distance, which measures how different two sentences are by counting changes needed to make them the same, and (2) a heuristic strategy, which is a rule of thumb that pairs starting sentences, likely summaries, from different news stories in the same group. We assess both data sets using a tool that aligns words and a scoring method from translating languages. Results show that the edit distance data is cleaner and easier to match with an alignment error rate (AER) of 11.58% on a similar test set. For data from the heuristic strategy, both training sets perform similarly, with AERs of 13.2% and 14.7%. Looking at 100 sentence pairs from each set shows that the edit distance data misses some of the complex word and sentence changes typical in paraphrases. The summary sentences, though harder to match, keep more of the interesting changes useful for learning paraphrases. We introduce the Microsoft Research Paraphrase Corpus (MSRPC), which uses combined news stories from the web to learn both sentence and word alignments.	investigated
33	Efficient Support Vector Classifiers For Named Entity Recognition Named Entity (NE) recognition is a task in which proper nouns and numerical information are extracted from documents and are classified into categories such as person, organization, and date. It is a key technology of Information Extraction and Open-Domain Question Answering. First, we show that an NE recognizer based on Support Vector Machines (SVMs) gives better scores than conventional systems. However, off-the-shelf SVM classifiers are too inefficient for this task. Therefore, we present a method that makes the system substantially faster. This approach can also be applied to other similar tasks such as chunking and part-of-speech tagging. We also present an SVM-based feature selection method and an efficient training method. We propose Kernel Expansion that is used to transform the d-degree polynomial kernel based classifier into a linear one, with a modified decision function. We propose an XQK (eXpand the Quadratic Kernel) which can make their Named-Entity recognizer drastically fast.	Efficient Support Vector Classifiers For Named Entity Recognition Named Entity (NE) recognition is a task where names and numbers are pulled from documents and sorted into groups like person, organization, and date. It's important for pulling information from text and answering questions. We first show that using Support Vector Machines (SVMs) for NE recognition works better than older methods. But, standard SVM tools are too slow for this job. So, we introduce a way to make the system much quicker. This method can also be used for similar tasks like breaking text into parts and identifying word types. We also introduce a way to pick important features using SVMs and a faster way to train the system. We suggest a method called Kernel Expansion to change a complex classifier into a simpler, faster one. We introduce XQK (eXpand the Quadratic Kernel) to make the NE recognizer much faster.	current technology
34	A Graph Model For Unsupervised Lexical Acquisition This paper presents an unsupervised method for assembling semantic knowledge from a part-of-speech tagged corpus using graph algorithms. The graph model is built by linking pairs of words which participate in particular syntactic relationships. We focus on the symmetric relationship between pairs of nouns which occur together in lists. An incremental cluster-building algorithm using this part of the graph achieves 82% accuracy at a lexical acquisition task, evaluated against WordNet classes. The model naturally realises domain and corpus specific ambiguities as distinct components in the graph surrounding an ambiguous word. We try to find graph regions that are more connected internally than externally.	A Graph Model For Unsupervised Lexical Acquisition This paper introduces a method that doesn't require labeled data for gathering word meaning from a text that is marked with parts of speech, using network methods. The network model is created by connecting pairs of words that have certain grammatical connections. We concentrate on the equal relationship between pairs of nouns that appear together in lists. A step-by-step grouping method using this part of the network achieves 82% success in a task of learning new words, compared to WordNet categories. The model naturally identifies different meanings in specific topics and text as separate parts in the network around a word with multiple meanings. We aim to find parts of the network that are more connected inside themselves than with the outside.	similar tasks
35	Identifying Anaphoric And Non-Anaphoric Noun Phrases To Improve Coreference Resolution We present a supervised learning approach o identification of anaphoric and non-anaphoric noun phrases and how how such information can be incorporated into a coreference resolution system. The resulting system outperforms the best MUC-6 and MUC-7 coreference resolution systems on the corresponding MUC coreference data sets, obtaining F-measures of 66.2 and 64.0, respectively. We train a separate anaphoricity classifier in addition to a coreference model. We challenge the motivation for the inclusion of such detectors, reporting no improvements, or even worse performance.	Identifying Anaphoric And Non-Anaphoric Noun Phrases To Improve Coreference Resolution We introduce a method using supervised learning (a technique where a computer learns from examples) to identify anaphoric noun phrases (words that refer back to something previously mentioned) and non-anaphoric noun phrases (words that don't refer back). This information is used to enhance a system that figures out which words in a text refer to the same thing. Our improved system performs better than the best existing coreference resolution systems from MUC-6 and MUC-7 (specific tests for measuring these systems) on their respective datasets, achieving F-measures (a combined measure of accuracy and completeness) of 66.2 and 64.0. We create a separate tool to identify anaphoricity (whether a word refers back or not) alongside the coreference model. We question the benefit of adding such tools, as we find they do not improve results and can even make them worse.	labeled
36	Concept Discovery From Text Broad-coverage lexical resources such as WordNet are extremely useful. However, they often include many rare senses while missing domain-specific senses. We present a clustering algorithm called CBC (Clustering By Committee) that automatically discovers concepts from text. It initially discovers a set of tight clusters called committees that are well scattered in the similarity space. The centroid of the members of a committee is used as the feature vector of the cluster. We proceed by assigning elements to their most similar cluster. Evaluating cluster quality has always been a difficult task. We present a new evaluation methodology that is based on the editing distance between output clusters and classes extracted from WordNet (the answer key). Our experiments show that CBC outperforms several well-known clustering algorithms in cluster quality. Mutual information (MI) is an information theoric measure and has been used in our method for  clustering words.	Concept Discovery From Text Broad-coverage lexical resources like WordNet are very helpful, but they often include many uncommon meanings while missing meanings specific to certain areas. We introduce a grouping method called CBC (Clustering By Committee) that automatically finds concepts from text. It starts by finding a set of closely related groups called committees that are well spread out in the space of similarity. The average of the committee members is used as the main feature of the group. We then assign items to the group they are most similar to. Checking the quality of these groups has always been tricky. We introduce a new way to evaluate them based on the editing distance, which is a way to measure how different two things are, between the groups we create and the categories taken from WordNet (the answer key). Our tests show that CBC works better than several well-known grouping methods in terms of quality. Mutual information (MI) is a measure from information theory, which is a study of how information is measured and communicated, and we used it in our method for grouping words.	systems
37	Building A Large-Scale Annotated Chinese Corpus In this paper we address issues related to building a large-scale Chinese corpus. We try to answer four questions: (i) how to speed up annotation, (ii) how to maintain high annotation quality, (iii) for what purposes is the corpus applicable, and finally (iv) what future work we anticipate.	Building A Large-Scale Annotated Chinese Corpus In this paper we discuss problems related to creating a large collection of Chinese language data. We try to answer four questions: (i) how to make the annotation process faster, (ii) how to keep the annotation quality high, (iii) what the corpus can be used for, and finally (iv) what future work we expect to do.	communicated
216	One Sense Per Discourse It is well-known that there are polysemous words like sentence whose "meaning" or "sense" depends on the context of use. We have recently reported on two new word-sense disambiguation systems, one trained on bilingual material (the Canadian Hansards) and the other trained on monolingual material (Roget's Thesaurus and Grolier's Encyclopedia). As this work was nearing completion, we observed a very strong discourse effect. That is, if a polysemous word such as sentence appears two or more times in a well-written discourse, it is extremely likely that they will all share the same sense. This paper describes an experiment which confirmed this hypothesis and found that the tendency to share sense in the same discourse is extremely strong (98%). This result can be used as an additional source of constraint for improving the performance of the word-sense disambiguation algorithm. In addition, it could also be used to help evaluate disambiguation algorithms that did not make use of the discourse constraint. We claim on the basis of corpus analysis that to a very large extent a word keeps the same meaning throughout a text.	One Sense Per Discourse It is well-known that there are words with multiple meanings, like "sentence," where the meaning depends on how it's used in a sentence. We have recently reported on two new systems that figure out which meaning of a word is intended. One system was trained using material in two languages (the Canadian Hansards), and the other used material in one language (Roget's Thesaurus and Grolier's Encyclopedia). As this work was almost finished, we noticed a strong effect related to the overall discussion or context. This means that if a word with multiple meanings, like "sentence," appears more than once in a well-written text, it is very likely that it will have the same meaning each time. This paper describes an experiment that confirmed this idea and found that the likelihood of sharing the same meaning in the same discussion is very high (98%). This result can help improve the systems that figure out word meanings by using this information as a guide. Additionally, it could help evaluate other systems that do not use context to determine word meanings. We claim, based on analyzing large collections of texts, that a word usually keeps the same meaning throughout a text.	model performs
38	Learning Question Classifiers In order to respond correctly to a free form factual question given a large collection of texts, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer. These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer. This paper presents a machine learning approach to question classification. We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into fine-grained classes. We show accurate results on a large collection of free-form questions used in TREC 10. We assign one of fifty possible types to a question based on features present in the question. We have developed a machine learning approach which uses the SNoW learning architecture.	Learning Question Classifiers In order to correctly answer a factual question from a large collection of texts, one needs to understand the question well enough to identify some limits or rules it places on the possible answer. These limits might include figuring out the type or category of answer needed and suggesting different ways to find and confirm a potential answer. This paper introduces a machine learning method for classifying questions. We create a step-by-step classifier that uses a structured guide of answer categories, eventually sorting questions into detailed groups. We show precise results using a large set of free-form questions from TREC 10. We categorize a question into one of fifty types based on certain features in the question. We have developed a machine learning method using the SNoW learning system.	Scale Annotated
39	The LinGO Redwoods Treebank: Motivation And Preliminary Applications The LinGO Redwoods initiative is a seed activity in the design and development of a new type of treebank. While several medium to large-scale treebanks exist for English (and for other major languages), pre-existing publicly available resources exhibit the following limitations: (i) annotation is mono-stratal, either encoding topological (phrase structure) or tectogrammatical (dependency) information, (ii) the depth of linguistic information recorded is comparatively shallow, (iii) the design and format of linguistic representation in the tree-bank hard-wires a small, predefined range of ways in which information can be extracted from the treebank, and (iv) representations in existing treebanks are static and over the (often year or decade-long) evolution of a large-scale treebank tend to fall behind the development of the field. LinGO Redwoods aims at the development of a novel treebanking methodology, rich in nature and dynamic both in the ways linguistic data can be retrieved from the treebank in varying granularity and in the constant evolution and regular updating of the treebank itself. Since October 2001, the project is working to build the foundations for this new type of treebank, to develop a basic set of tools for treebank construction and maintenance, and to construct an initial set of 10,000 annotated trees to be distributed together with the tools under an open-source license. We develop the HPSG LinGo Redwoods Treebank. The Redwoods treebank has been created to provide annotated training material to permit statistical models for ambiguity resolution to be combined with the precise interpretations produced by the ERG. Dynamic, discriminant-based treebanking is pioneered in the Redwoods treebank of English.	The LinGO Redwoods Treebank: Motivation And Preliminary Applications The LinGO Redwoods project is a starting activity for creating a new kind of treebank, which is a database of sentences analyzed for linguistic information. While there are already several medium to large treebanks for English and other major languages, the available resources have some limitations: (i) they only show one layer of information, either how phrases are structured or how words depend on each other, (ii) they don't go very deep into the language details, (iii) they have a fixed way of representing language that limits how information can be retrieved, and (iv) they don't change over time and tend to become outdated as language studies progress. LinGO Redwoods wants to create a new method for treebanks that is both rich in detail and flexible, allowing different levels of information to be accessed and regularly updated as language studies advance. Since October 2001, the project has been working on building this new type of treebank, developing basic tools to make and maintain it, and creating an initial set of 10,000 analyzed sentences to share publicly with the tools. We are developing the HPSG LinGo Redwoods Treebank. The Redwoods treebank was made to provide training material that helps combine statistical models to solve ambiguities with the precise meanings given by the ERG. The Redwoods treebank introduces new ways of treebanking that are dynamic and based on choosing between different possibilities in English.	machine learning
40	Deterministic Dependency Parsing Of English Text This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time. When trained and evaluated on the Wall Street Journal section of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%. Unlike most previous systems, the parser produces labeled dependency graphs, using as arc labels a combination of bracket labels and grammatical role labels taken from the Penn Treebank II annotation scheme. The best overall accuracy obtained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels). We propose a variant of the model of Yamada and Matsumoto that reduces the complexity, from the worst case quadratic to linear. Our deterministic shift/reduce classifier-based dependency parsing approach offers state-of-the-art accuracy with high efficiency due to a greedy search strategy.	Deterministic Dependency Parsing Of English Text This paper introduces a method for analyzing English sentences called a deterministic dependency parser, which uses memory-based learning and works quickly. When tested on a specific set of English sentences from the Wall Street Journal, the parser correctly connected words in sentences 87.1% of the time. Unlike older systems, this parser creates labeled diagrams that show how words depend on each other, using a mix of labels that show sentence structure and grammatical roles from a popular linguistic database. The parser's best accuracy for finding both the main word and its label was 86.0% using basic grammar labels (7 labels) and 84.4% using all possible labels (50 labels). We suggest a simpler version of a previous model by Yamada and Matsumoto that makes the process faster, from a slow quadratic pace to a faster linear pace. Our approach, which uses a quick decision-making process called shift/reduce classifier-based dependency parsing, is highly accurate and efficient.	database
41	Efficient Parsing Of Highly Ambiguous Context-Free Grammars With Bit Vectors An efficient bit-vector-based CKY-style parser for context-free parsing is presented. The parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences. The parser uses bit-vector operations to parallelise the basic parsing operations. The parser is particularly useful when all analyses are needed rather than just the most probable one. We apply the Viterbi algorithm, exploiting its ability to deal with highly-ambiguous grammars.	Efficient Parsing Of Highly Ambiguous Context-Free Grammars With Bit Vectors An efficient bit-vector-based CKY-style parser for context-free parsing is presented. The parser creates a condensed representation (called a parse forest) of all the possible ways a sentence can be analyzed using large sets of grammatical rules and long sentences. The parser uses bit-vector operations, which are a type of computer operation, to do multiple parsing tasks at the same time. This parser is especially useful when every possible analysis is needed, not just the most likely one. We use the Viterbi algorithm, which is good at handling situations with many possible grammar interpretations.	accurate
42	The Importance Of Supertagging For Wide-Coverage CCG Parsing This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis. The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training. It also dramatically increases the speed of the parser. We show that large increases in speed can be obtained by tightly integrating the supertagger with the CCG grammar and parser. This is the first work we are aware of to successfully integrate a supertagger with a full parser which uses an automatically extracted grammar. We also further reduce the derivation space using constraints on category combination. The result is an accurate wide-coverage CCG parser which is an order of magnitude faster than comparable systems for other linguistically motivated formalisms. Our scores give an indication of how supertagging accuracy corresponds to overall dependency recovery. We describe two log-linear parsing models for CCG: a normal-form derivation model and a dependency model. The CCG parsing consists of two phases: first the supertagger assigns the most probable categories toeach word, and then the small number of combinatory rules, plus the type-changing and punctuation rules, are used with the CKY algorithm to build a packed chart. We propose a method for integrating the supertagger with the parser: initially a small number of categories is assigned to each word, and more categories are requested if the parser can not find a spanning analysis.	The Importance Of Supertagging For Wide-Coverage CCG Parsing This paper explains the role of supertagging, which is like a label assigning process, in a CCG parser that uses a mathematical method to choose the best analysis. The supertagger makes the process more efficient by reducing the number of possible options the model needs to evaluate, which also makes training faster. It also significantly speeds up the parser. We demonstrate that you can achieve much faster speeds by closely linking the supertagger with the CCG grammar and parser. This is the first known successful attempt to combine a supertagger with a complete parser that uses a grammar created by a machine. We also cut down on options by using rules that limit how categories can be combined. The result is a very fast and accurate CCG parser compared to other systems that use similar complex rules. Our results show how the accuracy of supertagging is related to the overall accuracy of understanding sentence structure. We describe two mathematical models for CCG parsing: one for how sentences are built and another for understanding relationships between words. CCG parsing has two steps: first, the supertagger assigns likely categories to each word, and then a few rules, along with type-changing and punctuation rules, help build a structured representation using a method called the CKY algorithm. We suggest a way to combine the supertagger with the parser: at first, only a few categories are given to each word, and more are added if the parser can't find a complete analysis.	multiple parsing
43	Confidence Estimation For Machine Translation We present a detailed study of confidence estimation for machine translation. Various methods for determining whether MT output is correct are investigated, for both whole sentences and words. Since the notion of correctness is not intuitively clear in this context, different ways of defining it are proposed. We present results on data from the NIST 2003 Chinese-to-English MT evaluation. We introduce a sentence level QE system where an arbitrary threshold is used to classify the MT output as good or bad. We study sentence and word level features for translation error prediction.	Confidence Estimation For Machine Translation We present a detailed study of confidence estimation for machine translation. Various methods for determining whether MT (Machine Translation) output is correct are investigated, for both whole sentences and words. Since the idea of correctness is not easy to understand in this context, different ways of defining it are proposed. We present results on data from the NIST 2003 Chinese-to-English MT evaluation. We introduce a sentence level QE (Quality Estimation) system where a chosen limit is used to classify the MT output as good or bad. We study sentence and word level features for translation error prediction.	sentence
45	Language Model Adaptation For Statistical Machine Translation Via Structured Query Models We explore unsupervised language model adaptation techniques for Statistical Machine Translation. The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection. Specific language models are then build from the retrieved data and interpolated with a general background model. Experiments show significant improvements when translating with these adapted language models. We apply a slightly different sentence-level strategy to language model adaptation, first generating an nbest list with a baseline system, then finding similar sentences in a monolingual target language corpus. We construct specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. We convert initial SMT hypotheses to queries and retrieved similar sentences from a large monolingual collection.	Language Model Adaptation For Statistical Machine Translation Via Structured Query Models We explore methods to adjust language models for machine translation without human guidance. The results from the translation process are turned into search requests at different levels of detail and used to find similar sentences from a huge collection of text in one language. Specific language models are then created from this gathered data and combined with a general model. Tests show big improvements when translating with these adjusted language models. We use a slightly different method by first creating a list of possible translations, then finding similar sentences in a text collection of the target language. We make specific language models by using machine translation results as search requests to find similar sentences in large text collections. We change initial machine translation guesses into search requests and find similar sentences from a large collection of text in one language.	Paraphrase
46	ORANGE: A Method For Evaluating Automatic Evaluation Metrics For Machine Translation Comparisons of automatic evaluation metrics for machine translation are usually conducted on corpus level using correlation statistics such as Pearson’s product moment correlation coefficient or  Spearman’s rank order correlation coefficient between human scores and automatic scores. However, such comparisons rely on human judgments of translation qualities such as adequacy and fluency. Unfortunately, these judgments are often inconsistent and very expensive to acquire. In this paper, we introduce a new evaluation method, ORANGE, for evaluating automatic machine translation evaluation metrics automatically without extra human involvement other than using a set of reference translations. We also show the results of comparing several existing automatic metrics and three new automatic metrics using ORANGE. BLEU is smoothed (Lin and Och, 2004b), and it considers only matching up to bi grams because this has higher correlations with human judgments than when higher-ordered n-grams are included. Smoothed per-sentence BLEU was used as a similarity metric.	:ORANGE: A Method For Evaluating Automatic Evaluation Metrics For Machine Translation Comparisons of automatic evaluation measures for machine translation are usually done on a large set of texts using statistical methods like Pearson’s correlation coefficient or Spearman’s rank correlation coefficient. These methods compare the scores given by humans and the automatic scores. However, these comparisons depend on human opinions about how good the translation is, looking at things like how accurate and smooth it is. Unfortunately, these human opinions can be unreliable and costly to get. In this paper, we introduce a new method called ORANGE, which allows us to evaluate automatic translation evaluation tools without needing extra human input, except for using a set of reference translations. We also present the results of comparing some existing automatic tools and three new ones using ORANGE. BLEU, a common evaluation tool, is adjusted (Lin and Och, 2004b) and it only looks at two-word matches because this aligns better with human opinions than using longer word sequences. Adjusted per-sentence BLEU was used as a way to measure similarity.	Machine
47	Improving A Statistical MT System With Automatically Learned Rewrite Patterns Current clump-based statistical MT systems have two limitations with respect to word ordering: First, they lack a mechanism for expressing and using generalization that accounts for reorderings of linguistic phrases. Second, the ordering of target words in such systems does not respect linguistic phrase boundaries. To address these limitations, we propose to use automatically learned rewrite patterns to preprocess the source sentences so that they have a word order similar to that of the target langauge. Our system is a hybrid one. The basic model is statistical, but we use broad-coverage rule-based parsers in two ways - during training for learning rewrite patterns, and at runtime for reordering the source sentences. Our experiments show 10% relative improvement in Bleu measure. We describe an approach for translation from French to English, where reordering rules are acquired automatically. Our re-ordering rules are automatically learned from aligning parse trees for both the source and target sentences.	Improving A Statistical MT System With Automatically Learned Rewrite Patterns Current group-based statistical machine translation (MT) systems have two problems with word order: First, they don't have a way to handle changes in word order that cover different language phrases. Second, the order of words in the output doesn't follow natural language phrase breaks. To fix these problems, we suggest using automatically learned rewrite patterns to rearrange the words in the original sentences so they match the order of the target language. Our system combines different methods. The main model is statistical, but we also use rule-based parsers (tools that analyze sentence structure) in two ways - one during training to learn rewrite patterns, and the other while running to rearrange the original sentences. Our tests show a 10% improvement in the Bleu measure (a way to check translation quality). We explain a method for translating from French to English, where reordering rules are learned automatically. Our reordering rules are learned automatically by matching sentence structures in both the original and target sentences.	evaluate automatic
48	Part-Of-Speech Tagging In Context We present a new HMM tagger that exploits context on both sides of a word to be tagged, and evaluate it in both the unsupervised and supervised case. Along the way, we present the first comprehensive comparison of unsupervised methods for part-of-speech tagging, noting that published results to date have not been comparable across corpora or lexicons. Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms, we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable. Finally, we show how this new tagger achieves state-of-the-art results in a supervised, non-training intensive framework. While replicating earlier experiments, we discover that performance was highly dependent on cleaning tag dictionaries using statistics gleaned from the tokens. We show that he expectation maximization algorithm for bi tag HMMs is efficient and quite effective for acquiring accurate POS taggers given only a lexicon (tag dictionary) and certain favorable conditions. We observe that earlier unsupervised HMM-EM results were artificially high due to use of Optimized Lexicons, in which only frequent-enough analyses of each word were kept.	Part-Of-Speech Tagging In Context We introduce a new HMM tagger, a tool that labels words in a sentence, using the words around each word to make decisions. We test it in two different ways: without help (unsupervised) and with help (supervised). We also provide the first detailed comparison of unsupervised methods for labeling parts of speech, pointing out that previous results have not been consistent across different sets of texts or dictionaries. We find that the dictionary's quality greatly affects how accurate the labeling is, so we offer a way to train HMMs that makes the labeling more accurate when the dictionary is unstable. Lastly, we demonstrate that this new tagger achieves top results using a simpler method that doesn't require extensive training. While repeating previous tests, we find that success relies on cleaning up the tag dictionaries with helpful data from the text. We show that the expectation maximization algorithm, a method for improving accuracy in tagging, works well with HMMs when we have a dictionary and certain favorable conditions. We notice that past high results in unsupervised HMM-EM tests were due to using improved dictionaries, where only common word analyses were included.	learned
58	Estimation of Conditional Probabilities With Decision Trees and an Application to Fine-Grained POS Tagging We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state-of-the-art POS taggers. Our fine-grained tag set contains approximately 800 tags.	Estimation of Conditional Probabilities With Decision Trees and an Application to Fine-Grained POS Tagging We present a HMM part-of-speech tagging method which works well for POS tagsets with a large number of detailed tags. It is based on three ideas: (1) breaking down the POS tags into attribute lists and breaking down the context-based POS probabilities of the HMM into a combination of attribute probabilities, (2) estimating the context-based probabilities using decision trees, which are like flowcharts for making decisions, and (3) using high-order HMMs, which means using more complex models that consider more context. In tests on German and Czech data, our tagger performed better than the best POS taggers currently available. Our detailed tag set contains about 800 tags.	language
49	Chinese Segmentation And New Word Detection Using Conditional Random Fields Chinese word segmentation is a difficult, important and widely-studied sequence modeling problem. This paper demonstrates the ability of linear-chain conditional random fields (CRFs) to perform robust and accurate Chinese word segmentation by providing a principled framework that easily supports the integration of domain knowledge in the form of multiple lexicons of characters and words. We also present a probabilistic new word detection method, which further improves performance. Our system is evaluated on four datasets used in a recent comprehensive Chinese word segmentation competition. State-of-the-art performance is obtained. The superiority of CRFs on Chinese information processing was also demonstrated in word segmentation). CRF is a statistical sequence modeling framework introduced by Lafferty et al (2001), and we use it for the Chinese word segmentation task by treating word segmentation as a binary decision task. We first use this framework for Chinese word segmentation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one. We define the word segmentation problem as labeling each character as whether or not the previous character boundary of the current character is a word boundary.	Chinese Segmentation And New Word Detection Using Conditional Random Fields Chinese word segmentation is a challenging and important problem that involves breaking down sentences into words. This paper shows how a method called linear-chain conditional random fields (CRFs) can effectively handle Chinese word segmentation by providing a structured approach that easily incorporates specialized knowledge using various lists of characters and words. We also introduce a method for finding new words, which helps improve accuracy. Our system is tested on four sets of data from a recent Chinese word segmentation competition, achieving top-level results. The effectiveness of CRFs for processing Chinese information is also shown in word segmentation. CRF is a statistical tool for analyzing sequences, introduced by Lafferty et al (2001), and we use it for Chinese word segmentation by treating it as a task where you make yes-or-no decisions. We apply this method by labeling each character as either the start of a new word or a continuation of the current word. We define the word segmentation problem by deciding if each character is the start of a new word or continues from the previous character.	success relies
50	Question Answering Based On Semantic Structures The ability to answer complex questions posed in Natural Language depends on (1) the depth of the available semantic representations and (2) the inferential mechanisms they support. In this paper we describe a QA architecture where questions are analyzed and candidate answers generated by 1) identifying predicate argument structures and semantic frames from the input and 2) performing structured probabilistic inference using the extracted relations in the context of a domain and scenario model. A novel aspect of our system is a scalable and expressive representation of actions and events based on Coordinated Probabilistic Relational Models (CPRM). In this paper we report on the ability of the implemented system to perform several forms of probabilistic and temporal inferences to extract answers to complex questions. The results indicate enhanced accuracy over current state-of-the-art Q/A systems. We explore the role of semantic structures in question answering. We demonstrate that question answering can stand to benefit from broad coverage semantic processing. Our question answering system takes PropBank/FrameNet annotations as input, uses the PropBank targets to indicate which actions are being described with which arguments and produces an answer using probabilistic models of actions as the tools of inference.	Question Answering Based On Semantic Structures The ability to answer hard questions written in everyday language depends on (1) how detailed the meaning (semantic) information is and (2) the reasoning methods that this information supports. In this paper, we describe a question-answering (QA) system where questions are examined and possible answers are generated by 1) identifying the main parts of sentences and the meaning behind them from the input, and 2) using these extracted relationships to make informed guesses within a specific topic or situation. A new feature of our system is a flexible and clear way of showing actions and events using Coordinated Probabilistic Relational Models (CPRM), which is a method to predict outcomes based on relationships. In this paper, we share how well the system can make different types of informed guesses based on probability and time to find answers to difficult questions. The results show better accuracy compared to the best current QA systems. We look into how understanding the meaning behind words helps in answering questions. We show that answering questions can benefit from wide-ranging understanding of meanings. Our QA system uses PropBank/FrameNet labels as input, which help identify what actions are being described and with what details, and then it gives an answer by using models that predict actions based on chance and reasoning.	segmentation
51	Towards Terascale Semantic Acquisition Although vast amounts of textual data are freely available, many NLP algorithms exploit only a minute percentage of it. In this paper, we study the challenges of working at the terascale. We present an algorithm, designed for the terascale, for mining is-a relations that achieves similar performance to a state-of-the-art linguistically-rich method. We focus on the accuracy of these two systems as a function of processing time and corpus size. We propose a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performance and efficiency. We extend is-a relation acquisition towards terascale, and automatically identified hypernym patterns by minimal edit distance. We propose, in the scenario of extracting is-a relations, one pattern-based approach and compared it with a baseline syntactic distributional similarity method (called syntactic co-occurrence in their paper).	Towards Terascale Semantic Acquisition Although there is a lot of text data available for free, many natural language processing (NLP) methods use only a small part of it. In this paper, we look at the difficulties of working with very large amounts of data, called terascale. We introduce a method designed for handling terascale data that identifies "is-a" relationships (like "a dog is a mammal") and performs similarly to a very advanced language-based method. We examine how accurate these two systems are, based on how long they take to process and the size of the data set. We suggest a similar, very expandable method using an edit-distance technique (a way to measure how different two pieces of text are) to learn patterns of words and parts of speech (POS), demonstrating both good results and efficiency. We improve the process of finding "is-a" relationships on a large scale and automatically find broader category patterns using minimal text differences. We suggest a pattern-based method for finding "is-a" relationships and compare it to a basic method that uses how often words appear together in sentences (referred to as syntactic co-occurrence in their paper).	actions
52	Characterising Measures Of Lexical Distributional Similarity This work investigates the variation in a word's distributionally nearest neighbours with respect to the similarity measure used. We identify one type of variation as being the relative frequency of the neighbour words with respect to the frequency of the target word. We then demonstrate a three-way connection between relative frequency of similar words, a concept of distributional gnerality and the semantic relation of hyponymy. Finally, we consider the impact that this has on one application of distributional similarity methods (judging the compositionality of collocations). Abstracting from results for concrete test sets, we try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends. We also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently. We analyzed the variation in a word's distribution ally nearest neighbours with respect to a variety of similarity measures. We attempted to refine the distributional similarity goal to predict whether one term is a generalization/specification of the other.	Characterising Measures Of Lexical Distributional Similarity This study looks at how the closest related words to a given word change depending on how we measure similarity. We find that one type of change is how often these related words appear compared to the main word. We then show a three-way link between how often similar words appear, a broad idea of word usage, and a specific word meaning relationship called hyponymy (where one word is a more specific version of another). Finally, we examine how this affects a method used to judge how words work together in phrases. By looking beyond specific examples, we try to find patterns and language features that affect how well similarity measures work. We also discovered that how often words appear was important in deciding which word was more general, with the more general word often appearing more. We studied how the closest related words to a word change with different ways of measuring similarity. We aimed to improve the goal of similarity to predict if one word is a broader or narrower version of another.	Although
59	Learning Entailment Rules for Unary Templates Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rules - entailment rules between templates with a single variable. In this paper we investigate two approaches for unsupervised learning of such rules and compare the proposed methods with a binary rule learning method. The results show that the learned unary rule-sets outperform the binary rule-set. In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure. We propose a unary template, which is defined as a template consisting of one argument slot and one predicate phrase. We use the distributional similarity of arguments to detect unary template entailment. Two approaches for unsupervised learning of unary rules (i.e. between templates with a single variable) are investigated. In (Zhao et al, 2009), a pivot approach for extracting paraphrase patterns from bilingual parallel corpora is presented, while in (Callison-Burch,2008) the quality of paraphrase extraction from parallel corpora is improved by requiring that phrases and their paraphrases have the same syntactic type. Our approach is different from theirs in many respects: their goal is paraphrase extraction, while we are extracting directional entailment rules; as textual resources for pattern extraction they use parallel corpora (using patterns in another language as pivots), while we rely on monolingual Wikipedia revisions (taking benefit from its increasing size); the para phrases they extract are more similar to DIRT, while our approach allows to focus on the acquisition of rules for specific phenomena frequent in entailment pairs, and not covered by other resources. We try identifying the entailment relation between lexical-syntactic templates using WeedsPrec, but observed that it tends to promote unreliable relations involving infrequent templates.	Learning Entailment Rules for Unary Templates Most studies on learning entailment rules without supervision have focused on rules connecting templates with two variables, overlooking unary rules - which are rules connecting templates with just one variable. In this paper, we explore two methods for learning these unary rules without supervision and compare them to a method for learning binary rules. The findings show that the unary rules we learned perform better than the binary rules. Also, we introduce a new way to measure similarity for learning entailment called Balanced-Inclusion, which performs the best. We define a unary template as a template with one empty space for an argument and one action or description phrase. We detect unary template entailment using the similarity in how arguments are used. We explore two methods for learning unary rules without supervision (meaning between templates with one variable). In a study by Zhao et al. (2009), a method using a pivot approach to find similar phrases from bilingual texts was presented, while Callison-Burch (2008) improved the quality of finding similar phrases by ensuring that phrases and their similar phrases follow the same grammatical structure. Our method is different in several ways: they aim to find similar phrases, while we aim to find directional entailment rules; they use bilingual texts (using phrases in another language as a reference), while we use changes made to Wikipedia articles in the same language (benefiting from its growing content); the similar phrases they find are more like DIRT, while our approach focuses on finding rules for specific common patterns in entailment pairs not covered by other resources. We attempt to identify the entailment relationship between lexical-syntactic templates using a method called WeedsPrec, but noticed it often suggests unreliable relationships involving uncommon templates.	Estimation
53	Wide-Coverage Semantic Representations From A CCG Parser This paper shows how to construct semantic representations from the derivations produced by a wide-coverage CCG parser. Unlike the dependency structures returned by the parser itself, these can be used directly for semantic interpretation. We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text. We believe this is a major step towards wide-coverage semantic interpretation, one of the key objectives of the field of NLP. We present an algorithm that learns CCG lexicons with semantics but requires fully specified CCG derivations in the training data. We consider the challenging problem of constructing broad-coverage semantic representations with CCG, but do not learn the lexicon.	Wide-Coverage Semantic Representations From A CCG Parser This paper explains how to create meaning-based structures (semantic representations) from the results made by a comprehensive CCG parser. Unlike the basic connection structures (dependency structures) the parser gives, these can be used directly to understand meaning (semantic interpretation). We show that well-made semantic representations can be created for more than 97% of the sentences in new sections of The Wall Street Journal (WSJ) text. We think this is a big step towards understanding meaning in a wide range of texts, which is a main goal in the field of Natural Language Processing (NLP). We introduce a method (algorithm) that learns word lists with meanings (CCG lexicons with semantics) but needs fully detailed CCG outcomes (derivations) in the training examples. We address the difficult task of creating broad meaning-based structures (broad-coverage semantic representations) with CCG, but we do not learn the word list (lexicon).	often similar
54	Semantic Role Labeling Via Integer Linear Programming Inference We present a system for the semantic role labeling task. The system combines a machine learning technique with an inference procedure based on integer linear programming that supports the incorporation of linguistic and structural constraints into the decision process. The system is tested on the data provided in CoNLL-2004 shared task on semantic role labeling and achieves very competitive results. We formulate SRL as a constituent-by-constituent (C-by-C) tagging problem.	Semantic Role Labeling Via Integer Linear Programming Inference We present a system for identifying the roles words play in a sentence. The system uses a computer learning method with a decision-making process that uses math rules to include language and structure rules into its decisions. The system is tested on data from a specific competition in 2004 about identifying word roles and performs very well. We describe this task as labeling each part of the sentence one by one.	meanings
55	Determining The Sentiment Of Opinions Identifying sentiments (the affective parts of opinions) is a challenging problem. We present a system that, given a topic, automatically finds the people who hold opinions about that topic and the sentiment of each opinion. The system contains a module for determining word sentiment and another for combining sentiments within a sentence. We experiment with various models of classifying and combining sentiment at word and sentence levels, with promising results. We try to determine the final sentiment orientation of a given sentence by combining sentiment words within it. We start with two lists of positive and negative seed words. We use WordNet synonyms and antonyms to expand two lists of positive and negative seed words.	Determining The Sentiment Of Opinions Identifying sentiments (the emotional parts of opinions) is a tough problem. We introduce a system that, when given a topic, automatically finds the people who have opinions about that topic and the feeling or mood of each opinion. The system has a part for figuring out the mood of words and another for mixing those moods within a sentence. We test different ways of sorting and mixing feelings at both the word and sentence levels, with encouraging results. We attempt to find out the overall mood direction of a sentence by combining mood words in it. We begin with two lists of positive and negative starting words. We use WordNet (a large database of words) to find similar and opposite words to expand the two lists of positive and negative starting words.	labeling
56	Sentence Compression Beyond Word Deletion In this paper we generalise the sentence compression task. Rather than simply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion. We present a new corpus that is suited to our task and a discriminative tree-to-tree transduction model that can naturally account for structural and lexical mismatches. The model incorporates a novel grammar extraction method, uses a language model for coherent output, and can be easily tuned to a wide range of compression specific loss functions. Different from prior research, we achieve sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process. We present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. Our abstractive methods sheds more light on how people compress sentences, but do not always manage to outperform extractive methods. We expand the task to include substitutions, insertions and reorderings that are automatically learned from parallel texts. We propose the first abstractive compression method.	Sentence Compression Beyond Word Deletion In this paper, we expand the task of sentence compression. Instead of just shortening a sentence by removing words or parts, as done before, we change it by using other methods like swapping words, changing the order, and adding new words. We introduce a new collection of texts that is suitable for our task and a special model that can handle differences in structure and word choices. The model uses a new way of creating rules, applies a language model to make the sentences flow well, and can be adjusted to fit different ways of measuring compression quality. Unlike earlier studies, we compress sentences using a mix of methods including word removal, swapping, adding, and rearranging based on a statistical model, similar to how we create paraphrases. We provide a model that can both shorten and rephrase single sentences, but it does not create summaries for entire documents. Our new methods give more insight into how people shorten sentences, but they don't always do better than methods that just cut out parts. We include changes, additions, and rearrangements that are automatically learned from matching texts. We introduce the first method that creates new sentences while shortening them.	attempt
57	A Classifier-Based Approach to Preposition and Determiner Error Correction in L2 English In this paper, we present an approach to the automatic identification and correction of preposition and determiner errors in non-native (L2) English writing. We show that models of use for these parts of speech can be learned with an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing. In the context of automated preposition and determiner error correction in L2 English, we note that the process is often disrupted by misspellings.	A Classifier-Based Approach to Preposition and Determiner Error Correction in L2 English In this paper, we present a method to automatically find and fix mistakes with prepositions (like "in," "on," "at") and determiners (like "a," "an," "the") in writing by people learning English as a second language (L2). We show that we can teach computer models to identify the correct use of these words with 70.06% accuracy for prepositions and 92.15% accuracy for determiners in native (L1) English text, and we share initial results for finding errors in L2 writing. We also mention that fixing these errors automatically is often complicated by spelling mistakes.	paraphrases
60	The Ups and Downs of Preposition Error Detection in ESL Writing In this paper we describe a methodology for detecting preposition errors in the writing of non-native English speakers. Our system performs at 84% precision and close to 19% recall on a large set of student essays. In addition, we address the problem of annotation and evaluation in this domain by showing how current approaches of using only one rater can skew system evaluation. We present a sampling approach to circumvent some of the issues that complicate evaluation of error detection systems. We use the TOEFL data. We show that agreement between two native speakers on a cloze test targeting prepositions is about 76%, which demonstrates that there are many contexts that license multiple prepositions. Our model is trained with lexical features.	The Ups and Downs of Preposition Error Detection in ESL Writing In this paper we describe a method for finding preposition mistakes in the writing of people learning English. Our system is right 84% of the time (precision) but only finds about 19% of all mistakes (recall) in a large set of student essays. We also talk about the problem of grading and judging in this area by showing how using just one person to rate can make the system's performance look different. We present a sampling method to avoid some problems that make judging error detection systems difficult. We use data from the TOEFL test. We show that when two native English speakers take a fill-in-the-blank test focused on prepositions, they agree 76% of the time, showing that there are many situations where different prepositions can be used correctly. Our model is trained using word features.	entailment
61	A Uniform Approach to Analogies Synonyms Antonyms and Associations Recognizing analogies, synonyms, antonyms, and associations appear to be four distinct tasks, requiring distinct NLP algorithms. In the past, the four tasks have been treated independently, using a wide variety of algorithms. These four semantic classes, however, are a tiny sample of the full range of semantic phenomena, and we cannot afford to create ad hoc algorithms for each semantic phenomenon; we need to seek a unified approach. We propose to subsume a broad range of phenomena under analogies. To limit the scope of this paper, we restrict our attention to the subsumption of synonyms, antonyms, and associations. We introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT analogy questions, TOEFL synonym questions, ESL synonym-antonym questions, and similar-associated-both questions from cognitive psychology. We propose a simpler SVM-based algorithm for analogical classification called PairClass. We argue that many NLP tasks can be formulated in terms of analogical reasoning, and we apply our PairClass algorithm to a number of problems including SAT verbal analogy tests, synonym/antonym classification and distinction between semantically similar and semantically associated words. We advocate the need for a uniform approach to corpus-based semantic tasks.	A Uniform Approach to Analogies Synonyms Antonyms and Associations Recognizing analogies (like comparisons), synonyms (words with similar meanings), antonyms (opposite words), and associations (related words) seem like four different jobs, needing different language processing methods. In the past, these four jobs have been handled separately, using many different techniques. However, these four types are just a small part of all language meanings, and we can't make special methods for each one; we need a single way to handle them all. We suggest covering a wide range of these types using analogies. To keep this paper focused, we only look at including synonyms, antonyms, and associations. We introduce a teaching-based computer learning method using written examples to classify word pairs that work like analogies, and we show it can answer multiple-choice SAT analogy questions, TOEFL synonym questions, ESL synonym-antonym questions, and questions from psychology about words that are similar, related, or both. We suggest a simpler algorithm (a step-by-step problem-solving method) based on Support Vector Machine (SVM) for classifying analogies called PairClass. We believe many language tasks can be thought of in terms of analogical thinking, and we apply our PairClass method to various problems like SAT verbal analogy tests, identifying synonyms and antonyms, and telling apart words that are similar in meaning from those that are just related. We support the need for a single method for handling language meaning tasks using written examples.	English
62	Top Accuracy and Fast Dependency Parsing is not a Contradiction In addition to a high accuracy, short parsing and training times are the most important properties of a parser. However, parsing and training times are still relatively long. To determine why, we analyzed the time usage of a dependency parser. We illustrate that the mapping of the features onto their weights in the support vector machine is the major factor in time complexity. To resolve this problem, we implemented the passive-aggressive perceptron algorithm as a Hash Kernel. The Hash Kernel substantially improves the parsing times and takes into account the features of negative examples built during the training. This has lead to a higher accuracy. We could further increase the parsing and training speed with a parallel feature extraction and a parallel parsing algorithm. We are convinced that the Hash Kernel and the parallelization can be applied successful to other NLP applications as well such as transition based dependency parsers, phrase structrue parsers, and machine translation. We show that the Hash Kernel improves parsing speed and accuracy since the parser uses additionally negative features. The Mateparser is an efficient second order dependency parser that models the interaction between siblings as well as grandchildren.	Top Accuracy and Fast Dependency Parsing is not a Contradiction In addition to being very accurate, having short times for parsing (breaking down language structures) and training (teaching the system) are the most important features of a parser (a tool that analyzes sentences). However, these times are still pretty long. To find out why, we studied how a dependency parser (a tool that looks at how words depend on each other) uses time. We show that connecting features (important details) to their weights (importance) in the support vector machine (a type of computer model) is the main reason for long time usage. To fix this, we used the passive-aggressive perceptron algorithm (a method for training models) as a Hash Kernel (a smart way to organize and access data). The Hash Kernel makes parsing faster and considers the negative examples (unsuccessful cases) during training, which leads to better accuracy. We could make parsing and training even faster by using parallel (simultaneous) feature extraction and parsing. We believe the Hash Kernel and parallel processing can work well in other language processing tasks too, such as transition-based dependency parsers (another parsing method), phrase structure parsers (tools that understand sentence structures), and machine translation (converting text from one language to another). We show that the Hash Kernel speeds up parsing and improves accuracy by also using negative features. The Mateparser is a fast second-order dependency parser that looks at relationships between words like siblings and grandchildren (levels of word connections).	analogies
63	A Monolingual Tree-based Translation Model for Sentence Simplification In this paper, we consider sentence simplification as a special form of translation with the complex sentence as the source and the simple sentence as the target. We propose a Tree-based Simplification Model (TSM), which, to our knowledge, is the first statistical simplification model covering splitting, dropping, reordering and substitution integrally. We also describe an efficient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia. The evaluation shows that our model achieves better readability scores than a set of baseline systems. We use a tree-based simplification model which uses techniques from statistical machine translation (SMT) with this data set. We examine the use of paired documents in English Wikipedia and Simple Wikipediafor a data-driven approach to the sentence simplification task. We propose sentence simplification can be approached as a monolingual machine translation task, where the source and target languages are the same and where the output should be simpler in form from the input but similar in meaning.	A Monolingual Tree-based Translation Model for Sentence Simplification In this paper, we look at sentence simplification as a type of translation where the complicated sentence is changed into a simple sentence. We suggest a Tree-based Simplification Model (TSM), which we believe is the first statistical model that combines splitting sentences, removing unnecessary parts, changing the order of words, and replacing words all together. We also explain an effective way to train our model using a large set of matching data from Wikipedia and Simple Wikipedia. The testing shows that our model makes sentences easier to read compared to other basic systems. We use a tree-based simplification model which applies methods from statistical machine translation (SMT) with this data. We explore using paired articles from English Wikipedia and Simple Wikipedia to guide the sentence simplification process with data. We suggest that simplifying sentences can be seen as a task of translating within the same language, aiming for an output that is easier to read but has the same meaning as the input.	processing
64	Robust Sentiment Detection on Twitter from Biased and Noisy Data In this paper, we propose an approach to automatically detect sentiments on Twitter messages (tweets) that explores some characteristics of how tweets are written and meta-information of the words that compose these messages. Moreover, we leverage sources of noisy labels as our training data. These noisy labels were provided by a few sentiment detection websites over twitter data. In our experiments, we show that since our features are able to capture a more abstract representation of tweets, our solution is more effective than previous ones and also more robust regarding biased and noisy data, which is the kind of data provided by these sources. We propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features.	Robust Sentiment Detection on Twitter from Biased and Noisy Data In this paper, we suggest a way to automatically find feelings in Twitter messages (tweets) by looking at how tweets are written and extra information about the words in these messages. We also use imperfect data from some websites that judge emotions on Twitter as our training data. In our tests, we demonstrate that because our method captures a more general idea of tweets, it works better than older methods and handles imperfect and biased data better, which is the type of data from these sources. We introduce a two-step method to sort the feelings in tweets using SVM classifiers (a type of machine learning model) with general features.	sentences
65	Enhanced Sentiment Learning Using Twitter Hashtags and Smileys Automated identification of diverse sentiment types can be beneficial for many NLP systems such as review summarization and public media analysis. In some of these systems there is an option of assigning a sentiment value to a single sentence or a very short text. In this paper we propose a supervised sentiment classification framework which is based on data from Twitter, a popular microblogging service. By utilizing 50 Twitter tags and 15 smileys as sentiment labels, this framework avoids the need for labor intensive manual annotation, allowing identification and classification of diverse sentiment types of short texts. We evaluate the contribution of different feature types for sentiment classification and show that our framework successfully identifies sentiment types of untagged sentences. The quality of the sentiment identification was also confirmed by human judges. We also explore dependencies and overlap between different sentiment types represented by smileys and Twitter hashtags. We used 50 hash tags and 15 emoticons as noisy labels to create a dataset for twitter sentiment classification.	Enhanced Sentiment Learning Using Twitter Hashtags and Smileys Automated identification of different types of feelings can be helpful for many language processing systems like those that summarize reviews or analyze public media. In some of these systems, there is an option to give a feeling value to a single sentence or a very short text. In this paper, we suggest a supervised way to classify feelings using data from Twitter, a widely used microblogging platform. By using 50 Twitter tags and 15 smileys (faces showing emotions) as labels for feelings, this method avoids the need for time-consuming manual labeling, allowing it to identify and classify different feeling types in short texts. We check how different features help in classifying feelings and show that our method can successfully identify feelings in sentences that aren't tagged. The accuracy of identifying feelings was also confirmed by human judges. We also look at the connections and similarities between different feeling types shown by smileys and Twitter hashtags. We used 50 hashtags and 15 emoticons (simple pictures expressing emotions) as rough labels to create a dataset for classifying feelings on Twitter.	introduce
66	D-PATR: A Development Environment For Unification-Based Grammars We describe systems in which FSs may be modified by default statements in such a way that this property does not automatically hold.	D-PATR: A Development Environment For Unification-Based Grammars We explain systems where Feature Structures (FSs) can be changed by default rules so that this characteristic does not always apply automatically.	summarize
67	Categorial Unification Grammars Categorial unification grammars (CUGs) embody the essential properties of both unification and categorial grammar formalisms. Their efficient and uniform way of encoding linguistic knowledge in well-understood and widely used representations makes them attractive for computational applications and for linguistic research. In this paper, the basic concepts of CUGs and simple examples of their application will be presented. It will be argued that the strategies and potentials of CUGs justify their further exploration in the wider context of research on unification grammars. Approaches to selected linguistic phenomena such as long-distance dependencies, adjuncts, word order, and extraposition are discussed.	Categorial Unification Grammars Categorial unification grammars (CUGs) combine key features of both unification and categorial grammar systems. They offer an effective and consistent way to express language knowledge using familiar and widely accepted methods, making them useful for computer applications and language research. This paper will introduce the basic ideas of CUGs and give simple examples of how they are used. It will suggest that the methods and possibilities of CUGs make them worth further study in the larger field of research on unification grammars. The paper will also talk about how CUGs can handle certain language topics like long-distance dependencies (connections between parts of a sentence that are far apart), adjuncts (extra information in a sentence), word order (the arrangement of words in a sentence), and extraposition (moving parts of a sentence to a different place).	Development
68	A Statistical Approach To Language Translation An approach to automatic translation is outlined that utilizes techniques of statistical information extraction from large data bases. The method is based on the availability of pairs of large corresponding texts that are translations of each other. In our case, the texts are in English and French. Fundamental to the technique is a complex glossary of correspondence of fixed locutions. The steps of the proposed translation process are: (1) Partition the source text into a set of fixed locutions. (2) Use the glossary plus contextual information to select the corresponding set of fixed locutions into a sequence forming the target sentence. (3) Arrange the words of the target fixed locutions into a sequence forming the target sentence. We have developed statistical techniques facilitating both the automatic creation of the glossary, and the performance of the three translation steps, all on the basis of an alignment of corresponding sentences in the two texts. While we are not yet able to provide examples of French / English translation, we present some encouraging intermediate results concerning glossary creation and the arrangement of target word sequences. We develop the so-called IBM models, implementing a set of elementary operations, such as movement, duplication and translation, that independently act on individual words in the source sentence. In general a statistical machine translation system is composed of three components: a language model, a translation model, and a decoder. The language model tells how probable a given sentence is in the source language, the translation model indicates how likely it is that a particular target sentence is a translation of a given source sentence, and the decoder is what actually takes a source sentence as input and produces its translation as output.	A Statistical Approach To Language Translation An approach to automatic translation is explained that uses methods to gather information from large databases. The method relies on having large pairs of texts that are translations of each other. In our case, the texts are in English and French. Key to the technique is a detailed list (glossary) of matching phrases. The steps in the translation process are: (1) Break down the original text into set phrases. (2) Use the glossary and surrounding information to choose the matching set phrases to form the final sentence. (3) Arrange the words of the chosen phrases to form the final sentence. We have created statistical methods to help both make the glossary automatically and carry out these three translation steps, using aligned matching sentences in the two texts. Although we cannot yet show examples of French/English translation, we present promising initial results about creating the glossary and arranging word sequences. We develop certain models, using basic actions like moving, copying, and translating that work on individual words in the original sentence. Generally, a statistical machine translation system has three parts: a language model, a translation model, and a decoder. The language model shows how likely a sentence is in the original language, the translation model shows how likely a target sentence is a translation of the original, and the decoder is what actually translates the original sentence.	grammars
69	Parsing Strategies With 'Lexicalized' Grammars: Application To Tree Adjoining Grammars In this paper we present a general parsing strategy that arose from the development of an Earley-type parsing algorithm for TAGs (Schabes and Joshi 1988) and from recent linguistic work in TAGs (Abeille 1988). In our approach elementary structures are associated with their lexical heads. These structures specify extended domains of locality (as compared to a context-free grammar) over which constraints can be stated. These constraints either hold within the elementary structure itself or specify what other structures can be composed with a given elementary structure. We state the conditions under which context-free based grammars can be 'lexicalized' without changing the linguistic structures originally produced. We argue that even if one extends the domain of locality of CFGs to trees, using only substitution does not give the freedom to choose the head of each structure. We show how adjunction allows us to 'lexicalize' a CFG freely. We then show how a 'lexicalized' grammar naturally follows from the extended domain of locality of TAGs and present some of the linguistic advantages of our approach. A novel general parsing strategy for 'lexicalized' grammars is discussed. In a first stage, the parser builds a set structures corresponding to the input sentence and in a second stage, the sentence is parsed with respect to this set. The strategy is independent of the linguistic theory adopted and of the underlying grammar formalism. However, we focus our attention on TAGs. Since the set of trees needed to parse an input sentence is supposed to be finite, the parser can use in principle any search strategy. Thus, in particular, a top-down strategy can be used since problems due to recursive structures are eliminated. The parser is also able to use non-local information to guide the search. We then explain how the Earley-type parser for TAGs can be modified to take advantage of this approach. Lexicalized grammars offer significant parsing benefits as the number of applications of productions (i.e., derivation steps) is clearly bounded by the length of the input string.	Parsing Strategies With 'Lexicalized' Grammars: Application To Tree Adjoining Grammars. In this paper, we introduce a general method for breaking down sentences (parsing) that came from creating a parsing algorithm similar to Earley's for Tree Adjoining Grammars (TAGs) and from recent studies on TAGs. In our method, basic sentence structures are linked with their main words (lexical heads). These structures cover larger areas than traditional grammars, allowing us to set rules about them. These rules either apply to the structure itself or explain how it can connect with other structures. We explain when grammars based on simple rules can be turned into 'lexicalized' ones without changing the original sentence structures. We claim that even if you expand simple grammars to include whole trees, just using substitution doesn't let you choose the main word of each structure freely. We demonstrate how using 'adjunction' (adding extra elements) allows us to easily create 'lexicalized' grammars from simple ones. We then explain how 'lexicalized' grammars come naturally from the larger areas covered by TAGs and discuss some language benefits of our method. We talk about a new general method for parsing 'lexicalized' grammars. First, the parser creates structures corresponding to the sentence, then it analyzes the sentence based on these structures. This method works with any language theory or grammar system, but we focus on TAGs. Since the number of trees needed for parsing a sentence is limited, any search method can be used. Particularly, a top-down method can be used as it avoids problems with repeating patterns. The parser can also use information from outside the immediate area to help with the search. We then describe how the Earley-type parser for TAGs can be changed to use this new method. Lexicalized grammars provide major benefits because the number of steps needed to break down a sentence is clearly limited by the sentence's length.	databases
74	Toward Memory-Based Translation An essential problem of example-based translation is how to utilize more than one translation example for translating one source sentence. This 1)aper proposes a method to solve this problem. We introduce the representation, called matching expression, which represents the combination of fragments of translation examples. The translation process consists of three steps: (1) Make the source matching expression from the source sentence. (2) Transfer the source matching expression into the target matching expression. (3) Construct the target sentence from the target matching expression. This mechanism generates some candidates of translation. To select, the best translation out of them, we define the score of a translation. We combine a measure of structural similarity with a measure of word distance in order to obtain the overall distance measure that is used for matching.	Toward Memory-Based Translation An essential problem of example-based translation is how to use more than one translation example for translating one original sentence. This paper proposes a method to solve this problem. We introduce a tool, called matching expression, which shows how parts of translation examples fit together. The translation process has three steps: (1) Create the source matching expression from the original sentence. (2) Change the source matching expression into the target matching expression. (3) Build the target sentence from the target matching expression. This system creates some translation options. To choose the best translation from these options, we define a score for each translation. We combine a way to measure how similar the structure is with a way to measure the difference in words to get an overall distance measure used for matching.	Probabilities
70	A Uniform Architecture For Parsing And Generation The use of a single grammar for both parsing and generation is an idea with a certain elegance, the desirability of which several researchers have noted. In this paper, we discuss a more radical possibility: not only can a single grammar be used by different processes engaged in various "directions" of processing, but one and the same language-processing architecture can be used for processing the grammar in the various modes. In particular, parsing and generation can be viewed as two processes engaged in by a single parameterized theorem prover for the logical interpretation of the formalism. We discuss our current implementation of such an architecture, which is parameterized in such a way that it can be used for either purpose with grammars written in the PATR formalism. Furthermore, the architecture allows fine tuning to reflect different processing strategies, including parsing models intended to mimic psycholinguistic phenomena. This tuning allows the parsing system to operate within the same realm of efficiency as previous architectures for parsing alone, but with much greater flexibility for engaging in other processing regimes. We state that to guarantee completeness in using a precomputed entry in the chart, the entry must subsume the formula being generated top-down.	A Uniform Architecture For Parsing And Generation The idea of using one set of language rules (grammar) for both understanding (parsing) and creating (generation) sentences is attractive and has been considered by many researchers. In this paper, we explore a more daring idea: not only can one grammar be used by different processes working in different ways, but the same system for understanding and creating language can be used to handle the grammar in different modes. Specifically, understanding and creating language can be seen as two tasks performed by a single flexible system that figures out the logical meaning of the structure. We explain our current version of this system, which is designed to work for either task with language rules written in a specific format called PATR. Additionally, this system can be adjusted to match different methods of processing, including models for understanding language that are designed to imitate human language understanding. These adjustments allow the system to work as efficiently as previous systems focused only on understanding, but with much more ability to handle other tasks. We mention that to ensure all possibilities are covered when using a pre-prepared entry in the system's record, the entry must cover the formula being created from the top down.	rules about
71	Feature Structures Based Tree Adjoining Grammars We have embedded Tree Adjoining Grammars (TAG) in a feature structure based unification system. The resulting system, Feature Structure based Tree Adjoining Grammars (FTAG), captures the principle of factoring dependencies and recursion, fundamental to TAG's. We show that FTAG has an enhanced descriptive capacity compared to TAG formalism. We consider some restricted versions of this system and some possible linguistic stipulations that can be made. We briefly describe a calculus to represent the structures used by this system, extending on the work of Rounds, and Kasper [Rounds et al. 1986, Kasper et al. 1986] involving the logical formulation of feature structures. A Feature-based TAG consists of a set of (auxiliary or initial) elementary trees and of two tree-composition operations: substitution and adjunction.	Feature Structures Based Tree Adjoining Grammars We have combined Tree Adjoining Grammars (TAG), a type of grammar used in language processing, with a system that uses feature structures to unify information. This new system, called Feature Structure based Tree Adjoining Grammars (FTAG), simplifies the way dependencies and repetitive patterns are handled, which is a key aspect of TAGs. We demonstrate that FTAG can describe things more effectively than the original TAG system. We also explore simpler versions of this system and some possible language rules that can be applied. We provide a brief overview of a method to represent the structures used by this system, building upon previous work by Rounds and Kasper, which involved the logical setup of feature structures. A Feature-based TAG is made up of basic tree structures (either auxiliary or initial) and involves two main ways to combine these trees: substitution, which means replacing one part with another, and adjunction, which means adding extra parts.	version
72	Word Sense Disambiguation With Very Large Neural Networks Extracted From Machine Readable Dictionaries In this paper, we describe a means for automatically building very large neural networks (VLNNs) from definition texts in machine-readable dictionaries, and demonstrate the use of these networks for word sense disambiguation. Our method brings together two earlier, independent approaches to word sense disambiguation: the use of machine-readable dictionaries and spreading and activation models. The automatic construction of VLNNs enables real-size experiments with neural networks for natural language processing, which in turn provides insight into their behavior and design and can lead to possible improvements. We apply conventional spreading activation approaches to word sense disambiguation.	Word Sense Disambiguation With Very Large Neural Networks Extracted From Machine Readable Dictionaries In this paper, we explain a method to automatically create very large neural networks (VLNNs) from the text of definitions in digital dictionaries and show how these networks help in figuring out the correct meaning of words. Our method combines two earlier, separate techniques for understanding word meanings: using digital dictionaries and spreading activation models (a way to mimic how the brain processes information). Building these VLNNs automatically allows us to run experiments with real-sized neural networks for understanding human language, which helps us understand how they work and how we might make them better. We use traditional spreading activation methods to help determine word meanings.	combine
73	Constraint Grammar As A Framework For Parsing Running Text Grammars which are used in parsers are often directly imported from autonomous grammar theory and descriptive practice that were not exercised for the explicit purpose of parsing. Parsers have been designed for English based on e.g. Government and Binding Theory, Generalized Phrase Structure Grammar, and Lexical-Functional Grammar. We present a formalism to be used for parsing where the grammar statements are closer to real text sentences and more directly address some notorious parsing problems, especially ambiguity. The formalism is a linguistic one. It relies on transitional probabilities in an indirect way. The probabilities are not part of the description. The descriptive statements, constraints, do not have the ordinary task of defining the notion 'correct sentence in L'. They are less categorical in nature, more closely tied to morphological features, and more directly geared towards the basic task of parsing. We see this task as one of inferring surface structure from a stream of concrete tokens in a basically bottom-up mode. Constraints are formulated on the basis of extensive corpus studies. They may reflect absolute, rule-like facts, or probabilistic tendencies where a certain risk is judged to be proper to take. Constraints of the former rule-like type are of course preferable. The ensemble of constraints for language L constitute a Constraint Grammar (CG) for L. A CG is intended to be used by the Constraint Grammar Parser CGP, implemented as a Lisp interpreter. Our input tokens to CGP are morphologically analyzed word-forms. One central idea is to maximize the use of morphological information for parsing purposes. All relevant structure is assigned directly via lexicon, morphology, and simple mappings from morphology to syntax. The task of the constraints is basically to discard as many alternatives as possible, the optimum being a fully disambiguated sentence with one syntactic reading only. The second central idea is to treat morphological disambiguation and syntactic labelling by the same mechanism of discarding improper alternatives. A good parsing formalism should satisfy many requirements: the constraints should be declarative rather than procedural, they should be able to cope with any real-world text-sentence (i.e. with running text, not just with linguists' laboratory sentences), they should be clearly separated from the program code by which they are executed, the formalism should be language-independent, it should be reasonably easy to implement (optimally as finite-state automata), and it should also be efficient to run. The CG formalism adheres to these desiderata. we propose the Constraint Grammar framework.	Constraint Grammar As A Framework For Parsing Running Text Grammars used in parsers (tools that analyze sentence structure) are often taken from grammar theories and practices not specifically designed for parsing. Parsers have been created for English using theories like Government and Binding Theory, Generalized Phrase Structure Grammar, and Lexical-Functional Grammar. We introduce a system for parsing that makes grammar rules closer to real sentences and addresses common problems like ambiguity (when a sentence can have more than one meaning). This system is based on language rules. It indirectly uses probabilities (how likely something is). Probabilities aren't directly included in the rules. The rules (called constraints) don't just define what a 'correct sentence' is. They are more flexible and focus on word structure, helping with the main job of parsing. Parsing here means figuring out the structure of a sentence from individual words and phrases. Constraints are based on extensive study of language use. They can represent strict rules or tendencies that involve some level of risk. Strict rules are preferred. All the constraints for a language make up a Constraint Grammar (CG) for that language. A CG is used by the Constraint Grammar Parser (CGP), which is a program written in Lisp (a programming language). The input for CGP is words that have been analyzed for their structure. One key idea is to use as much word structure information as possible for parsing. The structure is assigned directly through a dictionary, word structure analysis, and simple connections from word structure to sentence structure. The goal of the constraints is to eliminate as many wrong options as possible, ideally leaving a sentence with only one clear meaning. Another key idea is to handle both word structure and sentence labeling by removing incorrect options. A good parsing system should meet several needs: constraints should be rules, not step-by-step instructions, they should handle any real-world text (not just examples made by linguists), they should be separate from the program code they run on, the system should work for any language, it should be easy to set up (ideally as simple automated processes), and it should work efficiently. The CG system meets these needs. We suggest using the Constraint Grammar framework.	spreading activation
217	Corpus-Based Statistical Sense Resolution The three corpus-based statistical sense resolution methods studied here attempt to infer the correct sense of a polysemous word by using knowledge about patterns of word co-occurrences. The techniques were based on Bayesian decision theory, neural networks, and content vectors as used in information retrieval. To understand these methods better, we posed a very specific problem: given a set of contexts, each containing the noun line in a known sense, construct a classifier that selects the correct sense of line for new contexts. To see how the degree of polysemy affects performance, results from three- and six-sense tasks are compared. The results demonstrate that each of the techniques is able to distinguish six senses of line with an accuracy greater than 70%. Furthermore, the response patterns of the classifiers are, for the most part, statistically indistinguishable from one another. Comparison of the two tasks suggests that the degree of difficulty involved in resolving individual senses is a greater performance factor than the degree of polysemy. We construct 2094-word line dataset for word sense disambiguation.	Corpus-Based Statistical Sense Resolution The three methods studied here try to figure out the right meaning of a word that has multiple meanings by looking at how words appear together. The methods were based on different techniques: using probability theory (Bayesian decision theory), computer systems that mimic the human brain (neural networks), and analyzing the content for finding information (content vectors). To understand these methods better, we focused on a specific problem: given a few examples where the word "line" is used clearly, create a tool that picks the right meaning of "line" in new situations. To see how having more meanings affects the results, we compared tasks with three meanings and six meanings. The results show that each method can identify six meanings of "line" with over 70% accuracy. Also, the way these tools responded was mostly similar to each other statistically. Comparing the two tasks suggests that how hard it is to decide on a specific meaning is more important than how many meanings a word has. We created a dataset with 2094 instances of the word "line" for working out its meaning.	which meaning
75	Synchronous Tree-Adjoining Grammars The unique properties of Tree-adjoining grammars (TAG) present a challenge for the application of TAGs beyond the limited confines of syntax, for instance, to the task of semantic interpretation or automatic translation of natural language. We present a variant of TAGs, called synchronous TAGs, which characterize correspondences between languages. The formalism's intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language, or to their translates in another natural language; in summary, we intend it to allow TAGs to be used beyond their role in syntax proper. We discuss the application of synchronous TAGs to concrete examples, mentioning primarily in passing some computational issues that arise in its interpretation. Synchronous Tree Adjoining Grammars is introduced primarily for semantics but will be later also proposed for translation. A synchronous derivation process for the two syntactic structures of both languages suggests the level of cross-lingual isomorphism between the two trees (e.g. Synchronous Tree Adjoining Grammars.	Synchronous Tree-Adjoining Grammars The special features of Tree-adjoining grammars (TAG) make it difficult to use them outside of their typical use in sentence structure, like for understanding meaning or translating languages automatically. We introduce a version of TAGs called synchronous TAGs that show connections between languages. This method is meant to link words and sentences in one language to their meanings in a logical form or to their translations in another language, allowing TAGs to be used for more than just sentence structure. We talk about using synchronous TAGs with real-life examples and briefly mention some computer-related challenges that come up when interpreting them. Synchronous Tree Adjoining Grammars is mainly introduced for understanding meaning but will also be suggested for translation later. A synchronous process for creating sentence structures in both languages shows how similar the sentence structures (trees) are between the two languages.	example
76	Typed Unification Grammars We introduce TFS, a computer formalism in the class of logic formalisms which integrates a powerful type system. Its basic data structures are typed feature structures. The type system encourages an object-oriented approach to linguistic description by providing a multiple inheritance mechanism and an inference mechanism which allows the specitication of relations between levels of linguistic description defined as classes of objects. We illustrate this approach starting from a very simple DCG, and show how to make use of the typing system to enforce general constraints and modularize linguistic descriptions, and how further abstraction leads to a HPSG-like grammar. The proposed approach inevitably leads to the consequence that the data structure becomes slightly complicated.	Typed Unification Grammars We introduce TFS, a computer system that is part of logic systems and includes a strong way to categorize information. Its main data structures are called typed feature structures, which are like organized templates. The type system supports an object-oriented method (a way of organizing information like objects) for describing language by providing a way to share properties (multiple inheritance) and a way to define relations between different language levels, treated as classes of objects. We explain this method starting from a simple DCG (a basic grammar model), and show how to use the type system to apply general rules and divide language descriptions into parts. We also explain how more general ideas lead to a grammar similar to HPSG (a complex grammar model). This approach can make the data structure a bit more complicated.	difficult
77	Automatic Processing Of Large Corpora For The Resolution Of Anaphora References Manual acquisition of semantic constraints in broad domains is very expensive. This paper presents an automatic scheme for collecting statistics on cooccurrence patterns in a large corpus. To a large extent, these statistics reflect, semantic constraints and thus are used to disambiguate anaphora references and syntactic ambiguities. The scheme was implemented by gathering statistics on the output of other linguistic tools. An experiment was performed to resolve references of the pronoun "it" in sentences that were randomly selected from the corpus. Title results of the experiment show that in most of the cases the cooccurrence statistics indeed reflect the semantic constraints and thus provide a basis for a useful disambiguation tool. We use the distribution of a pronoun's context to determine which candidate antecedents can fit the context. We present one of the earliest methods for using predicate-argument frequencies in pronoun resolution.	Automatic Processing Of Large Corpora For The Resolution Of Anaphora References Manual acquisition of semantic constraints in broad domains is very expensive. This paper presents an automatic method for collecting data on patterns that frequently occur together in a large collection of texts. To a large extent, these data show limitations based on meaning, and thus are used to clarify unclear references and sentence structure. The method was carried out by collecting data from the results of other language tools. An experiment was conducted to resolve the references of the pronoun "it" in sentences that were randomly chosen from the collection. Title results of the experiment show that in most cases, the frequently occurring data indeed reflect the limitations based on meaning and thus provide a basis for a useful tool to clarify confusion. We use the distribution of a pronoun's context to determine which possible previous words can fit the context. We present one of the earliest methods for using how often certain words appear together in resolving pronouns.	grammar similar
78	Word Identification For Mandarin Chinese Sentences Chinese sentences are composed with string of characters without blanks to mark words. However the basic unit for sentence parsing and understanding is word. Therefore the first step of processing Chinese sentences is to identify the words. The difficulties of identifying words include (l) the identification of complex words, such as Determinative-Measure, reduplications, derived words etc., (2) the identification of proper names,(3) resolving the ambiguous segmentations. In this paper, we propose the possible solutions for the above difficulties. We adopt a matching algorithm with 6 different heuristic rules to resolve the ambiguities and achieve a 99.77% of the success rate. The statistical data supports that the maximal matching algorithm is the most effective heuristics. we propose the forward maximum matching algorithm.	Word Identification For Mandarin Chinese Sentences Chinese sentences are made up of a series of characters without spaces to separate words. However, the basic unit needed to break down and understand a sentence is a word. So, the first step in processing Chinese sentences is to find the words. The challenges in finding words include (1) identifying complex words, like those that combine a word and a measure word, repeated words, words made from other words, etc., (2) recognizing proper names, (3) solving unclear separations between words. In this paper, we suggest possible solutions for these challenges. We use a matching method with 6 different practical rules to solve the unclear separations and achieve a 99.77% success rate. The data shows that the best matching method is the most effective approach. We suggest using the forward maximum matching method.	Large Corpora
79	Two-Level Morphology With Composition We recognize that a cascade of composed FSTs could implement the two-level model. We observe that the rule sets may be composed with the lexicon transducers in an efficient way and that the resulting transducer was roughly similar in size as the lexicon transducer itself.	Two-Level Morphology With Composition We understand that using a series of combined FSTs (Finite State Transducers, which are tools that map between different sets of symbols) could achieve the two-level model. We notice that the sets of rules can be combined with the dictionary transducers efficiently, and the combined transducer ended up being about the same size as the dictionary transducer itself.	between
80	A Fast Algorithm For The Generation Of Referring Expressions We simplify previous work in the development of algorithms for the generation of referring expressions while at the same time taking account of psycholinguistic findings and transcript data. The result is a straightforward algorithm that is computationally tractable, sensitive to the preferences of human users, and reasonably domain-independent. We provide a specification of the resources a host system must provide in order to make use of the algorithm, and describe an implementation used in the IDAS system. We apply generalizations about the salience of properties of objects and conventions about what words make base level attributions to incrementally select words for inclusion in a description.	A Fast Algorithm For The Generation Of Referring Expressions We make previous methods for creating algorithms that produce referring expressions (ways to describe things) simpler, while also considering findings from psychology about language and real-world examples. The result is a simple algorithm that is easy for computers to handle, respects what people prefer, and works in many different situations. We explain what resources a computer system needs to use this algorithm, and describe how it was used in the IDAS system. We use general ideas about what makes certain features of objects stand out and rules about which words are used to describe basic characteristics to gradually choose words for a description.	Finite State
81	Stochastic Lexicalized Tree-Adjoining Grammars The notion of stochastic lexicalized tree-adjoining grammar (SLTAG) is formally defined. The parameters of a SLTAG correspond to the probability of combining two structures each one associated with a word. The characteristics of SLTAG are unique and novel since it is lexically sensitive (as N-gram models or Hidden Markov Models) and yet hierarchical (as stochastic context-free grammars). Then, two basic algorithms for SLTAG arc introduced: an algorithm for computing the probability of a sentence generated by a SLTAG and an inside-outside- like iterative algorithm for estimating the parameters of a SLTAG given a training corpus. Finally, we should how SLTAG enables to define a lexicalized version of stochastic context-free grammars and we report preliminary experiments showing some of the advantages of SLTAG over stochastic context-free grammars. In stochastic tree-adjoining grammar, this lack of context-sensitivity is overcome by assigning probabilities to larger structural units.	Stochastic Lexicalized Tree-Adjoining Grammars The idea of stochastic lexicalized tree-adjoining grammar (SLTAG) is formally explained. The parameters of a SLTAG relate to the chance of combining two structures, each linked with a word. The features of SLTAG are special and new because it is sensitive to individual words (like models that predict word sequences or models that guess hidden states) and also structured in layers (like grammars that predict sentence structure). Then, two main methods for SLTAG are introduced: a method for calculating the chance of a sentence created by a SLTAG and a repeated process similar to the "inside-outside" method for figuring out the parameters of a SLTAG using a practice set of texts. Finally, we show how SLTAG allows creating a version of stochastic context-free grammars that focuses on words, and we share early tests showing some benefits of SLTAG over regular stochastic context-free grammars. In stochastic tree-adjoining grammar, the problem of not considering context is solved by giving probabilities to larger parts of the structure.	psychology about
82	Word-Sense Disambiguation Using Statistical Models Of Roget's Categories Trained On Large Corpora This paper describes a program that disambignates English word senses in unrestricted text using statistical models of the major Roget's Thesaurus categories. Roget's categories serve as approximations of conceptual classes. The categories listed for a word in Roger's index tend to correspond to sense distinctions; thus selecting the most likely category provides a useful level of sense disambiguation. The selection of categories is accomplished by identifying and weighting words that are indicative of each category when seen in context, using a Bayesian theoretical framework. Other statistical approaches have required special corpora or hand-labeled training examples for much of the lexicon. Our use of class models overcomes this knowledge acquisition bottleneck, enabling training on unrestricted monolingual text without human intervention. Applied to the 10 million word Grolier's Encyclopedia, the system correctly disambiguated 92% of the instances of 12 polysemous words that have been previously studied in the literature. We rely on the intuition that the senses of words are hinted at by their contextual information. From the perspective of a generative process, neighboring words of a target are generated by the target's underlying sense.	Word-Sense Disambiguation Using Statistical Models Of Roget's Categories Trained On Large Corpora This paper explains a program that figures out the meanings of English words in any type of text using statistical methods based on major groups from Roget's Thesaurus. Roget's groups act like rough categories for different ideas. The groups listed for a word in Roget's index often match up with different meanings; so, picking the most likely group helps us understand the word's meaning better. Choosing the groups is done by finding and giving importance to words that suggest each group when seen in context, using a method based on probability (Bayesian theory). Other statistical methods needed special text collections or examples labeled by hand for many words. Our use of group models avoids this problem of gathering knowledge, allowing us to train on any single-language text without human help. When applied to the 10 million-word Grolier's Encyclopedia, the system correctly figured out the meanings of 92% of the examples of 12 words with multiple meanings that have been studied before. We base our method on the idea that the meanings of words are suggested by the words around them. From the view of creating text, nearby words of a target word are produced by the target word's true meaning.	SLTAG allows
83	Automatic Acquisition Of Hyponyms From Large Text Corpora We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to attgment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested. We find individual pairs of hypernyms and hyponyms from text using pattern-matching techniques.	Automatic Acquisition Of Hyponyms From Large Text Corpora We explain a method to automatically find hyponyms (specific examples of broader categories) from any text. This method has two main goals: (i) to avoid needing pre-existing knowledge and (ii) to work with many types of text. We look for patterns in the text that are easy to spot, happen often, and clearly show the relationship we are interested in. We explain how to find these patterns and suggest that other word relationships can be found in the same way. Part of the method is tested, and the results help improve and review a large manually created thesaurus (a book of words grouped by meaning). We also suggest how this can help in areas like finding information. We identify pairs of hypernyms (general terms) and hyponyms (specific examples) from text using pattern-finding methods.	context
84	A Computational Model Of Language Performance: Data Oriented Parsing Data Oriented Parsing (DOP) is a model where no abstract rules, but language experiences in the form of an analyzed corpus, constitute the basis for language processing. Analyzing a new input means that the system attempts to find find the most probable way to reconstruct the input out of fragments that already exist in the corpus. Disambiguation occurs as a side-effect. DOP can be implemented by using conventional parsing strategies. In this work, super strong equivalence relations between other stochastic grammars are studied. We show that conventional context-free parsing techniques can be used in creating a parse forest for a sentence in DOP1.	A Computational Model Of Language Performance: Data Oriented Parsing Data Oriented Parsing (DOP) is a method where instead of using abstract rules, previous language experiences stored in a database called a corpus are used to understand language. When the system analyzes new information, it tries to rebuild it using pieces from the existing database. Clarifying meaning happens as a natural outcome. DOP can be carried out using standard parsing methods. In this study, we examine very strong relationships between other types of statistical language structures. We demonstrate that standard context-free parsing methods can be used to create a collection of possible interpretations, known as a parse forest, for a sentence in DOP1.	finding methods
85	Surface Grammatical Analysis For The Extraction Of Terminological Noun Phrases LEXTER is a software package for extracting terminology. A corpus of French language texts on any subject field is fed in, and LEXTER produces a list of likely terminological units to be submitted to an expert to be validated. To identify the terminological units, LEXTER takes their form into account and proceeds in two main stages : analysis, parsing. In the first stage, LEXTER uses a base of rules designed to identify frontier markers in view to analysing the texts and extracting maximal-length noun phrases. In the second stage, LEXTER parses these maximal-length noun phrases to extract subgroups which by virtue of their grammatical structure and their place in the maximal-length noun phrases are likely to be terminological units. In this article, the type of analysis used (surface grammatical analysis) is highlighted, as the methodological approach adopted to adapt the rules (experimental approach). We present a surface-syntactic analyser that extracts maximal length noun phrases mainly sequences of determiners, premodifiers, nominal heads, and certain kinds of post modifying prepositional phrases and adjectives from French texts for terminology applications. Our method relies purely on linguistic information, namely morpho-syntactic features of term candidates.	Surface Grammatical Analysis For The Extraction Of Terminological Noun Phrases LEXTER is a computer program used to find technical terms. It takes a collection of French texts on any topic and creates a list of possible technical terms for an expert to check. LEXTER identifies these terms by looking at their structure in two main steps: analysis and parsing. In the first step, LEXTER uses a set of rules to find boundary markers in the text to analyze and pull out the longest possible noun groups. In the second step, LEXTER breaks down these long noun groups to find smaller parts that could be technical terms based on their grammar and position within the phrase. This article focuses on the type of analysis used, called surface grammatical analysis, and the experimental methods used to adapt the rules. We introduce a tool that finds the longest noun groups, which are mainly sequences of words like determiners (words like "the" or "a"), premodifiers (descriptive words before the noun), main nouns, and certain types of phrases or adjectives that come after the noun, from French texts for use in finding technical terms. Our approach depends entirely on language details, especially the grammatical characteristics of potential terms.	methods
86	Part-Of-Speech Tagging With Neural Networks Text corpora which are tagged with part-of-speech in- formation are useful in many areas of linguistic research. In this paper, a new part-of-speech tagging method based on neural networks (Net-Tagger) is presented and its performance is compared to that of a HMM-tagger (Cutting et al. , 1992) and a trigram-based tagger (Kempe, 1993). It is shown that the Net-Tagger performs as well as the trigram-based tagger and better than the HMM-tagger. The correct rate of tagging has reached 95%, in part by using a very large amount of training data.	Part-Of-Speech Tagging With Neural Networks Text collections labeled with part-of-speech information are helpful in many areas of language research. In this paper, a new part-of-speech tagging method using neural networks (Net-Tagger) is introduced and its performance is compared to that of an HMM-tagger (a previous method from Cutting et al., 1992) and a trigram-based tagger (another method from Kempe, 1993). It is shown that the Net-Tagger works as well as the trigram-based tagger and better than the HMM-tagger. The accuracy of tagging has reached 95%, partly by using a very large amount of practice data.	adjectives
87	A Stochastic Japanese Morphological Analyzer Using A Forward-DP Backward-A* N-Best Search Algorithm We present a novel method for segmenting the input sentence into words and assigning parts of speech to the words. It consists of a statistical language model and an efficient two-pass N-best search algorithm. The algorithm does not require delimiters between words. Thus it is suitable for written Japanese. The proposed Japanese morphological analyzer achieved 95.l% recall and 94.6% precision for open text when it was trained and tested on the ATR Corpus. We propose a method to search for the N best sets.	A Stochastic Japanese Morphological Analyzer Using A Forward-DP Backward-A* N-Best Search Algorithm We introduce a new way to break down a sentence into words and label the words with their parts of speech (like nouns or verbs). It uses a statistical model of language and a fast two-step search method to find the best word options. This method doesn't need spaces between words, making it perfect for written Japanese. Our Japanese word analyzer correctly found 95.1% of the words it looked for and was accurate 94.6% of the time when tested on the ATR Corpus, a collection of texts. We suggest a way to find the top N best groups of words.	Tagging
88	Comlex Syntax: Building A Computational Lexicon We describe the design of Complex Syntax, a computational lexicon providing detailed syntactic information for approximately 38,000 English headwords. We consider the types of errors which arise in creating such a lexicon, and how such errors can be measured and controlled. Our COMLEX syntax dictionary provides verb subategorization information and syntactic paraphrases, but they are indexed by words thus not suitable to use in generation directly.	Comlex Syntax: Building A Computational Lexicon We explain how we created Complex Syntax, a computer-based dictionary that gives detailed grammar information for about 38,000 main English words. We look at the kinds of mistakes that can happen when making this dictionary, and how we can find and fix these mistakes. Our COMLEX syntax dictionary includes details about how verbs are used and different ways to say things in sentences, but because they are organized by words, it is not easy to use them for creating new sentences directly.	perfect
89	PRINCIPAR - An Efficient Broad-Coverage Principle-Based Parser We present an efficient, broad-coverage, principle-based parser for English. The parser has been implemented in C++ and runs on SUN Sparcstations with X-windows. It contains a lexicon with over 90,000 entries, constructed automatically by applying a set of extraction and conversion rules to entries from machine readable dictionaries. We release MiniPar, a fast and robust parser for grammatical dependency relations.	PRINCIPAR - An Efficient Broad-Coverage Principle-Based Parser We introduce an efficient English language parser that uses broad rules to understand sentences. The parser is built using C++ and works on SUN Sparcstations with a graphical interface called X-windows. It includes a word list (lexicon) with over 90,000 entries, created automatically by using rules to extract and convert data from digital dictionaries. We also offer MiniPar, a quick and reliable tool for analyzing grammatical connections between words.	COMLEX syntax
90	Recognizing Text Genres With Simple Metrics Using Discriminant Analysis A simple method for categorizing texts into pre-determined text genre categories using the statistical standard technique of discriminant analysis is demonstrated with application to the Brown corpus. Discriminant analysis makes it possible to use a large number of parameters taht may be specific for a certain corpus or information streatm, and combine them into a small number of function, with the parameters weighted on bais of how useful they are for discriminating text genres. An application to information retrieval is discussed. We word length as an indicator of formality for applications such as genre classification.	Recognizing Text Genres With Simple Metrics Using Discriminant Analysis A simple method for sorting texts into set categories based on their type or style is shown using a statistical method called discriminant analysis, applied to a collection of texts known as the Brown corpus. Discriminant analysis helps to take many different features that might be unique to a specific collection of texts or information and turn them into a few important calculations, giving more importance to features that are better at identifying text types. This method is also talked about in relation to finding information. We use word length as a sign of how formal something is for tasks like identifying text types.	language parser
91	K-Vec: A New Approach For Aligning Parallel Texts Various methods have been proposed for aligning texts in two or more languages such as the Canadian Parliamentary Debates (Hansards). Some of these methods generate a bilingual lexicon as a by-product. We present an alternative alignment strategy which we call K-vec, that starts by estimating the lexicon. For example, it discovers that the English word fisheries is similar to the French peches by noting that the distribution of fisheries in the English text is similar to the distribution of peches in the French. K-vec does not depend on sentence boundaries.	K-Vec: A New Approach For Aligning Parallel Texts Various methods have been proposed for aligning texts in two or more languages like the Canadian Parliamentary Debates (Hansards). Some of these methods create a list of words with their translations as a side effect. We introduce a different way to align texts called K-vec, which begins by figuring out this list of translations. For instance, it finds that the English word "fisheries" matches the French word "peches" by observing that "fisheries" appears in similar ways in English as "peches" does in French. K-vec does not rely on sentence breaks.	something
92	A Rule-Based Approach To Prepositional Phrase Attachment Disambiguation In this paper, we describe a new corpus-based approach to prepositional phrase attachment disambiguation, and present results comparing performance of this algorithm with other corpus-based approaches to this problem. We train a transformation-based learning algorithm on 12,766 quadruples from WSJ. We use the supervised transformation-based learning method and lexical and conceptual classes derived from WordNet, achieving 82% precision on 500 randomly selected examples.	A Rule-Based Approach To Prepositional Phrase Attachment Disambiguation In this paper, we describe a new method based on a collection of texts (corpus) to clarify where prepositional phrases belong in a sentence, and show how well this method works compared to other methods based on text collections for this issue. We train a learning algorithm on 12,766 sets of four related items from the Wall Street Journal (WSJ). We use a supervised learning method, which means it learns from examples, and we use word types and meanings from WordNet (a large dictionary), achieving 82% accuracy on 500 randomly chosen examples.	sentence
93	Word Sense Disambiguation Using Conceptual Density This paper presents a method for the resolution of lexical ambiguity of nouns and its automatic evaluation over the Brown Corpus. The method relies on the use oil' the wide-coverage noun taxonomy of WordNet and the notion of conceptual distance among concepts, captured by a Conceptual Density formula developed for this purpose. This fully automatic method requires no hand coding of lexical entries, hand tagging of text nor any kind of training process. The results of the experiments have been automatically evaluated against SemCor, the sense-tagged version of the Brown Corpus. Our Conceptual Density (CD) is a flexible semantic similarity which depends on the generalizations of word senses not referring to any fixed level of the hierarchy. We use a conceptual distance formula that was created to by sensitive to the length of the shortest path that connects the concepts involved, the depth of the hierarchy and the density of concepts in the hierarchy. To overcome the problem of varying link distances, we propose a semantic similarity measure (referred to conceptual density) which is sensitive to i) the length of the path, ii) the depth of the nodes in the hierarchy (deeper nodes are ranked closer) and iii) the density of nodes in the sub hierarchies (concepts involved in a denser sub hierarchy are ranked closer than those in a more sparse region).	Word Sense Disambiguation Using Conceptual Density This paper presents a way to solve the confusion of meaning for nouns and tests it automatically using the Brown Corpus, a large collection of texts. The method uses WordNet, a database that organizes nouns in a hierarchy, and measures how close concepts are to each other with a special formula called Conceptual Density. This automatic method doesn't need any manual input of words, tagging of text, or training process. The test results are automatically compared with SemCor, a version of Brown Corpus where meanings are already tagged. Our Conceptual Density (CD) is a flexible way to see how similar meanings are, without sticking to one level of the hierarchy. We use a formula that measures how close concepts are based on the shortest path between them, how deep they are in the hierarchy, and how many concepts are around them. To handle different distances between links, we suggest a measure of similarity (called conceptual density) that considers i) the path length, ii) how deep the points are in the hierarchy (deeper points are closer), and iii) how crowded the concepts are in their groups (more crowded groups are closer than less crowded ones).	other methods
94	Anaphora For Everyone: Pronominal Anaphora Resolution Without A Parser We present an algorithm for anaphora resolution which is a modified and extended version of that developed by (Lappin and Leass, 1994). In contrast to that work, our algorithm does not require in-depth, full, syntactic parsing of text. Instead, with minimal compromise in output quality, the modifications enable the resolution process to work from tile output of a part of speech tagger, enriched only with annotations of grammatical function of lexical items in the in-put text stream. Evaluation of the results of our implementation demonstrates that accurate anaphora resolution can be realized within natural language processing frameworks which do not -- cannot -- employ robust and reliable parsing components. We also suggest that anaphora resolution is part of the discourse referents resolution.	Anaphora For Everyone: Pronominal Anaphora Resolution Without A Parser We present a method for figuring out what pronouns refer to (anaphora resolution) which is an improved version of the one developed by (Lappin and Leass, 1994). Unlike that earlier work, our method doesn't need a detailed analysis of the sentence structure (full syntactic parsing). Instead, with only a small drop in result quality, our method can work using just the basic information about parts of speech (like nouns, verbs, etc.) and some extra notes on the role each word plays in sentences. Tests show that our method can accurately resolve what pronouns refer to even in systems that don't use strong and reliable sentence structure analysis tools. We also suggest that resolving pronouns is part of figuring out what words in a conversation refer to.	suggest
95	Role Of Word Sense Disambiguation In Lexical Acquisition: Predicting Semantics From Syntactic Cues This paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the construction of large-scale knowledge sources. We describe two experiments: one which ignored word-sense distinctions, resulting in 6.3% accuracy for semantic classification of verbs based on (Levin, 1993); and one which exploited word-sense distinctions, resulting in 97.9% accuracy. These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that verb semantics and syntactic behavior are predictably related; (2) to demonstrate that a 15-fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses. Finally, we show that we can provide effective acquisition techniques for novel word senses using a combination of online sources. We show that if we were given the perfect knowledge of the possible syntactic frames, verbs can be classified into the correct classes almost perfectly.	Role Of Word Sense Disambiguation In Lexical Acquisition: Predicting Semantics From Syntactic Cues This paper talks about the problem of words having multiple meanings when gathering information from computerized resources to build large knowledge bases. We explain two experiments: one where we didn't consider different meanings of words, leading to only 6.3% accuracy in sorting verbs by meaning based on (Levin, 1993); and another where we considered different word meanings, achieving 97.9% accuracy. These experiments had two goals: (1) to confirm the main idea from (Levin, 1993) that the meaning of verbs and how they are used in sentences are predictably connected; (2) to show that by separating sentence patterns into groups that match different word meanings, we can improve our understanding of word meanings by 15 times. Finally, we demonstrate that we can effectively learn new word meanings by using a mix of online resources. We show that with perfect knowledge of sentence structures, verbs can be sorted into the right categories almost flawlessly.	analysis
96	Three New Probabilistic Models For Dependency Parsing: An Exploration After presenting a novel O(n3) parsing algorithm for dependency grammar, we develop three contrasting ways to stochasticize it. We propose (a) a lexical affinity model where words struggle to modify each other, (b) a sense tagging model where words fluctuate randomly in their selectional preferences, and (e) a generative model where the speaker fleshes out each word's syntactic and conceptual structure without regard to the implications for the hearer. We also give preliminary empirical results from evaluating the three models' parsing performance on annotated Wall Street Journal training text (derived from the Penn Treebank). In these results, the generative model performs significantly better than the others, and does about equally well at assigning part-of-speech tags. The proposed parsing algorithm is sufficient for searching over all projective trees in O (n3) time.	Three New Probabilistic Models For Dependency Parsing: An Exploration After introducing a new O(n3) parsing algorithm for dependency grammar, we explore three different ways to make it random and unpredictable. We suggest (a) a lexical affinity model where words try to connect with each other, (b) a sense tagging model where words change unpredictably in their preferences, and (e) a generative model where the speaker develops each word's grammatical and meaning structure without thinking about how the listener will understand it. We also provide early test results from checking how well the three models work on parsing using annotated Wall Street Journal training text (from the Penn Treebank). In these tests, the generative model does much better than the others and performs about the same in assigning part-of-speech tags. The suggested parsing algorithm is capable of searching through all projective trees in O(n3) time.	knowledge
97	Message Understanding Conference-6: A Brief History We have recently completed the sixth in a series of "Message Understanding Conferences" which are designed to promote and evaluate research in information extraction. MUC-6 introduced several innovations over prior MUCs, most notably in the range of different tasks for which evaluations were conducted. We describe some of the motivations for the new format and briefly discuss some of the results of the evaluations. We demostrate that grammar-based IE systems can be effective in many scenarios. We introduce name recognition and classification tasks.	Message Understanding Conference-6: A Brief History We have recently finished the sixth "Message Understanding Conference," which aims to support and assess research in pulling information from texts. MUC-6 brought in new ideas compared to earlier conferences, especially with different types of tasks being evaluated. We explain why we changed the format and talk a little about the evaluation results. We show that systems based on language rules for information extraction (IE) can work well in many situations. We also introduce tasks for recognizing and sorting names.	parsing
98	HMM-Based Word Alignment In Statistical Translation In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora. We use a useful feature that assesses the goodness of the alignment path through the source sentence.	HMM-Based Word Alignment In Statistical Translation In this paper, we explain a new way to match words in statistical translation and share test results. The main idea is to base the matching chances on how word positions differ, not where they are exactly. To do this, we use a simple Hidden Markov model (HMM), which is a statistical model, similar to how it's used in speech recognition to match timing. Unlike speech models, this one doesn't require words to stay in order. We explain how the model works and test it on various sets of bilingual texts. We use a helpful feature that checks how well the words match in the original sentence.	Conference
99	Motivations And Methods For Text Simplification Long and complicated sentences prove to be a stumbling block for current systems relying on NL input. These systems stand to gain from methods that syntactically simplify such sentences. To simplify a sentence, we need an idea of the structure of the sentence, to identify the components to be separated out. Obviously a parser could be used to obtain the complete structure of the sentence. However, full parsing is slow prone to failure, especially on complex sentences. In this paper, we consider two alternatives to full parsing which could be used for simplification. The first approach uses a Finite State Grammar (FSG) to produce noun and verb groups while the second uses a Supertagging model to produce dependency linkages. We discuss the impact of these two input representations on the sim-plification process. We introduce a two stage process, first transforming from sentence to syntactic tree, then from syntactic tree to new sentence. Our text simplification techniques deal not only with helping readers with reading disabilities, but also to help NLP systems as a preprocessing tool.	Motivations And Methods For Text Simplification Long and complicated sentences can be difficult for current systems that rely on natural language (NL) input. These systems can benefit from methods that make such sentences simpler. To make a sentence simpler, we need to understand its structure to identify parts that can be separated. A parser, which analyzes sentence structure, could be used for this purpose. However, full parsing can be slow and may not work well with complex sentences. In this paper, we look at two alternatives to full parsing for simplification. The first method uses a Finite State Grammar (FSG) to create groups of nouns and verbs, while the second uses a Supertagging model to show how words depend on each other. We explore how these two ways of organizing input affect the simplification process. We introduce a two-step process: first changing a sentence into a syntactic tree (a diagram showing structure), then changing this tree into a new, simpler sentence. Our text simplification techniques are designed not only to help people with reading difficulties but also to assist NLP systems as a tool for preparing data.	speech recognition
100	Using Semantic Roles to Improve Question Answering Shallow semantic parsing, the automatic identification and labeling of sentential constituents, has recently received much attention. Our work examines whether semantic role information is beneficial to question answering. We introduce a general framework for answer extraction which exploits semantic role annotations in the FrameNet paradigm. We view semantic role assignment as an optimization problem in a bipartite graph and answer extraction as an instance of graph matching. Experimental results on the TREC datasets demonstrate improvements over state-of-the-art models. We show that shallow semantic information in the form of predicate argument structures (PASs) improves the automatic detection of correct answers to a target question. We also point out that the low coverage of the current version of FrameNet significantly limits the expected boost in performance.	Using Semantic Roles to Improve Question Answering Shallow semantic parsing, which is the automatic process of identifying and labeling parts of a sentence, has recently gained a lot of interest. Our study looks at whether using information about semantic roles (the parts words play in a sentence) can help improve question answering. We present a general method for finding answers that takes advantage of semantic role annotations, following the FrameNet model. We treat assigning semantic roles as a problem of finding the best solution in a two-part graph, and we see finding answers as a kind of matching in this graph. Tests on the TREC datasets show improvements compared to the best models available. We demonstrate that using basic semantic information in the form of predicate argument structures (PASs), which are relationships between verbs and their related words, helps automatically find the right answers to a given question. However, we also note that the current version of FrameNet doesn’t cover enough examples, which limits the potential improvement in performance.	diagram showing
101	What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA This paper presents a syntax-driven approach to question answering, specifically the answer-sentence selection problem for short-answer questions. Rather than using syntactic features to augment existing statistical classifiers (as in previous work), we build on the idea that questions and their (correct) answers relate to each other via loose but predictable syntactic transformations. We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust nonlexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model. Our model learns soft alignments as a hidden variable in discriminative training. Experimental results using the TREC dataset are shown to significantly outperform strong state-of-the-art baselines. We explore the use a formalism called quasi synchronous grammar (Smith and Eisner, 2006) in order to find a more explicit model for matching the set of dependencies, and yet still allow for looseness in the matching. We use quasi-synchronous translation to map all parent-child paths in a question to any path in an answer.	What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA This paper introduces a method based on sentence structure to help answer questions, focusing on choosing the right sentence for short answers. Instead of adding sentence structure features to existing statistical models (as done before), we work with the idea that questions and their correct answers have a pattern of predictable changes in structure. We suggest using a probabilistic grammar, which is a structured set of rules, inspired by one used for translating languages (by D. Smith and Eisner, 2006). This grammar is adjusted using a mix of a strong model that doesn't rely on word meanings with an optional model influenced by word meanings. Our model learns flexible connections as an unseen factor during specific training. Tests using the TREC dataset show it performs much better than other advanced methods. We look into using a system called quasi-synchronous grammar (by Smith and Eisner, 2006) to create a clearer model for matching related parts of sentences, while still allowing for some flexibility. We use this grammar to connect all related paths between a question and an answer.	answers
102	Improving Statistical Machine Translation Using Word Sense Disambiguation We show for the first time that incorporating the predictions of a word sense disambiguation system within a typical phrase-based statistical machine translation (SMT) model consistently improves translation quality across all three different IWSLT Chinese-English test sets, as well as producing statistically significant improvements on the larger NIST Chinese-English MT task - and moreover never hurts performance on any test set, according not only to BLEU but to all eight most commonly used automatic evaluation metrics. Recent work has challenged the assumption that word sense disambiguation (WSD) systems are useful for SMT. Yet SMT translation quality still obviously suffers from inaccurate lexical choice. In this paper, we address this problem by investigating a new strategy for integrating WSD into an SMT system, that performs fully phrasal multi-word disambiguation. Instead of directly incorporating a Senseval-style WSD system, we redefine the WSD task to match the exact same phrasal translation disambiguation task faced by phrase-based SMT systems. Our results provide the first known empirical evidence that lexical semantics are indeed useful for SMT, despite claims to the contrary. We provide a machine translation system with the WSD probabilities for a phrase translation as extra features in a log-linear model. We use rich context features based on position, syntax and local collocations to dynamically adapt the lexicons for each sentence and facilitate the choice of longer phrases. We use a state-of-the-art WSD engine (a combination of naive Bayes, maximum entropy, boosting and Kernel PCA models) to dynamically determine the score of a phrase pair under consideration and, thus, let the phrase selection adapt to the context of the sentence.	Improving Statistical Machine Translation Using Word Sense Disambiguation We demonstrate for the first time that adding predictions from a word sense disambiguation (WSD) system to a typical phrase-based statistical machine translation (SMT) model consistently makes the translation better across three different IWSLT Chinese-English test sets and also shows notable improvements on the larger NIST Chinese-English machine translation task. Importantly, it never harms performance on any test set, according to the BLEU score and all other eight commonly used automatic evaluation measures. Recent studies have questioned whether WSD systems are helpful for SMT. However, SMT still struggles with choosing the right words. In this paper, we tackle this issue by exploring a new way to combine WSD with an SMT system that handles phrases and multiple words together. Instead of using a traditional WSD system, we redefine the WSD task to suit the same phrase translation challenge faced by phrase-based SMT systems. Our findings provide the first known evidence that understanding word meanings is indeed helpful for SMT, even though some have argued otherwise. We give a machine translation system the WSD probabilities for a phrase translation as additional features in a log-linear model. We use detailed context features based on position, sentence structure, and nearby words to adjust the word choices for each sentence and help choose longer phrases. We use a top-notch WSD tool (which combines naive Bayes, maximum entropy, boosting, and Kernel PCA models) to determine the score of a phrase pair in context, allowing the phrase choice to fit the sentence more effectively.	choosing
103	Why Doesn't EM Find Good HMM POS-Taggers? This paper investigates why the HMMs estimated by Expectation-Maximization (EM) produce such poor results as Part-of-Speech (POS) taggers. We find that the HMMs estimated by EM generally assign a roughly equal number of word tokens to each hidden state, while the empirical distribution of tokens to POS tags is highly skewed. This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution. We investigate Gibbs Sampling (GS) and Variational Bayes (VB) estimators and show that VB converges faster than GS for this task and that VB significantly improves 1-to-1 tagging accuracy over EM. We also show that EM does nearly as well as VB when the number of hidden HMM states is dramatically reduced. We also point out the high variance in all of these estimators, and that they require many more iterations to approach convergence than usually thought. we demonstrate good performance on unsupervised part-of-speech tagging (using a dictionary) with a Bayesian model.	Why Doesn't EM Find Good HMM POS-Taggers? This paper looks into why Hidden Markov Models (HMMs) estimated by Expectation-Maximization (EM) perform poorly as Part-of-Speech (POS) taggers. We find that HMMs estimated by EM tend to assign a similar number of word tokens (units of text) to each hidden state, while in reality, the distribution of tokens to POS tags is very uneven. This suggests using a Bayesian approach with a sparse prior (a method that favors certain distributions) to guide the estimation towards this uneven distribution. We explore Gibbs Sampling (GS) and Variational Bayes (VB) methods and show that VB reaches results faster than GS for this task and that VB greatly improves the accuracy of matching words to their correct POS tags compared to EM. We also show that EM performs almost as well as VB when the number of hidden HMM states is greatly reduced. Additionally, we note the high variability in the results of all these methods, and that they need more iterations (repetitions of the process) to get close to stable results than commonly expected. We demonstrate effective performance on unsupervised POS tagging (without labeled data, using a dictionary) using a Bayesian model.	notable
104	V-Measure: A Conditional Entropy-Based External Cluster Evaluation Measure We present V-measure, an external entropy-based cluster evaluation measure. V-measure provides an elegant solution to many problems that affect previously defined cluster evaluation measures including 1) dependence on clustering algorithm or data set, 2) the problem of matching, where the clustering of only a portion of data points are evaluated and 3) accurate evaluation and combination of two desirable aspects of clustering, homogeneity and completeness. We compare V-measure to a number of popular cluster evaluation measures and demonstrate that it satisfies several desirable properties of clustering solutions, using simulated clustering results. Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering. F score is not suitable for comparing results with different cluster numbers. The V-measure (VM) is an information theoretic metric that reports the harmonic mean of homogeneity (each cluster should contain only instances of a single class) and completeness (all instances of a class should be members of the same cluster). A significant limitation of F-Score is that it does not evaluate the make up of clusters beyond the majority class.	V-Measure: A Conditional Entropy-Based External Cluster Evaluation Measure We introduce V-measure, a method to evaluate groupings of data using a concept called entropy, which is about measuring disorder. V-measure effectively addresses several issues found in older methods for judging data groupings, such as 1) relying too much on the specific method or data used to create groups, 2) only checking a part of the data, and 3) accurately assessing and balancing two important features of data grouping: homogeneity (each group should have items from only one category) and completeness (all items of a category should be in the same group). We compare V-measure with other well-known evaluation methods and show that it has several good qualities for judging how well data is grouped, using test data. Lastly, we use V-measure to evaluate two grouping tasks: organizing documents and organizing speech patterns. The F score is not good for comparing results when the number of groups differs. The V-measure is a way to measure how well data is grouped by calculating the average of homogeneity and completeness. A major drawback of the F-Score is that it doesn't look at the content of groups beyond the most common category.	greatly improves
105	Lexical Semantic Relatedness with Random Graph Walks Many systems for tasks such as question answering, multi-document summarization, and information retrieval need robust numerical measures of lexical relatedness. Standard thesaurus-based measures of word pair similarity are based on only a single path between those words in the thesaurus graph. By contrast, we propose a new model of lexical semantic relatedness that incorporates information from every explicit or implicit path connecting the two words in the entire graph. Our model uses a random walk over nodes and edges derived from WordNet links and corpus statistics. We treat the graph as a Markov chain and compute a word-specific stationary distribution via a generalized PageRank algorithm. Semantic relatedness of a word pair is scored by a novel divergence measure, ZKL, that outperforms existing measures on certain classes of distributions. In our experiments, the resulting relatedness measure is the WordNet-based measure most highly correlated with human similarity judgments by rank ordering at ? =.90. we use random walks over WordNet, incorporating information such as meronymy and dictionary glosses.	Lexical Semantic Relatedness with Random Graph Walks Many systems, like those used for answering questions, summarizing multiple documents, and retrieving information, need strong ways to measure how related words are. Traditional methods that use a thesaurus to measure how similar word pairs are only look at one direct connection between the words in the thesaurus. In contrast, we suggest a new model that looks at all possible direct and indirect connections between two words in a whole network. Our model uses a random walk, which is like taking a random path, through nodes and links from WordNet (a large database of words) and statistics from texts. We treat this network like a Markov chain, which is a type of mathematical model, and calculate a specific distribution of results for each word using a modified PageRank algorithm (a method originally used to rank web pages). We rate how related a pair of words is using a new method called ZKL, which is better than current methods for some types of data. In our tests, this new way of measuring word relatedness aligns most closely with how humans judge word similarity, with a high accuracy of 90%. We use random walks over WordNet, which includes information like parts of a whole and dictionary explanations.	older methods
106	Online Learning of Relaxed CCG Grammars for Parsing to Logical Form We consider the problem of learning to parse sentences to lambda-calculus representations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG). A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar - for example allowing flexible word order, or insertion of lexical items - with learned costs. We also present a new, online algorithm for inducing a weighted CCG. Results for the approach on ATIS data show 86% F-measure in recovering fully correct semantic analyses and 95.9% F-measure by a partial-match criterion, a more than 5% improvement over the 90.3% partial-match figure reported by He and Young (2006). We develop ATIS dataset for semantic parsing. We develop a set which includes features that are sensitive to lexical choices and the structure of the logical form that is constructed. We introduce the standard application, composition and coordination combinators, as well as type-shifting rules to model spontaneous, unedited text.	Online Learning of Relaxed CCG Grammars for Parsing to Logical Form We look at the challenge of teaching a computer to break down sentences into lambda-calculus, which is a way of representing their basic meanings, and we introduce a method that learns a type of grammar called weighted combinatory categorial grammar (CCG). A central idea is to add new, non-traditional CCG combinators that make parts of the grammar more flexible, such as allowing words to be in different orders or adding words, with these changes having learned costs. We also share a new method that updates the grammar as it learns, called an online algorithm, for creating a weighted CCG. When tested on ATIS data, our approach showed an 86% success rate in getting the exact correct meaning and a 95.9% success rate when a partial match was acceptable, which is over 5% better than a previous result of 90.3% reported by researchers He and Young in 2006. We create the ATIS dataset for teaching computers how to understand sentence meanings. This dataset includes features that pay attention to word choices and the structure of the logical meaning that is built. We include the usual methods for combining grammar parts, such as application, composition, and coordination, along with rules that help handle natural, unpolished text.	network
124	Dependency Parsing by Belief Propagation We formulate dependency parsing as a graphical model with the novel ingredient of global constraints. We show how to apply loopy belief propagation (BP), a simple and effective tool for approximate learning and inference. As a parsing algorithm, BP is both asymptotically and empirically efficient. Even with second-order features or latent variables, which would make exact parsing considerably slower or NP-hard, BP needs only O(n3) time with a small constant factor. Furthermore, such features significantly improve parse accuracy over exact first-order methods. Incorporating additional features would increase the runtime additively rather than multiplicatively. We can encapsulate common dynamic programming algorithms within special-purpose factors to efficiently globally constrain variable configurations. DEP-TREE is a global combinatorial factor which attaches to all Link (i, j) variables and similarly contributes a factor of 1 iff the configuration of Link variables forms a valid projective dependency graph.	Dependency Parsing by Belief Propagation We describe dependency parsing (finding relationships between words in a sentence) as a graphical model with a new feature of overall rules. We demonstrate how to use loopy belief propagation (BP), a straightforward and effective method for approximate learning and reasoning. As a parsing method, BP is both theoretically and practically fast. Even with more complex features or hidden elements, which would make exact parsing much slower or extremely difficult (NP-hard), BP only takes O(n3) time with a small added constant. Additionally, these features greatly improve accuracy compared to simpler first-order methods. Adding more features would only slightly increase the time it takes, rather than significantly. We can integrate common problem-solving algorithms within special-purpose factors to effectively control all variable setups. DEP-TREE is a global factor that connects to all Link (i, j) variables and contributes a factor of 1 only if the setup of Link variables creates a valid projective dependency graph.	Although research
107	The Infinite PCFG Using Hierarchical Dirichlet Processes We present a nonparametric Bayesian model of tree structures based on the hierarchical Dirichlet process (HDP). Our HDP-PCFG model allows the complexity of the grammar to grow as more training data is available. In addition to presenting a fully Bayesian model for the PCFG, we also develop an efficient variational inference procedure. On synthetic data, we recover the correct grammar without having to specify its complexity in advance. We also show that our techniques can be applied to full-scale parsing applications by demonstrating its effectiveness in learning state-split grammars. We find that because the latent variable grammars are not explicitly regularized, EM keeps fitting the training data and eventually begins over fitting.	The Infinite PCFG Using Hierarchical Dirichlet Processes We present a flexible Bayesian model of tree structures based on a method called the hierarchical Dirichlet process (HDP). Our HDP-PCFG model allows the grammar's complexity to increase as more training data becomes available. Besides introducing a complete Bayesian model for the PCFG, we also create an efficient method for estimating the model's parameters. With simulated data, we accurately identify the correct grammar without needing to decide its complexity beforehand. We also show that our methods work well for large-scale parsing applications by proving their effectiveness in learning detailed grammars. We find that because the hidden variable grammars are not clearly simplified, the EM algorithm (a method for finding parameters) keeps adjusting to fit the training data and eventually starts fitting too closely to the specific data.	weighted
108	Large-Scale Named Entity Disambiguation Based on Wikipedia Data This paper presents a large-scale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and Web search results. It describes in detail the disambiguation paradigm employed and the information extraction process from Wikipedia. Through a process of maximizing the agreement between the contextual information extracted from Wikipedia and the context of a document, as well as the agreement among the category tags associated with the candidate entities, the implemented system shows high disambiguation accuracy on both news stories and Wikipedia articles. We find that topical coherence between a candidate entity and other entities in the context will improve NED accuracy. We employ context vectors consisting of phrases and categories extracted from Wikipedia. We introduce an entity disambiguation data set. We make use of the explicit category information found within Wikipedia. For evaluation, we use 20 news stories from MSNBC with 642 entity mentions manually linked to Wikipedia and another 113 mentions not having any corresponding link to Wikipedia.	Large-Scale Named Entity Disambiguation Based on Wikipedia Data This paper introduces a big system that identifies and clarifies the meaning of names using information from a large online encyclopedia and web search results. It explains the method used to clarify the names and how information is taken from Wikipedia. By making sure the information from Wikipedia matches the information in a document and checking if the category tags for possible names agree, the system works accurately for both news stories and Wikipedia articles. We find that keeping the topic consistent between a possible name and other names in the text helps improve accuracy. We use context clues made of phrases and categories from Wikipedia. We introduce a set of data for checking names. We use the clear category information available in Wikipedia. For testing, we use 20 news stories from MSNBC with 642 names manually linked to Wikipedia and another 113 names without a matching Wikipedia link.	keeps adjusting
109	Tree Kernel-Based Relation Extraction with Context-Sensitive Structured Parse Tree Information This paper proposes a tree kernel with context-sensitive structured parse tree information for relation extraction. It resolves two critical problems in previous tree kernels for relation extraction in two ways. First, it automatically determines a dynamic context-sensitive tree span for relation extraction by extending the widely-used Shortest Path-enclosed Tree (SPT) to include necessary context information outside SPT. Second, it proposes a context-sensitive convolution tree kernel, which enumerates both context-free and context-sensitive sub-trees by considering their ancestor node paths as their contexts. Moreover, this paper evaluates the complementary nature between our tree kernel and a state-of-the-art linear kernel. Evaluation on the ACE RDC corpora shows that our dynamic context-sensitive tree span is much more suitable for relation extraction than SPT and our tree kernel outperforms the state-of-the-art Collins and Duffy's convolution tree kernel. It also shows that our tree kernel achieves much better performance than the state-of-the-art linear kernels. Finally, it shows that feature-based and tree kernel-based methods much complement each other and the composite kernel can well integrate both flat and structured features. Our composite kernel depends partially on a full parse, and partially on a collection of shallow syntactic features.	Tree Kernel-Based Relation Extraction with Context-Sensitive Structured Parse Tree Information This paper introduces a new method using tree structures to find relationships in data. It fixes two major issues with older methods in two ways. First, it automatically chooses the right part of the tree to look at by adding important information that was left out before. Second, it suggests a new way to look at tree parts by including the paths leading to them as important context. Additionally, the paper tests how well this new method works with another advanced method. Tests on specific data show that our new tree method is better at finding relationships than older methods and is also better than the best current method. It also shows that our method works much better than the best current simpler methods. Finally, it shows that using both our method and another method together works better because they help each other. Our combined method uses both detailed and simpler features from the data.	Wikipedia
110	Chinese Syntactic Reordering for Statistical Machine Translation Syntactic reordering approaches are an effective method for handling word-order differences between source and target languages in statistical machine translation (SMT) systems. This paper introduces a reordering approach for translation from Chinese to English. We describe a set of syntactic reordering rules that exploit systematic differences between Chinese and English word order. The resulting system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English in terms of their word order. We evaluated the reordering approach within the MOSES phrase-based SMT system (Koehn et al., 2007). The reordering approach improved the BLEU score for the MOSES system from 28.52 to 30.86 on the NIST 2006 evaluation data. We also conducted a series of experiments to analyze the accuracy and impact of different types of reordering rules. Our rule set substantially decreases the total times of rule application about 60%, compared with a constituent-based approach. Chinese ordering differs from English mainly in clause ordering.	Chinese Syntactic Reordering for Statistical Machine Translation Syntactic reordering approaches are a useful way to manage differences in word order (the sequence of words) between original and translated languages in statistical machine translation (SMT) systems. This paper presents a method for changing word order to help translate from Chinese to English. We explain a set of reordering rules that take advantage of the regular differences in how words are arranged in Chinese and English. The new system is used before processing both training and test sentences, changing Chinese sentences to be more like English ones in their word order. We tested this reordering method using the MOSES phrase-based SMT system (a specific translation system by Koehn et al., 2007). The reordering method increased the BLEU score (a metric for evaluating translation quality) for the MOSES system from 28.52 to 30.86 on the NIST 2006 evaluation data. We also did several tests to examine how accurate and effective different types of reordering rules are. Our set of rules significantly reduces the total number of times rules are applied by about 60%, compared to a method based on sentence parts. Chinese order is different from English mostly in how clauses (parts of sentences) are arranged.	because
111	Online Large-Margin Training for Statistical Machine Translation We achieved a state of the art performance in statistical machine translation by using a large number of features with an online large-margin training algorithm. The millions of parameters were tuned only on a small development set consisting of less than 1K sentences. Experiments on Arabic-to-English translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features. We perform BLUE computations in the context of a set O of previously-translated sentences. We find the possibility of overfitting in the dataset (Arabic-English newswire translation), especially when domain differences are present.	Online Large-Margin Training for Statistical Machine Translation We reached the best performance in statistical machine translation by using many features with an online large-margin training method. The millions of settings were adjusted using only a small test set with less than 1,000 sentences. Tests on translating from Arabic to English showed that a model trained with specific simple features did better than a regular SMT system with fewer features. We calculate BLUE scores, which measure translation quality, using a group of previously translated sentences. We noticed a risk of overfitting, which means the model might work well only on the test data, especially when there are differences in the topics of the text (like Arabic-English news articles).	approaches
112	Large Language Models in Machine Translation This paper reports on the benefits of large-scale statistical language modeling in machine translation. A distributed infrastructure is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams. It is capable of providing smoothed probabilities for fast, single-pass decoding. We introduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases. 5-gram word language models in English are trained on a variety of monolingual corpora. In the case of language models, we often have to remove low-frequency words because of a lack of computational resources, since the feature space of k grams tends to be so large that we sometimes need cutoffs even in a distributed environment. To scale LMs to larger corpora with higher-order dependencies we consider distributed language models that scale more readily. Stupid back off smoothing is significantly more efficient to train and deploy in a distributed framework than a context dependent smoothing scheme such as Kneser-Ney. We show that each doubling of the training data from the news domain (used to build the language model) leads to improvements of approximately 0.5 BLEU points. We used 1500 machines for a day to compute the relative frequencies of n-grams from 1.8TB of web data.	Large Language Models in Machine Translation This paper discusses how using large statistical language models can improve machine translation. We suggest a shared system to train on up to 2 trillion pieces of text, creating language models with up to 300 billion groups of words (n-grams). This system quickly gives smooth probabilities for translating text in one go. We introduce a new, cheaper smoothing method called Stupid Backoff, which works well on large data and gets close to the quality of a more complex method called Kneser-Ney Smoothing as more data is used. We train 5-word (5-gram) models in English using different single-language text collections. In language models, we often have to remove rare words due to limited computer power, as the space needed for features (k grams) is huge, sometimes requiring limits even when using a shared system. To handle larger text collections with complex connections, we use shared language models that can expand more easily. Stupid Backoff is much easier to train and use in a shared system than a more complex method like Kneser-Ney. We found that doubling the training data from news articles improves results by about 0.5 BLEU points, a measure of translation quality. We used 1500 computers for a day to calculate how often word groups appear in 1.8TB of web data.	Training
113	Factored Translation Models We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level - may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence. Any way to enforce linguistic constraints will result in a reduced need for data, and ultimately in more complete models, given the same amount of data. We also propose frameworks for the simultaneous use of different word-level representations. We propose a tight integration of morpho syntactic information into the translation model, where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. We generalise the phrase-based model's representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even to replace, surface forms during search. Factored translation models facilitate a more data-oriented approach to agreement modeling.	Factored Translation Models We introduce an improvement of phrase-based statistical machine translation models that allows for easy addition of extra information at the word level, like language labels or computer-generated word categories. In several tests, we demonstrate that using factored translation models improves translation quality, both in terms of automated scoring systems and in making sentences sound more naturally correct. Any method that applies language rules will need less data and will lead to more complete models with the same amount of information. We also suggest systems for using different word-level representations at the same time. We recommend closely incorporating detailed language information into the translation model, where the base form of a word and its grammar details are translated separately, and this information is combined to create the final translation. We expand the phrase-based model's way of representing a word from just a string of letters to a collection of features, allowing extra details like parts of speech and word structure to be linked with or to replace the word's basic form during the translation process. Factored translation models encourage a more data-focused method for handling word agreement, which is how words match in a sentence.	creating language
114	The CoNLL 2007 Shared Task on Dependency Parsing The Conference on Computational Natural Language Learning features a shared task, in which participants train and test their learning systems on the same data sets. In 2007, as in 2006, the shared task has been devoted to dependency parsing, this year with both a multilingual track and a domain adaptation track. In this paper, we define the tasks of the different tracks and describe how the data sets were created from existing treebanks for ten languages. In addition, we characterize the different approaches of the participating systems, report the test results, and provide a first analysis of these results. We note that languages with free word order and high morphological complexity are the most difficult for dependency parsing. Morphologically rich languages present new challenges, as the use of state of the art parsers for more configurational and non-inflected languages like English does not reach similar performance levels in languages like Basque, Greek or Turkish.	The CoNLL 2007 Shared Task on Dependency Parsing The Conference on Computational Natural Language Learning has a shared task where participants train and test their learning systems using the same data sets. In 2007, like in 2006, the shared task focused on dependency parsing, which is about understanding sentence structure. This year, it included a multilingual track (using multiple languages) and a domain adaptation track (adjusting to different topics). In this paper, we explain the different tasks and how the data sets were made from existing language databases called treebanks for ten languages. We also describe the different methods used by participants, show the test results, and give an initial look at these results. We find that languages with flexible word order and complex grammar are the hardest for dependency parsing. Languages with complex grammar bring new challenges because the top parsers (sentence analyzers) used for languages like English, which have a fixed word order and simpler grammar, do not perform as well in languages like Basque, Greek, or Turkish.	addition
115	Single Malt or Blended? A Study in Multilingual Parser Optimization We describe a two-stage optimization of the MaltParser system for the ten languages in the multilingual track of the CoNLL 2007 shared task on dependency parsing. The first stage consists in tuning a single-parser system for each language by optimizing parameters of the parsing algorithm, the feature model, and the learning algorithm. The second stage consists in building an ensemble system that combines six different parsing strategies, extrapolating from the optimal parameters settings for each language. When evaluated on the official test sets, the ensemble system significantly outperforms the single-parser system and achieves the highest average labeled attachment score. We extend the two-stage approach to a three-stage architecture where the parser and labeler generate an n-best list of parses which in turn is reranked. We point out that the official results for Chinese contained a bug, and the true performance of our system is actually much higher. We implement a left-to-right arc-eager parsing model in a way that the parser scan through an input sequence from left to right and the right dependents are attached to their heads as soon as possible.	Single Malt or Blended? A Study in Multilingual Parser Optimization We explain a two-step process to improve the MaltParser system for ten languages in the multilingual section of the CoNLL 2007 shared task on dependency parsing, which is a way to understand sentence structure. The first step is to adjust a single-parser system for each language by fine-tuning parts of the parsing method, the feature model (which determines what information the parser looks at), and the learning method. The second step is to create a combined system that uses six different parsing strategies, using the best settings for each language. When tested officially, this combined system does much better than the single-parser system and gets the highest average score for correct links between words. We expand this two-step method into a three-step process where the parser and labeler create a list of the best sentence structures, which is then sorted again to find the best one. We note that the official results for Chinese had an error, and our system actually performs much better than reported. We create a left-to-right arc-eager parsing model, which means the parser reads from left to right and quickly connects words to their main words as soon as possible.	grammar
116	Experiments with a Higher-Order Projective Dependency Parser We present experiments with a dependency parsing model defined on rich factors. Our model represents dependency trees with factors that include three types of relations between the tokens of a dependency and their children. We extend the projective parsing algorithm of Eisner (1996) for our case, and train models using the averaged perceptron. Our experiments show that considering higher-order information yields significant improvements in parsing accuracy, but comes at a high cost in terms of both time and memory consumption. In the multilingual exercise of the CoNLL-2007 shared task (Nivre et al., 2007), our system obtains the best accuracy for English, and the second best accuracies for Basque and Czech. We extend the first-order model to incorporate a sum over scores for pairs of adjacent arcs in the tree, yielding a second-order model. Our second-order models include head grandparent relations. Our second order algorithm uses the child of the dependent occurring in the sentence between the head and the dependent as well as the edge from the dependents to a grandchild. We introduce the left-most and right-most grandchild as factors.	Experiments with a Higher-Order Projective Dependency Parser We present experiments with a model that analyzes sentence structure using detailed elements. Our model represents sentence structures with elements that include three types of relationships between words and their related words. We improve upon an existing parsing method by Eisner (1996) to fit our needs and train models using a method called averaged perceptron. Our experiments show that using more detailed information greatly improves accuracy in understanding sentence structures, but it requires a lot of time and computer memory. In a multilingual competition (CoNLL-2007), our system achieved the highest accuracy for English, and the second highest for Basque and Czech. We enhance the basic model to include a combination of scores for pairs of connected word links, creating a more advanced model. Our advanced models include relationships involving the main word, its parent, and grandparent. Our advanced method considers the word between the main and related word, as well as the link from related words to their grandchild. We introduce the first and last grandchild as elements in our model.	eager parsing
117	Improving Translation Quality by Discarding Most of the Phrasetable It is possible to reduce the bulk of phrase-tables for Statistical Machine Translation using a technique based on the significance testing of phrase pair co-occurrence in the parallel corpus. The savings can be quite substantial (up to 90%) and cause no reduction in BLEU score. In some cases, an improvement in BLEU is obtained at the same time although the effect is less pronounced if state-of-the-art phrase table smoothing is employed. We use Fisher's exact test. We filter out statistically unreliable translation pairs.	Improving Translation Quality by Discarding Most of the Phrasetable It is possible to make phrase-tables, which are used in Statistical Machine Translation, much smaller by using a method that checks how often phrase pairs appear together in a text with two languages. This method can save a lot of space (up to 90%) and does not lower the BLEU score, which measures translation quality. Sometimes, it even improves the BLEU score, but this is less noticeable if advanced phrase table techniques are used. We use a method called Fisher's exact test to do this. We remove translation pairs that are not statistically reliable.	highest
118	Hierarchical Phrase-Based Translation with Suffix Arrays A major engineering challenge in statistical machine translation systems is the efficient representation of extremely large translation rulesets. In phrase-based models, this problem can be addressed by storing the training data in memory and using a suffix array as an efficient index to quickly lookup and extract rules on the fly. Hierarchical phrase-based translation introduces the added wrinkle of source phrases with gaps. Lookup algorithms used for contiguous phrases no longer apply and the best approximate pattern matching algorithms are much too slow, taking several minutes per sentence. We describe new lookup algorithms for hierarchical phrase-based translation that reduce the empirical computation time by nearly two orders of magnitude, making on-the-fly lookup feasible for source phrases with gaps. The basis of our method is to look for the occurrences of continuous substrings using a Suffix Array, and then intersect them to find the occurrences of discontinuous substrings.	Hierarchical Phrase-Based Translation with Suffix Arrays A big challenge in creating systems that automatically translate languages is finding a way to efficiently manage a very large set of translation rules. In phrase-based models, we can solve this problem by storing the learning data in memory and using a suffix array, which is a tool that helps quickly find and use these rules. Hierarchical phrase-based translation adds another layer of complexity because it deals with source phrases that have gaps. The usual methods for finding these phrases don't work, and the best available methods are too slow, taking several minutes for each sentence. We explain new methods for quickly finding phrases with gaps in hierarchical phrase-based translation, cutting down the time it takes by almost 100 times, making it possible to find phrases with gaps quickly. Our approach is based on looking for sequences of words that appear together using a suffix array and then combining these sequences to find phrases that don't appear together continuously.	which measures
119	A Topic Model for Word Sense Disambiguation We develop latent Dirichlet allocation with WORDNET (LDAWN), an unsupervised probabilistic topic model that includes word sense as a hidden variable. We develop a probabilistic posterior inference algorithm for simultaneously disambiguating a corpus and learning the domains in which to consider each word. Using the WORDNET hierarchy, we embed the construction of Abney and Light (1999) in the topic model and show that automatically learned domains improve WSD accuracy compared to alternative contexts. We use the document-level topics extracted with Latent Dirichlet Allocation (LDA) as indicators of meanings for word sense disambiguation. We describe a related topic model, LDAWN, for word sense disambiguation that adds an intermediate layer of latent variables Z on which the Markov model parameters are conditioned. We integrate semantics into the topic model framework.	A Topic Model for Word Sense Disambiguation We develop a version of a model called latent Dirichlet allocation with WORDNET (LDAWN), which is a system that helps understand word meanings without needing labeled examples, and treats word sense as something hidden. We create a method to help figure out meanings in a group of texts and learn which areas each word belongs in at the same time. By using the WORDNET structure, we incorporate the approach of Abney and Light (1999) into this model and show that automatically learned areas improve the accuracy of figuring out word meanings compared to other methods. We use topics found in documents with Latent Dirichlet Allocation (LDA) as clues to help understand word meanings. We explain a related model, LDAWN, for understanding word meanings that adds an extra layer of hidden elements Z, which affects how the system's parameters are set. We bring understanding of word meanings into the topic model framework.	suffix array
135	Joint Unsupervised Coreference Resolution with Markov Logic Machine learning approaches to coreference resolution are typically supervised, and require expensive labeled data. Some unsupervised approaches have been proposed (e.g., Haghighi and Klein (2007)), but they are less accurate. In this paper, we present the first unsupervised approach that is competitive with supervised ones. This is made possible by performing joint inference across mentions, in contrast to the pairwise classification typically used in supervised methods, and by using Markov logic as a representation language, which enables us to easily express relations like apposition and predicate nominals. On MUC and ACE datasets, our model outperforms Haghigi and Klein’s one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models. We empirically report that global approaches achieve performance better than the ones based on incrementally processing a text. Our method is based on the entity-mention model. In the predicate nominative construction, the object of a copular verb (forms of the verb be) is constrained to corefer with its subject.	Joint Unsupervised Coreference Resolution with Markov Logic Machine learning methods to solve coreference resolution usually need labeled data, which can be costly. Some methods that don't need labeled data have been tried (like by Haghighi and Klein in 2007), but they aren't as accurate. In this paper, we introduce the first method that doesn’t need labeled data and competes well with those that do. We achieve this by analyzing all mentions together, unlike the separate comparisons usually used in other methods, and by using Markov logic, a system that helps us easily describe relationships like apposition (placing things side by side) and predicate nominals (nouns following verbs like "is"). Using MUC and ACE datasets, our model does better than the one by Haghigi and Klein while using much less data, and often matches or is better than the best methods that use labeled data. We show that methods looking at the whole text perform better than those that look at parts of it one by one. Our method focuses on the entity-mention model. In sentences with a predicate nominative (where a noun follows a verb like "is"), the noun must refer back to the subject.	lattices
120	Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles We present a data-driven variant of the LR algorithm for dependency parsing, and extend it with a best-first search for probabilistic generalized LR dependency parsing. Parser actions are determined by a classifier, based on features that represent the current state of the parser. We apply this parsing framework to both tracks of the CoNLL 2007 shared task, in each case taking advantage of multiple models trained with different learners. In the multilingual track, we train three LR models for each of the ten languages, and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme. In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-of-domain training set, in a scheme similar to one iteration of co-training. We use a combination of co-learning and active learning by training two different parsers on the labeled training data, parsing the unlabeled domain data with both parsers, and adding parsed sentences to the training data only if the two parsers agreed on their analysis. We generalize the standard deterministic framework to probabilistic parsing by using a best-first search strategy.	Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles We introduce a method based on data for dependency parsing, using a variation of the LR algorithm. We enhance it with a best-first search, which is a way to find the most probable outcomes in a list of possibilities, for a more flexible way of parsing sentences. The parser makes decisions using a classifier, which is a tool that chooses actions based on specific features or characteristics of the current parsing situation. We use this parsing method on both parts of the CoNLL 2007 shared task, utilizing multiple models, which are trained using different techniques. In the multilingual part, we create three LR models for each of the ten languages and combine their results using a voting system called maximum spanning tree, which helps decide the best overall outcome. In the domain adaptation part, we use two models to work with data that hasn't been labeled in the new area we're studying, adding it to the labeled data from another area, in a process similar to co-training, where multiple learning methods improve each other. We combine co-learning, where models learn together, and active learning, where we only add data if two different parsers agree on it. We expand the usual straightforward method to include the best probable outcomes using a best-first search strategy.	WORDNET
121	Extracting Aspect-Evaluation and Aspect-Of Relations in Opinion Mining The technology of opinion extraction allows users to retrieve and analyze people's opinions scattered over Web documents. We define an opinion unit as a quadruple consisting of the opinion holder, the subject being evaluated, the part or the attribute in which the subject is evaluated, and the value of the evaluation that expresses a positive or negative assessment. We use this definition as the basis for our opinion extraction task. We focus on two important subtasks of opinion extraction: (a) extracting aspect-evaluation relations, and (b) extracting aspect-of relations, and we approach each task using methods which combine contextual and statistical clues. Our experiments on Japanese weblog posts show that the use of contextual clues improve the performance for both tasks. We analyze the labeled corpus for opinion expressions and observe that many opinion expressions are used in multiple domains. We adopt a supervised learning technique to search for useful syntactic patterns as contextual clues.	Extracting Aspect-Evaluation and Aspect-Of Relations in Opinion Mining The technology of opinion extraction helps users find and study people's opinions spread across web documents. We define an opinion unit as four parts: the person giving the opinion, the thing being talked about, the specific part or feature of that thing being discussed, and whether the opinion is good or bad. We use this definition as the base for our opinion extraction task. We concentrate on two key parts of opinion extraction: (a) finding connections between the aspect (part or feature) and the evaluation (opinion), and (b) finding connections between the aspect and what it belongs to, using methods that mix context (surrounding information) and statistical clues (data patterns). Our tests on Japanese blog posts show that using context clues makes both tasks better. We study a labeled set of data for opinion expressions and notice that many opinions are used in different areas. We use a supervised learning technique, which is a method where a computer learns from examples, to find useful sentence structures as context clues.	Dependency Parsing
122	Indirect-HMM-based Hypothesis Alignment for Combining Outputs from Machine Translation Systems This paper presents a new hypothesis alignment method for combining outputs of multiple machine translation (MT) systems. An indirect hidden Markov model (IHMM) is proposed to address the synonym matching and word ordering issues in hypothesis alignment. Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the parameters of the IHMM are estimated indirectly from a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty. The IHMM-based method significantly outperforms the state-of-the-art TER-based alignment model in our experiments on NIST benchmark datasets. Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the 2008 NIST Open MT Evaluation. we propose using an indirect hidden Markov model (IHMM) for pairwise alignment of system outputs.	Indirect-HMM-based Hypothesis Alignment for Combining Outputs from Machine Translation Systems This paper introduces a new way to align and combine results from different machine translation (MT) systems. It uses an indirect hidden Markov model (IHMM) to help match similar words and correct word order in the combined translations. Unlike usual hidden Markov models that are trained by finding the most likely patterns, the IHMM gets its information from various sources such as how similar words are in meaning, how they look, and a penalty for how far words are moved from their original place. The IHMM method works much better than the best existing model, called TER-based alignment, in our tests using NIST standard test sets. Our combined system using this method was the best in translating Chinese to English in a specific competition in 2008. We suggest using the IHMM model to align outputs from different systems two at a time.	context
123	Multilingual Subjectivity Analysis Using Machine Translation Although research in other languages is increasing, much of the work in subjectivity analysis has been applied to English data, mainly due to the large body of electronic resources and tools that are available for this language. In this paper, we propose and evaluate methods that can be employed to transfer a repository of subjectivity resources across languages. Specifically, we attempt to leverage on the resources available for English and, by employing machine translation, generate resources for subjectivity analysis in other languages. Through comparative evaluations on two different languages (Romanian and Spanish), we show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language. We demonstrate that machine translation can perform quite well when extending the subjectivity analysis to multilingual environment. We hypothesize that subjectivity is expressed differently in various languages due to lexicalization, formal versus informal markers, etc.	Multilingual Subjectivity Analysis Using Machine Translation Although research in other languages is increasing, much of the work in subjectivity analysis (studying opinions or feelings in text) has been applied to English data, mainly due to the large number of electronic resources and tools available for this language. In this paper, we propose and evaluate methods to transfer a collection of resources for studying opinions across languages. Specifically, we try to use the resources available for English and, by using machine translation (automatic translation by computers), create resources for studying opinions in other languages. Through comparisons on two different languages (Romanian and Spanish), we show that automatic translation is a practical option for creating resources and tools for studying opinions in a new language. We show that machine translation can work well when expanding opinion analysis to different languages. We suggest that opinions are expressed differently in various languages due to word choice, formal versus informal language markers, etc.	competition
125	Revisiting Readability: A Unified Framework for Predicting Text Quality We combine lexical, syntactic, and discourse features to produce a highly predictive model of human readers' judgments of text readability. This is the first study to take into account such a variety of linguistic factors and the first to empirically demonstrate that discourse relations are strongly associated with the perceived quality of text. We show that various surface metrics generally expected to be related to readability are not very good predictors of readability judgments in our Wall Street Journal corpus. We also establish that readability predictors behave differently depending on the task: predicting text readability or ranking the readability. Our experiments indicate that discourse relations are the one class of features that exhibits robustness across these two tasks. We propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality. When readability is targeted towards adult competent language users a more prominent role is played by discourse features. Five annotators have assessed the overall text quality of each article on a scale from 1 to 5. We find non-significant correlation for the mean number of words per sentence and the mean number of characters per word.	Revisiting Readability: A Unified Framework for Predicting Text Quality We put together word choices, sentence structure, and how ideas are connected to create a model that can guess how people will judge how easy a text is to read. This is the first research to look at so many language factors and to show with evidence that how ideas connect in a text is closely linked to how good people think the text is. We demonstrate that certain simple measurements, usually thought to be related to readability, don't actually predict how easy a text is to read in our Wall Street Journal study. We also show that what predicts readability can change depending on the goal: whether it's to guess how easy a text is or to compare different texts. Our tests show that how ideas connect is the only feature that stays reliable in both tasks. We suggest a combined approach that uses word choice, sentence structure, how well words fit together, how topics are carried through, and idea connections to judge text quality. When judging readability for adults who are fluent in the language, how ideas connect becomes more important. Five people rated the overall quality of each article on a scale from 1 to 5. We find that the average number of words in a sentence and the average number of letters in a word do not really relate to text quality.	solving algorithms
126	Syntactic Constraints on Paraphrases Extracted from Parallel Corpora We improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type. This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method. We show how the monolingual context of a sentence to paraphrase can be used to improve the quality of the acquired paraphrases. Human evaluators are asked to score each pair of an original sentence and a paraphrased sentence with the following two 5-point scale grades: Grammaticality: whether the paraphrased sentence is grammatical, Meaning: whether the meaning of the original sentence is properly retained by the paraphrased sentence. A problem of phrase-based methods to paraphrase or term variation acquisition is the fact that a large proportion of the term variations or paraphrases proposed by the system are superior sub-strings of the original term. We automatically acquire paraphrase dictionary.	Syntactic Constraints on Paraphrases Extracted from Parallel Corpora We improve the quality of paraphrases (different ways to say the same thing) taken from parallel corpora (collections of text in two languages) by ensuring that phrases and their paraphrases have the same grammatical structure. This is done by analyzing the English text in the collection and changing the way we extract phrases to include phrase labels along with pairs from both languages. To keep a wide range of phrases that are not full sentences, we use complex grammatical labels. A manual review shows a 19% absolute improvement in paraphrase quality compared to the basic method. We demonstrate how the context of a sentence in one language can be used to enhance the quality of the paraphrases obtained. Human evaluators (people who judge quality) are asked to rate each pair of original and paraphrased sentences with two scores: Grammaticality (if the paraphrased sentence follows grammar rules) and Meaning (if the paraphrased sentence keeps the original meaning). A problem with methods that change phrases or acquire term variations is that many of the suggested variations are just larger parts of the original phrase. We automatically create a paraphrase dictionary (a collection of different ways to say things).	sentence
127	Forest-based Translation Rule Extraction Translation rule extraction is a fundamental problem in machine translation, especially for linguistically syntax-based systems that need parse trees from either or both sides of the bitext. The current dominant practice only uses 1-best trees, which adversely affects the rule set quality due to parsing errors. So we propose a novel approach which extracts rules from a packed forest that compactly encodes exponentially many parses. Experiments show that this method improves translation quality by over 1 BLEU point on a state-of-the-art tree-to-string system, and is 0.5 points better than (and twice as fast as) extracting on 30-best parses. When combined with our previous work on forest-based decoding, it achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero by 0.7 points. We use viterbi algorithm to prune the forest.	Forest-based Translation Rule Extraction Translation rule extraction is a key issue in machine translation, especially for systems based on language structure that need sentence structure trees from one or both languages. The current main method only uses the best single tree, which lowers the quality of rules due to mistakes in analyzing sentence structure. We suggest a new way that extracts rules from a packed forest, a method that efficiently represents many possible sentence structures. Tests show that this new method improves translation quality by more than 1 point on the BLEU scale, a measurement of translation accuracy, in a top-tier system that converts tree structures to strings. It is also 0.5 points better and twice as fast as using the top 30 best sentence structures. When combined with our earlier work on decoding using forests, it results in a 2.5 BLEU points improvement over the basic system and even beats the Hiero system, which is a well-known translation method, by 0.7 points. We use the Viterbi algorithm, a procedure for finding the most likely sequence of hidden states, to simplify the forest.	phrases
128	Online Large-Margin Training of Syntactic and Structural Translation Features Minimum-error-rate training (MERT) is a bottleneck for current development in statistical machine translation because it is limited in the number of weights it can reliably optimize. Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT. We first show that by parallel processing and exploiting more of the parse forest, we can obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost. We then test the method on two classes of features that address deficiencies in the Hiero hierarchical phrase-based model: first, we simultaneously train a large number of Marton and Resnik's soft syntactic constraints, and, second, we introduce a novel structural distortion model. In both cases we obtain significant improvements in translation performance. Optimizing them in combination, for a total of 56 feature weights, we improve performance by 2.6 BLUE on a subset of the NIST 2006 Arabic-English evaluation data. We introduce structural distortion features into a hierarchical phrase-based model, aimed at modeling nonterminal reordering given source span length. We show that MERT is competitive with small numbers of features compared to high-dimensional optimizers such as MIRA. Our feature explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-side terminals.	Online Large-Margin Training of Syntactic and Structural Translation Features Minimum-error-rate training (MERT) is a challenge for improving statistical machine translation because it can't effectively adjust many weights at once. Building on the work of Watanabe et al., we explore using the MIRA algorithm by Crammer et al. instead of MERT. We first show that by using parallel processing and making better use of the parse forest, we can achieve results with MIRA that are as good as or better than with MERT in terms of translation quality and cost. We then test the method on two types of features that fix problems in the Hiero model, which is a system that uses hierarchical phrases: first, we train many of Marton and Resnik's soft syntactic constraints at the same time; second, we introduce a new structural distortion model, which helps with word order changes. In both cases, we see big improvements in translation quality. By optimizing all these together, with a total of 56 feature weights, we improve performance by 2.6 BLUE points (a measure of translation quality) on some of the NIST 2006 Arabic-English test data. We add structural distortion features into a model that uses hierarchical phrases to better handle rearranging parts of a sentence based on the length of the source text. We show that MERT works well with a small number of features compared to MIRA, which can handle many features. Our feature specifically addresses over-counting of translation rules and problems with poorly matched or inserted words.	points better
129	Cheap and Fast â€“ But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon’s Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense. We compare the quality of labels produced by non-expert Turkers against those made by experts for a variety of NLP tasks and find that they required only four responses per item to emulate expert annotations. We show that obtaining multiple low quality labels (through Mechanical Turk) can approach high-quality editorial labels. We work with a majority rule where ties are broken uniformly at random and report an observed agreement (accuracy) between the majority rule and the gold standard of 89.7.	Cheap and Fast â€“ But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks Human language tagging is important for many tasks in natural language processing, but it can be costly and take a lot of time. We look into using Amazon's Mechanical Turk system, which is a much cheaper and quicker way to gather annotations from a wide range of non-expert paid workers online. We examine five tasks: recognizing emotions, finding word similarities, understanding if one text implies another, figuring out the order of events, and clarifying word meanings. For all five, we find that non-expert annotations from Mechanical Turk agree well with existing top-quality labels given by expert annotators. For recognizing emotions, we also show that using these non-expert labels to train computer programs can work just as well as using the best expert labels. We suggest a way to fix biases that greatly improves the quality of annotations for two tasks. We conclude that many large labeling tasks can be successfully planned and done using this method for much less money. We compare the quality of labels made by non-expert workers with those made by experts for different natural language processing tasks and find that only four responses per item were needed to match expert annotations. We show that getting many low-quality labels (through Mechanical Turk) can come close to high-quality expert labels. We use a system where the most common answer wins, and ties are broken randomly, and we report a match (accuracy) between this system and the expert standard of 89.7%.	Watanabe
130	Understanding the Value of Features for Coreference Resolution In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques. These works often show that complex models improve over a weak pairwise baseline. However, less attention has been given to the importance of selecting strong features to support learning a coreference model. This paper describes a rather simple pair-wise classification model for coreference resolution, developed with a well-designed set of features. We show that this produces a state-of-the-art system that outperforms systems built with complex models. We suggest that our system can be used as a baseline for the development of more complex models – which may have less impact when a more robust set of features is used. The paper also presents an ablation study and discusses the relative contributions of various features. Our algorithm runs in time quadratic in the number of mentions.	Understanding the Value of Features for Coreference Resolution In recent years, there has been a lot of work on the important problem of coreference resolution, which is figuring out when different words in a text refer to the same thing. Most of this work has focused on creating new models and techniques for solving this problem. These studies often show that complicated models do better than simpler ones that just compare pairs of words. However, not much focus has been on how important it is to choose strong features, or characteristics, that help in learning a coreference model. This paper describes a fairly simple method that compares pairs of words for coreference resolution, using a carefully chosen set of features. We show that this creates a top-performing system that does better than systems using complex models. We suggest that our system can be used as a starting point for developing more complex models, which might not be needed if strong features are already being used. The paper also includes a study that removes one feature at a time to see which ones are most important. Our method takes time that increases with the square of the number of mentions, or references, in the text.	standard
131	Bayesian Unsupervised Topic Segmentation This paper describes a novel Bayesian approach to unsupervised topic segmentation. Unsupervised systems for this task are driven by lexical cohesion: the tendency of well-formed segments to induce a compact and consistent lexical distribution. We show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation. This contrasts with previous approaches, which relied on hand-crafted cohesion metrics. The Bayesian framework provides a principled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems. Our model yields consistent improvements over an array of state-of-the-art systems on both text and speech datasets. We also show that both an entropy-based analysis and a well-known previous technique can be derived as special cases of the Bayesian framework. We present a dynamic program for linear segmentation. If the actual number of segments is known and only a linear discourse structure is acceptable, then a single move, shift of the segment border, is sufficient. We find the richer model beneficial for a meetings corpus but not for a textbook.	Bayesian Unsupervised Topic Segmentation This paper explains a new way using Bayesian methods for dividing topics without needing prior examples. Systems that don't need training often rely on lexical cohesion, which is how well words stick together in a topic. We show that this can be understood using Bayesian methods by treating words in a topic as if they come from a specific language model for that topic; getting the best fit in this model gives a good topic division. This is different from older methods that used manually created measures of cohesion. The Bayesian method also allows us to include other helpful features like cue phrases, which are strong hints about the structure of the conversation and were not used before in such systems. Our model shows consistent improvements over the latest systems on both written and spoken data. We also show that both a way of analyzing unpredictability (entropy) and a well-known previous technique can be seen as examples of the Bayesian method. We present a method for segmenting in a straight line. If the number of segments is known and only a straight-line structure is allowed, then just moving the border between segments is enough. We find the more detailed model is helpful for meeting notes but not for a textbook.	compares pairs
132	A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers There is growing interest in applying Bayesian techniques to NLP problems. There are a number of different estimators for Bayesian models, and it is useful to know what kinds of tasks each does well on. This paper compares a variety of different Bayesian estimators for Hidden Markov Model POS taggers with various numbers of hidden states on data sets of different sizes. Recent papers have given contradictory results when comparing Bayesian estimators to Expectation Maximization (EM) for unsupervised HMM POS tagging, and we show that the difference in reported results is largely due to differences in the size of the training data and the number of states in the HMM. We investigate a variety of samplers for HMMs, including some that these earlier papers did not study. We find that all of Gibbs samplers do well with small data sets and few states, and that Variational Bayes does well on large data sets and is competitive with the Gibbs samplers. In terms of times of convergence, we find that Variational Bayes was the fastest of all the estimators, especially on large data sets, and that explicit Gibbs sampler (both pointwise and sentence-blocked) were generally faster than their collapsed counterparts on large data sets. We consider three evaluation criteria: M-to-1 and 1-to-1 are the tagging accuracies under the best many-to-one map and the greedy one-to-one map respectively; VI is a map-free information theoretic criterion. We induce a many-to-one mapping of state identifiers to PoS tags from one half of the corpus and evaluate on the second half, which is referred to as cross-validation accuracy. We show that sparse priors can gain 4% (.62 to .66 with a 1M word corpus) in cross-validated many to-one accuracy.	A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers There is increasing interest in using Bayesian methods for Natural Language Processing (NLP) challenges. There are several different methods (estimators) for Bayesian models, and it's helpful to know which tasks each method is good at. This paper looks at different Bayesian methods for Hidden Markov Model (HMM) POS taggers, which use different numbers of hidden states, on data sets of various sizes. Recent studies have shown mixed results when comparing Bayesian methods to a technique called Expectation Maximization (EM) for unsupervised HMM POS tagging. We demonstrate that the difference in these findings is mainly due to the size of the training data and the number of states in the HMM. We examine different sampling methods for HMMs, including some not explored in earlier research. We discover that all Gibbs samplers work well with small data sets and few states, while Variational Bayes performs well on large data sets and matches the performance of Gibbs samplers. In terms of speed, Variational Bayes was the quickest of all methods, especially on large data sets, and the detailed Gibbs sampler (both pointwise and sentence-blocked) was typically faster than their simplified versions on large data sets. We use three evaluation methods: M-to-1 and 1-to-1 measure tagging accuracy with different mapping strategies; VI is a method that doesn’t rely on mapping. We create a many-to-one linking of state identifiers to Parts of Speech (PoS) tags from one half of the data and test on the other half, known as cross-validation accuracy. We show that using sparse priors can improve accuracy by 4% (from .62 to .66 with a 1 million word data set) in cross-validated many-to-one accuracy.	known previous
133	A Tale of Two Parsers: Investigating and Combining Graph-based and Transition-based Dependency Parsing Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations. We study both approaches under the framework of beam-search. By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods. More importantly, we propose a beam-search-based parser that combines both graph-based and transition-based parsing into a single system for training and decoding, showing that it outperforms both the pure graph-based and the pure transition-based parsers. Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuracies of 92.1% and 86.2%, respectively. We define head rules to convert phrase structures into dependency structures. We combine beam search with a globally normalized discriminative model, using structured perceptron learning and the early update strategy of Collins and Roark (2004), and also explore the addition of graph based features to a transition-based parser.	A Tale of Two Parsers: Investigating and Combining Graph-based and Transition-based Dependency Parsing Graph-based and transition-based methods for dependency parsing look at the problem in different ways, each having its own benefits and drawbacks. We examine both methods using a technique called beam-search. By creating a graph-based and a transition-based dependency parser, we demonstrate that beam-search is a strong option for both techniques. More importantly, we introduce a new parser that uses beam-search to combine both graph-based and transition-based parsing into one system for learning and understanding, and it performs better than using just one method alone. When tested on English and Chinese language data sets, the combined system achieved top-level accuracy scores of 92.1% and 86.2%, respectively. We establish rules to change phrase structures into dependency structures. We mix beam search with a globally adjusted model that makes decisions based on differences, using a learning method called structured perceptron learning and an early update strategy from Collins and Roark (2004). We also look into adding graph-based features to a transition-based parser.	increasing interest
134	Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation We present Minimum Bayes-Risk (MBR) decoding over translation lattices that compactly encode a huge number of translation hypotheses. We describe conditions on the loss function that will enable efficient implementation of MBR decoders on lattices. We introduce an approximation to the BLEU score (Papineni et al., 2001) that satisfies these conditions. The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata. Our experiments show that the Lattice MBR decoder yields moderate, consistent gains in translation performance over N-best MBR decoding on Arabic-to-English, Chinese-to-English and English-to-Chinese translation tasks. We conduct a range of experiments to understand why Lattice MBR improves upon N-best MBR and study the impact of various parameters on MBR performance. We consider Taylor approximations to the logarithm of BLEU. We extend MBR to word lattices, which improves performance over k-best list MBR. The log-BLEU function must be modified slightly to yield a linear Taylor approximation: we replace the clipped n-gram count with the product of an n gram count and an n-gram indicator function. We compute expected feature values by intersecting the translation lattice with a lattices for each n-gram t.	Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation We introduce a method called Minimum Bayes-Risk (MBR) decoding that helps choose the best translations from a large set of possible translations stored in a compact form called "translation lattices." We explain how to set up the rules for measuring translation errors so that the MBR method works well with these lattices. We create a simplified version of the BLEU score, which is a way to measure how good a translation is, to fit these rules. The MBR decoding using this simplified BLEU score is done with a tool called Weighted Finite State Automata. Our tests show that this Lattice MBR method gives better and more reliable translation results compared to the older method called N-best MBR decoding when translating between Arabic, Chinese, and English. We run various tests to find out why Lattice MBR is better than N-best MBR and see how different settings affect its performance. We use a mathematical shortcut, called Taylor approximation, to simplify the calculation of the BLEU score. We expand the MBR method to work with word lattices, which further improves its performance over the k-best list MBR method. We make a small change to the BLEU score calculation by replacing a part of it with a simpler multiplication method to make the math easier. We calculate expected values for certain features by combining the translation lattice with another lattice for each part of the translation.	performs
136	Lattice-based Minimum Error Rate Training for Statistical Machine Translation Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature function its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and representing the exact error surface of all translations that are encoded in a phrase lattice. Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experiments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT. We find that first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository. We present a procedure for conducting line optimisation directly over a word lattice encoding the hypotheses in Cs. We apply the SweepLine algorithm to the union to discard redundant linear functions and their associated hypotheses. We theorize that an upper bound for the number of linear functions in the upper envelope at the final state is equal to the number of edges in the lattice. In our MERT algorithm we compute the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space. We extend the MERT algorithm so as to use the whole set of candidate translations compactly represented in the search lattice produced by the decoder, instead of only a N-best list of candidates extracted from it. We find that the Down hill Simplex Algorithm loses its robustness as the dimension goes up by more than 10.	Lattice-based Minimum Error Rate Training for Statistical Machine Translation Minimum Error Rate Training (MERT) is a useful way to find the best settings for a model by directly improving how well it works using an automatic test. During training, the process checks each feature to see how it affects errors in a list of possible translations. The settings are then changed to reduce errors by looking at all sentences together. Usually, MERT uses N-best lists, which are the top N possible translations from a decoder. This paper introduces a new method that efficiently handles all possible translations in a phrase lattice, rather than just the top ones. This means considering many more translation options. This method is used to train a system that translates languages using a statistical approach. Tests showed it runs faster and slightly better than the usual N-best MERT. However, early attempts to adjust settings often result in poor choices, likely because they focus too much on the set of possibilities. We suggest a way to optimize directly over a group of word choices. We use a method to remove unnecessary functions and options. We guess that the most functions at the end are the same as the number of paths in the lattice. Our MERT method checks errors by looking at all possible translations in a packed forest, which allows for small changes in settings. We improve the MERT method to consider all possible translations, not just the top few. We find that a method called the Downhill Simplex Algorithm doesn't work as well when there are more than 10 factors to consider.	first method
137	A Generative Model for Parsing Natural Language to Meaning Representations In this paper, we present an algorithm for learning a generative model of natural language sentences together with their formal meaning representations with hierarchical structures. The model is applied to the task of mapping sentences to hierarchical representations of their underlying meaning. We introduce dynamic programming techniques for efficient training and decoding. In experiments, we demonstrate that the model, when coupled with a discriminative reranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models. Our hybrid tree model use a tree transformation based approach. We present a joint generative process that produces a hybrid tree structure containing words, syntactic structures, and meaning representations, where the meaning representations are in a variable-free tree-structured form. We propose 3 models for generative semantic parsing :unigram, bigram, and mix gram (interpolation between the two).	A Generative Model for Parsing Natural Language to Meaning Representations In this paper, we introduce a method for teaching a computer program to understand natural language sentences and their complex meaning in a structured way. The method is used to convert sentences into organized representations that show their true meaning. We use special techniques to make the training and decoding processes fast and efficient. In tests, we show that our method, when combined with a technique that helps choose the best answer, performs better than previous methods on two publicly available datasets. The method still works well even when given new examples that are different from those it learned from. This leads to a noticeable improvement in remembering information compared to older methods. Our method uses a tree-based approach that combines different elements. We introduce a combined process that creates a tree structure including words, grammar structures, and meanings, with meanings in a simple tree structure. We suggest three methods for understanding the meaning of sentences: unigram (single unit), bigram (pairs of units), and mix gram (a combination of the two).	translations
138	Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis Determining the polarity of a sentiment-bearing expression requires more than a simple bag-of-words approach. In particular, words or constituents within the expression can interact with each other to yield a particular overall polarity. In this paper, we view such subsentential interactions in light of compositional semantics, and present a novel learning-based approach that incorporates structural inference motivated by compositional semantics into the learning procedure. Our experiments show that (1) simple heuristics based on compositional semantics can perform better than learning-based methods that do not incorporate compositional semantics (accuracy of 89.7% vs. 89.1%), but (2) a method that integrates compositional semantics into learning performs better than all other alternatives (90.7%). We also find that “content-word negators”, not widely employed in previous work, play an important role in determining expression-level polarity. Finally, in contrast to conventional wisdom, we find that expression-level classification accuracy uniformly decreases as additional, potentially disambiguating, context is considered. Content-word negators are words that are not function words, but act semantically as negators. We combine different kinds of negators with lexical polarity items through various compositional semantic models, both heuristic and machine learned, to improve phrasal sentiment analysiss. We propose an algorithm for phrase-based sentiment analysis that learns proper assignments of intermediate sentiment analysis decision variables given the a priori (i.e., out of context) polarity of the words in the phrase and the (correct) phrase-level polarity. We hand-code compositional rules in order to model compositional effects of combining different words in the phrase. We categorized polarity reversing words into two categories: function-word negators such as not and content-word negators such as eliminate.	Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis Determining the positive or negative feeling of an expression requires more than just looking at individual words. Words or parts of the expression can interact to create a specific overall feeling. In this paper, we look at these interactions using compositional semantics (how word meanings combine) and introduce a new learning approach that uses this idea in the learning process. Our experiments show that (1) simple rules based on compositional semantics can be better than learning methods that don't use this idea (89.7% accuracy vs. 89.1%), and (2) using compositional semantics in learning works best (90.7% accuracy). We also find that "content-word negators," which are not often used in past research, are important in figuring out the overall feeling of expressions. Surprisingly, we discover that accuracy in classifying expressions decreases when more context is added. Content-word negators are words that aren't basic words but act to reverse meaning. We combine different kinds of negators with words showing feelings using various models, both simple and machine-learned, to improve understanding of phrases. We suggest a method for analyzing phrase sentiment that learns how to decide the sentiment of phrases based on the basic feeling of the words and the correct overall phrase feeling. We manually create rules to model how combining different words affects the overall meaning. We divide words that reverse meaning into two types: function-word negators like "not" and content-word negators like "eliminate."	structures
139	A Simple and Effective Hierarchical Phrase Reordering Model While phrase-based statistical machine translation systems currently deliver state-of-the- art performance, they remain weak on word order changes. Current phrase reordering models can properly handle swaps between adjacent phrases, but they typically lack the ability to perform the kind of long-distance reorderings possible with syntax-based systems. In this paper, we present a novel hierarchical phrase reordering model aimed at improving non-local reorderings, which seamlessly integrates with a standard phrase-based system with little loss of computational efficiency. We show that this model can successfully handle the key examples often used to motivate syntax-based systems, such as the rotation of a prepositional phrase around a noun phrase. We contrast our model with reordering models commonly used in phrase-based systems, and show that our approach provides statistically significant BLEU point gains for two language pairs: Chinese-English (+0.53 on MT05 and +0.71 on MT08) and Arabic-English (+0.55 on MT05). Our hierarchical orientation model captures non-local phrase reordering by a shift reduce algorithm. We introduce a deterministic shift-reduce parser into decoding, so that the decoder always has access to the largest possible previous block, given the current translation history. We introduce three orientation models for lexicalized reordering: word-based, phrase-based and hierarchical orientation model.	A Simple and Effective Hierarchical Phrase Reordering Model While current translation systems are good, they struggle with changing word order. Present models can manage swaps between phrases next to each other but aren't good at handling long-distance rearrangements like systems that use grammar rules. In this paper, we introduce a new model that helps with these long-distance changes and works well with existing systems without slowing them down much. We demonstrate that this model can handle key examples that usually need complex grammar-based systems, like moving a group of words around another group. We compare our model with those commonly used and show it improves translation quality for Chinese-English and Arabic-English. Our model uses a method called a shift-reduce algorithm to handle non-local phrase changes. We add a straightforward shift-reduce parser to the translation process so it can always see the biggest part of the sentence already translated. We propose three models for reordering words: based on single words, groups of words, and a hierarchical approach.	negators
140	Two Languages are Better than One (for Syntactic Parsing) We show that jointly parsing a bitext can substantially improve parse quality on both sides. In a maximum entropy bitext parsing model, we define a distribution over source trees, target trees, and node-to-node alignments between them. Features include monolingual parse scores and various measures of syntactic divergence. Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables. The resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 F at predicting English side trees and 1.8 F at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation. In bitext parsing, we use feature functions defined on triples of (parse tree in language 1, parse tree in language 2, word alignment), combined in a log-linear model trained to maximize parse accuracy. We use word alignment density features which measure how well the aligned entity pair matches up with alignments from an independent word aligner.	Two Languages are Better than One (for Syntactic Parsing) We show that analyzing two languages together can greatly improve the accuracy of understanding sentence structure in both languages. In a model that predicts the structure of sentences in two languages (bitext parsing), we define a method to look at sentence trees in each language and how their parts match up. Features include scores for sentence structure in one language and measures of how different the structures are between the two languages. Using translated parts of a Chinese language dataset, our model is trained in steps to best guess the sentence structures, treating the connections between languages as hidden information. The resulting two-language parser performs better than the best single-language parsers by 2.5 F points for English trees and 1.8 F points for Chinese trees (the best numbers published for these data sets). Also, these better sentence structures improve the quality of translated text by 2.4 BLEU points. In two-language parsing, we use features that look at combinations of sentence structures in both languages and word connections, combined in a model trained to improve accuracy. We use features that measure how well the connected words match with pairings from another word matching tool.	changing
141	Unsupervised Semantic Parsing We present the first unsupervised approach to the problem of learning a semantic parser, using Markov logic. Our USP system transforms dependency trees into quasi-logical forms, recursively induces lambda forms from these, and clusters them to abstract away syntactic variations of the same meaning. The MAP semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them. We evaluate our approach by using it to extract a knowledge base from biomedical abstracts and answer questions. USP substantially outperforms TextRunner, DIRT and an informed baseline on both precision and recall on this task. We consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. We model joint probability of the dependency tree and its latent semantic representation using Markov Logic Networks (MLNs) (Richardson and Domingos, 2006), selecting parameters (weights of first-order clauses) to maximize the probability of the observed dependency structures. We group parameters and impose local normalization constraints within each group.	Unsupervised Semantic Parsing We introduce the first method that doesn't need labeled data to teach a semantic parser, using Markov logic, which helps computers understand language meanings. Our USP system changes sentence structures into simple logic expressions, creates lambda forms (a way to express functions) from these, and groups them to focus on the same meaning despite different phrasing. The most likely semantic structure of a sentence is found by matching its parts to these groups and combining them. We test this method by using it to build a database from scientific papers and answer questions about them. USP does much better than TextRunner, DIRT, and a smart starting point in both correctness and coverage for this task. We look at a situation where the aim is to (1) break down the sentence structure into parts, (2) match each part to a group of similar sentence structures, and (3) figure out the relationships between these parts. We use Markov Logic Networks (MLNs) to model the combined probability of the sentence structure and its hidden meaning, choosing settings (weights of rules) to make the observed structures most likely. We organize settings and apply local rules within each group.	Parsing
906	Phrasetable Smoothing For Statistical Machine Translation We discuss different strategies for smoothing the phrasetable in Statistical MT, and give results over a range of translation settings. We show that any type of smoothing is a better idea than the relative-frequency estimates that are often used. The best smoothing techniques yield consistent gains of approximately 1% (absolute) according to the BLEU metric.	Phrasetable Smoothing For Statistical Machine Translation We talk about different ways to make the phrasetable smoother in Statistical Machine Translation (MT) and share results from various translation situations. We demonstrate that using any smoothing method is better than using relative-frequency estimates, which are commonly used. The best smoothing techniques lead to consistent improvements of about 1% according to the BLEU metric (a way to measure translation quality).	Statistical
142	First- and Second-Order Expectation Semirings with Applications to Minimum-Risk Training on Translation Forests Many statistical translation models can be regarded as weighted logical deduction. Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hypergraphs). We then introduce a novel second-order expectation semiring, which computes second-order statistics (e.g., the variance of the hypothesis length or the gradient of entropy). This second-order semiring is essential for many interesting training paradigms such as minimum risk, deterministic annealing, active learning, and semi-supervised learning, where gradient descent optimization requires computing the gradient of entropy or risk. We use these semirings in an open-source machine translation toolkit, Joshua, enabling minimum-risk training for a benefit of up to 1.0 BLEU point. We consider minimum risk training using a linearly decomposable approximation of BLEU. The sufficient statistics for graph expected BLEU can be computed using expectation semirings. We extend the work of Smith and Eisner and obtain much better estimates of feature expectations by using a packed chart instead of an n-best list. We perform expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007).	First- and Second-Order Expectation Semirings with Applications to Minimum-Risk Training on Translation Forests Many statistical translation models can be seen as logical processes where each step has a weight or importance. In this way, we use weights from a mathematical tool called the expectation semiring to calculate basic statistics, like the expected length of a translation or how often certain features appear, over groups of translations. We then introduce a new tool called the second-order expectation semiring, which helps calculate more complex statistics, like how much the length of a translation varies or how much uncertainty there is. This second-order tool is important for advanced training methods, such as minimum risk training, which aims to minimize mistakes, and other strategies that require understanding changes in uncertainty and risk. We use these tools in a free machine translation software, Joshua, which can improve translation quality by up to 1.0 BLEU point, a score used to measure translation accuracy. We explore minimum risk training using a simplified method to calculate BLEU scores. Important numbers needed for calculating expected BLEU scores on graphs can be found using expectation semirings. We improve on previous work by Smith and Eisner by using a comprehensive chart instead of a simple list to get better estimates of expected features. We conduct training to improve BLEU scores using a method called deterministic annealing on translation forests created by a system called Hiero.	which helps
143	Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora A significant portion of the world's text is tagged by readers on social bookmarking websites. Credit attribution is an inherent problem in these corpora because most pages have multiple tags, but the tags do not always apply with equal specificity across the whole document. Solving the credit attribution problem requires associating each word in a document with the most appropriate tags and vice versa. This paper introduces Labeled LDA, a topic model that constrains Latent Dirichlet Allocation by defining a one-to-one correspondence between LDA's latent topics and user tags. This allows Labeled LDA to directly learn word-tag correspondences. We demonstrate Labeled LDA's improved expressiveness over traditional LDA with visualizations of a corpus of tagged web pages from del.icio.us. Labeled LDA outperforms SVMs by more than 3 to 1 when extracting tag-specific document snippets. As a multi-label text classifier, our model is competitive with a discriminative baseline on a variety of datasets. L-LDA extends standard LDA to include supervision for specific target categories, and the generative process includes a second observed variable, i.e. each document is explicitly labeled with a target category.	Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora A large part of the world's text is tagged by users on social bookmarking websites. Credit attribution is a problem in these collections of text because most pages have many tags, but the tags don't always match the whole document equally well. Solving this problem means matching each word in a document with the best tags and vice versa. This paper presents Labeled LDA, a topic model that adjusts Latent Dirichlet Allocation (a type of statistical model) by linking each hidden topic to user tags directly. This helps Labeled LDA to learn how words and tags are related. We show how Labeled LDA is better at expressing ideas than traditional LDA using visual examples from a collection of tagged web pages from del.icio.us. Labeled LDA is more than three times better than SVMs (support vector machines, a type of algorithm) at finding document parts related to specific tags. As a tool for sorting text with multiple labels, our model competes well with a strong baseline on various sets of data. L-LDA takes standard LDA and adds guidance for specific target categories, and the process involves a second observed factor, meaning each document is clearly marked with a target category.	expectation semiring
144	Fast Cheap and Creative: Evaluating Translation Quality Using Amazon&rsquo;s Mechanical Turk Manual evaluation of translation quality is generally thought to be excessively time consuming and expensive. We explore a fast and inexpensive way of doing it using Amazon’s Mechanical Turk to pay small sums to a large number of non-expert annotators. For $10 we redundantly recreate judgments from a WMT08 translation task. We find that when combined non-expert judgments have a high-level of agreement with the existing gold-standard judgments of machine translation quality, and correlate more strongly with expert judgments than Bleu does. We go on to show that Mechanical Turk can be used to calculate human-mediated translation edit rate (HTER), to conduct reading comprehension experiments with machine translation, and to create high quality reference translations. We find that lazy annotators tended to stay longer and do more annotations. We treat evaluation as a weighted voting problem where each annotator's contribution is weighted by agreement with either a gold standard or with other annotators. We show the effectiveness of crowd sourcing as a method of accomplishing labor intensive natural language processing tasks.	Fast Cheap and Creative: Evaluating Translation Quality Using Amazon’s Mechanical Turk Manual evaluation of translation quality is often seen as very time-consuming and costly. We look into a quick and cheap method using Amazon’s Mechanical Turk, where we pay small amounts of money to many non-experts to rate translations. For $10, we recreated the evaluations from a WMT08 translation task. We discovered that combined ratings from non-experts matched well with the top-quality standards and were more similar to expert opinions than Bleu (a common translation evaluation metric). We also show that Mechanical Turk can be used to calculate human-mediated translation edit rate (HTER), do reading comprehension tests with machine translation, and produce high-quality reference translations. We noticed that less careful raters tended to stay longer and do more ratings. We handle evaluation like a weighted voting problem, where each rater's input is given importance based on how much they agree with a high standard or with other raters. We demonstrate the success of using a large group of people (crowdsourcing) to complete demanding language tasks.	bookmarking
145	An Empirical Study of Semi-supervised Structured Conditional Models for Dependency Parsing This paper describes an empirical study of high-performance dependency parsers based on a semi-supervised learning approach. We describe an extension of semi-supervised structured conditional models (SS-SCMs) to the dependency parsing problem, whose framework is originally proposed in (Suzuki and Isozaki, 2008). Moreover, we introduce two extensions related to dependency parsing: The first extension is to combine SS-SCMs with another semi-supervised approach, described in (Koo et al., 2008). The second extension is to apply the approach to second-order parsing models, such as those described in (Carreras, 2007), using a two-stage semi-supervised learning approach. We demonstrate the effectiveness of our proposed methods on dependency parsing experiments using two widely used test collections: the Penn Treebank for English, and the Prague Dependency Treebank for Czech. Our best results on test data in the above datasets achieve 93.79% parent-prediction accuracy for English, and 88.05% for Czech. We present a very effective semi-supervised approach in which features from multiple generative models estimated on unlabeled data are combined in a discriminative system for structured prediction.	An Empirical Study of Semi-supervised Structured Conditional Models for Dependency Parsing This paper describes a practical study of high-performing tools for analyzing sentence structure using a semi-supervised learning method (a way of teaching computers using a mix of labeled and unlabeled data). We explain how we expanded a method called semi-supervised structured conditional models (SS-SCMs) to tackle the problem of dependency parsing (analyzing how words in a sentence relate to each other), originally proposed by researchers Suzuki and Isozaki in 2008. We also introduce two new ways to enhance dependency parsing: The first is by combining SS-SCMs with another semi-supervised method, as explained by Koo and others in 2008. The second improvement applies this method to advanced parsing models, such as those described by Carreras in 2007, using a two-step semi-supervised learning method. We show how effective our new methods are by testing them on two well-known sets of data: the Penn Treebank for English and the Prague Dependency Treebank for Czech. Our best results on these tests show 93.79% accuracy for predicting word relationships in English and 88.05% for Czech. We introduce a highly effective semi-supervised method that combines features from various models trained on unlabeled data into a system that makes structured predictions.	Quality
146	Parser Adaptation and Projection with Quasi-Synchronous Grammar Features We connect two scenarios in structured learning: adapting a parser trained on one corpus to another annotation style, and projecting syntactic annotations from one language to another. We propose quasi-synchronous grammar (QG) features for these structured learning tasks. That is, we score an aligned pair of source and target trees based on local features of the trees and the alignment. Our quasi-synchronous model assigns positive probability to any alignment of any trees, in contrast to asynchronous grammar, which would insist on some form of structural parallelism. In monolingual dependency parser adaptation, we achieve high accuracy in translating among multiple annotation styles for the same sentence. On the more difficult problem of cross-lingual parser projection, we learn a dependency parser for a target language by using bilingual text, an English parser, and automatic word alignments. Our experiments show that unsupervised QG projection improves on parses trained using only high-precision projected annotations and far outperforms, by more than 35% absolute dependency accuracy, learning an unsupervised parser from raw target-language text alone. When a few target-language parse trees are available, projection gives a boost equivalent to doubling the number of target-language trees. We think of cross-language adaptation as unsupervised projection using word aligned parallel text to construct training material for the target language.	Parser Adaptation and Projection with Quasi-Synchronous Grammar Features We link two learning situations: changing a parser (a tool for understanding sentences) trained on one set of rules to work with a different set of rules, and transferring sentence structures from one language to another. We introduce quasi-synchronous grammar (QG) features, which means we look at pairs of sentences from two different languages and score them based on specific parts of the sentences and how they match. Unlike other methods that require strict matching, our model allows any kind of match between sentences. In adapting to different styles within the same language, we get very accurate results when changing how the same sentence is understood. For the harder task of using one language to help understand another, we create a sentence-understanding tool for a new language by using texts that have been translated, an English sentence tool, and automatic word matches. Our tests show that our method, which doesn’t need direct guidance, does better than methods that only use very precise matches and is much more accurate by over 35% when compared to just using raw text in the new language. When we have a few examples from the new language, our method improves results as if we had twice as many examples. We view adapting across languages as using word-matched translated text to help train tools for the new language without needing direct guidance.	Empirical
147	Polylingual Topic Models Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts. Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages. We introduce a polylingual topic model that discovers topics aligned across multiple languages. We explore the model’s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages. We retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations. We show that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) does not degrade significantly. We extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles). Polylingual topic models learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic.	Polylingual Topic Models Topic models are tools that help analyze large collections of text, but until now, they have been used mostly for texts in one language or sometimes two. However, with huge collections of linked documents in many languages, like Wikipedia, there's a need for tools that can understand content in several languages. We introduce a polylingual topic model that finds topics common across different languages. We test the model using two large text groups, each with texts in over ten languages, and show that it helps in translating languages and tracking topic changes between languages. We find possible translations by choosing a few likely words in both languages and combining them for each topic to create translation options. We show that as long as the number of related documents to unrelated ones is more than 0.25, the topic patterns (measured by how similar they are using a method called Jensen-Shannon Divergence) don’t get worse by much. We expand the original idea of LDA (a type of topic model) to work for many languages, both for texts that are exactly the same (like legal documents) and somewhat similar texts (like Wikipedia articles). Polylingual topic models learn about topics in different languages, creating groups of language-specific word arrangements for each topic.	Unlike other
148	Web-Scale Distributional Similarity and Entity Set Expansion Computing the pairwise semantic similarity between all words on the Web is a computationally challenging task. Parallelization and optimizations are necessary. We propose a highly scalable implementation based on distributional similarity, implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web. The pairwise similarity between 500 million terms is computed in 50 hours using 200 quadcore nodes. We apply the learned similarity matrix to the task of automatic set expansion and present a large empirical study to quantify the effect on expansion performance of corpus size, corpus quality, seed composition and seed size. We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia. Our DASH stores the case for each phrase in Wikipedia. We find that 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found. Given the seeds set S, a seeds centroid vector is produced using the surrounding word contexts of all occurrences of all the seeds in the corpus.	Web-Scale Distributional Similarity and Entity Set Expansion Computing how similar all words are to each other on the Web is very difficult for computers. We need to use methods that allow computers to work faster and smarter. We suggest a way that can handle lots of data, using a method that looks at how words are used similarly. This is done with a system called MapReduce, which can handle huge amounts of data, like 200 billion words from the Web. We compare how similar 500 million words are in 50 hours using 200 powerful computers. We use this information to automatically grow sets of related items and show a study that measures how different factors affect this process, like the size and quality of the data, and the number and type of examples we start with. We share a tool for testing this process that includes many different groups of items from Wikipedia. Our DASH system keeps track of each phrase in Wikipedia. We find that starting with 10 to 20 examples is enough in our method to find many new correct examples. From these starting examples, we create a main example profile using the words that appear around them in the data.	similar
149	Supervised Models for Coreference Resolution Traditional learning-based coreference resolvers operate by training a mention-pair classifier for determining whether two mentions are coreferent or not. Two independent lines of recent research have attempted to improve these mention-pair classifiers, one by learning a mention-ranking model to rank preceding mentions for a given anaphor, and the other by training an entity-mention classifier to determine whether a preceding cluster is coreferent with a given mention. We propose a cluster-ranking approach to coreference resolution that combines the strengths of mention rankers and entity-mention models. We additionally show how our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution. Experimental results on the ACE data sets demonstrate its superior performance to competing approaches. In each query we include a null-cluster instance, to allow joint learning of discourse-new detection. We show that the CR model is stronger than the MP model. Our cluster ranking model proceeds in a left-to-right fashion and adds the current discourse old mention to the highest scoring preceding cluster.	Supervised Models for Coreference Resolution Traditional learning-based coreference resolvers work by teaching a system to identify if two words or phrases (called mentions) refer to the same thing. Recently, two separate research methods have tried to improve these systems: one by ranking earlier mentions to find the right one for a reference (anaphor), and the other by checking if a group of previous mentions (a cluster) matches a current mention. We suggest a new method that combines the best of both these approaches. Our method also helps in identifying new references in a text at the same time as solving coreferences. Tests on the ACE data sets show that our method performs better compared to others. During each query, we include an option that assumes no previous mention, which helps in learning to detect new references. We demonstrate that our new model (CR) is better than the old one (MP). Our model works by looking at the text from left to right and adds the current mention to the most relevant previous group.	profile
150	Simple Coreference Resolution with Rich Syntactic and Semantic Features Coreference systems are driven by syntactic, semantic, and discourse constraints. We present a simple approach which completely modularizes these three aspects. In contrast to much current work, which focuses on learning and on the discourse component, our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large, unlabeled corpus. Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and most supervised ones. Primary contributions include (1) the presentation of a simple-to-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems). We show that coreference errors in state-of-the art systems are frequently due to poor models of semantic compatibility. In our SYN-CONSTR setting, each referring mention is coreferent with any past mention with the same head or in a deterministic syntactic configuration (appositives or predicative nominatives constructions). When searching for an antecedent for mk, its candidate antecedents are visited in an order determined by their positions in the associated parse tree.	Simple Coreference Resolution with Rich Syntactic and Semantic Features Coreference systems rely on rules about sentence structure (syntactic), meaning (semantic), and conversation flow (discourse constraints). We introduce a straightforward method that separates these three areas completely. Unlike many current methods that focus on learning and conversation flow (discourse), our approach is predictable (deterministic) and based solely on how well the sentence structure and meaning match, learned from a large collection of text without labels (unlabeled corpus). Even though it's simple and not focused on conversation flow, our system performs much better than all systems that don't rely on labeled data (unsupervised) and better than most that do (supervised). Our main contributions are (1) offering an easy-to-copy, high-performing starting point and (2) showing that most mistakes are due to sentence structure and meaning issues not related to linking references (coreference) and might be better fixed by systems not designed for coreference. We demonstrate that top systems often make mistakes because they don't handle meaning matching well. In our SYN-CONSTR setup, each reference connects with any earlier mention with the same main word (head) or in a predictable sentence structure (like appositives or sentences that rename the subject). When looking for a previous reference (antecedent) for a mention (mk), possible matches are checked based on their order in the sentence diagram (parse tree).	learning
151	Bilingually-Constrained (Monolingual) Shift-Reduce Parsing Jointly parsing two languages has been shown to improve accuracies on either or both sides. However, its search space is much bigger than the monolingual case, forcing existing approaches to employ complicated modeling and crude approximations. Here we propose a much simpler alternative, bilingually-constrained monolingual parsing, where a source-language parser learns to exploit reorderings as additional observation, but not bothering to build the target-side tree as well. We show specifically how to enhance a shift-reduce dependency parser with alignment features to resolve shift-reduce conflicts. Experiments on the bilingual portion of Chinese Treebank show that, with just 3 bilingual features, we can improve parsing accuracies by 0.6% (absolute) for both English and Chinese over a state-of-the-art baseline, with negligible (∼6%) efficiency overhead, thus much faster than biparsing. We keep the probabilities of a natural rule unchanged and set those of a virtual rule to 1. We improve English prepositional phrase attachment using features from an unparsed Chinese sentence.	Bilingually-Constrained (Monolingual) Shift-Reduce Parsing Jointly parsing two languages has been shown to improve accuracies on either or both sides. However, its search space is much bigger than the monolingual case, forcing existing approaches to use complex models and rough estimates. Here we propose a much simpler alternative, bilingually-constrained monolingual parsing, where a parser for the source language learns to use reorderings as extra information, but does not build the tree for the target language. We show specifically how to improve a shift-reduce dependency parser with alignment features to solve shift-reduce conflicts. Experiments on the bilingual part of the Chinese Treebank show that, with just 3 bilingual features, we can improve parsing accuracies by 0.6% (absolute) for both English and Chinese compared to a leading baseline, with very little (around 6%) extra work, making it much faster than parsing both languages together. We keep the chances of a natural rule the same and set those of a virtual rule to 1. We improve English prepositional phrase attachment using features from an unparsed Chinese sentence.	subject
152	Phrase Dependency Parsing for Opinion Mining In this paper, we present a novel approach for mining opinions from product reviews, where it converts opinion mining task to identify product features, expressions of opinions and relations between them. By taking advantage of the observation that a lot of product features are phrases, a concept of phrase dependency parsing is introduced, which extends traditional dependency parsing to phrase level. This concept is then implemented for extracting relations between product features and expressions of opinions. Experimental evaluations show that the mining task can benefit from phrase dependency parsing. We utilize the dependency parser to extract the noun phrases and verb phrases from the reviews as the aspect candidates. For a monolingual task, we use a shallow parser to convert lexical dependencies from a dependency parser into phrase dependencies.	Phrase Dependency Parsing for Opinion Mining In this paper, we introduce a new method for finding opinions in product reviews. This method changes the task of opinion mining into finding product features, opinion expressions, and how they are connected. Noting that many product features are phrases, we introduce "phrase dependency parsing," which expands regular dependency parsing to include phrases. This idea is used to find connections between product features and opinion expressions. Tests show that using phrase dependency parsing improves the opinion mining task. We use the dependency parser to pick out noun phrases (nouns and related words) and verb phrases (verbs and related words) from reviews as possible aspects. For tasks involving one language, we use a simple parser to turn word-level dependencies from a dependency parser into phrase-level dependencies.	approaches
153	On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing This paper introduces dual decomposition as a framework for deriving inference algorithms for NLP problems. The approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. The approach provably solves a linear programming (LP) relaxation of the global inference problem. It leads to algorithms that are simple, in that they use existing decoding algorithms; efficient, in that they avoid exact algorithms for the full model; and often exact, in that empirically they often recover the correct solution in spite of using an LP relaxation. We give experimental results on two problems: 1) the combination of two lexicalized parsing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger. We use the highest scoring output of the parsing submodel over all iterations.	On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing This paper introduces dual decomposition as a method for creating problem-solving algorithms for NLP (Natural Language Processing). The approach uses well-known dynamic-programming (a method for solving problems by breaking them down into simpler sub-problems) algorithms as tools for solving smaller parts of the problem, along with an easy way to make sure these tools agree with each other. This method can solve a simplified version of a bigger problem called a linear programming (LP) relaxation. It results in algorithms that are easy to use because they rely on existing problem-solving methods; efficient because they avoid using complete methods for the entire model; and often correct, because in practice, they frequently find the right solution despite using an LP relaxation. We show test results on two issues: 1) combining two detailed sentence structure models; and 2) combining a detailed sentence structure model with a three-word sequence part-of-speech tagger (a tool that labels words with their roles, like noun or verb). We use the highest scoring result from the sentence structure submodel in all attempts.	expressions
154	Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not. This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure. We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines. We rank the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models. We apply linear interpolation to combine the instance weighted out-of-domain model with an in-domain model. We propose a method for machine translation that uses features to capture degrees of generality.	Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation We explain a new way to improve Statistical Machine Translation (SMT) by giving importance to phrases from other topics based on how relevant they are to the target topic. This relevance is judged by how similar they are to the target topic and whether they are common language or not. This method builds on earlier work by adding more detail and focusing on specific examples rather than large parts of text, while also using a simpler way to train the system. We add this technique of giving importance to examples into a system that mixes different models, and we find it consistently improves results compared to many standard methods. We order the sentence pairs in a general language collection based on scores that measure how well the sentences fit with the specific topic language. We then use a simple method to combine the phrases from other topics with the specific topic model. We suggest a method for translating languages that uses features to show how general or specific they are.	combining
155	A Multi-Pass Sieve for Coreference Resolution Most coreference resolution models determine if two mentions are coreferent using a single function over a set of constraints or features. This approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones. To overcome this problem, we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference models one at a time from highest to lowest precision. Each tier builds on the previous tier’s entity cluster output. Further, our model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster. This cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time. The framework is highly modular: new coreference modules can be plugged in without any change to the other modules. In spite of its simplicity, our approach outperforms many state-of-the-art supervised and unsupervised models on several standard corpora. This suggests that sieve-based approaches could be applied to other NLP tasks. Our rule based model obtains competitive result with less time. The candidate antecedents for the pronoun are ordered based on a notion of discourse salience that favors syntactic salience and document proximity. We develop accurate unsupervised systems that exploit simple but robust linguistic principles.	A Multi-Pass Sieve for Coreference Resolution Most models for coreference resolution, which is figuring out if two words refer to the same thing, use one method with rules or features to decide. This can cause mistakes because the less accurate features often overshadow the fewer, more accurate ones. To fix this, we suggest a simple system called a sieve, which uses different layers of coreference models, starting from the most accurate to the least. Each layer improves on the results from before. Also, our model spreads overall information by sharing details like gender and number among words in the same group. This careful sieve makes sure stronger features are prioritized over weaker ones and each decision uses all the available information. The system is very flexible: new parts can be added without changing the rest. Despite being simple, our method performs better than many advanced models on several standard tests. This shows sieve-based methods might work for other tasks in language processing. Our rule-based model gives competitive results quickly. We organize potential matches for pronouns based on how noticeable they are in the text, focusing on sentence structure and closeness in the document. We create accurate unsupervised systems that use basic but strong language rules.	language
156	Nouns are Vectors Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space We propose an approach to adjective-noun composition (AN) for corpus-based distributional semantics that, building on insights from theoretical linguistics, represents nouns as vectors and adjectives as data-induced (linear) functions (encoded as matrices) over nominal vectors. Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training. A small post-hoc analysis further suggests that, when the model-generated AN vector is not similar to the corpus-observed AN vector, this is due to anomalies in the latter. We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task. We find that the mult method can be expected to perform better in the original, non reduced semantic space because the SVD dimensions can have negative values, leading to counter intuitive results with component-wise multiplication (multiplying large opposite-sign values results in large negative values instead of being cancelled out). The adjective-specific linear map (alm) model performed far better than add and mult in approximating the correct vectors for unseen ANs, while on this (in a sense, more meta linguistic) task add and mult work better, while alm is successful only in the more sophisticated measure of neighbor density.	Nouns are Vectors Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space We suggest a method for understanding how adjectives and nouns combine in language using a technique called distributional semantics, which is based on analyzing large collections of text (corpus). In simple terms, we represent nouns as points in space (vectors) and adjectives as processes (functions) that change these points, which are shown as grids of numbers (matrices). Our model works better than others at predicting combinations of adjectives and nouns that it hasn’t seen before. A small follow-up analysis shows that if our model’s prediction is different from what is found in the text collection, it’s often because of unusual examples in the text. We also show that our method offers two new ways to understand what adjectives mean, which are better than the traditional method based on how often words appear together in text. We find that one method, called "mult," works better in the original setup because some mathematical adjustments (SVD) can cause confusing results when multiplying numbers. The "alm" model, which is specific to each adjective, was much better than two other methods ("add" and "mult") at guessing the right combinations of unseen adjective-noun pairs. However, for a different task that looks at the general relationship between words, add and mult were more effective, while alm was successful only when using a more complex method of measuring how closely words are grouped together.	coreference
157	Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning. Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method. The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences. We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations. We present an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method. We initialise lexical weights in their learning algorithm using corpus-wide alignment statistics across words and meaning elements.	Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification This paper focuses on how to teach a system to convert sentences into logical forms, using examples that pair sentences with their logical meanings. Previous methods were specific to certain languages or meanings; we introduce a more universal technique. Our method creates a probabilistic CCG grammar, which captures the meaning of words and shows how these meanings can be joined to understand full sentences. We use a technique called higher-order unification to define a range of possible grammars that match the examples provided. We also create an online learning method that efficiently explores this range while determining the best parameters for a model that interprets sentences. Tests show high accuracy on standard data sets in four languages with two different meanings. We offer a method for learning that works across languages, replacing custom templates with a universal method to learn word meanings. We start by setting initial weights for learning using overall alignment statistics between words and their meanings.	Adjective
158	Using Universal Linguistic Knowledge to Guide Grammar Induction We present an approach to grammar induction that utilizes syntactic universals to improve dependency parsing across a range of languages. Our method uses a single set of manually-specified language-independent rules that identify syntactic dependencies between pairs of syntactic categories that commonly occur across languages. During inference of the probabilistic model, we use posterior expectation constraints to require that a minimum proportion of the dependencies we infer be instances of these rules. We also automatically refine the syntactic categories given in our coarsely tagged input. Across six languages our approach outperforms state-of-the-art unsupervised methods by a significant margin. Our system is  weakly supervised, in which manually defined universal syntactic rules (USR) are used to constrain a probabilistic Bayesian model.	Using Universal Linguistic Knowledge to Guide Grammar Induction We present a method for teaching computers to understand grammar by using common grammar rules found in many languages to help improve how well they can identify sentence structure in different languages. Our method uses one set of general rules that point out relationships between types of words that are common in most languages. When the computer tries to learn these relationships, we make sure that a certain amount of the connections it finds matches our rules. We also improve the basic word categories we start with automatically. In tests with six languages, our method works better than the best existing methods that don't use prior information. Our system uses a little bit of human guidance, where we use predefined universal grammar rules to guide a statistical model.	determining
159	A Latent Variable Model for Geographic Lexical Variation The rapid growth of geotagged social media raises new computational possibilities for investigating geographic linguistic variation. In this paper, we present a multi-level generative model that reasons jointly about latent topics and geographical regions. High-level topics such as “sports” or “entertainment” are rendered differently in each geographic region, revealing topic-specific regional distinctions. Applied to a new dataset of geotagged microblogs, our model recovers coherent topics and their regional variants, while identifying geographic areas of linguistic consistency. The model also enables prediction of an author’s geographic location from raw text, outperforming both text regression and supervised topic models. We gathered the text and geographical locations of 9,250 microbloggers on the website twitter.com to construct a dataset. We collected about 380,000 tweets from Twitter's official API. We predict locations based on Gaussian distributions over the earth's surface as part of a hierarchical Bayesian model. We consider all tweets of a user concatenated as a single document, and use the earliest collected GPS-assigned location as the gold location.	A Latent Variable Model for Geographic Lexical Variation The rapid increase in social media posts that include location data (geotagged) opens up new ways to study how language changes in different places. In this paper, we introduce a detailed model that looks at hidden themes and locations at the same time. Big themes like “sports” or “entertainment” are expressed differently depending on the area, showing unique regional features for each topic. When used on a new collection of geotagged short messages (microblogs), our model finds clear topics and their local versions, while also highlighting areas where language is similar. The model can also guess where a writer is located just from their text, doing better than other methods that use direct calculations or guided topic analysis. We collected texts and locations of 9,250 people posting short messages (microbloggers) on Twitter to create a dataset. We gathered around 380,000 short messages (tweets) from Twitter's official data tool (API). We estimate locations using a statistical method (Gaussian distributions) over the earth's surface as part of a structured (hierarchical) statistical approach (Bayesian model). We treat all of a user's tweets as one document and use the first GPS-based location as the correct location.	teaching
160	Dual Decomposition for Parsing with Non-Projective Head Automata This paper introduces algorithms for non-projective parsing based on dual decomposition. We focus on parsing algorithms for non-projective head automata, a generalization of head-automata models to non-projective structures. The dual decomposition algorithms are simple and efficient, relying on standard dynamic programming and minimum spanning tree algorithms. They provably solve an LP relaxation of the non-projective parsing problem. Empirically the LP relaxation is very often tight: for many languages, exact solutions are achieved on over 98% of test sentences. The accuracy of our models is higher than previous work on a broad range of datasets. We consider third-order features such as grand-siblings and tri-siblings.	Dual Decomposition for Parsing with Non-Projective Head Automata This paper introduces methods for non-projective parsing (a way of analyzing sentence structure) using dual decomposition (a strategy to break down complex problems). We focus on parsing methods for non-projective head automata, which extend head-automata models to work with more complex sentence structures. The dual decomposition methods are straightforward and effective, using common techniques like dynamic programming (a method for solving problems by breaking them down into simpler steps) and minimum spanning tree algorithms (a way to connect points with the shortest path). They are proven to solve a simplified version of the non-projective parsing problem. In practice, this simplified version usually works well: for many languages, it finds exact solutions for over 98% of test sentences. The accuracy of our models is better than past research across many types of data. We also look at more detailed features like grand-siblings and tri-siblings (relationships between words in a sentence).	analysis
161	Multi-Source Transfer of Delexicalized Dependency Parsers We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-the-art performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. We show that part-of-speech tags contain significant amounts of information for unlabeled dependency parsing. We demonstrate an alternative to grammar induction by projecting reference parse trees from languages that have annotations to ones that are resource-poor. Tree banks in other languages can still serve as a kind of proxy for learning which features generally transfer useful in formation. We demonstrate that projecting from a single oracle chosen language can lead to good parsing performance.	Multi-Source Transfer of Delexicalized Dependency Parsers We present a simple way to move dependency parsers, which help understand sentence structure, from languages with labeled data to those without. We show that delexicalized parsers, which don't rely on specific words, can be moved between languages, leading to better accuracy than unsupervised parsers, which learn without labeled data. We use a learning method driven by rules from similar texts in different languages to build the final parser. Unlike past methods, we show that using multiple source languages can greatly improve the quality of the parsers. Our system's parsers perform the best when compared to other systems that don't use labels or use projected techniques in eight different languages. We show that part-of-speech tags, which label words as nouns, verbs, etc., hold a lot of information for parsing in languages without labeled data. We offer another way to create grammar by transferring parse trees, which show how sentences are structured, from languages with data to those without. Language data from other languages can still help learn useful features. We show that choosing one well-matched language to transfer from can lead to good parsing results.	decomposition methods
167	Named Entity Recognition in Tweets: An Experimental Study People tweet more than 100 Million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner. The performance of standard NLP tools is severely degraded on tweets. This paper addresses this issue by re-building the NLP pipeline beginning with part-of-speech tagging, through chunking, to named-entity recognition. Our novel T-NER system doubles F1 score compared with the Stanford NER system. T-NER leverages the redundancy inherent in tweets to achieve this performance, using LabeledLDA to exploit Freebase dictionaries as a source of distant supervision. LabeledLDA outperforms co-training, increasing F1 by 25% over ten common entity types. Our NLP tools are available at: http:// github.com/aritter/twitter_nlp We use token unigrams as features, including any hash tags, but ignoring twitter mentions, URLs and purely numeric tokens. Our system exploits a CRF model to segment named entities and then uses a distantly supervised approach based on LabeledLDA to classify named entities.	Named Entity Recognition in Tweets: An Experimental Study People send over 100 million tweets every day, creating a large amount of messy, informal, but sometimes useful short messages that reflect current trends in a way never seen before. Regular language processing tools don't work well on tweets. This paper solves this problem by improving the language processing steps, starting with identifying parts of speech, then grouping words, and finally recognizing named entities (like names of people, places). Our new T-NER system performs twice as well as the Stanford NER system. T-NER takes advantage of repeated information in tweets to work better, using a technique called LabeledLDA to use Freebase dictionaries as a guide without needing direct supervision. LabeledLDA works better than another method called co-training, improving results by 25% for ten common types of entities. You can find our language processing tools at: http://github.com/aritter/twitter_nlp. We use single words as features, including hashtags, but we skip Twitter mentions, web links, and purely number-based tokens. Our system uses a model called CRF to identify named entities in text and then uses a distantly supervised method based on LabeledLDA to categorize these named entities.	applying
162	Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model’s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines. we introduce a semi-supervised approach that uses recursive autoencoders to learn the hierarchical structure and sentiment distribution of a sentence.	Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions We introduce a new machine learning method that uses recursive autoencoders to predict how people feel about sentences. Our method creates numerical representations (like summaries in numbers) for phrases with multiple words. In tasks where we guess how people feel, these representations work better than other top methods on popular datasets, like movie reviews, without needing any pre-set lists of emotional words or rules for changing opinions. We also test how well the model can guess people's feelings on a new dataset made from personal stories shared on the Experience Project website. This dataset includes personal stories labeled with various emotions, which together form a pattern showing how people react emotionally. Our algorithm predicts these emotion patterns more accurately than several other strong methods. We introduce a semi-supervised approach that uses recursive autoencoders to learn the sentence's layered structure and how feelings are spread throughout it.	accuracy
163	Domain Adaptation via Pseudo In-Domain Data Selection We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large general-domain parallel corpus that are most relevant to the target domain. These sentences may be selected with simple cross-entropy based methods, of which we present three. As these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora. These subcorpora - 1% the size of the original - can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus. Performance is further improved when we use these domain-adapted models in combination with a true in-domain model. The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining in- and general-domain systems during decoding. We improve the perplexity based approach and propose bilingual cross entropy difference as a ranking function with in- and general-domain language models. We propose a bilingual cross-entropy difference to select data from parallel corpus for domain adaptation which captures the contextual information slightly, and outperform monolingual cross-entropy difference.	Domain Adaptation via Pseudo In-Domain Data Selection We look at an effective way to improve translation software by picking sentences from a big collection of translated texts that best match the topic we’re interested in. We use simple methods based on cross-entropy, a statistical measure, to choose these sentences, and we share three such methods. Since these sentences aren't exactly like our desired topic's data, we call them "pseudo" in-domain data groups. These smaller groups, which are just 1% of the original size, can be used to train translation systems that work better than systems trained on the entire collection. The performance gets even better when we use these specialized models together with a real in-domain model. The findings show that more data isn't always better; instead, choosing the right data for the topic and combining different sources during translation gives the best results. We have improved the perplexity approach, another statistical measure, and suggest using bilingual cross-entropy difference, which considers both languages involved, to rank and select data. This method captures some context and performs better than using cross-entropy for just one language.	labeled
164	Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora We address the creation of cross-lingual textual entailment corpora by means of crowdsourcing. Our goal is to define a cheap and replicable data collection methodology that minimizes the manual work done by expert annotators, without resorting to preprocessing tools or already annotated monolingual datasets. In line with recent works emphasizing the need of large-scale annotation efforts for textual entailment, our work aims to: i) tackle the scarcity of data available to train and evaluate systems, and ii) promote the recourse to crowdsourcing as an effective way to reduce the costs of data collection without sacrificing quality. We show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators. The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German.	Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora We focus on making cross-language datasets using crowdsourcing, which means getting help from lots of people online. Our aim is to find a cheap and repeatable way to gather data that requires little work from experts, without using special tools or pre-done data in one language. Following recent studies that highlight the need for lots of data for understanding text relationships, our work seeks to: i) deal with the lack of data needed to train and test systems, and ii) encourage the use of crowdsourcing as a cost-effective method to gather data without losing quality. We demonstrate that a difficult data creation task, where even experts often disagree, can be broken down into simple tasks for non-experts. The final dataset, made through a series of tasks on Amazon Mechanical Turk, includes more than 1,600 matched pairs for each set of texts and hypotheses in English, Italian, and German.	These smaller
165	Tuning as Ranking We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chiang et al., 2008b), PRO is easy to implement. It uses off-the-shelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours. We establish PRO’s scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios. PRO casts the problem of tuning as a ranking problem between pairs of translation candidates. We optimize ranking in n-best lists, but learn parameters in an online fashion. We minimize logistic loss sampled from the merged n-bests, and sentence-BLEU is used for determining ranks.	Tuning as Ranking We present an easy, effective, and scalable method for adjusting settings in statistical machine translation, using a ranking approach based on comparing pairs (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method can handle many settings and easily work with systems that have thousands of features. Also, unlike recent methods based on the MIRA algorithm by Crammer and Singer (2003) (Watanabe et al., 2007; Chiang et al., 2008b), PRO is simple to set up. It uses ready-made software for linear binary classification and can be added to an existing MERT setup in just a few hours. We show PRO's ability to work on a large scale and its effectiveness by comparing it to MERT and MIRA, and show it works equally well on both phrase-based and syntax-based systems in different languages with large amounts of data. PRO treats the task of tuning as a ranking problem between pairs of translation options. We improve ranking in lists of top choices but learn settings as we go. We reduce logistic loss, which is a measure of prediction error, from the combined top choices, and use sentence-BLEU, a scoring method that measures translation quality, to determine ranks.	experts
166	Experimental Support for a Categorical Compositional Distributional Model of Meaning Modeling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences. Our model matches the results of its competitors in the first experiment, and betters them in the second. The general improvement in results with increase in syntactic complexity showcases the compositional power of our model. We suggest that relational word vectors live in a space the dimensionality of which be a function of the arity of the relation.	Experimental Support for a Categorical Compositional Distributional Model of Meaning Modeling how sentences mean what they do using real-world data methods has been a challenge for language experts who work with computers. We use the theoretical model by Coecke and others (2010) with data from the BNC (British National Corpus) and test it. Our approach is based on teaching a computer system to learn relationships between words without direct instruction and applying these learned relationships to the words they describe. We test this by seeing how well our model can figure out the meaning of words in simple sentences (following the work by Mitchell and Lapata, 2008) and a new test for more complex sentences. Our model performs just as well as others in the first test and does even better in the second test. The fact that our results get better with more complex sentence structures shows how well our model can understand sentence meaning. We suggest that the space where word relationships exist should depend on how many things the word relates to.	comparing
204	Recognizing Contextual Polarity In Phrase-Level Sentiment Analysis This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline. We propose supervised learning, dividing the resources into prior polarity and context polarity. Our experiments indicate that lexicon-lookup approaches to subjectivity analysis will have limited success on general texts. We manually construct polarity lexicon, in which each entry is annotated with its degree of subjectivity (strong, weak), as well as its sentiment polarity (positive, negative and neutral). Our MPQA lexicon contains separate lexicons for subjectivity clues, intensifiers and valence shifters, which are used for identifying opinion roots, modifiers and negation words.	Recognizing Contextual Polarity In Phrase-Level Sentiment Analysis This paper introduces a new method for analyzing feelings in phrases. It first checks if a phrase is neutral (without emotion) or polar (with emotion) and then figures out if the polar expression is positive or negative. With this method, the system can automatically find out the emotional context for many expressions, performing much better than simple methods. We suggest using supervised learning, which means training the system with examples, by splitting resources into initial emotion (prior polarity) and context-based emotion (context polarity). Our tests show that looking up words in a dictionary won't work well for understanding emotions in general writing. We create a list of words with their emotional levels labeled as strong or weak, and their emotion type as positive, negative, or neutral. Our MPQA list includes separate lists for clues that show emotions, words that make emotions stronger, and words that change the feeling, which help in finding the main opinion words, modifying words, and words that show opposites.	Pointwise
168	Identifying Relations for Open Information Extraction Open Information Extraction (IE) is the task of extracting assertions from massive corpora without requiring a pre-specified vocabulary. This paper shows that the output of state-of-the-art Open IE systems is rife with uninformative and incoherent extractions. To overcome these problems, we introduce two simple syntactic and lexical constraints on binary relations expressed by verbs. We implemented the constraints in the REVERB Open IE system, which more than doubles the area under the precision-recall curve relative to previous extractors such as TEXTRUNNER and WOEpos. More than 30% of REVERB’s extractions are at precision 0.8 or higher compared to virtually none for earlier systems. The paper concludes with a detailed analysis of REVERB’s errors, suggesting directions for future work. We show that verbal phrases uncover a large fraction of binary predicates while reducing the amount of noisy phrases that do not denote any relations. We develop a large scale web-based ReVerb corpus, comprising tuple extractions of predicate templates with their argument instantiations. Our ReVerb corpus is a large scale publicly available web based open extractions data set, containing about 15 million unique template extractions, automatically extracted from the ClueWeb09 web crawl.	Identifying Relations for Open Information Extraction Open Information Extraction (IE) is about taking statements from large collections of text without needing a specific set of words in advance. This paper explains that current Open IE systems often produce results that are not clear or useful. To fix these issues, we introduce two simple rules based on sentence structure and word choice for connections made by verbs. We added these rules to the REVERB Open IE system, which significantly improves accuracy compared to older systems like TEXTRUNNER and WOEpos. Over 30% of REVERB's results are very accurate (at least 80% correct), while earlier systems had almost none at this level. The paper ends with a detailed look at REVERB's mistakes and suggests ways to improve. We demonstrate that using action phrases reveals many connections between two things while cutting down on irrelevant phrases that don't show any real links. We created a large online collection called the ReVerb corpus, which includes extracted templates of connections and their specific examples. Our ReVerb corpus is a big public data set available on the web, with about 15 million unique extractions, automatically taken from the ClueWeb09 web archive.	sometimes useful
169	A Comparison of Vector-based Representations for Semantic Composition In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods. We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication. Some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora. We find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection. The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method. We compute a weighted linear combination of the embeddings for words that appear in the document to be classified. We compare count and predict representations as input to composition functions. For paraphrase detection, we use cosine similarity between sentence pairs together with two shallow similarity cues: word overlap between the two sentences and difference in sentence length. Add and mult attained the top performance with the simple models for both figures of merit.	A Comparison of Vector-based Representations for Semantic Composition In this paper, we tackle the challenge of figuring out the combined meaning of phrases and sentences using methods that analyze word usage in large text collections. We try different ways of representing and combining words, with some methods being simple and others being more complex and requiring learning from large text collections or understanding sentence structure. We discover that simple methods work just as well as more complex ones for two specific tasks: (1) checking how similar phrases are, and (2) identifying if sentences mean the same thing in different words. The amount of text used for training and the size of word representations are less important than how well the method of combining words matches the way their meanings are represented. We calculate a weighted sum of word meanings for words in a document to classify it. We compare two types of word representations as inputs for combining them. For identifying similar meaning sentences, we use a measure called cosine similarity between pairs of sentences, along with two simple tricks: checking how many words the sentences share and the difference in their lengths. The "Add" and "Mult" methods performed the best with these simple models for both tasks.	older systems
170	A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins. We present a transition-based system for joint part-of-speech tagging and labeled dependency parsing with non-projective trees. Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-the-art results for all languages. we introduce a transition-based system that jointly performed POS tagging and dependency parsing.	A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing Most current dependency parsers assume that input words have been clarified using a part-of-speech tagger before they start working. We present a transition-based system that does both part-of-speech tagging (identifying the role of each word in a sentence) and labeled dependency parsing (analyzing the sentence structure) with non-projective trees (a flexible tree structure). Tests on Chinese, Czech, English, and German show consistent improvements in both tagging and parsing accuracy compared to a step-by-step system, leading to better results for all languages. We introduce a transition-based system that performs both POS tagging and dependency parsing together.	classify
171	An Efficient Implementation Of A New DOP Model Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank. This paper proposes an integration of the two models which outperforms each of them separately. Together with a PCFG-reduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank. Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence. We note that it is the highest ranking parse, not derivation, that is desired. We show that DOP models that select the preferred parse of a test sentence using the shortest derivation criterion perform very well. We redress subtree probabilit by a simple correction factor.	An Efficient Implementation Of A New DOP Model Two seemingly opposite DOP models are discussed in studies: one finds the parse tree (structure of a sentence) with the most common subtrees from a collection of tree patterns, and the other finds the parse tree with the fewest subtrees. This paper suggests combining these two models to perform better than each one alone. By also simplifying DOP with a PCFG (a grammar-based technique), we get better accuracy and speed with the Wall Street Journal tree data. Our findings show an 11% lower error rate than earlier models, with an average processing time of 3.6 seconds for each WSJ sentence. We emphasize that the best parse (sentence structure), not the way it is created, is what we aim for. We demonstrate that DOP models that choose the best parse using the shortest creation method work very well. We adjust subtree probability with a simple correction factor.	tagging
177	Computing Consensus Translation For Multiple Machine Translation Systems Using Enhanced Hypothesis Alignment This paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation (MT) systems. The outputs are combined and a possibly new translation hypothesis can be generated. Similarly to the well-established ROVER approach of (Fiscus, 1997) for combining speech recognition hypotheses, the consensus translation is computed by voting on a confusion network. To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering. The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment. The proposed alignment and voting approach was evaluated on several machine translation tasks, including a large vocabulary task. The method was also tested in the framework of multi-source and speech translation. On all tasks and conditions, we achieved significant improvements in translation quality, increasing e.g. the BLEU score by as much as 15% relative. We align synonyms and different morphological forms of words to each other but this is done implicitly, relying on the parallel text to learn word alignments. We use pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model. Different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003). We propose using a statistical word alignment algorithm as a more robust way of aligning (monolingual) outputs into a confusion network for system combination.	Computing Consensus Translation For Multiple Machine Translation Systems Using Enhanced Hypothesis Alignment This paper talks about a new way to find a common translation from the results of different machine translation (MT) systems. The results are mixed together to possibly create a new translation option. Like the well-known ROVER approach by Fiscus in 1997, which combines speech recognition attempts, the common translation is figured out by voting on a confusion network, which is a system that helps organize different options. To make this network, we match words from the original machine translation attempts using a better method that specifically deals with words being in different orders. Instead of just looking at one sentence, we look at the whole document to help with matching words. The new method of matching and voting was tested on many translation tasks, including one with a big vocabulary. It was also tested with translations from different sources and speech translations. In all tests, we saw big improvements in translation quality, like increasing the BLEU score, which measures translation quality, by up to 15%. We match similar words and different word forms to each other by learning from texts in different languages. We use pair-by-pair matching methods based on balanced matches from an HMM (Hidden Markov Model) alignment model. We consider different word orders by training match models using all pairs of attempts as a parallel text group with GIZA++ (a tool for aligning words). We suggest using a statistical word matching method as a stronger way to line up (same language) outputs into a confusion network to combine systems.	encyclopedia
172	Bootstrapping Statistical Parsers From Small Datasets We present a practical co-training method for bootstrapping statistical parsers using a small amount of manually parsed training material and a much larger pool of raw sentences. Experimental results show that unlabelled sentences can be used to improve the performance of statistical parsers. In addition, we consider the problem of bootstrapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material. We show that bootstrapping continues to be useful, even though no manually produced parses from the target domain are used. We examine self-training for PCFG parsing in the small seed case (< 1k labeled data). We report either minor improvements or significant damage from using self-training for parsing. We find degradation using a lexicalized tree adjoining grammar parser and minor improvement using Collins lexicalized PCFG parser; however, this gain was obtained only when the parser was trained on a small labeled set.	Bootstrapping Statistical Parsers From Small Datasets We introduce a useful method called co-training for improving statistical parsers, which are tools that help computers understand sentences. This method uses a small set of sentences that have been manually analyzed and a larger set of plain sentences that haven’t been analyzed yet. Tests show that these plain sentences can help make the parsers work better. We also look into the challenge of improving parsers when the manually analyzed sentences come from a different area or topic than the plain sentences or the sentences we test on. We demonstrate that this method is still helpful even if no analyzed sentences from the specific area are used. We explore a method called self-training for a specific type of parsing (PCFG) when starting with a very small amount of data (less than 1,000 analyzed sentences). Our results show either slight improvements or noticeable problems when using self-training for parsing. We observe that a certain parser (lexicalized tree adjoining grammar parser) performs worse, while another type (Collins lexicalized PCFG parser) shows a small improvement, but only when it was trained with a small set of analyzed sentences.	adjust subtree
173	Combining Distributional And Morphological Information For Part Of Speech Induction In this paper we discuss algorithms for clustering words into classes from unlabelled text using unsupervised algorithms, based on distributional and morphological information. We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages. We propose a perplexity based test for the quality of the POS induction algorithm. We find that many-to-1 accuracy has several defects.	Combining Distributional And Morphological Information For Part Of Speech Induction In this paper we discuss methods for grouping words into categories from text that hasn't been labeled, using algorithms that don't need prior training, based on how words are used (distributional information) and their structure (morphological information). We show how using word structure information can boost accuracy, especially for uncommon words, and that this works well for many languages. We suggest using a complexity test to judge how good the part of speech (POS) grouping method is. We discover that measuring accuracy by matching many words to one category has some problems.	parsing
174	Investigating GIS And Smoothing For Maximum Entropy Taggers This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (Gis) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar. Our supertagger finds the single most probable category sequence given the sentenc and uses additional features defined in terms of the previously assigned categories.	Investigating GIS And Smoothing For Maximum Entropy Taggers This paper examines two parts of Maximum Entropy tagging: using a correction feature in the Generalised Iterative Scaling (GIS) estimation method, and ways to make models more reliable (smoothing). We show through analysis and testing that the correction feature, thought to be necessary for GIS to work correctly, is not needed. We also look into using a Gaussian prior (a mathematical method) and a simple cutoff (a limit to simplify the model) for smoothing. The experiments are done with two sets of tags: the usual Penn Treebank POS tagset and a larger set of word types from Combinatory Categorial Grammar. Our supertagger (a tool that assigns tags) finds the most likely category sequence for a sentence and uses extra features based on the categories already assigned.	morphological
175	Empirical Methods For Compound Splitting Compounded words are a challenge for NLP applications such as machine translation (MT). We introduce methods to learn splitting rules from monolingual and parallel corpora. We evaluate them against a gold standard and measure their impact on performance of statistical MT systems. Results show accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a German-English noun phrase translation task. We present a method requiring no linguistically motivated morphological analysis to split compounds. We split German compound words, based on the frequency of the words in the potential decompositions.	Empirical Methods For Compound Splitting Compounded words are tricky for NLP (Natural Language Processing) applications like machine translation (MT), which is translating text from one language to another. We introduce methods to learn splitting rules from large collections of text in one language and collections in two languages. We test these methods against a high-quality standard and see how they affect the performance of statistical MT systems. Results show a high accuracy of 99.1% and improvements in MT performance of 0.039 BLEU (a score for measuring the quality of machine-translated text) on a task translating German-English noun phrases. We present a method that doesn't need complex language structure analysis to break down compound words. We break down German compound words based on how often the words appear in possible breakdowns.	tagging
176	Using Encyclopedic Knowledge For Named Entity Disambiguation We present a new method for detecting and disambiguating named entities in open domain text. A disambiguation SVM kernel is trained to exploit the high coverage and rich structure of the knowledge encoded in an online encyclopedia. The resulting model significantly outperforms a less informed baseline. We measure similarity between the textual context of the NE mention and the Wikipedia categories of the candidate. We use context matching to link noun phrase subjects into Wikipedia.	Using Encyclopedic Knowledge For Named Entity Disambiguation We present a new way to identify and clarify the meaning of specific names or terms in general text. We train a machine learning model, called a disambiguation SVM kernel, to take advantage of the detailed and well-organized information found in an online encyclopedia. This model works much better than a simpler version with less information. We compare how similar the surrounding text of the named entity is to the Wikipedia categories of possible matches. We use context matching to connect groups of nouns into Wikipedia.	Compounded
178	Online Learning Of Approximate Dependency Parsing Algorithms In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word. We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms. We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish. We propose a second-order graph-based dependency parsing model which incorporates features from the two kinds of subtrees. We use the Viterbi decoding algorithm to achieve O (n3) parsing time. We show that non-projective dependency parsing with horizontal Markovization is FNP-hard. We define a second-order dependency parsing model in which interactions between adjacent siblings are allowed.	Online Learning Of Approximate Dependency Parsing Algorithms In this paper, we expand on the maximum spanning tree (MST) method for understanding sentence structure, originally by McDonald et al. (2005c). We include more detailed features and allow for sentence structures where words can have more than one parent. We acknowledge that these changes make the MST approach very complex and difficult to compute, but we solve this issue with new simpler parsing methods. Our experiments demonstrate that using these simpler methods with online learning techniques gives the highest accuracy for analyzing Czech and Danish languages. We introduce a model that looks at the connections between parts of a sentence in two different ways. We use a method called the Viterbi algorithm to process this in a reasonable amount of time, specifically O(n3). We find that a specific type of sentence analysis, called non-projective parsing with horizontal Markovization, is extremely challenging to solve, known as FNP-hard. We describe a model that allows interactions between nearby sibling words in a sentence.	speech recognition
179	Making Tree Kernels Practical For Natural Language Learning In recent years tree kernels have been proposed for the automatic learning of natural language applications. Unfortunately, they show (a) an inherent super linear complexity and (b) a lower accuracy than traditional attribute/value methods. In this paper, we show that tree kernels are very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods. Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis. We introduce a fast implementation of tree kernels, where a node pair set is first constructed for those associated with same production rules.	Making Tree Kernels Practical For Natural Language Learning In recent years, tree kernels have been suggested for automatically teaching computers to understand language. Unfortunately, they have (a) a complex and time-consuming process and (b) are less accurate than traditional methods that use specific features and values. In this paper, we show that tree kernels are useful for understanding language because (a) we have a simple method to compute tree kernels quickly and (b) our research on how different tree kernels classify language shows that combining them always improves on traditional methods. Tests with Support Vector Machines (a type of machine learning model) on classifying predicates and arguments (parts of sentences) support our idea. We introduce a quick way to use tree kernels, where we first create a set of node pairs that follow the same rules.	understanding sentence
180	Determining Term Subjectivity And Term Orientation For Opinion Mining Opinion mining is a recent subdiscipline of computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses. To aid the extraction of opinions from text, recent work has tackled the issue of determining the orientation of “subjective” terms contained in text, i.e. deciding whether a term that carries opinionated content has a positive or a negative connotation. This is believed to be of key importance for identifying the orientation of documents, i.e. determining whether a document expresses a positive or negative opinion about its subject matter. We contend that the plain determination of the orientation of terms is not a realistic problem, since it starts from the nonrealistic assumption that we already know whether a term is subjective or not; this would imply that a linguistic resource that marks terms as “subjective” or “objective” is available, which is usually not the case. In this paper we confront the task of deciding whether a given term has a positive connotation, or a negative connotation, or has no subjective connotation at all; this problem thus subsumes the problem of determining subjectivity and the problem of determining orientation. We tackle this problem by testing three different variants of a semi-supervised method previously proposed for orientation detection. Our results show that determining subjectivity and orientation is a much harder problem than determining orientation alone.	Determining Term Subjectivity And Term Orientation For Opinion Mining Opinion mining is a new area of study in computational linguistics, which focuses on the opinions expressed in a document rather than the topic it discusses. To help pull out opinions from text, recent studies have addressed how to find out if "subjective" terms (words showing opinions) in text have a positive or negative meaning. This is important for figuring out if a document has a positive or negative opinion about its topic. We argue that just figuring out if terms are positive or negative isn't realistic because it assumes we already know if a term is subjective (showing opinion) or not. This means we would need a resource that labels terms as "subjective" or "objective" (neutral), which usually isn't available. In this paper, we take on the challenge of deciding if a term has a positive, negative, or no opinion at all, which includes finding out both subjectivity and direction (positive or negative meaning). We address this by testing three different versions of a semi-supervised method (a method using both labeled and unlabeled data) that was suggested before for finding direction. Our results show that figuring out both subjectivity and direction is much harder than just finding the direction.	understanding language
181	Mining WordNet For A Fuzzy Sentiment: Sentiment Tag Extraction From WordNet Glosses Many of the tasks required for semantic tagging of phrases and texts rely on a list of words annotated with some semantic features. We present a method for extracting sentiment-bearing adjectives from WordNet using the Sentiment Tag Extraction Program (STEP). We did 58 STEP runs on unique non-intersecting seed lists drawn from manually annotated list of positive and negative adjectives and evaluated the results against other manually annotated lists. The 58 runs were then collapsed into a single set of 7,813 unique words. For each word we computed a Net Overlap Score by subtracting the total number of runs assigning this word a negative sentiment from the total of the runs that consider it positive. We demonstrate that Net Overlap Score can be used as a measure of the words degree of membership in the fuzzy category of sentiment: the core adjectives, which had the highest Net Overlap scores, were identified most accurately both by STEP and by human annotators, while the words on the periphery of the category had the lowest scores and were associated with low rates of inter-annotator agreement. We find that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets. Non-neutral adjectives were extracted from WordNet and assigned fuzzy sentiment category membership/centrality scores and tags. WordNet synonyms, antonyms, and glosses are used to iteratively expand a list of seeds.	Mining WordNet For A Fuzzy Sentiment: Sentiment Tag Extraction From WordNet Glosses Many tasks needed to label phrases and texts with meaning rely on a list of words marked with certain features. We introduce a way to find adjectives that show feelings from WordNet using a program called STEP. We ran STEP 58 times on different lists of words that were checked by people to be either positive or negative and compared the results to other checked lists. These 58 runs were combined into one group of 7,813 different words. For each word, we calculated a Net Overlap Score by subtracting the number of times the word was marked negative from the number of times it was marked positive. We show that the Net Overlap Score can indicate how strongly a word belongs to the fuzzy category of sentiment: main adjectives with high scores were identified accurately by STEP and humans, while words on the edges had low scores and low agreement among people. We discovered that machines can struggle to correctly label words with feelings if the words are unclear in meaning during training. Adjectives that are not neutral were taken from WordNet and given scores to show how strongly they fit into a fuzzy category of sentiment. WordNet's similar words, opposites, and definitions are used to expand a list of starting words step by step.	opinions
182	CDER: Efficient MT Evaluation Using Block Movements Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks. In many cases though such movements still result in correct or almost correct sentences. In this paper, we will present a new evaluation measure which explicitly models block reordering as an edit operation. Our measure can be exactly calculated in quadratic time. We consider edit distance for word substitution and reordering. Our CDER measure is based on edit distance, such as the well-known WER, but allows reordering of blocks.	CDER: Efficient MT Evaluation Using Block Movements Most advanced methods for judging machine translation give high penalties for moving groups of words around. However, these movements often still create correct or nearly correct sentences. In this paper, we introduce a new way to evaluate that specifically treats moving word groups as an editing step. Our method can be calculated accurately in a time that grows with the square of the number of words. We look at edit distance, which means counting changes needed for word swapping and moving. Our CDER measure is based on edit distance, like the well-known Word Error Rate (WER), but it allows for moving word groups around.	meaning during
183	Re-Evaluation The Role Of Bleu In Machine Translation Research We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric. We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu's correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores. The problems of Blue include: (1) synonyms and paraphrases are only handled if they are in the set of multiple reference translations [available]; (2) The scores for words are equally weighted so missing out on content-bearing material brings no additional penalty; (3) The brevity penalty is a stop-gap measure to compensate for the fairly serious problem of not being able to calculate recall. Blue has certain shortcomings for comparing different machine translation systems, especially if comparing conceptually different systems, e.g. phrase-based versus rule-based systems. We find that BLEU and NIST favour n-gram-based MT models such as Pharaoh (Koehn, 2004), so the translations produced by rule-based systems score lower on the automatic evaluation, even though human judges consistently rate their output higher than Pharaoh's translation.	Re-Evaluation The Role Of Bleu In Machine Translation Research We believe that people working on machine translation rely too much on the Bleu score, a tool that measures how well a machine translates. We demonstrate that getting a better Bleu score doesn't always mean that the translation is actually better, and we provide two strong examples where Bleu doesn't match what humans think is good translation. This creates opportunities for new research that was previously seen as not worth pursuing because it couldn't improve Bleu scores. The problems with Bleu include: (1) it only recognizes similar words and phrases if they are already provided in several correct translations; (2) it treats all words as equally important, so missing important content doesn't affect the score much; (3) the "brevity penalty" is a temporary fix for not being able to measure how much of the original content is actually translated. Bleu has limitations when comparing different types of machine translation systems, especially if the systems work in different ways, like those using phrases versus those following specific rules. We find that both BLEU and another metric called NIST tend to favor systems that use short word sequences, such as the Pharaoh model (Koehn, 2004). As a result, translations from rule-based systems get lower automatic scores even though human judges often think they are better than those from Pharaoh.	calculated
184	Discriminative Sentence Compression With Soft Syntactic Evidence We present a model for sentence compression that uses a discriminative large-margin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers. The parsers are trained out-of-domain and contain a significant amount of noise. We argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly. This differs from current state-of-the-art models (Knight and Marcu, 2000) that treat noisy parse trees, for both compressed and uncompressed sentences, as gold standard when calculating model parameters. We provide a Viterbi-like dynamic programming algorithm to recover the highest scoring sequence of order-preserving bigrams from a lattice, either in unconstrained form or with a specific length constraint. We use the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words. We use semi-Markov model which allows incorporating a language model for the compression.	Discriminative Sentence Compression With Soft Syntactic Evidence We introduce a method for shortening sentences that uses a learning system designed to focus on differences, combined with new tools that work on pairs of words and deep grammatical structures provided by extra tools that look at sentence structure and word relationships. These tools are trained on unrelated topics and include a lot of errors. We believe that because our learning system can focus on differences, it can adjust to any errors in the tools to improve accuracy in shortening sentences. This approach is different from the best current methods (Knight and Marcu, 2000) that consider error-filled sentence structures as perfect when setting up the model. We offer a method similar to one used for finding the best path in decision-making to find the best sequence of word pairs that keeps their order, with or without a specific length limit. We use the results from two tools (one focusing on sentence parts and another on how words depend on each other) as inputs in a model that breaks down sentences into pairs of consecutive words. We use a type of model that allows adding a language model to help with shortening sentences.	content doesn
185	Comparing Automatic And Human Evaluation Of NLG Systems We consider the evaluation problem in Natural Language Generation (NLG) and present results for evaluating several NLG systems with similar functionality, including a knowledge-based generator and several statistical systems. We compare evaluation results for these systems by human domain experts, human non-experts, and several automatic evaluation metrics, including NIST, BLEU, and ROUGE. We find that NIST scores correlate best (> 0.8) with human judgments, but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone. We conclude that automatic evaluation of NLG systems has considerable potential, in particular where high-quality reference texts and only a small number of human evaluators are available. However, in general it is probably best for automatic evaluations to be supported by human-based evaluations, or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain. We use several different evaluation techniques (human and corpus-based) to evaluate the output of five NLG systems which generated wind descriptions for weather forecasts. We demonstrate that automatic metrics can correlate highly with human ratings if the training dataset is of high quality. The two automatic metrics used in the evaluations, NIST and BLEU have been shown to correlate highly with expert judgments (Pearson correlation coefficients 0.82 and 0.79 respectively) in this domain.	Comparing Automatic And Human Evaluation Of NLG Systems We look at the challenge of evaluating Natural Language Generation (NLG) systems and share results from testing several NLG systems that do similar tasks, including one that uses knowledge to generate text and others that rely on statistics. We compare how these systems were evaluated by experts in the field, regular people who are not experts, and several automatic scoring methods, such as NIST, BLEU, and ROUGE. We discover that NIST scores align best (above 0.8) with human opinions, but all the automatic methods we checked seem to favor systems that choose words based on how often they appear. We suggest that automatic evaluation of NLG systems has a lot of promise, especially when there are high-quality example texts and only a few human reviewers. However, generally, it's probably best if automatic evaluations are backed up by human-based reviews, or at least by research that shows a particular method matches human opinions well in a certain area. We use several different evaluation methods (by humans and by text analysis) to assess the results from five NLG systems that created wind descriptions for weather forecasts. We show that automatic scoring methods can match human ratings closely if the data used for training is of high quality. The two automatic methods used, NIST and BLEU, have been shown to match expert opinions well (with Pearson correlation scores of 0.82 and 0.79 respectively) in this area.	differences
186	A Clustering Approach For Nearly Unsupervised Recognition Of Nonliteral Language In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques. TroFi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies. It also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning. We adapt a word-sense disambiguation algorithm to our task and augment it with multiple seed set learners, a voting schema, and additional features like SuperTags and extra-sentential context. Detailed experiments on hand-annotated data show that our enhanced algorithm outperforms the baseline by 24.4%. Using the TroFi algorithm, we also build the TroFi Example Base, an extensible resource of annotated literal/nonliteral examples which is freely available to the NLP research community. For scoring, Literal recall is defined as (correct literals in literal cluster/ total correct literals); Literal precision is defined as (correct literals in literal cluster/ size of literal cluster). We model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two automatically constructed seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set.	A Clustering Approach For Nearly Unsupervised Recognition Of Nonliteral Language In this paper, we introduce TroFi (Trope Finder), a system that automatically sorts verbs into literal (exact meaning) and nonliteral (figurative meaning) uses using methods that require little human guidance. TroFi relies on the context of sentences rather than complex grammatical rules or structures. It uses initial sets of literal and nonliteral examples that are collected and refined without human help to start the learning process. We modify an existing method for understanding word meanings to fit our needs and enhance it with multiple initial learning sets, a system for decision-making, and extra features like detailed grammar tags and context outside the sentence. Tests on data checked by humans show our improved method works 24.4% better than the basic one. With TroFi, we also create the TroFi Example Base, a growing resource of labeled examples of literal and nonliteral uses, which is available to researchers in language technology. For evaluation, Literal recall is calculated as (correct literal meanings found / total correct literal meanings); Literal precision is calculated as (correct literal meanings found / total literal meanings identified). We treat the task of distinguishing literal from nonliteral meanings as a problem of figuring out word meanings and use a grouping method that compares new examples to two sets created automatically (one for literal and one for nonliteral meanings), labeling the example based on the closest match.	scoring
197	Inducing Multilingual Text Analysis Tools Via Robust Projection Across Aligned Corpora This paper describes a system and set of algorithms for automatically inducing stand-alone monolingual part-of-speech taggers, base noun-phrase bracketers, named-entity taggers and morphological analyzers for an arbitrary foreign language. Case studies include French, Chinese, Czech and Spanish. Existing text analysis tools for English are applied to bilingual text corpora and their output projected onto the second language via statistically derived word alignments. Simple direct annotation projection is quite noisy, however, even with optimal alignments. Thus this paper presents noise-robust tagger, bracketer and lemmatizer training procedures capable of accurate system bootstrapping from noisy and incomplete initial projections. Performance of the induced stand-alone part-of-speech tagger applied to French achieves 96% core part-of-speech (POS) tag accuracy, and the corresponding induced noun-phrase bracketer exceeds 91% F-measure. The induced morphological analyzer achieves over 99% lemmatization accuracy on the complete French verbal system. This achievement is particularly noteworthy in that it required absolutely no hand-annotated training data in the given language, and virtually no language-specific knowledge or resources beyond raw text. Performance also significantly exceeds that obtained by direct annotation projection. We perform early work in the cross-lingual projection of part-of-speech tag annotations from English to French and Czech, by way of word-aligned parallel bilingual corpora.	Inducing Multilingual Text Analysis Tools Via Robust Projection Across Aligned Corpora This paper explains a system and set of methods for automatically creating language tools, like part-of-speech taggers (which label words as nouns, verbs, etc.), noun-phrase bracketers (which find groups of words that act together), named-entity taggers (which find names of people, places, etc.), and morphological analyzers (which study word forms) for any foreign language. Examples include French, Chinese, Czech, and Spanish. Existing English language tools are used on bilingual texts, and their results are transferred to the second language using statistical word matches. Simple direct transfer of these results is often messy, even with the best word matches. Therefore, this paper introduces methods that can handle this messiness and still train accurate language tools from these imperfect initial results. The French part-of-speech tagger achieves 96% accuracy, and the noun-phrase bracketer scores over 91% in precision and recall. The morphological analyzer gets over 99% accuracy in analyzing French verbs. This is impressive because it didn't need any manually labeled data in the new language, and almost no specific knowledge or resources beyond basic text. This performance is much better than what simple result transfer would give. The paper also discusses early work on transferring part-of-speech information from English to French and Czech using aligned bilingual texts.	describe
187	Automatically Constructing A Lexicon Of Verb Phrase Idiomatic Combinations We investigate the lexical and syntactic flexibility of a class of idiomatic expressions. We develop measures that draw on such linguistic properties, and demonstrate that these statistical, corpus-based measures can be successfully used for distinguishing idiomatic combinations from non-idiomatic ones. We also propose a means for automatically determining which syntactic forms a particular idiom can appear in, and hence should be included in its lexical representation. To measure fixedness, we use statistical measures of lexical, syntactic, and overall fixedness. We come up with a dozen possible syntactic forms for verb-object pairs (based on passivization, determiner, and object pluralization) and use a corpus based statistical measure to determine the canonical form (s).	Automatically Constructing A Lexicon Of Verb Phrase Idiomatic Combinations We study how flexible idiomatic expressions (phrases with special meanings) are in terms of words used and sentence structure. We create methods that use these language features and show that these methods, based on analyzing large collections of text, can effectively tell apart idiomatic phrases from regular ones. We also suggest a way to automatically find out which sentence structures a specific idiom can be used in, so it can be correctly represented in a dictionary. To check how fixed these expressions are, we use statistical methods to look at word choice, sentence structure, and overall consistency. We identify twelve possible sentence structures for verb-object pairs (like using passive voice, different articles, or plural forms) and use a statistical method based on text analysis to find the standard form(s).	introduce
188	Exploiting Shallow Linguistic Information For Relation Extraction From Biomedical Literature We propose an approach for extracting relations between entities from biomedical literature based solely on shallow linguistic information. We use a combination of kernel functions to integrate two different information sources: (i) the whole sentence where the relation appears, and (ii) the local contexts around the interacting entities. We performed experiments on extracting gene and protein interactions from two different data sets. The results show that our approach outperforms most of the previous methods based on syntactic and semantic information. In addition to word features, we extract shallow linguistic information such as POS tag, lemma, and orthographic features of tokens for PPI extraction.	Exploiting Shallow Linguistic Information For Relation Extraction From Biomedical Literature We suggest a method to find connections between things in medical texts using only simple language details. We combine special mathematical tools called kernel functions to bring together two types of information: (i) the entire sentence where the connection is, and (ii) the nearby words around the related things. We tested this on finding how genes and proteins interact in two different sets of data. The findings show that our method works better than most older methods that use complex grammar and meaning details. Besides word features, we also use simple language details like POS tag (which tells what part of speech a word is), lemma (basic word form), and the way words look for extracting PPI (protein-protein interaction).	Idiomatic
189	Personalizing PageRank for Word Sense Disambiguation In this paper we propose a new graph-based method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambiguation. Our algorithm uses the full graph of the LKB efficiently, performing better than previous approaches in English all-words datasets. We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet. In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster. We propose Personalized PageRank (PPR) that tries to trade-off between the amount of the employed lexical information and the overall efficiency. We initialize the ranks of the vertex at a uniform value (usually 1/N for a graph with N vertices). We present a novel use of PageRank for word sense disambiguation. The key idea is to adapt the matrix initialization step in order to exploit the available contextual evidence.	Personalizing PageRank for Word Sense Disambiguation In this paper, we suggest a new method using a graph (a network of connected points) that relies on a knowledge base (LKB) like WordNet to perform Word Sense Disambiguation without needing any pre-labeled examples. Our method uses the entire graph from the LKB in a smart way, working better than older methods on English all-words datasets. We also demonstrate that it can be easily adapted to other languages effectively, as long as a wordnet (a database of words) is available. Additionally, we examine how well the method works, showing that it is effective and can be adjusted to work faster. We introduce Personalized PageRank (PPR), which aims to balance the amount of word information used with the speed of the process. We start by giving each vertex (point in the graph) an equal initial value, usually 1/N for a graph with N points. We show a new way to use PageRank for figuring out the meanings of words based on their context. The main idea is to change how we start the process to make use of the surrounding information available.	Linguistic Information
190	Bayesian Word Sense Induction Sense induction seeks to automatically identify word senses directly from a corpus. A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning. Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word's contexts into different classes, each representing a word sense. Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words. The Bayesian framework provides a principled way to incorporate a wide range of features beyond lexical co-occurrences and to systematically assess their utility on the sense induction task. The proposed approach yields improvements over state-of-the-art systems on a benchmark dataset. Our latent variable formulation serves as a foundation for more robust models of other linguistic phenomena. We extract pseudo documents from a 10-word window centered on the corresponding word token for each word type. We combine different feature sets using a probabilistic Word Sense Induction model and find that only some combinations produced an improved system.	Bayesian Word Sense Induction Sense induction tries to automatically find different meanings of a word by looking at a large collection of texts. In the past, it was believed that the words around a confusing word help to figure out what it means. So, sense induction is usually seen as a task where we group different uses of a word into categories, each showing a different meaning. Our work uses a method called Bayesian to look at this by treating the words around a confusing word as examples from a type of statistical model, which sees word meanings as groups of related words. This Bayesian method offers a clear way to include many different features, not just those that involve nearby words, and to test how useful they are for figuring out word meanings. Our new method does better than the best current systems when tested on a standard set of data. Our approach, which involves hidden variables, sets the stage for creating stronger models for understanding other language features. We take out sections of text that are 10 words long, centered around the word we are studying. We mix different sets of features using a probability-based Word Sense Induction model and discover that only some mixes actually make the system better.	surrounding
191	Nonconcatenative Finite-State Morphology Instead of modeling morphology along the more traditional finite-state transducer, we suggest modeling it with a n-tape automaton, where tapes would carry precisely this interleaving that is called for in Semitic interdigitation. We propose a framework with which each of the auto segmental tiers is assigned a tape in a multi-tape finite state machine, with an additional tape for the surface form.	Nonconcatenative Finite-State Morphology Instead of using the usual finite-state transducer (a tool for language patterns), we suggest using a n-tape automaton (a type of machine with multiple tracks) that can handle the mixing patterns seen in Semitic languages. We propose a system where each layer of language structure gets its own track in a multi-track finite state machine, with one extra track for the final word form.	creating stronger
203	Extracting Product Features And Opinions From Reviews Consumers are often forced to wade through many on-line reviews in order to make an informed product choice. This paper introduces OPINE, an unsupervised information-extraction system which mines reviews in order to build a model of important product features, their evaluation by reviewers, and their relative quality across products. Compared to previous work, OPINE achieves 22% higher precision (with only 3% lower recall) on the feature extraction task. OPINE's novel use of relaxation labeling for finding the semantic orientation of words in context leads to strong performance on the tasks of finding opinion phrases and their polarity. Our dictionary-based method utilizes Wikipedia to find an entry page for a phrase or a single term in a query. We not only analyze polarity of opinions regarding product features but also rank opinions based on their strength. We present a method that identifies product features for using corpus statistics, WordNet relations and morphological cues. The relevance ranking and extraction was performed with Pointwise Mutual Information.	Extracting Product Features And Opinions From Reviews Consumers often have to read through many online reviews to make a smart product choice. This paper talks about OPINE, a system that automatically goes through reviews to find important product features, what reviewers think about them, and how they compare to other products. OPINE does a better job than previous methods, getting 22% more precise results (but 3% less in finding everything) when finding product features. OPINE uses a new way to understand the meaning of words based on their context, which helps it find opinion phrases and whether they are positive or negative. Our method uses Wikipedia to find a page for a phrase or single term in a question. We not only look at whether opinions about product features are positive or negative but also rank them based on how strong they are. We show a method that discovers product features using data from a large collection of texts, connections from WordNet (a large database of words), and hints from word forms. The importance ranking and finding were done using a method called Pointwise Mutual Information, which measures how often two things happen together.	local reordering
192	Inference In DATR DATR is a declarative language for representing a restricted class of inheritance networks, permitting both multiple and default inheritance. The principal intended area of application is the representation of lexical entries for natural language processing, and we use examples from this domain throughout. In this paper we present the syntax and inference mechanisms for the language. The goal of the DATR enterprise is the design of a simple language that (i) has the necessary expressive power to encode the lexical entries presupposed by contemporary work in the unification grammar tradition, (ii) can express all the evident generalizations about such entries, (iii) has an explicit theory of inference, (iv) is computationally tractable, and (v) has an explicit declarative semantics. The present paper is primarily concerned with (iii), though the examples used may hint at our strategy in respect of (i) and (ii). we introduce DATR, a formal language for representing lexical knowledge.	Inference In DATR DATR is a language used to show how ideas are connected in a specific way, allowing for multiple layers of ideas and fallback options when needed. It is mainly used to describe words and their meanings for processing natural language, and examples are taken from this area. In this paper, we explain the rules and methods of using the language. The purpose of creating DATR is to have a simple language that (i) can describe word meanings in a detailed way, (ii) can show all common patterns about these meanings, (iii) has a clear method for drawing conclusions, (iv) is easy to use with computers, and (v) has clear rules for how it works. This paper focuses mainly on (iii), but the examples might also give an idea of how we handle (i) and (ii). We introduce DATR as a formal way to represent word knowledge.	Semitic
193	Translation By Structural Correspondences We sketch and illustrate an approach to machine translation that exploits the potential of simultaneous correspondences between separate levels of linguistic representation, as formalized in the LFG notion of codescriptions. The approach is illustrated with examples from English, German and French where the source and the target language sentence show noteworthy differences in linguistic analysis. The architecture can provide a formal basis for specifying complex source-target translation relationships in a declarative fashion that builds on monolingual grammars and lexicons that are independently motivated and theoretically justified.	Translation By Structural Correspondences We outline and show a method for machine translation that takes advantage of matching patterns between different levels of language representation, as explained in the LFG (Lexical Functional Grammar) idea of codescriptions. This method is demonstrated with examples from English, German, and French where the original and translated sentences have significant differences in language analysis. The system offers a formal way to describe complex translation connections between source and target languages in a clear manner, using monolingual (single language) grammar rules and word lists that are independently supported and logically sound.	focuses
194	Named Entity Recognition Without Gazetteers It is often claimed that Named Entity recognition systems need extensive gazetteers - lists of names of people, organisations, locations, and other named entities. Indeed, the compilation of such gazetteers is sometimes mentioned as a bottleneck in the design of Named Entity recognition systems. We report on a Named Entity recognition system which combines rule-based grammars with statistical (maximum entropy) models. We report on the system's performance with gazetteers of different types and different sizes, using test material from the MUC-7 competition. We show that, for the text type and task of this competition, it is sufficient to use relatively small gazetteers of well-known names, rather than large gazetteers of low-frequency names. We conclude with observations about the domain independence of the competition and of our experiments. We utilize the discourse level to disambiguate items in non predictive contexts. We exploit label consistency information within a document using relatively ad hoc multi-stage labeling procedures.	Named Entity Recognition Without Gazetteers It is often said that systems for recognizing named entities (important names like people, companies, or places) need long lists of these names, called gazetteers. Creating these lists can sometimes slow down the development of these systems. We discuss a system that uses both rule-based methods and statistical models (ways to predict based on data) to recognize named entities. We tested the system's performance with different kinds and sizes of gazetteers, using material from the MUC-7 competition. We found that for this competition, using small lists of well-known names works just as well as using large lists of less common names. We also share thoughts on how our findings apply to different areas beyond the competition. We use the context of the text to clarify confusing items. We take advantage of consistency (keeping things the same) within a document using flexible, multi-step labeling methods.	translation
195	An Efficient Method For Determining Bilingual Word Classes In statistical natural language processing we always face the problem of sparse data. One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling. In this paper we describe a method to determine bilingual word classes suitable for statistical machine translation. We develop an optimization criterion based on a maximum-likelihood approach and describe a clustering algorithm. We will show that the usage of the bilingual word classes we get can improve statistical machine translation. We show improvements on perplexity of bilingual corpus, and word translation accuracy using a template-based translation model. We describe a method for determining bilingual word classes, used to improve the extraction of alignment templates through alignments between classes, not only between words.	An Efficient Method For Determining Bilingual Word Classes In statistical natural language processing, we often deal with the issue of having too little data. To help solve this, we can group words into similar categories, which is a common method in statistical language modeling. In this paper, we explain a way to find bilingual word groups that work well for statistical machine translation. We create a rule for optimizing based on the best fit approach and explain a method for grouping words. We will demonstrate that using these bilingual word groups can enhance statistical machine translation. We show improvements on the complexity of understanding a bilingual text collection and the accuracy of translating words using a pattern-based translation model. We explain a method for finding bilingual word groups, which helps improve the process of finding alignment patterns through connections between groups, not just between individual words.	important names
196	Representing Text Chunks Dividing sentences in chunks of words is a useful preprocessing step for parsing, information extraction and information retrieval. (Ramshaw and Marcus, 1995) have introduced a "convenient" data representation for chunking by converting it to a tagging task. In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks. We will show that the data representation choice has a minor influence on chunking performance. However, equipped with the most suitable data representation, our memory-based learning chunker was able to improve the best published chunking results for a standard data set. We describe in detail the IOB schemes.	Representing Text Chunks Dividing sentences into groups of words is a helpful first step for understanding sentence structure, finding specific information, and searching for information. (Ramshaw and Marcus, 1995) introduced an "easy" way to represent this by turning it into a task where each word is labeled. In this paper, we will look at seven different ways to represent data for finding groups of words that describe things (noun phrase chunks). We will show that the choice of how to represent data has a small effect on how well this works. However, using the best way to represent data, our memory-based learning system was able to get better results than the best ones previously published for a standard set of data. We explain in detail the IOB methods, which are a way of marking the beginning, inside, and outside of these groups.	complexity
198	On Coreference Resolution Performance Metrics The paper proposes a Constrained Entity-Alignment F-Measure (CEAF) for evaluating coreference resolution. The metric is computed by aligning reference and system entities (or coreference chains) with the constraint that a system (reference) entity is aligned with at most one reference (system) entity. We show that the best alignment is a maximum bipartite matching problem which can be solved by the Kuhn-Munkres algorithm. Comparative experiments are conducted to show that the widely-known MUC F-measure has serious flaws in evaluating a coreference system. The proposed metric is also compared with the ACE-Value, the official evaluation metric in the Automatic Content Extraction (ACE) task, and we conclude that the proposed metric possesses some properties such as symmetry and better interpretability missing in the ACE-Value. We use a Bell tree to score and store the searching path.	On Coreference Resolution Performance Metrics The paper introduces a new way to measure how well computers can identify references to the same thing in different parts of a text, called Constrained Entity-Alignment F-Measure (CEAF). This measurement works by matching groups of related words (coreference chains) from both the reference and the computer system, with the rule that each group can only match with one group from the other side. We demonstrate that finding the best match is a type of problem called maximum bipartite matching, which can be solved using a specific method known as the Kuhn-Munkres algorithm. We perform tests that show the commonly used MUC F-measure has significant problems in evaluating these systems. The new measurement is also compared with another standard method used in a task called Automatic Content Extraction (ACE), and we find that our method has advantages like being fair and easier to understand, which the ACE method lacks. We use a structure called a Bell tree to keep track of and score the different ways of searching.	include
199	A Discriminative Matching Approach To Word Alignment We present a discriminative, large-margin approach to feature-based matching for word alignment. In this framework, pairs of word tokens receive a matching score, which is based on features of that pair, including measures of association between the words, distortion between their positions, similarity of the orthographic form, and so on. Even with only 100 labeled training examples and simple features which incorporate counts from a large unlabeled corpus, we achieve AER performance close to IBM Model 4, in much less time. Including Model 4 predictions as features, we achieve a relative AER reduction of 22% in over intersected Model 4 alignments. We use a large margin approach by factoring the structure level constraints to constraints at the level of an alignment link. We use a one-to-one constraint, where words in either sentence can participate in at most one link. We cast the problem of alignment as a maximum weight bipartite matching problem, where nodes correspond to the words in the two sentences.	A Discriminative Matching Approach To Word Alignment We present a method that focuses on distinguishing features (key characteristics) to match words from different languages. In this method, each pair of words is given a score based on certain features, like how often the words are used together, how far apart they are in a sentence, how similar they look, and more. Even with just 100 examples that we know are correct and using simple features that count word occurrences from a large text collection that hasn't been specifically labeled, we reach results close to a well-known method called IBM Model 4, but we do it faster. By including predictions from Model 4 as part of our features, we reduce errors by 22% when compared to Model 4 alone. We use a method that focuses on ensuring that the overall alignment rules are followed by making sure each specific word-to-word link follows these rules. We use a rule that each word can only be linked once, meaning each word can connect to only one other word. We approach the task of aligning words as a problem of finding the best match between two groups, with each word acting as a point that needs to be connected to the best possible match in the other group.	significant problems
200	A Discriminative Framework For Bilingual Word Alignment Bilingual word alignment forms the foundation of most approaches to statistical machine translation. Current word alignment methods are predominantly based on generative models. In this paper, we demonstrate a discriminative approach to training simple word alignment models that are comparable in accuracy to the more complex generative models normally used. These models have the the advantages that they are easy to add features to and they allow fast optimization of model parameters using small amounts of annotated data. LLR can still be used for extracting positive associations by filtering in a pre-processing step words with possibly negative associations. We train two models we call stage 1 and stage 2, both in the form of a weighted linear combination of feature values extracted from a pair of sentences and a proposed word alignment of them. We use statistics like log-likelihood-ratio and conditional likelihood-probability to measure word associations.	A Discriminative Framework For Bilingual Word Alignment Bilingual word alignment is a key part of most methods for statistical machine translation, which is how computers translate languages using data. Current methods for aligning words are mostly based on generative models, which are like recipes that predict word links based on certain patterns. In this paper, we show a different method called a discriminative approach, which is another way to train simple models for aligning words. These simple models work just as well as the more complex generative models usually used. These models are easy to improve by adding new features and they allow quick tuning of model settings with just a little bit of labeled data. LLR, which stands for log-likelihood ratio, can still be used to find positive word connections by first removing words that might have negative connections. We train two models, named stage 1 and stage 2. Both models work by combining weighted feature values, which are characteristics taken from a pair of sentences and their suggested word alignment. We use statistics like log-likelihood-ratio and conditional likelihood-probability to measure how words are connected.	using simple
201	A Maximum Entropy Word Aligner For Arabic-English Machine Translation This paper presents a maximum entropy word alignment algorithm for Arabic-English based on supervised training data. We demonstrate that it is feasible to create training material for problems in machine translation and that a mixture of supervised and unsupervised methods yields superior performance. The probabilistic model used in the alignment directly mod-els the link decisions. Significant improvement over traditional word alignment techniques is shown as well as improvement on several machine translation tests. Performance of the algorithm is contrasted with human annotation performance. We present a discriminatively trained 1-to-N model with feature functions specifically designed for Arabic. We train a discriminative model on a corpus of ten thousand word aligned Arabic-English sentence pairs that outperforms a GIZA++ baseline.	A Maximum Entropy Word Aligner For Arabic-English Machine Translation This paper introduces a method for connecting words between Arabic and English using a technique called maximum entropy, which is based on examples that have been carefully prepared. We show it's possible to create materials to help solve translation problems and that using a mix of guided and self-learning methods works better. The model, which uses probability, directly focuses on deciding which words should be linked. We show that this method is much better than older ways of linking words and improves several translation tests. We compare how well the algorithm works to how well humans do the same task. We introduce a specially trained model that can connect one word to many others, using specific features for Arabic. We trained this model on a set of 10,000 pairs of Arabic and English sentences, and it performs better than a previous standard tool called GIZA++.	combining
202	Local Phrase Reordering Models For Statistical Machine Translation We describe stochastic models of local phrase movement that can be incorporated into a Statistical Machine Translation (SMT) system. These models provide properly formulated, non-deficient, probability distributions over reordered phrase sequences. They are implemented by Weighted Finite State Transducers. We describe EM-style parameter re-estimation procedures based on phrase alignment under the complete translation model incorporating reordering. Our experiments show that the reordering model yields substantial improvements in translation performance on Arabic-to-English and Chinese-to-English MT tasks. We also show that the procedure scales as the bitext size is increased. We present a polynomial-time strategy. We define two local reordering models for their Translation Template Model (TTM): In the first one, called MJ-1, only adjacent phrases are allowed to swap, and the movement has to be done within a window of 2.	Local Phrase Reordering Models For Statistical Machine Translation We explain random models of moving phrases around locally that can be added to a Statistical Machine Translation (SMT) system. These models give well-defined, complete probability patterns for rearranging phrases. They use Weighted Finite State Transducers, which are a way to process sequences. We explain a method to adjust parameters using EM-style (a statistical technique) based on aligning phrases in the full translation model that includes reordering. Our tests show that this reordering model greatly improves translation quality from Arabic to English and Chinese to English. We also show that this method works well even when the text size increases. We offer a method that works efficiently in a reasonable amount of time. We define two local reordering models for their Translation Template Model (TTM): In the first one, called MJ-1, only phrases next to each other can switch places, and the switch must happen within a small range of 2 phrases.	previous standard
205	Identifying Sources Of Opinions With Conditional Random Fields And Extraction Patterns Recent systems have been developed for sentiment classification, opinion recognition, and opinion analysis (e.g., detecting polarity and strength). We pursue another aspect of opinion analysis: identifying the sources of opinions, emotions, and sentiments. We view this problem as an information extraction task and adopt a hybrid approach that combines Conditional Random Fields (Lafferty et al., 2001) and a variation of AutoSlog (Riloff, 1996a). While CRFs model source identification as a sequence tagging task, AutoSlog learns extraction patterns. Our results show that the combination of these two methods performs better than either one alone. The resulting system identifies opinion sources with 79.3% precision and 59.5% recall using a head noun matching measure, and 81.2% precision and 60.6% recall using an overlap measure.	Identifying Sources Of Opinions With Conditional Random Fields And Extraction Patterns Recent systems have been made to figure out if something has a positive or negative feeling, recognize opinions, and analyze them (like finding out how strong those feelings are). We are focusing on another part of opinion analysis: finding out who is expressing these opinions, feelings, and thoughts. We see this as a task of finding specific pieces of information and use a mixed method that uses Conditional Random Fields (a tool for predicting sequence data) and a version of AutoSlog (a method for learning patterns to extract information). CRFs help identify who is giving the opinion by looking at sequences, while AutoSlog helps learn the patterns of how information is shared. Our findings show that using both methods together works better than using each one separately. The system we developed can identify where opinions come from with 79.3% accuracy and finds 59.5% of the sources correctly using a method that matches certain key words, and with 81.2% accuracy and 60.6% correct findings using a method that looks for overlapping information.	emotion
206	Domain-Specific Sense Distributions And Predominant Sense Acquisition Distributions of the senses of words are often highly skewed. This fact is exploited by word sense disambiguation (WSD) systems which back off to the predominant sense of a word when contextual clues are not strong enough. The domain of a document has a strong influence on the sense distribution of words, but it is not feasible to produce large manually annotated corpora for every domain of interest. In this paper we describe the construction of three sense annotated corpora in different domains for a sample of English words. We apply an existing method for acquiring predominant sense information automatically from raw text, and for our sample demonstrate that (1) acquiring such information automatically from a mixed-domain corpus is more accurate than deriving it from SemCor, and (2) acquiring it automatically from text in the same domain as the target domain performs best by a large margin. We also show that for an all words WSD task this automatic method is best focussed on words that are salient to the domain, and on words with a different acquired predominant sense in that domain compared to that acquired from a balanced corpus. Our dataset is made up of 3 collections of documents: a domain-neutral corpus (BNC), and two domain-specific corpora (SPORTS and FINANCE).	Domain-Specific Sense Distributions And Predominant Sense Acquisition Distributions of the meanings of words are often very uneven. This fact is used by word meaning understanding systems, which rely on the most common meaning of a word when the context isn't clear enough. The topic of a document strongly affects how words are understood, but it's not practical to create large collections of manually labeled texts for every topic. In this paper, we explain how we built three collections of texts with labeled meanings in different topics for a sample of English words. We use an existing method to automatically find the most common meaning from plain text, and we show that (1) getting this information automatically from a mixed-topic collection is more accurate than getting it from SemCor (a specific text collection), and (2) getting it automatically from text in the same topic as the target topic works best by a large difference. We also show that for a task where all words need to be understood in context, this automatic method works best for words that are important to the topic, and for words that have a different most common meaning in that topic compared to one from a general collection. Our dataset includes 3 collections of documents: a topic-neutral collection (BNC), and two topic-specific collections (SPORTS and FINANCE).	AutoSlog
207	Bidirectional Inference With The Easiest-First Strategy For Tagging Sequence Data This paper presents a bidirectional inference algorithm for sequence labeling problems such as part-of-speech tagging, named entity recognition and text chunking. The algorithm can enumerate all possible decomposition structures and find the highest probability sequence together with the corresponding decomposition structure in polynomial time. We also present an efficient decoding algorithm based on the easiest-first strategy, which gives comparably good performance to full bidirectional inference with significantly lower computational cost. Experimental results of part-of-speech tagging and text chunking show that the proposed bidirectional inference methods consistently outperform unidirectional inference methods and bidirectional MEMMs give comparable performance to that achieved by state-of-the-art learning algorithms including kernel support vector machines. We propose easiest-first deterministic decoding.	Bidirectional Inference With The Easiest-First Strategy For Tagging Sequence Data This paper introduces a two-way thinking method for labeling sequences, like identifying parts of speech, recognizing names in text, and breaking text into chunks. The method can list all possible ways to break down the sequence and find the most likely one quickly. We also introduce a fast decoding method using an easiest-first strategy, which performs almost as well as the full two-way method but uses much less computing power. Tests on part-of-speech tagging and text chunking show that this two-way method consistently beats one-way methods, and two-way MEMMs perform as well as top learning methods like advanced support vector machines. We suggest using an easiest-first straightforward decoding method.	collection
208	Non-Projective Dependency Parsing Using Spanning Tree Algorithms We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al., 2003; McDonald et al., 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies. The key idea is to build a complete graph consisting of tokens of the sentence where each edge is weighted by a learned scoring function.	Non-Projective Dependency Parsing Using Spanning Tree Algorithms We describe weighted dependency parsing as finding the largest spanning trees (MSTs) in directed graphs (a network of points connected by arrows). With this setup, the parsing method by Eisner (1996) works well for finding all projective trees (a type of structured tree) in O(n3) time, which means it takes a certain amount of time based on the number of elements. Surprisingly, this setup can be easily adapted for non-projective parsing (more flexible tree structures) using the Chu-Liu-Edmonds MST algorithm (created by Chu and Liu, 1965; and Edmonds, 1967), leading to a faster O(n2) parsing process. We test these methods on the Prague Dependency Treebank using online large-margin learning techniques (methods for improving prediction accuracy) by Crammer et al., 2003; and McDonald et al., 2005, and find that MST parsing improves efficiency and accuracy for languages with non-projective dependencies (languages where relationships don't follow a simple tree structure). The main idea is to create a complete graph (a network where every point is connected to every other point) using the words of the sentence, where each connection (edge) is given a score by a learned scoring function.	introduces
209	Emotions From Text: Machine Learning For Text-Based Emotion Prediction In addition to information, text contains attitudinal, and more specifically, emotional content. This paper explores the text-based emotion prediction problem empirically, using supervised machine learning with the SNoW learning architecture. The goal is to classify the emotional affinity of sentences in the narrative domain of children's fairy tales, for subsequent usage in appropriate expressive rendering of text-to-speech synthesis. Initial experiments on a preliminary data set of 22 fairy tales show encouraging results over a narrative baseline and BOW approach for classification of emotional versus non-emotional contents, with some dependency on parameter tuning. We also discuss results for a tripartite model which covers emotional valence, as well as feature set alternations. In addition, we present plans for a more cognitively sound sequential model, taking into consideration a larger set of basic emotions.	Emotions From Text: Machine Learning For Text-Based Emotion Prediction In addition to information, text contains feelings and emotions. This paper looks into predicting emotions from text using a method called supervised machine learning, specifically with the SNoW learning system. The aim is to identify the emotional tone of sentences in children's fairy tales to help with making text-to-speech sound more expressive. Initial tests on a small set of 22 fairy tales show positive results compared to a simple narrative approach and a Bag of Words (BOW) method for telling apart emotional and non-emotional content, though it depends somewhat on adjusting certain settings. We also talk about results for a model that includes emotional positivity or negativity, and changes to the features used. Additionally, we share plans for a more advanced model that considers a wider range of basic emotions.	projective
210	Recognising Textual Entailment With Logical Inference We use logical inference techniques for recognising textual entailment. As the performance of theorem proving turns out to be highly dependent on not readily available background knowledge, we incorporate model building, a technique borrowed from automated reasoning, and show that it is a useful robust method to approximate entailment. Finally, we use machine learning to combine these deep semantic analysis techniques with simple shallow word overlap; the resulting hybrid model achieves high accuracy on the RTE test set, given the state of the art. Our results also show that the different techniques that we employ perform very differently on some of the subsets of the RTE corpus and as a result, it is useful to use the nature of the dataset as a feature. It is often the case that the lack of sufficient linguistic knowledge causes failure of inference, thus the system outputs "no entailment" for almost all pairs. Our system is based on logical representation and automatic theorem proving, but utilizes only WordNet (Fellbaum, 1998) as a lexical knowledge resource.	Recognising Textual Entailment With Logical Inference We use logical reasoning techniques to identify if one piece of text logically follows from another. Since the performance of proving logical statements relies heavily on extra information that isn't easily accessible, we add model building, a technique from automated reasoning, to show it is a strong method to estimate entailment. We also use machine learning to combine these deep understanding methods with simple word matching; this combined model performs very well on the RTE test set, based on the latest standards. Our results show that the different methods we use work very differently on some parts of the RTE dataset, so it's helpful to consider the type of data as a feature. Often, not having enough language knowledge causes the system to fail in making inferences, so it often predicts "no entailment" for most pairs. Our system is based on logical representation and automatic logical statement proving, but only uses WordNet (a large database of English words) as a language knowledge source.	method called
211	A Shortest Path Dependency Kernel For Relation Extraction We present a novel approach to relation extraction, based on the observation that the information required to assert a relationship between two named entities in the same sentence is typically captured by the shortest path between the two entities in the dependency graph. Experiments on extracting top-level relations from the ACE (Automated Content Extraction) newspaper corpus show that the new shortest path dependency kernel outperforms a recent approach based on dependency tree kernels. This work on relation extraction shows that the shortest dependency path between any two entities captures the in formation required to assert a relationship between them.	A Shortest Path Dependency Kernel For Relation Extraction We introduce a new method for finding relationships between two named things in a sentence. This method is based on the idea that the shortest connection between them in a sentence's structure usually contains the needed information. Tests using news articles from the ACE (Automated Content Extraction) dataset show that this new method is better than a recent one that used a different approach to sentence structure. This research indicates that the shortest path between two entities in a sentence effectively captures the information needed to identify their relationship.	Recognising Textual
212	OpinionFinder: A System For Subjectivity Analysis We provide a subjectivity lexicon. We provide a pre-trained classifier for annotating the phrases in a sentence with their contextual polarity values.	OpinionFinder: A System For Subjectivity Analysis We provide a subjectivity lexicon, which is a list of words that show opinions or feelings. We provide a pre-trained classifier, which is a tool that has already learned how to tag or label the phrases in a sentence with their contextual polarity values, meaning it can identify if the phrases are positive, negative, or neutral in the context they are used.	information
213	Identifying Word Correspondences In Parallel Texts Motivated by the need to reduce on the memory requirement and to insure robustness in estimation of probability, we propose an alternative algorithm in which probabilities are not estimated and stored for all word pairs. We propose a statistic to measure the strength of correlation between source and target words. We use the phi2 statistics as the correspondence level of the word pairs and show that it was more effective than the mutual information.	Identifying Word Correspondences In Parallel Texts Motivated by the need to use less computer memory and ensure the method is strong and reliable when guessing chances, we suggest a different method where chances are not guessed and saved for every pair of words. We suggest a way to measure how strongly words from one language match with words from another language. We use a measure called phi2 statistics to show how well word pairs match and demonstrate that it worked better than the mutual information method.	subjectivity lexicon
214	A Procedure For Quantitatively Comparing The Syntactic Coverage Of English Grammars We define PARSEVAL measures for parsing: labelled precision, labelled recall, and labelled F-measure (Prec., Rec., and F1, respectively), which are based on the number of non-terminal items in the parser's output that match those in the gold-standard parse.	A Procedure For Quantitatively Comparing The Syntactic Coverage Of English Grammars We explain PARSEVAL measures for parsing: labelled precision, labelled recall, and labelled F-measure (Prec., Rec., and F1, respectively). These are based on counting how many parts in the parser's output match the correct or "gold-standard" parse.	another language
215	Towards History-Based Grammars: Using Richer Models For Probabilistic Parsing We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Tree-bank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error. We present history-based parsing, using features of the parsing history to predict the next parser action.	Towards History-Based Grammars: Using Richer Models For Probabilistic Parsing. We explain a model for understanding natural language, called HBG, which uses detailed language information to clear up confusion. HBG uses information about words, sentence structure, meaning, and organization from the parse tree (a diagram showing sentence structure) in a new way to help decide the correct meaning of a sentence. We use a collection of labeled sentences, known as a Tree-bank, along with decision trees (a method for making choices) to identify important parts of a sentence tree that help find the right interpretation of a sentence. This is different from the usual method of adjusting grammar by thinking about language in the hope of finding the right interpretation. In direct comparisons with one of the best existing strong language understanding models, called P-CFG, the HBG model performs much better, improving understanding accuracy from 60% to 75%, which is a 37% reduction in mistakes. We introduce history-based parsing, using previous steps in understanding to predict the next move in the process.	Procedure
218	One Sense Per Collocation Previous work [Gale, Church and Yarowsky, 1992] showed that with high probability a polysemous word has one sense per discourse. In this paper we show that for certain definitions of collocation, a polysemous word exhibits essentially only one sense per collocation. We test this empirical hypothesis for several definitions of sense and collocation, and discover that it holds with 90-99% accuracy for binary ambiguities. We utilize this property in a disambiguation algorithm that achieves precision of 92% using combined models of very local context. We define collocation as a co-occurrence of two words in a defined relation. In order to analyze and compare the behavior of several kinds of collocations, we use a measure of entropy as well as the results obtained when tagging heldout data with the collocations organized as decision lists. We find that the objects of verbs play a more dominant role than their subjects in WSD and nouns acquire more stable disambiguating information from their noun or adjective modifiers.	One Sense Per Collocation Previous work [Gale, Church and Yarowsky, 1992] showed that with high probability a word with multiple meanings (polysemous) has one meaning in a particular piece of writing (discourse). In this paper, we show that for certain ways of grouping words together (collocation), a word with multiple meanings mostly has just one meaning in that group. We test this idea for different meanings and groupings, and find it works 90-99% of the time for situations with two possible meanings (binary ambiguities). We use this in a method to figure out meanings (disambiguation algorithm) that is 92% accurate by looking at the immediate context. We define collocation as when two words appear together in a specific way. To study and compare different word groupings, we measure unpredictability (entropy) and check the results when applying these groupings to new data using organized lists (decision lists). We find that the objects (things affected by verbs) are more important than the subjects (doers of actions) in figuring out word meanings (WSD), and nouns get clearer meaning information from nearby nouns or adjectives.	methods
219	A Semantic Concordance A semantic concordance is a textual corpus and a lexicon so combined that every substantive word in the text is linked to its appropriate sense in the lexicon. Thus it can be viewed either as a corpus in which words have been tagged syntactically and semantically, or as a lexicon in which example sentences can be found for many definitions. A semantic concordance is being constructed to use in studies of sense resolution in context (semantic disambiguation). The Brown Corpus is the text and WordNet is the lexicon. Semantic tags (pointers to WordNet synsets) are inserted in the text manually using an interface, ConText, that was designed to facilitate the task. Another interface supports searches of the tagged text. Some practical uses for semantic concordances are proposed. We present SemCor, a balanced, semantically annotated dataset, with all content words manually tagged by trained lexicographers.	A Semantic Concordance A semantic concordance is a collection of texts and a dictionary combined in a way that every important word in the text is connected to its correct meaning in the dictionary. This means it can be seen as a collection of texts where words are marked with their grammar and meaning, or as a dictionary where you can find example sentences for many meanings. A semantic concordance is being created to help study how the meaning of words can be understood in different contexts. The Brown Corpus is the text used, and WordNet is the dictionary. Special tags that point to WordNet definitions are added to the text manually using a tool called ConText, which was made to make this job easier. Another tool helps search through the tagged text. Some practical uses for these semantic concordances are suggested. We introduce SemCor, a well-balanced dataset with meanings marked by experts, where all important words are manually tagged.	clearer meaning
220	The Penn Treebank: Annotating Predicate Argument Structure The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure. This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides a set of coindexed null elements in what can be thought of as "underlying" position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides some non-context free annotational mechanism to allow the structure of discontinuous constituents to be easily recovered, and allows for a clear, concise tagging system for some semantic roles. Our Switchboard corpus contains transcriptions of spoken, spontaneous conversation annotated with phrase-structure trees. The PennTreebank II marks subjects (SBJ), logical objects of passives (LGS), some reduced relative clauses (RRC), as well as other grammatical information, but does not mark each constituent with a grammatical role.	The Penn Treebank: Annotating Predicate Argument Structure The Penn Treebank has recently added a new way of labeling sentences to show the relationship between verbs and their arguments (like the subject and object). This paper explains important features of this new labeling method. It offers a more consistent way to handle different grammar rules, includes special markers for things like questions, passive voice, and subjects of phrases without "to" verbs, allows for some complex grammar structures to be easily identified, and provides a simple labeling system for some meanings in sentences. Our Switchboard collection has transcripts of natural spoken conversations with labeled sentence structures. The Penn Treebank II identifies subjects, logical objects in passive sentences, some shortened relative clauses, and other grammar details, but it doesn’t label each part of a sentence with its specific grammar role.	dictionary
221	Using A Semantic Concordance For Sense Identification This paper proposes benchmarks for systems of automatic sense identification. A textual corpus in which open-class words had been tagged both syntactically and semantically was used to explore three statistical strategies for sense identification: a guessing heuristic, a most-frequent heuristic, and a co-occurrence heuristic. When no information about sense-frequencies was available, the guessing heuristic using the numbers of alternative senses in WordNet was correct 45% of the time. When statistics for sense-frequancies were derived from a semantic concordance, the assumption that each word is used in its most frequently occurring sense was correct 69% of the time; when that figure was calculated for polysemous words alone, it dropped to 58%. And when a co-occurence heuristic took advantage of prior occurrences of words together in the same sentences, little improvement was observed. The semantic concordance is still too small to estimate the potential limits of a co-occurrence heuristic. We prepare a sense-tagged corpus SEMCOR containing a substantial subset of the Brown corpus tagged with the refined senses of WORDNET.	Using A Semantic Concordance For Sense Identification This paper suggests standards for systems that automatically figure out word meanings. A collection of written texts, where important words were marked for grammar and meaning, was used to test three strategies for identifying meanings: a guessing method, a most-common method, and a method based on words appearing together. When there was no information on how often meanings were used, the guessing method using the number of possible meanings in WordNet was correct 45% of the time. When data on meaning frequency was taken from a meaning database, assuming each word is used in its most common meaning was correct 69% of the time; for words with multiple meanings, it dropped to 58%. And when a method based on words appearing together used previous occurrences of words in the same sentences, there was little improvement. The meaning database is still too small to determine the potential limits of using words appearing together. We prepare a sense-marked text collection SEMCOR, which includes a large part of the Brown text collection marked with the detailed meanings from WORDNET.	relative
222	A Maximum Entropy Model For Prepositional Phrase Attachment We construct a benchmark dataset of 27,937 pp-attachment quadruples extracted from the Wall Street Journal corpus. We train a maximum entropy model on (v, n1, p, n2) quadruples extracted from the Wall Street Journal corpus and achieve 81.6% accuracy. Our maximum entropy approach uses the mutual information clustering algorithm.	A Maximum Entropy Model For Prepositional Phrase Attachment We create a standard set of 27,937 examples of how prepositional phrases (like "on the table") attach to sentences, taken from the Wall Street Journal articles. We train a model that predicts how these phrases attach by using groups of four elements (verb, noun1, preposition, noun2) from these articles and reach 81.6% accuracy. Our method uses a clustering algorithm, which groups similar things together, based on mutual information, which measures how much knowing one thing tells us about another.	guessing method
223	Syntax Annotation for the GENIA Corpus Linguistically annotated corpus based on texts in biomedical domain has been constructed to tune natural language processing (NLP) tools for bio-textmining. As the focus of information extraction is shifting from "nominal" information such as named entity to "verbal" information such as function and interaction of substances, application of parsers has become one of the key technologies and thus the corpus annotated for syntactic structure of sentences is in demand. A subset of the GENIA corpus consisting of 500 MEDLINE abstracts has been annotated for syntactic structure in an XML-based format based on Penn Treebank II (PTB) scheme. Inter-annotator agreement test indicated that the writing style rather than the contents of the research abstracts is the source of the difficulty in tree annotation, and that annotation can be stably done by linguists without much knowledge of biology with appropriate guidelines regarding to linguistic phenomena particular to scientific texts. Our GENIA Treebank Corpus is estimated to have no imperative sentences and only seven interrogative sentences.	Syntax Annotation for the GENIA Corpus Linguistically annotated corpus based on texts in biomedical domain has been constructed to improve natural language processing (NLP) tools for finding information in biology texts. As the focus of finding information is shifting from "nominal" information like names to "verbal" information like functions and interactions of substances, using parsers (tools that analyze sentence structure) has become important, making a collection of sentences with marked sentence structure needed. A part of the GENIA corpus, containing 500 short scientific articles from MEDLINE, has been marked for sentence structure in an XML-based format using the Penn Treebank II (PTB) method. A test to see how much different people agree on this annotation showed that the style of writing, rather than the content of the research summaries, makes it hard to annotate sentence structure. It also showed that people who study languages can do the annotations consistently without needing much biology knowledge, as long as they have good guidelines for scientific text language issues. Our GENIA Treebank Corpus is estimated to have no command sentences and only seven question sentences.	elements
224	The Second International Chinese Word Segmentation Bakeoff The second international Chinese word segmentation bakeoff was held in the summer of 2005 to evaluate the current state of the art in word segmentation. Twenty three groups submitted 130 result sets over two tracks and four different corpora. We found that the technology has improved over the intervening two years, though the out-of-vocabulary problem is still of paramount importance. In the Second International Chinese Word Segmentation Bakeoff, two of the highest scoring systems in the closed track competition were based on a CRF model.	The Second International Chinese Word Segmentation Bakeoff The second international Chinese word segmentation bakeoff was held in the summer of 2005 to evaluate the current best methods in breaking down Chinese text into words. Twenty-three groups submitted 130 sets of results over two types of tests and four different collections of text. We found that the technology has gotten better over the past two years, but the issue of dealing with new, unknown words remains very important. In the event, two of the top systems in the closed track competition used a CRF model, which stands for Conditional Random Field, a type of statistical model used for predicting patterns.	sentences
225	A Maximum Entropy Approach to Chinese Word Segmentation We participated in the Second International Chinese Word Segmentation Bakeoff. Specifically, we evaluated our Chinese word segmenter in the open track, on all four corpora, namely Academia Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (MSR), and Peking University (PKU). Based on a maximum entropy approach, our word segmenter achieved the highest F measure for AS, CITYU, and PKU, and the second highest for MSR. We found that the use of an external dictionary and additional training corpora of different segmentation standards helped to further improve segmentation accuracy. We present a post processing method to enhance the unknown word segmentation. We use templates representing numbers, dates, letters etc.	A Maximum Entropy Approach to Chinese Word Segmentation We took part in the Second International Chinese Word Segmentation Bakeoff. We tested our Chinese word splitter in the open category on all four datasets: Academia Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (MSR), and Peking University (PKU). Using a method called maximum entropy, our word splitter got the highest accuracy score (F measure) for AS, CITYU, and PKU, and the second highest for MSR. We discovered that using an extra dictionary and more training data with different word-splitting rules helped improve how accurately words were split. We introduce a final step method to better split words that are not known. We use patterns that represent things like numbers, dates, and letters.	international
226	A Conditional Random Field Word Segmenter for Sighan Bakeoff 2005 We present a Chinese word segmentation system submitted to the closed track of Sighan bakeoff 2005. Our segmenter was built using a conditional random field sequence model that provides a framework to use a large number of linguistic features such as character identity, morphological and character reduplication features. Because our morphological features were extracted from the training corpora automatically, our system was not biased toward any particular variety of Mandarin. Thus, our system does not overfit the variety of Mandarin most familiar to the system's designers. Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR). We develop the Stanford Chinese word segmenter.	A Conditional Random Field Word Segmenter for Sighan Bakeoff 2005 We present a Chinese word segmentation system submitted to the closed track of Sighan bakeoff 2005. Our segmenter was built using a model called conditional random field, which allows us to use many language features like character identity (what each character is), how words change form (morphological features), and when characters repeat (character reduplication). Because we got our language features automatically from the training texts, our system wasn't biased towards any specific type of Mandarin. This means our system didn't become too focused on just one familiar type of Mandarin. Our final system got high scores: 0.947 (AS), 0.943 (HK), 0.950 (PK), and 0.964 (MSR). We developed the Stanford Chinese word segmenter.	Chinese
227	Using Contextual Speller Techniques and Language Modeling for ESL Error Correction We present a modular system for detection and correction of errors made by non-native (English as a Second Language = ESL) writers. We focus on two error types: the incorrect use of determiners and the choice of prepositions. We use a decision-tree approach inspired by contextual spelling systems for detection and correction suggestions, and a large language model trained on the Gigaword corpus to provide additional information to filter out spurious suggestions. We show how this system performs on a corpus of non-native English text and discuss strategies for future enhancements. We use a language model in addition to a classifier and combine the classifier output and language model scores in a meta classifier. We use a single language model score on hypothesized error and potential correction to filter out unlikely correction candidates.	Using Contextual Speller Techniques and Language Modeling for ESL Error Correction We introduce a flexible system to find and fix mistakes made by people who are learning English. We mainly look at two types of mistakes: using articles (like "a" and "the") incorrectly and picking the wrong prepositions (like "in," "at," or "on"). We use a method similar to spell checkers to find mistakes and suggest corrections. We also use a big language model, trained on a large collection of written texts called the Gigaword corpus, to help remove bad suggestions. We demonstrate how well this system works on text written by English learners and talk about how we can improve it in the future. We combine a language model with a classifier (a tool that sorts data) to make a better decision system. We use a single score from the language model to decide if a correction is likely to be wrong, helping us choose the best correction.	Conditional Random
239	A Machine Learning Approach To Coreference Resolution Of Noun Phrases In this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of "organization," "person," or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data sets. We include all noun phrases returned by their NP identifier and report an F-measure of 62.6% for MUC-6 data and 60.4% for MUC-7 data. We construct this entity-mention graph by learning to decide for each mention which preceding mention, if any, belongs in the same equivalence class; this approach is commonly called the pairwise coreference model.	A Machine Learning Approach To Coreference Resolution Of Noun Phrases In this paper, we introduce a method using machine learning to figure out when different noun phrases (like names or things) in a text refer to the same thing. Our method learns from a small collection of text examples that are marked up with information and works on all kinds of noun phrases, not just specific ones like pronouns. It doesn't limit the types of things the nouns can refer to, such as "organization" or "person." We test our method on well-known data sets (called MUC-6 and MUC-7) and get good results, showing that our method is promising and its accuracy is similar to methods that don't use learning. Our system is the first to use learning that performs as well as the best non-learning systems on these data sets. We include all noun phrases identified by their NP (noun phrase) detector and report scores of 62.6% for MUC-6 and 60.4% for MUC-7. We create a graph that connects mentions by learning to decide which previous mention belongs with a new mention, a process known as the pairwise coreference model.	makes mistakes
228	Learning Dependency Translation Models As Collections Of Finite-State Head Transducers The paper defines weighted head transducers, finite-state machines that perform middle-outstring transduction. These transducers are strictly more expressive than the special case of standard left-to-right finite-state transducers. Dependency transduction models are then defined as collections of weighted head transducers that are applied hierarchically. A dynamic programming search algorithm is described for finding the optimal transduction of an input string with respect to a dependency transduction model. A method for automatically training a dependency transduction model from a set of input-output example strings is presented. The method first searches for hierarchical alignments of the training examples guided by correlation statistics, and then constructs the transitions of head transducers that are consistent with these alignments. Experimental results are given for applying the training method to translation from English to Spanish and Japanese. We treat translation as a process of simultaneous induction of source and target dependency trees using head transduction. We present a two-level arranged word ordering and chunk ordering by a hierarchically organized collection of finite state transducers. We induce parallel tree structures from unbracketed parallel text, modeling the generation of each node's children with a finite-state transducer.	Learning Dependency Translation Models As Collections Of Finite-State Head Transducers The paper explains weighted head transducers, which are machines that process and transform strings from the middle outwards. These machines are more capable than the usual type that works strictly from left to right. Dependency transduction models use groups of these weighted head transducers, applied in a step-by-step manner. The paper describes a smart search method to find the best way to change an input string using this model. It also explains how to automatically train these models using examples of strings that show both input and output. The training method first looks for patterns in the examples using statistical guidance, then builds the head transducers based on those patterns. Experiments were conducted using this training approach to translate English into Spanish and Japanese. The study treats translation as creating linked tree structures from both the source and target languages using head transduction. It shows a two-step process for organizing words and chunks using a collection of these machines. The paper also discusses how to create matching tree structures from pairs of texts that don't have predefined sentence parts, using a machine to generate each part's connections.	improve
229	Models Of Translational Equivalence Among Words Parallel texts (bitexts) have properties that distinguish them from other kinds of parallel data. First, most words translate to only one other word. Second, bitext correspondence is typically only partial - many words in each text have no clear equivalent in the other text. This article presents methods for biasing statistical translation models to reflect these properties. Evaluation with respect to independent human judgments has confirmed that translation models biased in this fashion are significantly more accurate than a baseline knowledge-free model. This article also shows how a statistical translation model can take advantage of preexisting knowledge that might be available about particular language pairs. Even the simplest kinds of language-specific knowledge, such as the distinction between content words and function words, are shown to reliably boost translation model performance on some tasks. Statistical models that reflect knowledge about the model domain combine the best of both the rationalist and empiricist paradigms. We measure the orthographic similarity using longest common subsequence ratio (LCSR). We define a direct association as an association between two words where the two words are indeed mutual translations. We propose Competitive Linking Algorithm (CLA) to align the words to construct confusion network. We use competitive linking to greedily construct matchings where the pair score is a measure of word-to-word association. We argue that there are ways to determine the boundaries of some multi-words phrases, allowing to treat several words as a single token.	Models Of Translational Equivalence Among Words Parallel texts (bitexts), which are texts translated side-by-side, have special features different from other types of similar data. First, most words translate to only one other word. Second, the match between these texts is usually incomplete - many words in each text don't have a clear counterpart in the other text. This article introduces ways to adjust statistical translation models to consider these features. Tests using independent human opinions have shown that translation models adjusted this way are much more accurate than basic models that don't use prior knowledge. This article also explains how a statistical translation model can use any existing knowledge about specific language pairs. Even the simplest language-specific knowledge, like knowing the difference between important words and helper words, can improve the model's performance on certain tasks. Statistical models that use knowledge about the area they are applied to combine the strengths of both logical thinking and experience-based methods. We measure how words look similar using the longest common subsequence ratio (LCSR), which checks how much of a word sequence is shared between words. We define a direct association as a clear match between two words that are true translations of each other. We suggest a Competitive Linking Algorithm (CLA) to connect words and build a confusion network, which helps in translation. We use competitive linking to quickly create pairings where the pair score measures how well two words are connected. We suggest there are ways to find the edges of some phrases made of multiple words, allowing us to treat several words as one unit.	languages
230	Dialogue Act Modeling For Automatic Tagging And Recognition Of Conversational Speech We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech-act-like units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, and APOLOGY. Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence. The dialogue model is based on treating the discourse structure of a conversation as a hidden Markov model and the individual dialogue acts as observations emanating from the model states. Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram. The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act. We develop a probabilistic integration of speech recognition with dialogue modeling, to improve both speech recognition and dialogue act classification accuracy. Models are trained and evaluated using a large hand-labeled database of 1,155 conversations from the Switchboard corpus of spontaneous human-to-human telephone speech. We achieved good dialogue act labeling accuracy (65% based on errorful, automatically recognized words and prosody, and 71% based on word transcripts, compared to a chance baseline accuracy of 35% and human accuracy of 84%) and a small reduction in word recognition error. We use HMMs as a general model of discourse with an application to speech acts (or dialog acts) in conversations.	Dialogue Act Modeling For Automatic Tagging And Recognition Of Conversational Speech We explain a method using statistics to understand dialogue acts in conversation speech. Dialogue acts are parts of speech like STATEMENT, QUESTION, BACKCHANNEL (listening sounds like "uh-huh"), AGREEMENT, DISAGREEMENT, and APOLOGY. Our method finds and guesses these acts using word choices, word patterns, voice tone, and how well the conversation makes sense. The method uses a hidden Markov model, which is a way to see conversation structure as hidden steps with dialogue acts as visible actions. We also use a dialogue act n-gram, which is a way to predict the likely order of dialogue acts. The grammar for dialogue is mixed with word patterns, decision trees (which help make decisions based on data), and neural networks (computer systems that learn) to understand the unique way of speaking each dialogue act. We create a way to combine speech recognition with understanding dialogue acts to make both more accurate. The models are trained and tested with a big database of 1,155 conversations from the Switchboard collection of natural telephone talks. We reached good accuracy in labeling dialogue acts (65% with automatically recognized words and voice tone, and 71% with correct word transcripts, compared to a random accuracy of 35% and human accuracy of 84%) and slightly reduced errors in word recognition. We use HMMs (hidden Markov models) as a general way to understand conversation structure applied to speech acts in talks.	existing
231	A Compression-Based Algorithm For Chinese Word Segmentation Chinese is written without using spaces or other word delimiters. Although a text may be thought of as a corresponding sequence of words, there is considerable ambiguity in the placement of boundaries. Interpreting a text as a sequence of words is beneficial for some information retrieval and storage tasks: for example,full-text search, word-based compression, and keyphrase extraction. We describe a scheme that infers appropriate positions for word boundaries using an adaptive language model that is standard in text compression. It is trained on a corpus of presegmented text, and when applied to new text, interpolates word boundaries so as to maximize the compression obtained. This simple and general method performs well with respect to specialized schemes for Chinese language segmentation. Our n-gram generative language modeling based approach does not use domain knowledge.	A Compression-Based Algorithm For Chinese Word Segmentation. Chinese is written without spaces or markers to show where words end. Even though text can be thought of as a series of words, it's often unclear where one word ends and another begins. Breaking text into words is helpful for tasks like searching through text, compressing data to use less space, and picking out important phrases. We explain a method that guesses where word boundaries should be using a flexible language model, which is commonly used to compress text. This model is trained on already divided text, and when used on new text, it finds where words should be split to make the text as small as possible when compressed. This straightforward and widely applicable method works well compared to other specialized methods for dividing Chinese text. Our approach, based on n-gram language modeling, doesn't rely on specific knowledge about the subject area.	accuracy
232	An Empirically Based System For Processing Definite Descriptions We present an implemented system for processing definite descriptions in arbitrary domains. The design of the system is based on the results of a corpus analysis previously reported, which highlighted the prevalence of discourse-new descriptions in newspaper corpora. The annotated corpus was used to extensively evaluate the proposed techniques for matching definite descriptions with their antecedents, discourse segmentation, recognizing discourse-new descriptions, and suggesting anchors for bridging descriptions. A major obstacle in the resolution of definite noun phrases with full lexical heads is that only a small proportion of them is actually anaphoric (ca. 30%). In our system, WordNet is consulted to obtain the synonymy, hypernymy and meronymy relations for resolving the definite anaphora. We classify each definite description as either direct anaphora, discourse-new, or bridging description. We distinguish restrictive from non-restrictive post modification by ommitting modifiers that occur between commas, which should not be classified as chain starting. For the discourse-new classification task, the model's most important feature is whether the head word of the NP to be classified has occurred previously.	An Empirically Based System For Processing Definite Descriptions We introduce a system that deals with specific descriptions in any subject area. The system's design is based on previous analysis of text collections, showing that many new descriptions appear in newspapers. The marked text collection was used to test techniques for matching specific descriptions to their earlier mentions, dividing text into sections, identifying new descriptions, and suggesting links for connecting descriptions. A big challenge in resolving specific noun phrases with detailed main words is that only a small part of them are actually referring back to something earlier (about 30%). In our system, we use WordNet, a tool for finding synonyms, broader terms, and part-whole relationships, to help resolve these references. We categorize each specific description as either a direct reference, new in the text, or a connecting description. We separate important details from less important ones by ignoring words between commas, which shouldn't start a chain of references. For deciding if something is new in the text, the most important factor is if the main word has been used before.	without
233	On Coreferring: Coreference In MUC And Related Annotation Schemes In this paper, it is argued that "coreference" annotations, as performed in the MUC community for example, go well beyond annotation of the relation of coreference proper. As a result, it is not always clear what semantic relation these annotations are encoding. The paper discusses a number of problems with these annotations and concludes that rethinking of the coreference task is needed before the task is expanded. In particular, it suggests a division of labor whereby annotation of the coreference relation proper is separated from other tasks such as annotation of bound anaphora and of the relation between a subject and a predicative NP. It suffers however from a number of problems (van Deemter and Kibble, 2000), chief among which is the fact that the one semantic relation expressed by the scheme, ident, conflates a number of relations that semanticists view as distinct: besides COREFERENCE proper, there are IDENTITY ANAPHORA, BOUND ANAPHORA, and even PREDICATION.	On Coreferring: Coreference In MUC And Related Annotation Schemes In this paper, it is argued that "coreference" annotations, as done in the MUC community for example, go far beyond just marking when two things refer to the same entity. As a result, it is not always clear what meaning these annotations are showing. The paper talks about several issues with these annotations and concludes that we need to rethink the coreference task before making it bigger. Specifically, it suggests dividing the work so that identifying when two things refer to the same entity is separate from other tasks like marking when a word refers back to something else (bound anaphora) and showing the relationship between a subject and a descriptive noun phrase (predicative NP). However, it has several problems (van Deemter and Kibble, 2000), especially that the one meaning relation defined by the scheme, called ident, mixes up several relations that language experts see as different: besides actual coreference, there are identity anaphora, bound anaphora, and even predication.	description
234	Unsupervised Learning Of The Morphology Of A Natural Language This study reports the results of using minimum description length (MDL) analysis to model unsupervised learning of the morphological segmentation of European languages, using corpora ranging in size from 5,000 words to 500,000 words. We develop a set of heuristics that rapidly develop a probabilistic morphological grammar, and use MDL as our primary tool to determine whether the modifications proposed by the heuristics will be adopted or not. The resulting grammar matches well the analysis that would be developed by a human morphologist. In the final section, we discuss the relationship of this style of MDL grammatical analysis to the notion of evaluation metric in early generative grammar. We propose a recursive structure such that stems can consist of a sub-stem and a suffix. We use a morphological representation based on signatures, which are sets of affixes that represent a family of words sharing an inflectional or derivational morphology. We observe that less frequent and shorter affixes are more likely to be erroneous.	Unsupervised Learning Of The Morphology Of A Natural Language This study talks about using a method called minimum description length (MDL) to understand how to break down words in European languages into smaller parts without prior examples, using text collections ranging from 5,000 to 500,000 words. We create a set of rules (heuristics) that quickly build a system to predict word parts, and we use MDL to decide if these changes should be kept. The resulting word analysis closely matches what a human expert would do. In the last part, we talk about how this way of using MDL relates to early ways of judging grammar rules. We suggest a repeated pattern where the main part of a word (stem) can include a smaller part and an ending (suffix). We use a method to show word parts based on "signatures," which are groups of word endings that show how words are related by their word form changes. We notice that less common and shorter word endings are more likely to be mistakes.	Related Annotation
235	Improving Accuracy In Word Class Tagging Through The Combination Of Machine Learning Systems We examine how differences in language models, learned by different data-driven systems performing the same NLP task, can be exploited to yield a higher accuracy than the best individual system. We do this by means of experiments involving the task of morphosyntactic word class tagging, on the basis of three different tagged corpora. Four well-known tagger generators (hidden Markov model, memory-based, transformation rules, and maximum entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second-stage classifiers. All combination taggers outperform their best component. The reduction in error rate varies with the material in question, but can be as high as 24.3% with the LOB corpus. We report on accuracy of arounf 97% with in-domain training data for POS tagging using the Penn Treebank.	Improving Accuracy In Word Class Tagging Through The Combination Of Machine Learning Systems We look at how using different language models, learned by different computer systems doing the same Natural Language Processing (NLP) job, can help achieve better accuracy than using just one system alone. We test this by doing experiments on morphosyntactic word class tagging, which means identifying word types like nouns and verbs, using three different collections of text that have already been tagged. We train four well-known tools (hidden Markov model, memory-based, transformation rules, and maximum entropy) on the same text data. Then, we compare their results and combine them using different voting methods and extra classifiers to improve performance. All the combined systems work better than the best single system. The decrease in mistakes depends on the material used, but can be as much as 24.3% with the LOB text collection. We report an accuracy of about 97% for identifying parts of speech (POS) using the Penn Treebank when the training and testing data are from similar sources.	repeated
236	Probabilistic Top-Down Parsing And Language Modeling This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition. The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling. A lexicalized probabilistic top-down parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers. A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity. Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model. A small recognition experiment also demonstrates the utility of the model. Our parser works left-to-right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning. At each word in the string, our top-down parser provides access to the weighted set of partial analyses in the beam.	Probabilistic Top-Down Parsing And Language Modeling This paper explains how a wide-ranging probabilistic top-down parser works and how it helps with creating language models used in speech recognition. The paper starts by explaining important ideas in language modeling and probabilistic parsing and gives a quick overview of how others have used sentence structure for language modeling before. It then introduces a specific type of probabilistic top-down parser that uses word information (lexicalized) and performs very well in terms of how accurately it interprets sentences and how efficiently it does this compared to other top statistical parsers. A new language model that uses this type of parsing is explained, showing through tests that it does better than older methods in understanding test texts (corpus perplexity). Mixing it with a trigram model (a model that looks at sequences of three words) leads to a big improvement, showing that the information our model captures is different from what the trigram model captures. A small test also shows how useful the model is. Our parser reads a sentence from start to finish (left-to-right) and skips using a method called dynamic programming, instead using a method called beam search, which keeps many possible sentence interpretations open by adding the next word, then cutting down the choices. At each word in the sentence, our top-down parser offers a set of partial interpretations with weights, showing how likely each one is.	language models
237	The Interaction Of Knowledge Sources In Word Sense Disambiguation Word sense disambiguation (WSD) is a computational linguistics task likely to benefit from the tradition of combining different knowledge sources in artificial intelligence research. An important step in the exploration of this hypothesis is to determine which linguistic knowledge sources are most useful and whether their combination leads to improved results. We present a sense tagger which uses several knowledge sources. Tested accuracy exceeds 94% on our evaluation corpus Our system attempts to disambiguate all content words in running text rather than limiting itself to treating a restricted vocabulary of words. It is argued that this approach is more likely to assist the creation of practical systems. We present a classifier combination framework where disambiguation methods (simulated annealing, subject codes and selectional restrictions) were combined using the TiMBL memory-based approach (Daelemans et al, 1999). We use Longman Dictionary of Contemporary English (LDOCE) as sense inventory. We use POS tags of the focus word itself to aid sense disambiguations related to syntactic differences. We suggest that use of both syntactic and lexical features will improve disambiguation accuracies.	The Interaction Of Knowledge Sources In Word Sense Disambiguation Word sense disambiguation (WSD) is a task in computer language studies that can benefit from using different types of knowledge, similar to how artificial intelligence research does. A key step in testing this idea is figuring out which types of language knowledge are most helpful and if combining them makes results better. We show a sense tagger, a tool that labels meanings, which uses several kinds of knowledge. Tested accuracy is above 94% on our test data. Our system tries to figure out the meaning of all main words in a text, instead of just focusing on a limited set of words. It's argued that this method is more likely to help make useful systems. We present a framework that combines different methods of figuring out word meanings, like simulated annealing (a problem-solving method), subject codes, and selectional restrictions (rules about which words can go together), using the TiMBL memory-based approach (a method developed by Daelemans and others in 1999). We use the Longman Dictionary of Contemporary English (LDOCE) as our list of word meanings. We use POS (part of speech) tags of the main word to help with disambiguation related to differences in grammar. We suggest that using both grammar-related and word-related features will make it easier to correctly figure out word meanings.	Language
238	Automatic Verb Classification Based On Statistical Distributions Of Argument Structure Automatic acquisition of lexical knowledge is critical to a wide range of natural language processing tasks. Especially important is knowledge about verbs, which are the primary source of relational information in a sentence--the predicate-argument structure that relates an action or state to its participants (i.e., who did what to whom). In this work, we report on supervised learning experiments to automatically classify three major types of English verbs, based on their argument structure--specifically, the thematic roles they assign to participants. We use linguistically-motivated statistical indicators extracted from large annotated corpora to train the classifier, achieving 69.8% accuracy for a task whose baseline is 34%, and whose expert-based upper bound we calculate at 86.5%. A detailed analysis of the performance of the algorithm and of its errors confirms that the proposed features capture properties related to the argument structure of the verbs. Our results validate our hypotheses that knowledge about thematic relations is crucial for verb classification, and that it can be gleaned from a corpus by automatic means. We thus demonstrate an effective combination of deeper linguistic knowledge with the robustness and scalability of statistical techniques. We work with a Decision Tree and selected linguistic cues to classify English verbs into three classes: unaccusative, unergative and object-drop.	Automatic Verb Classification Based On Statistical Distributions Of Argument Structure Automatic collection of word knowledge is important for many tasks involving how computers understand language. It's especially important to know about verbs, which are the main words that show how actions or states connect people or things in a sentence (like who did what to whom). In this study, we describe experiments using supervised learning (a method where computers learn from examples) to automatically sort three main types of English verbs, based on how they use other words in a sentence (their argument structure) and the roles they give to other words. We use statistical clues inspired by language rules, taken from large collections of text that have been labeled, to train the computer system. We achieved 69.8% accuracy for this task, where guessing would give 34% accuracy, and experts could get up to 86.5%. A closer look at how the system performs and where it makes mistakes shows that the features we used successfully identify how verbs structure sentences. Our findings support our ideas that knowing about these relationships is key for sorting verbs and that this knowledge can be gathered from texts automatically. We show a successful mix of deep language understanding with the strength and ability to handle large amounts of data using statistical methods. We use a Decision Tree (a model that makes decisions based on questions) and specific language clues to sort English verbs into three categories: unaccusative, unergative, and object-drop.	meanings
240	A Critique And Improvement Of An Evaluation Metric For Text Segmentation The Pk evaluation metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms. However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size distribution. We propose a simple modification to the Pk metric that remedies these problems. This new metric—called WindowDiff — moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text. As a measure for segmentation quality we develop WindowDiff, which only evaluates segment boundaries not the labels assigned to them.	A Critique And Improvement Of An Evaluation Metric For Text Segmentation The Pk evaluation metric, first suggested by Beeferman, Berger, and Lafferty (1997), is becoming the usual way to judge how well text segmentation algorithms work. But, a detailed study of this metric shows several issues: it is harsher on false negatives (missing a boundary) than false positives (adding an extra boundary), it punishes close guesses too much, and it is influenced by changes in the size of text parts. We suggest a simple change to the Pk metric to fix these problems. This new metric—called WindowDiff—uses a fixed-sized window that moves across the text and penalizes the algorithm when the number of boundaries within the window does not match the actual number of boundaries in that part of the text. As a way to measure how well text is divided, we develop WindowDiff, which only looks at where the boundaries are, not what labels are given to them.	methods
241	Generating Referring Expressions: Boolean Extensions Of The Incremental Algorithm This paper brings a logical perspective to the generation of referring expressions, addressing the incompleteness of existing algorithms in this area. After studying references to individual objects, we discuss references to sets, including Boolean descriptions that make use of negated and disjoined properties. To guarantee that a distinguishing description is generated whenever such descriptions exist, the paper proposes generalizations and extensions of the Incremental Algorithm of Dale and Reiter (1995).	Generating Referring Expressions: Boolean Extensions Of The Incremental Algorithm This paper looks at creating ways to talk about things from a logical viewpoint, fixing gaps in current methods. After looking at how we refer to single objects, we talk about how we refer to groups, including using words like "not" or "or" to describe them. To ensure we always create a unique description when possible, the paper suggests improving and expanding on the Incremental Algorithm by Dale and Reiter (1995).	guesses
242	Class-Based Probability Estimation Using A Semantic Hierarchy This article concerns the estimation of a particular kind of probability, namely, the probability of a noun sense appearing as a particular argument of a predicate. In order to overcome the accompanying sparse-data problem, the proposal here is to define the probabilities in terms of senses from a semantic hierarchy and exploit the fact that the senses can be grouped into classes consisting of semantically similar senses. There is a particular focus on the problem of how to determine a suitable class for a given sense, or, alternatively, how to determine a suitable level of generalization in the hierarchy. A procedure is developed that uses a chi-square test to determine a suitable level of generalization. In order to test the performance of the estimation method, a pseudo-disambiguation task is used, together with two alternative estimation methods. Each method uses a different generalization procedure; the first alternative uses the minimum description length principle, and the second uses Resnik’s measure of selectional preference. In addition, the performance of our method is investigated using both the standard Pearson chisquare statistic and the log-likelihood chi-square statistic. Briefly, we populate the WordNet hierarchy based on corpus frequencies (of all nouns for a verb/slot pair), and then determine the appropriate probability estimate at each node in the hierarchy by using chi square to determine whether to generalize an estimate to a parent node in the hierarchy.	Class-Based Probability Estimation Using A Semantic Hierarchy This article is about estimating a certain type of probability, specifically, the chance of a noun meaning appearing as a specific part of a sentence. To deal with the issue of not having enough data, the idea is to define probabilities using a system of related meanings and take advantage of the fact that these meanings can be grouped into categories with similar meanings. The focus is on how to find the right category for a given meaning or how to decide on the right level of generalization in this system. A method is created that uses a statistical test called the chi-square test to find the right level of generalization. To test how well the estimation method works, we use a task that simulates choosing between meanings, along with two other estimation methods. Each method uses a different way to generalize: the first uses a principle called the minimum description length, and the second uses Resnik’s method for choosing preferences. We also check how well our method works using both the standard and a different version of the chi-square statistic. In short, we fill in the WordNet hierarchy based on how often nouns appear with a verb or slot, and then find the right probability estimate at each point in the hierarchy by using the chi-square test to decide if we should generalize an estimate to a higher level in the hierarchy.	Expressions
243	Automatic Labeling Of Semantic Roles We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles, such as Agent or Patient, or more domain-specific semantic roles, such as Speaker, Message, and Topic. The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and its position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible fillers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classifiers. Our system achieves 82% accuracy in identifying the semantic role of presegmented constituents. At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall. Our study also allowed us to compare the usefulness of different features and feature combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicates unseen in the training data. We propose the first SRL model on FrameNet.	Automatic Labeling Of Semantic Roles We present a system for identifying the meaning-based relationships, or roles, that parts of a sentence play within a context. Given a sentence, a key word, and a context, the system labels parts of the sentence with general roles, like Agent (doer) or Patient (receiver), or more specific roles like Speaker, Message, and Topic. The system uses statistical programs trained on about 50,000 sentences that were manually labeled with these roles by the FrameNet project. We then broke down each training sentence into a structure tree and picked out different word and structure features, including the type of phrase, its role in the grammar, and its spot in the sentence. These features were mixed with information about the main verb, noun, or adjective, and other details like the chances of different role combinations. We used various methods to group similar words to cover all possible role fillers. Test sentences were structured, labeled with these features, and then run through the programs. Our system correctly identifies 82% of the roles for already divided sentence parts. For the harder task of dividing and identifying roles at the same time, the system did 65% correct labeling and 61% in finding all correct labels. Our study also let us compare which features and ways of combining them are most helpful for this task. We also look into mixing role labeling with sentence structure analysis and try to apply it to new situations not covered in training. We propose the first model for this task using FrameNet.	Semantic
244	Summarizing Scientific Articles: Experiments With Relevance And Rhetorical Status In this article we propose a strategy for the summarization of scientific articles that concentrates on the rhetorical status of statements in an article: Material for summaries is selected in such a way that summaries can highlight the new contribution of the source article and situate it with respect to earlier work. We provide a gold standard for summaries of this kind consisting of a substantial corpus of conference articles in computational linguistics annotated with human judgments of the rhetorical status and relevance of each sentence in the articles. We present several experiments measuring our judges’ agreement on these annotations. We also present an algorithm that, on the basis of the annotated training material, selects content from unseen articles and classifies it into a fixed set of seven rhetorical categories. The output of this extraction and classification system can be viewed as a single-document summary in its own right; alternatively, it provides starting material for the generation of task-oriented and user-tailored summaries designed to give users an overview of a scientific field. We examine the problem of summarizing scientific articles using rhetorical analysis of sentences. We summarize scientific articles by selecting rhetorical elements that are commonly present in scientific abstracts.	Summarizing Scientific Articles: Experiments With Relevance And Rhetorical Status In this article, we suggest a way to summarize scientific articles by focusing on the importance of statements in an article. The summaries are created to show the new ideas of the article and compare them to past research. We offer a model for these summaries using a large collection of conference articles in computational linguistics, marked with human opinions on the importance and relevance of each sentence. We show several tests on how much our judges agree on these markings. We also introduce a program that uses the marked training material to pick out content from new articles and sorts it into seven fixed categories based on their rhetorical, or persuasive, style. The result of this system can be seen as a summary of a single document; or it can be used as a base to create summaries tailored to specific tasks or users, helping them understand a scientific field. We look at the challenge of summarizing scientific articles by analyzing the persuasive style of sentences. We summarize scientific articles by choosing common persuasive elements found in scientific summaries.	group similar
245	A Systematic Comparison Of Various Statistical Alignment Models We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented. The trial and test data had been manually aligned at the word level, noting particular pairs of words either as 'sure' or 'possible' alignments.	A Systematic Comparison Of Various Statistical Alignment Models We present and compare different methods for finding word alignments using statistical or rule-based (heuristic) models. We look at the five alignment models introduced by Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov model (a statistical model that predicts sequences), techniques to smooth out data, and improvements. These statistical models are compared with two rule-based models based on the Dice coefficient (a mathematical formula to compare how similar two sets are). We show different ways to combine word alignments to make a balanced version of directed statistical alignment models. To judge them, we look at how good the resulting Viterbi alignment (a method for finding the most likely sequence of states) is compared to a hand-made reference alignment. We test the models using tasks that involve translating between German and English (Verbmobil) and French and English (Hansards). We carefully examine different choices in designing our statistical alignment system and test these on training texts of different sizes. An important finding is that improved alignment models that consider the order of words and how often they appear together give much better results than simple rule-based models. In the Appendix, we show a fast training method for the alignment models discussed. The trial and test data were manually aligned at the word level, marking specific word pairs as 'sure' or 'possible' alignments.	specific
246	Graph-Based Generation Of Referring Expressions This article describes a new approach to the generation of referring expressions. We propose to formalize a scene (consisting of a set of objects with various properties and relations) as a labeled directed graph and describe content selection (which properties to include in a referring expression) as a subgraph construction problem. Cost functions are used to guide the search process and to give preference to some solutions over others. The current approach has four main advantages: (1) Graph structures have been studied extensively, and by moving to a graph perspective we get direct access to the many theories and algorithms for dealing with graphs; (2) many existing generation algorithms can be reformulated in terms of graphs, and this enhances comparison and integration of the various approaches; (3) the graph perspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches. One of the strengths of the Graph-Based Algorithm is its ability to generate expressions that involve relations between objects, and these include spatial ones (next to, on top of, etc.).	Graph-Based Generation Of Referring Expressions This article explains a new way to create referring expressions, which are phrases that describe specific things. We suggest representing a scene (a group of things with different characteristics and connections) as a labeled directed graph, which is a type of diagram. Deciding which details to include in a referring expression is like solving a puzzle where you only use part of this diagram. We use cost functions, which are rules that help us choose the best solution, to make the search easier. This method has four main benefits: (1) Graph structures have been well-studied, so by using graphs we can use many existing theories and techniques; (2) many current methods can be reworked using graphs, allowing us to compare and combine different methods more easily; (3) using graphs helps solve problems that older methods had when creating referring expressions; and (4) using graphs along with cost functions helps mix traditional rule-based methods with newer random-based ones. A key strength of this graph-based method is its ability to create expressions that show how things are related, including where they are in space (like next to or on top of something).	English
247	Word Reordering And A Dynamic Programming Beam Search Algorithm For Statistical Machine Translation In this article, we describe an efficient beam search algorithm for statistical machine translation based on dynamic programming (DP). The search algorithm uses the translation model presented in Brown et al. (1993). Starting from a DP-based solution to the traveling-salesman problem, we present a novel technique to restrict the possible word reorderings between source and target language in order to achieve an efficient search algorithm. Word reordering restrictions especially useful for the translation direction German to English are presented. The restrictions are generalized, and a set of four parameters to control the word reordering is introduced, which then can easily be adopted to new translation directions. The beam search procedure has been successfully tested on the Verbmobil task (German to English, 8,000-word vocabulary) and on the Canadian Hansards task (French to English, 100,000-word vocabulary). For the medium-sized Verbmobil task, a sentence can be translated in a few seconds, only a small number of search errors occur, and there is no performance degradation as measured by the word error criterion used in this article. In our work, a beam-search algorithm used for TSP is adapted to work with an IBM-4 word-based model and phrase-based model respectively.	Word Reordering And A Dynamic Programming Beam Search Algorithm For Statistical Machine Translation In this article, we explain an effective method called a beam search algorithm, which helps in translating languages using a system called dynamic programming (DP). This search method uses a translation model from Brown and others (1993). We start with a DP solution to a common problem called the traveling-salesman problem and introduce a new way to limit how words can be rearranged between the original and target language to make the search process better. These word rearrangement limits are especially helpful when translating from German to English. We make these limits more general and introduce four simple settings to control how words are rearranged, which can easily be used for new translation tasks. This beam search method has been tested successfully on the Verbmobil task (translating German to English, with 8,000 different words) and on the Canadian Hansards task (translating French to English, with 100,000 different words). For the moderate-sized Verbmobil task, a sentence can be translated quickly in just a few seconds, with only a few mistakes, and no drop in quality, according to the error measurement used in this article. In our research, we adapted a beam-search method used for TSP to work with an IBM-4 word-based model and a phrase-based model respectively.	reworked using
248	Introduction To The Special Issue On The Web As Corpus The Web, teeming as it is with language data, of all manner of varieties and languages, in vast quantity and freely available, is a fabulous linguists’ playground. This special issue of Computational Linguistics explores ways in which this dream is being explored. It is natural to question the appropriateness of web data for research purposes, because web data is inevitably noisy and search engines themselves can introduce certain idiosyncracies which can distort results.	Introduction To The Special Issue On The Web As Corpus The Web, full of language data in different forms and languages, is a great place for linguists to explore. This special issue of Computational Linguistics looks at how this exciting idea is being investigated. It is normal to wonder if web data is suitable for research because it can be messy and search engines can add unique quirks that might change the results.	article
249	The Web As A Parallel Corpus Parallel corpora have become an essential resource for work in multilingual natural language processing. In this article, we report on our work using the STRAND system for mining parallel text on the World Wide Web, first reviewing the original algorithm and results and then presenting a set of significant enhancements. These enhancements include the use of supervised learning based on structural features of documents to improve classification performance, a new content based measure of translational equivalence, and adaptation of the system to take advantage of the Internet Archive for mining parallel text from the Web on a large scale. Finally, the value of these techniques is demonstrated in the construction of a significant parallel corpus for a low-density language pair. We mine parallel web documents within bilingual web sites first and then extract bilingual sentences from mined parallel documents using sentence alignment method. We exploit the similarities in URL structure, document structure and other clues for mining the Web for parallel documents.	The Web As A Parallel Corpus Parallel corpora, which are collections of texts in different languages that are matched, are important for working with multiple languages in natural language processing (teaching computers to understand human languages). In this article, we share our work using the STRAND system to find parallel texts on the internet. We first look at the original method and results, then show important improvements. These improvements include using supervised learning (a way to train computers) based on the structure of documents to better sort them, a new way to measure how well texts match in translation, and adjusting the system to use the Internet Archive to find parallel texts on the Web in a big way. Finally, we show how useful these techniques are by building a large collection of matched texts for a pair of languages that don't have many resources. We first find parallel documents on bilingual websites and then extract matching sentences using a method to align sentences. We use similarities in how URLs (web addresses) and documents are structured, along with other hints, to find parallel documents on the Web.	different
250	Using The Web To Obtain Frequencies For Unseen Bigrams This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudodisambiguation task. Our study reveals that the large amount of data available for the web counts could outweigh the noisy problems.	Using The Web To Obtain Frequencies For Unseen Bigrams This article shows that the Web can be used to find how often certain pairs of words appear when they haven't been seen in a specific collection of texts. We explain a way to get numbers for pairs like adjective-noun, noun-noun, and verb-object from the Web by using a search engine. We test this way by showing: (a) a strong link between Web numbers and those from a text collection; (b) a dependable link between Web numbers and how believable people think the word pairs are; (c) a dependable link between Web numbers and numbers recreated using a method to fill in missing data; (d) good results from Web numbers in a task to check word pair meanings. Our study shows that the huge amount of information on the Web can be more useful than the issues caused by messy data.	Parallel
251	Head-Driven Statistical Models For Natural Language Parsing This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models. We propose to generate the head of a phrase first and then generate its sisters using Markovian processes, thereby exploiting head/sister-dependencies.	Head-Driven Statistical Models For Natural Language Parsing This article explains three statistical methods for understanding natural language structure. These methods build on techniques from simple grammar rules that involve probabilities, adding more detail by focusing on important words in sentences. This creates a structure where a sentence is broken down step-by-step, starting with the main word. The assumptions made lead to rules that capture different grammar patterns like sentence structure, word order, and connections between words. These rules are expressed using chances based on key words. The models are tested using a collection of sentence structures from the Wall Street Journal, showing that they are as accurate as other known models. To understand the models better, we also look at how they perform with different parts of sentences and how well they identify various connections between words. We study the models' features by checking how accurately they can pick up sentence structures, counting how often different structures appear, and using examples based on language rules. Lastly, we compare these models with others to explain why some work better than others. We suggest creating the main word of a phrase first and then the related words, using a process that takes advantage of connections between these words.	information
252	Disambiguating Nouns Verbs And Adjectives Using Automatically Acquired Selectional Preferences Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information. We evaluate WSD using selectional preferences acquired for English adjective–noun, subject, and direct object grammatical relationships with respect to a standard test corpus. The selectional preferences are specific to verb or adjective classes, rather than individual word forms, so they can be used to disambiguate the co-occurring adjectives and verbs, rather than just the nominal argument heads. We also investigate use of the one-sense-per-discourse heuristic to propagate a sense tag for a word to other occurrences of the same word within the current document in order to increase coverage. Although the preferences perform well in comparison with other unsupervised WSD systems on the same corpus, the results show that for many applications, further knowledge sources would be required to achieve an adequate level of accuracy and coverage. In addition to quantifying performance, we analyze the results to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance. We report that the Word-Class Model performs well in unsupervised WSD.	Disambiguating Nouns Verbs And Adjectives Using Automatically Acquired Selectional Preferences Selectional preferences, which are patterns showing how words typically go together, have been used by word sense disambiguation (WSD) systems to help figure out the right meaning of a word. We test WSD using selectional preferences collected for English adjective–noun pairs, and sentence parts like subjects and direct objects, using a standard test set. These preferences are linked to types of verbs or adjectives, not just specific words, so they help clarify which adjectives and verbs go together, not just the main noun in a sentence. We also look at using a method called one-sense-per-discourse, which assumes a word has the same meaning throughout a document, to spread a word’s meaning to all its uses in that document, aiming to improve results. While these preferences work well compared to other systems that don’t rely on pre-labeled data, more information sources are needed to improve accuracy and effectiveness for many tasks. Besides measuring how well the system works, we look at when these preferences are most precise and when using one-sense-per-discourse boosts performance. We find that the Word-Class Model, which groups words by type, works effectively in unsupervised WSD, meaning it doesn't need pre-labeled data.	suggest creating
253	CorMet: A Computational Corpus-Based Conventional Metaphor Extraction System CorMet is a corpus-based system for discovering metaphorical mappings between concepts. It does this by finding systematic variations in domain-specific selectional preferences, which are inferred from large, dynamically mined Internet corpora. Metaphors transfer structure from a source domain to a target domain, making some concepts in the target domain metaphorically equivalent to concepts in the source domain. The verbs that select for a concept in the source domain tend to select for its metaphorical equivalent in the target domain. This regularity, detectable with a shallow linguistic analysis, is used to find the metaphorical interconcept mappings, which can then be used to infer the existence of higher-level conventional metaphors. Most other computational metaphor systems use small, hand-coded semantic knowledge bases and work on a few examples. Although CorMet’s only knowledge base is WordNet (Fellbaum 1998) it can find the mappings constituting many conventional metaphors and in some cases recognize sentences instantiating those mappings. CorMet is tested on its ability to find a subset of the Master Metaphor List (Lakoff, Espenson, and Schwartz 1991). The CorMet system dynamically mines domain specific corpora to find less frequent usages and identifies conceptual metaphors. We show how statistical analysis can automatically detect and extract conventional metaphors from corpora, though creative metaphors still remain a tantalizing challenge.	CorMet: A Computational Corpus-Based Conventional Metaphor Extraction System CorMet is a system that uses large collections of text to find metaphorical connections between ideas. It does this by looking for patterns in how words are used in different subject areas, based on information gathered from the internet. Metaphors take ideas from one area (source) and apply them to another (target), making some ideas seem similar in both areas. The verbs (action words) used with a concept in the source area also often apply to its metaphorical equivalent in the target area. This pattern, which can be spotted with a simple language analysis, helps identify metaphorical links between ideas, which can then suggest the presence of common metaphors. Most other systems use small, manually created databases of word meanings and focus on a few examples. Although CorMet only uses a resource called WordNet, it can find many common metaphorical connections and sometimes recognize sentences that use these connections. CorMet is tested on its ability to find a part of the Master Metaphor List. The CorMet system gathers specific subject area texts to find less common uses and identifies metaphorical ideas. We show how using statistical analysis can automatically find and pull out common metaphors from texts, although coming up with new and creative metaphors is still a difficult task.	adjective
254	The Kappa Statistic: A Second Look In recent years, the kappa coefficient of agreement has become the de facto standard for evaluating intercoder agreement for tagging tasks. In this squib, we highlight issues that affect κ and that the community has largely neglected. First, we discuss the assumptions underlying different computations of the expected agreement component of κ. Second, we discuss how prevalence and bias affect the κ measure.	The Kappa Statistic: A Second Look In recent years, the kappa coefficient (a measure for how much two or more people agree when they label or tag things) has become the usual standard for checking how much people agree in tagging tasks. In this short article, we point out problems that affect kappa, which many people have mostly ignored. First, we talk about the assumptions or things taken for granted when calculating the expected agreement part of kappa. Second, we talk about how commonness (prevalence) and unfairness (bias) affect the kappa measure.	sometimes
255	Statistical Machine Translation With Scarce Resources Using Morpho-Syntactic Information In statistical machine translation, correspondences between the words in the source and the target language are learned from parallel corpora, and often little or no linguistic knowledge is used to structure the underlying models. In particular, existing statistical systems for machine translation often treat different inflected forms of the same lemma as if they were independent of one another. The bilingual training data can be better exploited by explicitly taking into account the interdependencies of related inflected forms. We propose the construction of hierarchical lexicon models on the basis of equivalence classes of words. In addition, we introduce sentence-level restructuring transformations which aim at the assimilation of word order in related sentences. We have systematically investigated the amount of bilingual training data required to maintain an acceptable quality of machine translation. The combination of the suggested methods for improving translation quality in frameworks with scarce resources has been successfully tested: We were able to reduce the amount of bilingual training data to less than 10% of the original corpus, while losing only 1.6% in translation quality. The improvement of the translation results is demonstrated on two German-English corpora taken from the Verbmobil task and the Nespole! task. We decompose German words into a hierarchical representation using lemmas and morphological tags, and use a MaxEnt model to combine the different levels of representation in the translation model. We describe a method that combines morphologically split verbs in German, and also reorders questions in English and German.	Statistical Machine Translation With Scarce Resources Using Morpho-Syntactic Information In statistical machine translation, the system learns how words in one language relate to words in another language by using collections of texts that exist in both languages. Often, these systems don't use much language knowledge to build their models. Many current translation systems wrongly treat different forms of a word as if they were unrelated. We can use the bilingual text data more effectively by recognizing how these word forms are connected. We suggest building word models based on groups of similar words. We also introduce changes to sentence structure to make the word order in related sentences more similar. We studied how much bilingual text data is needed to keep translation quality acceptable. Our methods for improving translation with limited data have been tested successfully: we reduced the bilingual text data to less than 10% of the original, losing only 1.6% in quality. The translation improvements are shown using two German-English text collections from the Verbmobil and Nespole! tasks. We break down German words into a structure using basic word forms and language tags, and use a MaxEnt model to combine these for translation. We explain a method that reconnects split verbs in German and rearranges questions in both English and German.	commonness
256	Learning Subjective Language Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization. The goal of this work is learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity. The features are also examined working together in concert. The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets. In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article. We show that low-frequency words and some collocations are a good indicators of subjectivity.	Learning Subjective Language Subjectivity in natural language is about using words to show personal opinions, thoughts, or guesses. It is important for many tasks in natural language processing, like pulling out information or sorting text. This study aims to learn how to identify subjective language from large collections of written text. Signs of subjectivity are found and checked, including rare words, word pairs that often appear together, and descriptions and actions identified by comparing word usage. These characteristics are also analyzed to see how they work together. These characteristics, drawn from various data using different methods, show consistent results, meaning they perform similarly across different data sets. Moreover, this article shows that the amount of subjective clues around a word greatly influences how likely it is to be subjective, and it shares the findings of a study that rated how subjective sentences with many such clues are. Finally, these clues are used to identify opinion-based writing (a way to sort text and detect writing style) to show the usefulness of what was learned in this article. We demonstrate that rare words and some common word pairs are strong signs of subjectivity.	original
257	The Alignment Template Approach To Statistical Machine Translation A phrase-based statistical machine translation approach — the alignment template approach — is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source–channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German–English speech Verbmobil task, we analyze the effect of various system components. On the French–English Canadian Hansards task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese–English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems. We describe a phrase-extract algorithm for extracting phrase pairs from a sentence pair annotated with a 1-best alignment.	The Alignment Template Approach To Statistical Machine Translation A method called the alignment template approach is explained, which is a way to use computers to translate languages by looking at groups of words together. This method helps understand how words relate to each other in both languages and learns how to change the order of words correctly. The model uses a flexible way of adjusting rules, making it easier to improve than older translation systems. We detail how to learn and find good translations of phrases, the steps used, and how the system searches for translations. This method is tested on three tasks. For the German-English speech task called Verbmobil, we study how different parts of the system work. In the French-English Canadian Hansards task, this system works much better than translating one word at a time. In a Chinese-English test in 2002 by the National Institute of Standards and Technology (NIST), it scores much higher than other translation systems. We explain a method to find pairs of phrases from sentences that have been matched up with their best translation.	methods
258	Intricacies Of Collins Parsing Model This article documents a large set of heretofore unpublished details Collins used in his parser, such that, along with Collins’ (1999) thesis, this article contains all information necessary to duplicate Collins’ benchmark results. Indeed, these as-yet-unpublished details account for an 11% relative increase in error from an implementation including all details to a clean-room implementation of Collins’ model. We also show a cleaner and equally well-performing method for the handling of punctuation and conjunction and reveal certain other probabilistic oddities about Collins’ parser. We not only analyze the effect of the unpublished details, but also reanalyze the effect of certain well-known details, revealing that bilexical dependencies are barely used by the model and that head choice is not nearly as important to overall parsing performance as once thought. Finally, we perform experiments that show that the true discriminative power of lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and its part of speech. The results suggest that the power of Collins-style parsing models did not lie primarily with the use of bilexical dependencies as was once thought, but in lexico-structural dependencies, that is, predicting syntactic structures conditioning on head words. We show that bilexical-information is used in only 1.49% of the decisions in Collins' Model-2 parser, and that removing this information results in an exceedingly small drop in performance.	Intricacies Of Collins Parsing Model This article shares many details about Collins' parser that were not previously published. Together with Collins' 1999 thesis, this article gives all the information needed to recreate Collins' results. These previously unpublished details explain why there is an 11% increase in errors when not using all the details in a simpler version of Collins’ model. We also present a simpler and equally effective way to handle punctuation and connecting words, and we show some surprising things about how Collins’ parser works. We not only look at the impact of the unpublished details but also reassess some well-known details, showing that the model barely uses word-to-word connections and that choosing the main word is not as crucial as previously thought. Finally, we run tests that show the real strength of adding word details is in using the main word and its type to create sentence structures. The results suggest that the strength of Collins-style parsing models is not mainly due to word-to-word connections, but in predicting structures based on main words. We show that word-to-word information is used in only 1.49% of decisions in Collins' Model-2 parser, and removing this information causes only a very small decrease in performance.	system works
259	Discriminative Reranking For Natural Language Parsing This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75% F-measure, a 13% relative decrease in F-measure error over the baseline model’s score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative—in terms of both simplicity and efficiency—to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation. We show that applying reranking techniques to the n-best output of a base parser can improve parsing performance. We propose a method only updates values of features co-occurring with a rule feature on examples at each iteration.	Discriminative Reranking For Natural Language Parsing This article looks at methods to improve the results of a probabilistic parser, which is a tool that analyzes sentence structure based on likelihood. The initial parser gives different possible sentence structures, each with a probability score. A second model tries to make the ranking of these structures better by using extra details in the tree structure. The advantage of our method is that it can use any set of features (details) of a tree without worrying about how they might interact or overlap, and without needing a complex model to explain how they work together. We introduce a new technique to improve ranking, based on a method called boosting, which is a way to enhance results by Freund et al. (1998). We use this boosting method on a set of data from the Wall Street Journal. This method combines a basic model's prediction with extra information from 500,000 additional features that weren't in the original model. The new model achieved a score of 89.75% in F-measure, which is a metric for accuracy, showing a 13% better error rate than the basic model’s score of 88.2%. We also present a new algorithm for the boosting method that efficiently handles the large number of features in the data. Tests show that this new algorithm is much faster than the usual way of using boosting. We believe this method is a good alternative because it's simple and efficient, compared to other methods that choose which features to use in complex models. Although our tests focus on natural language parsing (understanding sentence structure), this method could also be used in other language tasks that involve ranking, like speech recognition, translating languages, or generating text. We demonstrate that enhancing the ranking of the top options from a basic parser can improve how well it works. We suggest a method that only changes the feature values that appear together with a rule feature during each step of the process.	information
269	Feature Forest Models for Probabilistic HPSG Parsing Probabilistic modeling of lexicalized grammars is difficult because these grammars exploit complicated data structures, such as typed feature structures. This prevents us from applying common methods of probabilistic modeling in which a complete structure is divided into substructures under the assumption of statistical independence among sub-structures. For example, part-of-speech tagging of a sentence is decomposed into tagging of each word, and CFG parsing is split into applications of CFG rules. These methods have relied on the structure of the target problem, namely lattices or trees, and cannot be applied to graph structures including typed feature structures. This article proposes the feature forest model as a solution to the problem of probabilistic modeling of complex data structures including typed feature structures. The feature forest model provides a method for probabilistic modeling without the independence assumption when probabilistic events are represented with feature forests. Feature forests are generic data structures that represent ambiguous trees in a packed forest structure. Feature forest models are maximum entropy models defined over feature forests. A dynamic programming algorithm is proposed for maximum entropy estimation without unpacking feature forests. Thus probabilistic modeling of any data structures is possible when they are represented by feature forests. This article also describes methods for representing HPSG syntactic structures and predicate–argument structures with feature forests. Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing. The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed.	Feature Forest Models for Probabilistic HPSG Parsing Probabilistic modeling of lexicalized grammars (a type of grammar where each word has a specific role) is hard because these grammars use complex data structures, like typed feature structures (organized data with specific types). This makes it hard to use standard methods of probabilistic modeling, where complex structures are broken into smaller parts and assumed to work independently. For instance, breaking down sentence tagging into tagging each word or splitting CFG parsing (a type of grammar parsing) into applying rules. These methods depend on simpler structures like sequences or trees, and don't work well for graph-like structures including typed feature structures. This article suggests the feature forest model as a solution for modeling complex data structures like typed feature structures probabilistically. The feature forest model allows for probabilistic modeling without assuming independence between events when these events are shown as feature forests. Feature forests are general structures that pack multiple possible tree outcomes into one. Feature forest models use maximum entropy models (a type of statistical model) for feature forests. A dynamic programming algorithm (an efficient calculation method) is suggested for estimating maximum entropy without taking apart feature forests. This way, probabilistic modeling of any data structure is possible when using feature forests. The article also explains how to represent HPSG (a type of grammar) and predicate-argument structures with feature forests. Thus, it provides a full strategy for creating probabilistic models for HPSG parsing. The effectiveness of these methods is tested through experiments with the Penn Treebank (a database of annotated sentences), and the potential for using them on real-world sentences is discussed.	article turns
260	The Proposition Bank: An Annotated Corpus Of Semantic Roles The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated. We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty "trace" categories of the treebank. As proposition banks are semantically annotated versions of a Penn-style tree bank, they provide consistent semantic role labels across different syntactic realizations of the same verb.	The Proposition Bank: An Annotated Corpus Of Semantic Roles The Proposition Bank project takes a practical approach to showing meaning, adding a layer of information about who is doing what to whom (semantic role labels) to the sentence structures of the Penn Treebank. The resulting resource is considered simple because it doesn't show things like references to earlier mentioned items, amounts, and other complex features, but it is also broad because it includes every use of every verb in the collection of texts and allows for useful statistics to be created. We talk about the guidelines used to define the sets of roles in the labeling process and to study how often sentence structures change their meaning in the collection of texts. We explain an automatic system for tagging these roles, trained using the collection, and discuss how different types of information affect its performance, including comparing complete sentence analysis with a simpler form and the role of the "trace" categories that don't have a direct word in the treebank. As proposition banks are semantically annotated versions of a Penn-style tree bank, they provide consistent labels for roles across different sentence structures using the same verb.	gives different
261	Sentence Fusion For Multidocument News Summarization A system that can produce informative summaries, highlighting common information found in many online documents, will help Web users to pinpoint information that they need without extensive reading. In this article, we introduce sentence fusion, a novel text-to-text generation technique for synthesizing common information across documents. Sentence fusion involves bottom-up local multisequence alignment to identify phrases conveying similar information and statistical generation to combine common phrases into a sentence. Sentence fusion moves the summarization field from the use of purely extractive methods to the generation of abstracts that contain sentences not found in any of the input documents and can synthesize information across sources. We represent the inputs by dependency trees, align some words to merge the input trees into a lattice, and then extract a single, connected dependency tree as the output. We introduce the problem of converting multiple sentences into a single summary sentence.	Sentence Fusion For Multidocument News Summarization A system that can create informative summaries by showing shared information from many online articles will help people find what they need without having to read a lot. In this article, we introduce sentence fusion, a new way to create text by combining common information from different documents. Sentence fusion includes a step-by-step process to find phrases that have similar meanings and uses statistics to mix these phrases into one sentence. Sentence fusion improves summarization from just copying sentences to creating new summaries with sentences not found in the original documents, combining information from different sources. We show the input as dependency trees (a way to organize sentence structure), align some words to merge these trees into a network, and then take out a single, connected tree as the final result. We present the challenge of turning multiple sentences into one summary sentence.	includes
262	Improving Machine Translation Performance By Exploiting Non-Parallel Corpora We present a novel method for discovering parallel sentences in comparable, non-parallel corpora. We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other. Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus. Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available. We use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles. We filter out negative examples with high length difference or low word overlap (based on a bilingual dictionary). We define features primarily based on IBM Model 1 alignments (Brown et al, 1993).	Improving Machine Translation Performance By Exploiting Non-Parallel Corpora We introduce a new way to find matching sentences in texts that aren't already matched up. We teach a computer program to decide if two sentences mean the same thing in different languages. We use this method to gather matching sentences from large collections of Chinese, Arabic, and English newspapers that don't already have paired translations. We check the quality of these sentences by showing they make a top-notch translation system work better. We also demonstrate that a good translation system can be created from the ground up by starting with a small set of matched sentences (100,000 words) and making use of a large collection of unmatched texts. This means our method is useful for language pairs with limited translation resources. We use article dates and word similarity (after translating words with a bilingual dictionary) to find related news stories. We remove bad matches that are too different in length or have few words in common (using a bilingual dictionary). We use features mainly based on a specific word alignment method from IBM's research (Brown et al, 1993).	original
263	Evaluating WordNet-based Measures of Lexical Semantic Relatedness The quantification of lexical semantic relatedness has many applications in NLP, and many different measures have been proposed. We evaluate five of these measures, all of which use WordNet as their central resource, by comparing their performance in detecting and correcting real-word spelling errors. An information-content–based measure proposed by Jiang and Conrath is found superior to those proposed by Hirst and St-Onge, Leacock and Chodorow, Lin, and Resnik. In addition, we explain why distributional similarity is not an adequate proxy for lexical semantic relatedness. WordNet based measures are well known to be better suited to measure similarity than relatedness due to its hierarchical, taxonomic structure.	Evaluating WordNet-based Measures of Lexical Semantic Relatedness The measurement of how words are related in meaning has many uses in Natural Language Processing (NLP), and many different methods have been suggested. We review five of these methods, all of which use WordNet (a large database of words) as their main tool, by testing how well they find and fix real-word spelling mistakes. A method based on information content, suggested by Jiang and Conrath, is found to be better than those suggested by Hirst and St-Onge, Leacock and Chodorow, Lin, and Resnik. Additionally, we discuss why using distributional similarity (looking at how often words appear together) is not a good substitute for measuring how closely related words are in meaning. Measures based on WordNet are known to be better at measuring similarity (how alike things are) than relatedness (how things are connected) because of its organized, category-based structure.	English
264	Similarity of Semantic Relations There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood. This article introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector of frequencies of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1) The patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM. We develop a corpus based approach to model relational similarity, addressing (among other tasks) the distinction between synonyms and antonyms. We describe a method (Latent Relational Analysis) that extracts subsequence patterns for noun pairs from a large corpus, using query expansion to increase the recall of the search and feature selection and dimensionality reduction to reduce the complexity of the feature space.	Similarity of Semantic Relations There are at least two types of similarity. Relational similarity is a match between relationships, while attributional similarity is a match between characteristics. When two words are very similar in characteristics, we call them synonyms. When two pairs of words have very similar relationships, we say their relationships are analogous, meaning they are similar in a specific way. For example, the word pair mason:stone is similar in relationship to the pair carpenter:wood. This article introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA can be used in many fields, such as extracting information, understanding different meanings of words, and finding information. Recently, the Vector Space Model (VSM) used for finding information has been adjusted to measure relational similarity, reaching a score of 47% on a set of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relationship between a pair of words is shown by a vector, which is a series of numbers, that counts how often certain patterns appear in a large collection of texts. LRA improves the VSM approach in three ways: (1) The patterns are automatically taken from the collection of texts, (2) the Singular Value Decomposition (SVD), a mathematical method, is used to simplify the frequency data, and (3) automatically generated synonyms are used to explore different variations of the word pairs. LRA achieves 56% on the 374 analogy questions, which is statistically similar to the average human score of 57%. On the related problem of classifying semantic relations, which means understanding different kinds of word relationships, LRA shows similar improvements over the VSM. We develop a method based on a collection of texts to model relational similarity, addressing tasks like telling the difference between synonyms and antonyms. We describe a method (Latent Relational Analysis) that finds pattern sequences for noun pairs from a large collection of texts, using query expansion to increase the range of the search and feature selection and dimensionality reduction to make the feature space less complex.	similarity
265	Hierarchical Phrase-Based Translation   We present a statistical machine translation model that uses hierarchical phrases—phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations. Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation. We describe our system’s training and decoding methods in detail, and evaluate it for translation speed and translation accuracy. Using BLEU as a metric of translation accuracy, we find that our system performs significantly better than the Alignment Template System, a state-of-the-art phrase-based system. The hierarchical phrase-based model makes an advance of statistical machine translation by employing hierarchical phrases, which not only uses phrases to learn local translations but also uses hierarchical phrases to capture reorderings of words and subphrases which can cover a large scope.	Hierarchical Phrase-Based Translation We introduce a statistical machine translation model that uses hierarchical phrases, which are phrases containing smaller phrases within them. The model is technically a synchronous context-free grammar (a type of grammar used to describe languages) but it learns from a parallel text (text in two languages side by side) without needing any syntax rules. This means it combines basic ideas from both syntax-based (rules of sentence structure) and phrase-based (using groups of words) translation. We explain in detail how our system learns and processes translations, and we test its speed and accuracy. By using BLEU (a measure for checking translation quality), we find that our system works much better than the Alignment Template System, which is a leading phrase-based system. The hierarchical phrase-based model improves statistical machine translation by using hierarchical phrases, which not only helps in learning translations of small parts but also helps in rearranging words and phrases over a larger context.	measure
266	CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations augmented with local and long-range word–word dependencies. The resulting corpus, CCGbank, includes 99.4% of the sentences in the Penn Treebank. It is available from the Linguistic Data Consortium, and has been used to train wide coverage statistical parsers that obtain state-of-the-art rates of dependency recovery. In order to obtain linguistically adequate CCG analyses, and to eliminate noise and inconsistencies in the original annotation, an extensive analysis of the constructions and annotations in the Penn Treebank was called for, and a substantial number of changes to the Treebank were necessary. We discuss the implications of our findings for the extraction of other linguistically expressive grammars from the Treebank, and for the design of future treebanks. The CCGbank-style dependency is a directed graph of head-child relations labelled with the head's lexical category and the argument slot filled by the child. CCGbank is a corpus of CCG derivations that was semiautomatically converted from the Wall Street Journal section of the Penn treebank.	CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank This article introduces a method for converting the Penn Treebank, a collection of structured sentences, into a set of Combinatory Categorial Grammar (CCG) derivations, which are detailed sentence structures. These derivations are improved with connections between nearby and distant words. The new collection, called CCGbank, covers 99.4% of the sentences in the Penn Treebank. It is available from the Linguistic Data Consortium and has been used to train advanced computer programs that understand sentence structures very well. To make sure the CCG analyses were correct and to remove errors and inconsistencies in the original data, a detailed review of the sentence structures and labels in the Penn Treebank was needed, and many adjustments were made. We explore what our findings mean for extracting other types of detailed grammar from the Treebank and for creating future collections of structured sentences. The CCGbank-style dependency is a detailed map showing how words connect, labeled with the main word's category and the role of the connected word. CCGbank is a collection of CCG sentence structures that was partly converted automatically from the Wall Street Journal section of the Penn Treebank.	phrases
267	Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models This article describes a number of log-linear parsing models for an automatically extracted lexicalized grammar. The models are "full" parsing models in the sense that probabilities are defined for complete parses, rather than for independent events derived by decomposing the parse tree. Discriminative training is used to estimate the models, which requires incorrect parses for each sentence in the training data as well as the correct parse. The lexicalized grammar formalism used is Combinatory Categorial Grammar (CCG), and the grammar is automatically extracted from CCGbank, a CCG version of the Penn Treebank. The combination of discriminative training and an automatically extracted grammar leads to a significant memory requirement (up to 25 GB), which is satisfied using a parallel implementation of the BFGS optimization algorithm running on a Beowulf cluster. Dynamic programming over a packed chart, in combination with the parallel implementation, allows us to solve one of the largest-scale estimation problems in the statistical parsing literature in under three hours. A key component of the parsing system, for both training and testing, is a Maximum Entropy supertagger which assigns CCG lexical categories to words in a sentence. The supertagger makes the discriminative training feasible, and also leads to a highly efficient parser. Surprisingly, given CCG’s 'spurious ambiguity,' the parsing speeds are significantly higher than those reported for comparable parsers in the literature. We also extend the existing parsing techniques for CCG by developing a new model and efficient parsing algorithm which exploits all derivations, including CCG’s nonstandard derivations. This model and parsing algorithm, when combined with normal-form constraints, give state-of-the-art accuracy for the recovery of predicate–argument dependencies from CCGbank. The parser is also evaluated on DepBank and compared against the RASP parser, outperforming RASP overall and on the majority of relation types. The evaluation on DepBank raises a number of issues regarding parser evaluation. This article provides a comprehensive blueprint for building a wide-coverage CCG parser. We demonstrate that both accurate and highly efficient parsing is possible with CCG. From a parsing perspective, the C & C parser has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrase structure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al, 2009).	Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models This article talks about different methods for analyzing sentences using log-linear models, which are mathematical models, for a type of grammar that is automatically created. These models evaluate entire sentences instead of looking at parts separately. They are trained by comparing correct sentence structures with incorrect ones. The grammar type used is called Combinatory Categorial Grammar (CCG), and it is taken from a resource called CCGbank, a version of the Penn Treebank. Using this method requires a lot of computer memory (up to 25 GB), which is managed by using a special computer system called a Beowulf cluster that uses parallel computing (multiple computers working together). A method called dynamic programming helps solve one of the biggest problems in sentence analysis in less than three hours. An important part of this system is a tool called a Maximum Entropy supertagger, which assigns specific labels to words in a sentence, making the training process efficient and the analysis faster. Surprisingly, despite CCG's tendency for 'spurious ambiguity' (unnecessary multiple interpretations), the process is quicker than other similar systems. We also improve current methods by creating a new model and algorithm to handle all types of sentence structures, even unusual ones, achieving high accuracy in understanding sentence parts and their relationships. The parser, or sentence analyzer, is tested against another called RASP and performs better overall. The testing on DepBank, a resource for evaluating parsers, highlights some issues with how parsers are usually tested. This article serves as a detailed guide for building a comprehensive CCG parser, proving that accurate and efficient sentence analysis is achievable. The C & C parser matches up well with other top-notch sentence analyzers across various tests, including those involving grammatical relations, Penn Treebank sentence structures, and complex dependencies.	detailed review
268	Modeling Local Coherence: An Entity-Based Approach This article proposes a novel framework for representing and measuring local coherence. Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text. The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences and records distributional, syntactic, and referential information about discourse entities. We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment. An entity grid is constructed for each document, and is represented as a matrix in which each row represents a sentence, and each column represents an entity. We experiment on two datasets: news articles on the topic of earthquakes (Earthquakes) and narratives on the topic of aviation accidents (Accidents).	Modeling Local Coherence: An Entity-Based Approach This article introduces a new way to show and measure how smoothly ideas connect in a piece of writing. The main part of this method is the entity-grid, which is a way to see how often and where important things (entities) are mentioned in a text. The algorithm in the article turns a text into a series of steps showing how these things change and keeps track of different kinds of information about them. We rethink checking smoothness of ideas as a learning task and show that our method works well for tasks like making good text order and sorting texts. Using this method, we do well in organizing text, checking how well summaries flow, and judging how easy texts are to read. An entity grid is created for each document, shown as a table where each line is a sentence and each column is an important thing. We test on two sets of data: news stories about earthquakes and stories about aviation accidents.	dynamic programming
298	Accurate Methods For The Statistics Of Surprise And Coincidence Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results. This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text. However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical. This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text. Since it was first introduced to the NLP community by us, the G log-likelihood-ratio statistic has been widely used in statistical NLP as a measure of strength of association, particularly lexical associations.	Accurate Methods For The Statistics Of Surprise And Coincidence Much work has been done on analyzing text using statistics. In some studies, wrong statistical methods have been used, and how important the results are has not been considered. Specifically, assuming data follows a normal distribution (a common bell curve) is often done without reason, causing incorrect results. This normal distribution assumption makes it hard to study rare events, which actually happen a lot in real text. Thankfully, there are better methods based on likelihood ratio tests (a way to compare two hypotheses) that work well even with small samples. These tests can be done quickly and have been used to find combined terms or specific terms related to a topic. Sometimes, these methods work much better than older methods. When traditional methods with contingency tables (a type of table used in statistics) work well, the likelihood ratio tests work almost the same. This paper explains a method using likelihood ratios for analyzing text. Since we first introduced it to the natural language processing (NLP) community, the G log-likelihood-ratio statistic has been widely used in statistical NLP to measure connection strength, especially word connections.	Unification
270	A Global Joint Model for Semantic Role Labeling We present a model for semantic role labeling that effectively captures the linguistic intuition that a semantic argument frame is a joint structure, with strong dependencies among the arguments. We show how to incorporate these strong dependencies in a statistical joint model with a rich set of features over multiple argument phrases. The proposed model substantially outperforms a similar state-of-the-art local model that does not include dependencies among different arguments. We evaluate the gains from incorporating this joint information on the Propbank corpus, when using correct syntactic parse trees as input, and when using automatically derived parse trees. The gains amount to 24.1% error reduction on all arguments and 36.8% on core arguments for gold-standard parse trees on Propbank. For automatic parse trees, the error reductions are 8.3% and 10.3% on all and core arguments, respectively. We also present results on the CoNLL 2005 shared task data set. Additionally, we explore considering multiple syntactic analyses to cope with parser noise and uncertainty. We present a re-ranking model to jointly learn the semantic roles of multiple constituents in the SRL task.	A Global Joint Model for Semantic Role Labeling We introduce a model for identifying semantic roles that understands that a group of related words (semantic argument frame) is connected, with strong links between them. We explain how to include these strong links in a statistical model with many features that look at different groups of words. This new model performs much better than a similar advanced model that doesn't consider these links between different groups. We test the improvements from using this connected information on the Propbank dataset, both when using perfect sentence structures (parse trees) and when using automatically generated ones. The improvements result in a 24.1% reduction in errors for all groups of words and 36.8% for main groups when using perfect sentence structures. For automatically generated sentence structures, the error reductions are 8.3% for all groups and 10.3% for main groups. We also show results on the CoNLL 2005 shared task data set. Additionally, we look into using multiple sentence analyses to handle errors and uncertainty from the parser. We introduce a model that ranks these analyses to learn the semantic roles of multiple parts in the Semantic Role Labeling task.	Feature
271	The Importance of Syntactic Parsing and Inference in Semantic Role Labeling We present a general framework for semantic role labeling. The framework combines a machine learning technique with an integer linear programming–based inference procedure, which incorporates linguistic and structural constraints into a global decision process. Within this framework, we study the role of syntactic parsing information in semantic role labeling. We show that full syntactic parsing information is, by far, most relevant in identifying the argument, especially, in the very first stage—the pruning stage. Surprisingly, the quality of the pruning stage cannot be solely determined based on its recall and precision. Instead, it depends on the characteristics of the output candidates that determine the difficulty of the downstream problems. Motivated by this observation, we propose an effective and simple approach of combining different semantic role labeling systems through joint inference, which significantly improves its performance. Our system has been evaluated in the CoNLL-2005 shared task on semantic role labeling, and achieves the highest F1 score among 19 participants. The verb SRL system consists of four stages: candidate generation, argument identification, argument classification and inference.	The Importance of Syntactic Parsing and Inference in Semantic Role Labeling We present a basic system for semantic role labeling, which means identifying the roles words play in a sentence. This system uses a computer learning method combined with a special problem-solving process that uses rules about language and structure to make decisions. We examine how important sentence structure information is when identifying word roles. We find that complete sentence structure information is crucial for identifying roles, especially in the first step called the pruning stage, where unnecessary parts are removed. Interestingly, the success of this stage isn't just about how many correct roles it finds (recall) or how accurate it is (precision). Instead, it depends on the types of possible answers it gives, which affects later steps. Based on this, we suggest a simple and effective way to improve by combining different systems that label word roles, which significantly boosts performance. Our system was tested in a competition called CoNLL-2005 and got the highest score out of 19 participants. The verb SRL system has four steps: generating possible candidates, identifying roles, classifying roles, and making final decisions.	consider
272	Algorithms for Deterministic Incremental Dependency Parsing Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a nonprojective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework. We give a systematic description of the arc-standard and arc-eager algorithms, currently two popular transition-based parsing methods for word-level dependency parsing.	Algorithms for Deterministic Incremental Dependency Parsing Parsing methods that go through text from start to finish and create one single structure have often been seen as not good enough for understanding natural language because languages are usually very confusing. However, it's been found that when these methods are combined with tools that help make choices, they can be very good at sorting out language structures, especially those that focus on relationships between words. In this article, we first introduce a basic way to explain and study these methods, known as transition systems. We then discuss two types of these methods: stack-based and list-based. The stack-based method deals only with simpler, straightforward structures and includes two versions: arc-eager and arc-standard. The list-based method includes both straightforward and more complex versions. For each of these four methods, we provide explanations to show they work correctly and how fast they operate. We also test all methods using SVM classifiers, which predict the next step in parsing, across data from thirteen languages. We find that all four methods are fairly accurate, but the more complex list-based method generally works better for languages that have many complex sentence structures. However, the simpler methods can still be effective when used with a special technique called pseudo-projective parsing. The stack-based methods are faster in terms of both learning and parsing, but the simpler list-based method is also quite fast in real situations. When the simpler methods use pseudo-projective parsing, they can become slower in parsing but not in learning compared to the complex list-based method. Although parts of these methods have been discussed before, this is the first detailed review and testing of them all together. We provide a clear explanation of the arc-standard and arc-eager methods, which are currently popular for understanding word relationships in sentences.	Labeling
273	Survey Article: Inter-Coder Agreement for Computational Linguistics This article is a survey of methods for measuring agreement among corpus annotators. It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff’s alpha as well as Scott’s pi and Cohen’s kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappa-like measures in computational linguistics, may be more appropriate for many corpus annotation tasks — but that their use makes the interpretation of the value of the coefficient even harder. A comprehensive overview of methods for measuring the inter-annotator agreement in various areas of computational linguistics was given in this work.	Survey Article: Inter-Coder Agreement for Computational Linguistics This article reviews methods to check how much different people agree when labeling a set of language data. It explains the math and ideas behind agreement scores, talking about Krippendorff’s alpha, Scott’s pi, and Cohen’s kappa; looks at how these scores are used in different labeling tasks; and suggests that using weighted scores, like alpha, which are not as common as kappa scores in language studies, might be better for many labeling tasks, but they make understanding the score value more difficult. This work gives a full overview of methods for measuring how well people agree when labeling language data in different areas of language studies.	methods using
274	Articles: Recognizing Contextual Polarity: An Exploration of Features for Phrase-Level Sentiment Analysis Many approaches to automatic sentiment analysis begin with a large lexicon of words marked with their prior polarity (also called semantic orientation). However, the contextual polarity of the phrase in which a particular instance of a word appears may be quite different from the word’s prior polarity. Positive words are used in phrases expressing negative sentiments, or vice versa. Also, quite often words that are positive or negative out of context are neutral in context, meaning they are not even being used to express a sentiment. The goal of this work is to automatically distinguish between prior and contextual polarity, with a focus on understanding which features are important for this task. Because an important aspect of the problem is identifying when polar terms are being used in neutral contexts, features for distinguishing between neutral and polar instances are evaluated, as well as features for distinguishing between positive and negative contextual polarity. The evaluation includes assessing the performance of features across multiple machine learning algorithms. For all learning algorithms except one, the combination of all features together gives the best performance. Another facet of the evaluation considers how the presence of neutral instances affects the performance of features for distinguishing between positive and negative polarity. These experiments show that the presence of neutral instances greatly degrades the performance of these features, and that perhaps the best way to improve performance across all polarity classes is to improve the system’s ability to identify when an instance is neutral. We explore the difference between prior and contextual polarity: words that lose polarity in context, or whose polarity is reversed because of context.	Articles: Recognizing Contextual Polarity: An Exploration of Features for Phrase-Level Sentiment Analysis Many methods for automatic sentiment analysis start with a large list of words labeled with their usual emotional meaning (also called semantic orientation). However, the emotional meaning of a phrase where a word appears can be very different from the word's usual meaning. Positive words can be used in phrases that express negative feelings, or the other way around. Often, words that are positive or negative outside of a phrase are neutral in a phrase, meaning they do not express any emotion. The aim of this study is to automatically tell the difference between usual and in-context emotional meanings, focusing on what features are important for this. A key part of the problem is figuring out when emotional words are used in neutral contexts, so features that help tell apart neutral and emotional uses are tested, as well as features for telling apart positive and negative in-context meanings. The assessment involves checking how well features work with different machine learning methods. For all but one learning method, using all features together works best. Another part of the assessment looks at how neutral uses affect the performance of features for telling apart positive and negative meanings. These tests show that neutral uses greatly reduce the effectiveness of these features, and perhaps the best way to improve effectiveness for all meaning types is to better identify when a use is neutral. We explore the difference between usual and in-context emotional meanings: words that lose meaning in context, or have their meaning changed because of context.	language
310	Japanese Discourse And The Process Of Centering This paper has three aims: (1) to generalize a computational account of the discourse process called CENTERING, (2) to apply this account to discourse processing in Japanese so that it can be used in computational systems for machine translation or language understanding, and (3) to provide some insights on the effect of syntactic factors in Japanese on discourse interpretation. We argue that while discourse interpretation is an inferential process, syntactic cues constrain this process; we demonstrate this argument with respect to the interpretation of ZEROS, unexpressed arguments of the verb, in Japanese. The syntactic cues in Japanese discourse that we investigate are the morphological markers for grammatical TOPIC, the postposition wa, as well as those for grammatical functions such as SUBJECT, ga, OBJECT, o and OBJECT2, ni. In addition, we investigate the role of speaker's EMPATHY, which is the viewpoint from which an event is described. This is syntactically indicated through the use of verbal compounding, i.e. the auxiliary use of verbs such as kureta, kita. Our results are based on a survey of native speakers of their interpretation of short discourses, consisting of minimal pairs, varied by one of the above factors. We demonstrate that these syntactic cues do indeed affect the interpretation of ZEROS, but that having previously been the TOPIC and being realized as a ZERO also contributes to the salience of a discourse entity. We propose a discourse rule of ZERO TOPIC ASSIGNMENT, and show that CENTERING provides constraints on when a ZERO can be interpreted as the ZERO TOPIC. We propose forward center ranking for Japanese.	Japanese Discourse And The Process Of Centering This paper has three goals: (1) to explain a computer-based method called CENTERING, which helps understand conversations, (2) to use this method for understanding Japanese conversations in computer systems that translate languages or help understand them, and (3) to share some thoughts on how grammar affects understanding conversations in Japanese. We believe that understanding conversations involves making guesses, but grammar clues limit these guesses; we show this using examples of ZEROS, which are missing parts of a sentence, in Japanese. The grammar clues we study in Japanese conversations are words that show the main topic, like the word "wa," and words that show grammatical roles like "ga" for subject, "o" for object, and "ni" for another object. We also look at the speaker's EMPATHY, which is the angle from which a story is told. This is shown through verb combinations, like using extra verbs such as "kureta" or "kita." Our findings come from asking native speakers how they understand short conversations with small changes. We show that grammar clues affect how ZEROS are understood, but being the main topic before and showing up as a ZERO also makes a part of the conversation stand out. We suggest a rule for assigning ZERO as the main topic and show that CENTERING guides when a ZERO can be seen as the main topic. We suggest an order of importance for topics in Japanese conversations.	expectation maximization
275	Generating Phrasal and Sentential Paraphrases: A Survey of Data-Driven Methods The task of paraphrasing is inherently familiar to speakers of all languages. Moreover, the task of automatically generating or extracting semantic equivalences for the various units of language — words, phrases, and sentences — is an important part of natural language processing (NLP) and is being increasingly employed to improve the performance of several NLP applications. In this article, we attempt to conduct a comprehensive and application-independent survey of data-driven phrasal and sentential paraphrase generation methods, while also conveying an appreciation for the importance and potential use of paraphrases in the field of NLP research. Recent work done in manual and automatic construction of paraphrase corpora is also examined. We also discuss the strategies used for evaluating paraphrase generation techniques and briefly explore some future trends in paraphrase generation. We survey a variety of data driven paraphrasing techniques, categorizing them based on the type of data that they use.	Generating Phrasal and Sentential Paraphrases: A Survey of Data-Driven Methods The task of paraphrasing, which means rewording a sentence or phrase, is something that people who speak any language are familiar with. Additionally, creating or finding different ways of saying the same thing in words, phrases, and sentences automatically is a key part of natural language processing (NLP), which is a technology that helps computers understand human language better. This is being used more and more to make various language-related applications work better. In this article, we aim to provide a thorough review of methods that use data to create paraphrases of phrases and sentences, regardless of specific applications. We also highlight why paraphrases are valuable and how they can be used in NLP. We look at recent efforts in both manual and automatic creation of collections of paraphrases. We also talk about how paraphrase generation techniques are evaluated and briefly mention some future directions in this area. We review different techniques that use data to create paraphrases, organizing them based on the kind of data they use.	automatic sentiment
276	Distributional Memory: A General Framework for Corpus-Based Semantics Research into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus. As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems. In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes. Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods. The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature. We use a representation based on third order tensors and provide a general framework for distributional semantics in which it is possible to represent several aspects of meaning using a single data structure.	Distributional Memory: A General Framework for Corpus-Based Semantics Research into understanding language using collections of texts (corpus-based semantics) has often focused on creating specific models for each task. Each task is treated as a separate problem, using different types of information from the text collection. Instead of this "one task, one model" method, the Distributional Memory framework collects this information once from the text collection as a set of word connections that are organized in a large data structure called a third-order tensor. From this data structure, different tables are made, where rows and columns help solve different language understanding tasks. This means the same information can be used for various tasks like judging word similarities, finding synonyms, grouping concepts, predicting how verbs will be used, solving analogy problems, identifying relationships between words, discovering typical qualities of concepts, and sorting verbs into action types. Extensive testing shows that this method works well compared to specialized programs designed for these tasks, and also compared to top modern methods. The Distributional Memory approach is proven to be effective despite having to serve multiple purposes. We use a complex data structure (third-order tensors) to create a general system for understanding word meanings in different contexts using a single setup.	phrases
277	A Plan-Based Analysis Of Indirect Speech Act We propose an account of indirect forms of speech acts to request and inform based on the hypothesis that language users can recognize actions being performed by others, infer goals being sought, and cooperate in their achievement. This cooperative behaviour is independently motivated and may or may not be intended by speakers. If the hearer believes it is intended, he or she can recognize the speech act as indirect; otherwise it is interpreted directly. Heuristics are suggested to decide among the interpretations.	A Plan-Based Analysis Of Indirect Speech Act We suggest a way to understand indirect speech acts, like when people hint at requests or information, by assuming that people can identify what others are doing, guess what they want, and work together to achieve it. This teamwork happens naturally and might not always be planned by the speaker. If the listener thinks it's planned, they see the speech as indirect; if not, they take it literally. We propose simple rules to help choose between these interpretations.	understanding
278	Extraposition Grammars Extraposition grammars are an extension of definite clause grammars, and are similarly defined in terms of logic clauses. The extended formalism makes it easy to describe left extraposition of constituents, an important feature of natural language syntax. Whereas head grammars provide for an account of verb fronting and cross-serial dependencies, we, introducing extraposition grammars, is focused on displacement of noun phrases in English.	Extraposition Grammars Extraposition grammars build on definite clause grammars, which are a type of logical rules. This new version helps describe how parts of a sentence can be moved to the front, which is important for understanding sentence structure in languages. While head grammars explain how verbs can move to the front and handle complex sentence parts, extraposition grammars focus on moving noun phrases (groups of words acting as a noun) around in English.	Indirect
279	Coping With Syntactic Ambiguity Or How To Put The Block In The Box On The Table Sentences are far more ambiguous than one might have thought. There may be hundreds, perhaps thousands, of syntactic parse trees for certain very natural sentences of English. This fact has been a major problem confronting natural language processing, especially when a large percentage of the syntactic parse trees are enumerated during semantic/pragmatic processing. In this paper we propose some methods for dealing with syntactic ambiguity in ways that exploit certain regularities among alternative parse trees. These regularities will be expressed as linear combinations of ATN networks, and also as sums and products of formal power series. We believe that such encoding of ambiguity will enhance processing, whether syntactic and semantic constraints are processed separately in sequence or interleaved together. The number of possible binary-branching parses of a sentence is defined by the Catalan number, an exponential combinatoric function.	Coping With Syntactic Ambiguity Or How To Put The Block In The Box On The Table Sentences are much more unclear than you might think. There can be many different ways to structure certain simple English sentences. This has been a big issue for computer programs trying to understand language, especially when many of these structures are considered during understanding the meaning or context of the sentences. In this paper, we suggest some ways to handle this confusion by using patterns found among different sentence structures. These patterns will be shown as combinations of ATN networks (a type of language model), and also as sums and products of mathematical series. We believe this way of dealing with confusion will improve understanding, whether the sentence structure and meaning are handled one after the other or mixed together. The number of possible ways to break down a sentence is defined by the Catalan number, which is a mathematical formula that grows rapidly.	version
280	Attention Intentions And The Structure Of Discourse In this paper we explore a new theory of discourse structure that stresses the role of purpose and processing in discourse. In this theory, discourse structure is composed of three separate but interrelated components: the structure of the sequence of utterances (called the linguistic structure), a structure of purposes (called the intentional structure), and the state of focus of attention (called the attentional state). The linguistic structure consists of segments of the discourse into which the utterances naturally aggregate. The intentional structure captures the discourse-relevant purposes, expressed in each of the linguistic segments as well as relationships among them. The attentional state is an abstraction of the focus of attention of the participants as the discourse unfolds. The attentional state, being dynamic, records the objects, properties, and relations that are salient at each point of the discourse. The distinction among these components is essential to provide an adequate explanation of such discourse phenomena as cue phrases, referring expressions, and interruptions. The theory of attention, intention, and aggregation of utterances is illustrated in the paper with a number of example discourses. Various properties of discourse are described, and explanations for the behavior of cue phrases, referring expressions, and interruptions are explored. This theory provides a framework for describing the processing of utterances in a discourse. Discourse processing requires recognizing how the utterances of the discourse aggregate into segments, recognizing the intentions expressed in the discourse and the relationships among intentions, and tracking the discourse through the operation of the mechanisms associated with attentional state. This processing description specifies in these recognition tasks the role of information from the discourse and from the participants' knowledge of the domain. We proposed a theory of discourse structure to account for why an utterance was said and what was meant by it.	Attention Intentions And The Structure Of Discourse In this paper, we look into a new theory about how conversations are organized, focusing on the purpose and how they are processed. This theory says conversation structure has three parts that work together: the order of what is said (linguistic structure), the goals behind what is said (intentional structure), and what people are focusing on while talking (attentional state). The linguistic structure is about how parts of the conversation fit together naturally. The intentional structure is about the goals in each part of the conversation and how they connect. The attentional state is about where the participants' focus is during the conversation. It changes and records important things, traits, and connections at each moment. These parts are important to explain things in conversations like hints, how we refer to things, and interruptions. The paper shows examples of how attention, intention, and grouping of what is said work together. Different features of conversations are explained, and the paper looks into why certain things like hints, references, and interruptions happen. This theory provides a way to understand how conversations are processed. To process a conversation, you need to see how parts fit together, understand the goals, and follow the conversation by knowing what people focus on. This process uses information from the conversation and what people know about the topic. We suggest a theory to explain why something was said and what it means.	using patterns
281	An Efficient Augmented-Context-Free Parsing Algorithm An efficient parsing algorithm for augmented context-free grammars is introduced, and its application to on-line natural language interfaces discussed. The algorithm is a generalized LR parsing algorithm, which precomputes an LR shift-reduce parsing table (possibly with multiple entries) from a given augmented context-free grammar. Unlike the standard LR parsing algorithm, it can handle arbitrary context-free grammars, including ambiguous grammars, while most of the LR efficiency is preserved by introducing the concept of a "graph-structured stack". The graph-structured stack allows an LR shift-reduce parser to maintain multiple parses without parsing any part of the input twice in the same way. We can also view our parsing algorithm as an extended chart parsing algorithm efficiently guided by LR parsing tables. The algorithm is fast, due to the LR table precomputation. In several experiments with different English grammars and sentences, timings indicate a five- to tenfold speed advantage over Earley's context-free parsing algorithm. The algorithm parses a sentence strictly from left to right on-line, that is, it starts parsing as soon as the user types in the first word of a sentence, without waiting for completion of the sentence. A practical on-line parser based on the algorithm has been implemented in Common Lisp, and running on Symbolics and HP AI workstations. The parser is used in the multi-lingual machine translation project at CMU. Also, a commercial on-line parser for Japanese language is being built by Intelligent Technology Incorporation, based on the technique developed at CMU.	An Efficient Augmented-Context-Free Parsing Algorithm An efficient method for analyzing complex language structures is introduced, and its use in real-time language systems is discussed. The method is a type of LR parsing algorithm, which prepares a table in advance to help with parsing from a given set of language rules. Unlike the regular LR parsing method, it can handle any set of language rules, even confusing ones, while keeping much of the speed by using a "graph-structured stack," a special way of storing multiple possible meanings without repeating work. This method can also be seen as a fast version of another parsing method, guided by pre-made tables. It is quick because it prepares the tables ahead of time. In tests with different English language rules and sentences, it was found to be five to ten times faster than another well-known method called Earley's algorithm. The algorithm processes a sentence as it is typed, without waiting for the whole sentence to be completed. A working version of this parser has been made using the Common Lisp programming language and runs on certain computers. It is used in a multi-language translation project at Carnegie Mellon University (CMU). Additionally, a company called Intelligent Technology Incorporation is developing a version for Japanese, based on CMU's method.	conversations
282	An Algorithm For Generating Quantifier Scopings The syntactic structure of a sentence often manifests quite clearly the predicate-argument structure and relations of grammatical subordination. But scope dependencies are not so transparent. As a result, many systems for representing the semantics of sentences have ignored scoping or generated scopings with mechanisms that have often been inexplicit as to the range of scopings they choose among or profligate in the scopings they allow. This paper presents, along with proofs of some of its important properties, an algorithm that generates scoped semantic forms from unscoped expressions encoding predicate-argument structure. The algorithm is not profligate as are those based on permutation of quantifiers, and it can provide a solid foundation for computational solutions where completeness is sacrificed for efficiency and heuristic efficacy. We extend this formalism to support operators (such as not) and present an enumeration algorithm that is more efficient than the naive wrapping approach. We presented an algorithm to generate quantifier scopings from a representation of predicate-argument relations and the relations of grammatical subordination. We introduce an algorithm for generating all possible quantifier scopings.	An Algorithm For Generating Quantifier Scopings The structure of a sentence often clearly shows the main verb and its related parts, as well as how different parts of the sentence depend on each other. But understanding which parts of a sentence affect others is not so obvious. Because of this, many systems that try to understand the meaning of sentences have either ignored this aspect or have used unclear methods that either don't show how they choose from different interpretations or allow too many interpretations. This paper presents, along with proofs of some of its important properties, a method that creates interpretations with clear meanings from expressions that show the main verb and its related parts. Unlike other methods that simply rearrange parts, this method is careful and can be a strong base for computer solutions where being complete is less important than being efficient and effective. We expand this method to include operators (like not) and present a more efficient way to list possibilities than simple approaches. We showed a method to create interpretations from representations of how verbs and their parts relate and how different parts of a sentence depend on each other. We introduce a method for creating all possible interpretations of quantifiers.	language structures
283	Grammatical Category Disambiguation By Statistical Optimization Several algorithms have been developed in the past that attempt to resolve categorial ambiguities in natural language text without recourse to syntactic or semantic level information. An innovative method (called "CLAWS") was recently developed by those working with the Lancaster-Oslo/Bergen Corpus of British English. This algorithm uses a systematic calculation based upon the probabilities of co-occurrence of particular tags. Its accuracy is high, but it is very slow, and it has been manually augmented in a number of ways. The effects upon accuracy of this manual augmentation are not individually known. The current paper presents an algorithm for disambiguation that is similar to CLAWS but that operates in linear rather than in exponential time and space, and which minimizes the unsystematic augments. Tests of the algorithm using the million words of the Brown Standard Corpus of English are reported; the overall accuracy is 96%. This algorithm can provide a fast and accurate front end to any parsing or natural language processing system for English.	Grammatical Category Disambiguation By Statistical Optimization Several algorithms have been developed in the past that try to solve the confusion over word categories in natural language text without using grammar or meaning information. A new method (called "CLAWS") was recently created by researchers working with the Lancaster-Oslo/Bergen Collection of British English. This method uses a systematic calculation based on the chances that certain tags appear together. It is very accurate but very slow, and it has been manually improved in several ways. The effects on accuracy of these manual improvements are not known individually. The current paper presents a method for removing confusion over word categories that is similar to CLAWS but works faster and uses less memory, while reducing random improvements. Tests of the method using the million words of the Brown Standard Collection of English are reported; the overall accuracy is 96%. This method can provide a fast and accurate start to any system that processes or analyzes English language.	related
284	Temporal Ontology And Temporal Reference A semantics of temporal categories in language and a theory of their use in defining the temporal relations between events both require a more complex structure on the domain underlying the meaning representations than is commonly assumed. This paper proposes an ontology based on such notions as causation and consequence, rather than on purely temporal primitives. A central notion in the ontology is that of an elementary event-complex called a "nucleus." A nucleus can be thought of as an association of a goal event, or "culmination," with a "preparatory process" by which it is accomplished, and a "consequent state," which ensues. Natural-language categories like aspects, futurates, adverbials, and when-clauses are argued to change the temporal/aspectual category of propositions under the control of such a nucleic knowledge representation structure. The same concept of a nucleus plays a central role in a theory of temporal reference, and of the semantics of tense, which we follow McCawley, Partee, and Isard in regarding as an anaphoric category. We claim that any manageable formalism for natural language temporal descriptions will have to embody such an ontology, as will any usable temporal database for knowledge about events which is to be interrogated using natural language. We describe temporal expressions relating to changes of state.	Temporal Ontology And Temporal Reference A semantics (meaning) of temporal (time-related) categories in language and a theory of their use in defining the temporal relations between events both require a more complex structure on the domain underlying the meaning representations than is commonly assumed. This paper proposes an ontology (a way to categorize things) based on ideas like causation (cause and effect) and consequence, rather than just time-based basic concepts. A central idea in the ontology is an elementary event-complex called a "nucleus." A nucleus can be thought of as a combination of a goal event, or "culmination," with a "preparatory process" (the steps leading to it), and a "consequent state," which follows after. Natural-language categories like aspects, futurates, adverbials, and when-clauses are argued to change the time-related category of ideas based on such a nucleic knowledge representation structure. The same concept of a nucleus plays a central role in a theory of temporal reference, and of the semantics of tense, which we follow McCawley, Partee, and Isard in regarding as an anaphoric category (referring back to something previously mentioned). We claim that any manageable system for describing time in natural language will need to include such an ontology, as will any usable temporal database for knowledge about events that is to be queried using natural language. We describe temporal expressions relating to changes of state.	developed
285	Tense As Discourse Anaphor In this paper, I consider a range of English expressions and show that their context-dependency can be characterized in terms of two properties: 1. They specify entities in an evolving model of the discourse that the listener is constructing; 2. The particular entity specified depends on another entity in that part of the evolving "discourse model" that the listener is currently attending to. Such expressions have been called anaphors. I show how tensed clauses share these characteristics, usually just attributed to anaphoric noun phrases. This not only allows us to capture in a simple way the oft-stated but difficult-to-prove intuition that tense is anaphoric, but also contributes to our knowledge of what is needed for understanding narrative text. We improve upon the above work by specifying rules for how events are related to one another in a discourse and Sing and Sing defined semantic constraints through which events can be related (Sing, 1997).	Tense As Discourse Anaphor In this paper, I look at different English expressions and explain that how they depend on context can be explained by two things: 1. They point out specific things in a changing story the listener is creating in their mind; 2. The specific thing they point out depends on another thing in the part of the "story model" the listener is focusing on. These types of expressions are called anaphors, which means they refer back to something mentioned earlier. I explain how sentences with tenses share these features, usually thought to only apply to anaphoric noun phrases, which are words that refer back to nouns used before. This helps us easily understand the often-mentioned but hard-to-prove idea that tense, or the timing of actions in sentences, also refers back to earlier parts of the story. It also helps us know more about what we need to understand stories better. We improve the previous work by setting rules for how events are connected in a story and Sing and Sing defined limits for how events can be linked (Sing, 1997).	related
291	The Generative Lexicon In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives, I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole. We propose the Generative Lexicon Theory (GLT), which can be said to take advantage of both linguistic and conceptual approaches, providing a framework which arose from the integration of linguistic studies and of techniques found in AI.	The Generative Lexicon In this paper, I will discuss four main topics related to current research in the meanings of words: the methods used, how well they describe meanings, how accurately they show meanings, and how useful these representations are for computers. In addressing these issues, I will talk about some main problems the word meanings research community faces and suggest the best ways to handle these issues. Then, I will provide a way to break down word types and explain a theory of word meanings that includes ideas like cocompositionality (how words combine meaning together) and type coercion (forcing words into certain categories), along with different levels of meaning description, where the meaning work is shared more equally across the dictionary. I argue that breaking down word meanings is possible if done in a creative way. Instead of assuming a fixed set of basic elements, I will assume a fixed number of creative tools that can build meaning expressions. I develop a theory called Qualia Structure, a language for representing word meanings, which makes much word confusion unnecessary, while still explaining the multiple meanings that words have. Finally, I discuss how individual word structures can be included in the larger word knowledge system through a theory of passing down word traits. This gives us the needed rules for organizing the dictionary, allowing us to fully combine our natural language dictionary into a complete concept. We propose the Generative Lexicon Theory (GLT), which can be said to use both language and idea approaches, providing a framework that came from merging language studies and techniques used in artificial intelligence (AI).	philosophy
286	Word Association Norms Mutual Information And Lexicography The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor.) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand :mbjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words. In our work, the significance of an association (x, y) is measured by the mutual information I (x, y) ,i.e. the probability of observing x and y together, compared with the probability of observing x and y independently.	Word Association Norms Mutual Information And Lexicography The term word association is used in a specific way in the study of how our brains process language. (Basically, people respond faster to the word nurse if it comes after the word doctor, because they are closely related.) We will use this term to help describe different interesting language patterns, like the connection between related words like doctor and nurse, and the rules about how verbs and prepositions go together. This paper will suggest a clear way to measure how words are related using a concept from information theory called mutual information, which helps us understand how often words are linked based on large text databases. (The usual way of finding out how words are related involves testing a few thousand people on a few hundred words, which is both expensive and not very reliable.) The suggested measure, called the association ratio, figures out how words are related directly from large text databases, allowing us to find these connections for tens of thousands of words. In our research, the importance of a word pair (x, y) is measured by mutual information I (x, y), which looks at how often x and y appear together compared to how often they appear separately.	stories
287	Semantic-Head-Driven Generation We present an algorithm for generating strings from logical form encodings that improves upon previous algorithms in that it places fewer restrictions on the class of grammars to which it is applicable. In particular, unlike a previous bottom-up generator, it allows use of semlantically nonmonotonic grammars, yet unlike top-down methods, it also permits left-recursion. The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion. We introduce a head-driven algorithm for generating from logical forms.	Semantic-Head-Driven Generation We present a method for creating sentences from encoded logical forms that is better than older methods because it works with more types of grammar rules. Unlike an older method that builds from the bottom up, this new method can use grammar rules that aren't straightforward, and unlike top-down methods, it allows for certain kinds of repeated steps. The key idea of this method is that it works by following the structure of the sentence in a way that focuses on the main meaning parts first. We introduce a main-idea-focused method for creating sentences from logical forms.	suggest
288	A Statistical Approach To Machine Translation In this paper, we present a statistical approach to machine translation. We describe the application of our approach to translation from French to English and give preliminary results. We estimate parameters for a model of word-to-word correspondences and word re-orderings directly from large corpora of parallel bilingual text.	A Statistical Approach To Machine Translation In this paper, we present a statistical approach to machine translation. We explain how we use this method to translate from French to English and share early results. We calculate values for a model that links words and changes their order, using large collections of bilingual texts that match each other.	grammar
289	Lexical Cohesion Computed By Thesaural Relations As An Indicator Of The Structure Of Text In text, lexical cohesion is the result of chains of related words that contribute to the continuity of lexical meaning. These lexical chains are a direct result of units of text being 'about the same thing,' and finding text structure involves finding units of text that are about the same thing. Hence, computing the chains is useful, since they will have a correspondence to the structure of the text. Determining the structure of text is an essential step in determining the deep meaning of the text. In this paper, a thesaurus is used as the major knowledge base for computing lexical chains. Correspondences between lexical chains and structural elements are shown to exist. Since the lexical chains are computable, and exist in non-domain-specific text, they provide a valuable indicator of text structure. The lexical chains also provide a semantic context for interpreting words, concepts, and sentences. We propose the idea of using lexical chains as indicators of lexical cohesion. We propose the concept of Lexical Chains to explore the discourse structure of a text.	Lexical Cohesion Computed By Thesaural Relations As An Indicator Of The Structure Of Text In text, lexical cohesion happens when related words connect with each other, helping the text flow smoothly. These connections, called lexical chains, occur because parts of the text talk about the same topic, so finding these chains helps understand the text's organization. Calculating these chains is helpful because they relate to how the text is organized. Figuring out the organization of a text is important for understanding its deeper meaning. In this paper, a thesaurus (a book or tool listing words with similar meanings) is used as the main resource for creating these lexical chains. It is shown that there is a link between these chains and how the text is structured. Because these chains can be calculated and found in any type of text, they are useful for showing how the text is organized. They also help understand the meanings of words, ideas, and sentences. We suggest using lexical chains to indicate how words are connected. We suggest using Lexical Chains to study how a text is organized and flows.	bilingual
290	met*: A Method For Discriminating Metonymy And Metaphor By Computer The met* method distinguishes selected examples of metonymy from metaphor and from literalness and anomaly in short English sentences. In the met* method, literalness is distinguished because it satisfies contextual constraints that the nonliteral others all violate. Metonymy is discriminated from metaphor and anomaly in a way that [1] supports Lakoff and Johnson's (1980) view that in metonymy one entity stands for another whereas in metaphor one entity is viewed as another, [2] permits chains of metonymies (Reddy 1979), and [3] allows metonymies to co-occur with instances of either literalness, metaphor, or anomaly. Metaphor is distinguished from anomaly because the former contains a relevant analogy, unlike the latter. The met* method is part of Collative Semantics, a semantics for natural language processing, and has been implemented in a computer program called meta5. Some examples of meta5's analysis of metaphor and metonymy are given. The met* method is compared with approaches from artificial intelligence, linguistics, philosophy, and psychology. We use selectional preference violation technique to detect metaphors. We developed a system called met*, capable of discriminating between literalness, metonymy, metaphor and anomaly. We build a system met*, which is designed to distinguish both metaphor and metonymy from literal text, providing special techniques for processing these instances of figurative language. We developed a system called met*, capable of discriminating between literalness, metonymy, metaphor and anomaly.	:met*: A Method For Discriminating Metonymy And Metaphor By Computer The met* method helps a computer tell apart examples of metonymy (using one thing to represent another) from metaphor (comparing two things) and from literal (exact) and unusual sentences in short English. The method identifies literal sentences because they fit normal word use while the others do not. Metonymy is separated from metaphor and strange sentences by [1] supporting the idea that metonymy uses one thing in place of another, while a metaphor sees one thing as something else; [2] allowing for sequences of metonymies; and [3] letting metonymies occur with literal, metaphorical, or unusual sentences. Metaphors are different from strange sentences because they have a meaningful comparison, unlike strange ones. The met* method is part of a system for understanding language and is used in a computer program called meta5. Some examples of how meta5 analyzes metaphor and metonymy are provided. The met* method is compared with methods from artificial intelligence, language studies, philosophy, and psychology. We use a technique that notices when word meanings don't match to find metaphors. We made a system called met* to separate literal, metonymy, metaphor, and unusual sentences. We created a system met* to tell metaphor and metonymy apart from literal text, using special methods to handle these types of language. We developed a system called met*, capable of discriminating between literalness, metonymy, metaphor, and anomaly.	similar
292	Using Multiple Knowledge Sources For Word Sense Discrimination This paper addresses the problem of how to identify the intended meaning of individual words in unrestricted texts, without necessarily having access to complete representations of sentences. To discriminate senses, an understander can consider a diversity of information, including syntactic tags, word frequencies, collocations, semantic context, role-related expectations, and syntactic restrictions. However, current approaches make use of only small subsets of this information. Here we will describe how to use the whole range of information. Our discussion will include how the preference cues relate to general lexical and conceptual knowledge and to more specialized knowledge of collocations and contexts. We will describe a method of combining cues on the basis of their individual specificity, rather than a fixed ranking among cue-types. We will also discuss an application of the approach in a system that computes sense tags for arbitrary texts, even when it is unable to determine a single syntactic or semantic representation for some sentences. We are one of the first to use multiple kinds of features for word sense disambiguation in the semantic interpretation system, TRUMP. We describe a study of different sources useful for word sense disambiguation, including morphological information.	Using Multiple Knowledge Sources For Word Sense Discrimination This paper looks at how to figure out what a specific word means in any text, even if we don't have the full details of the sentences. To tell apart word meanings, we can use different kinds of information like grammar tags, how often words appear, common word pairings, meaning context, expected roles, and grammar rules. But current methods only use a small part of this information. Here, we will explain how to use all the available information. Our talk will cover how the hints we use connect to general word and idea knowledge and more specific knowledge about common word pairings and contexts. We will explain a way to combine these hints based on how specific each one is, instead of following a fixed order of importance. We will also talk about using this method in a system that assigns meaning tags to any text, even when it can't find a single grammar or meaning structure for some sentences. We are among the first to use many different features for figuring out word meanings in the TRUMP system. We describe a study of different sources that help figure out word meanings, including word structure information.	theory called
293	TINA: A Natural Language System For Spoken Language Applications A new natural language system, TINA, has been developed for applications involving spoken language tasks. TINA integrates key ideas from context free grammars, Augmented Transition Networks (ATN's), and the unification concept. TINA provides a seamless interface between syntactic and semantic analysis, and also produces a highly constraining probabilistic language model to improve recognition performance. An initial set of context-free rewrite rules provided by hand is first converted to a network structure. Probability assignments on all arcs in the network are obtained automatically from a set of example sentences. The parser uses a stack decoding search strategy, with a top-down control flow, and includes a feature-passing mechanism to deal with long-distance movement, agreement, and semantic constraints. TINA provides an automatic sentence generation capability that has been effective for identifying overgeneralization problems as well as in producing a word-pair language model for a recognizer. The parser is currently integrated with MIT's SUMMIT recognizer for use in two application domains, with the parser screening recognizer outputs either at the sentential level or to filter partial theories during the active search process. We propose the language understanding system, TINA, that integrates key ideas context free grammar, augmented transition network and unification concepts.	TINA: A Natural Language System For Spoken Language Applications A new natural language system, TINA, has been developed for tasks involving spoken language. TINA combines important ideas from simple grammar rules, Augmented Transition Networks (ATN's), and the idea of unification. TINA smoothly connects understanding of sentence structure (syntax) and meaning (semantics) and creates a detailed language model to help improve accuracy in recognizing speech. A starting set of simple rules is first turned into a network format. Probabilities for each part of the network are automatically learned from example sentences. The system uses a method that searches using a stack (like a list) from top to bottom and includes a way to handle complex sentence parts and meaning limitations. TINA can automatically generate sentences, which helps find overly broad interpretations and create a model of word pairs for speech recognition. The system is currently used with MIT's SUMMIT recognizer in two specific areas, helping to check and filter outputs either by whole sentences or during the search for solutions. We suggest the TINA language system, which combines important ideas from simple grammar, network paths for language, and combining information concepts.	available
294	Class-Based N-Gram Models Of Natural Language We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics. We propose a window method introducing the concept of semantic stickiness of two words as the relatively frequent close occurrence between them (less than 500 words distance).	Class-Based N-Gram Models Of Natural Language We talk about the challenge of guessing a word based on the words that came before it in a piece of text. Specifically, we look at "n-gram models" which are methods that use groups of words. We also explore different statistical methods (mathematical techniques) to group words together based on how often they appear together. We discover that we can create groups of words that seem to be based on either grammar (how words are used together) or meaning (what words mean), depending on the data we use. We suggest a "window method," which is a new idea that looks at how often two words appear close together, within 500 words, to understand their connection.	Language
295	A Problem For RST: The Need For Multi-Level Discourse Analysis We note that Rhetorical Structure Theory conflates the informational (the information being conveyed) and intentional (the effects on the reader's beliefs or attitudes) levels of discourse. We argue that both informational (semantic) and intentional relations can hold between clause simultaneously and independently.	A Problem For RST: The Need For Multi-Level Discourse Analysis We note that Rhetorical Structure Theory (RST) mixes together the information being shared and the intended effect on the reader's thoughts or feelings. We argue that both the meaning of the information and the intended effects can exist between sentences at the same time and separately from each other.	meaning
296	Introduction To The Special Issue On Computational Linguistics Using Large Corpora A historical account of this empirical renaissance is provide in this work. Much recent research in the field of natural language processing (NLP) has focused on an empirical, corpus-based approach.	Introduction To The Special Issue On Computational Linguistics Using Large Corpora A historical account of this empirical renaissance is provided in this work. Much recent research in the field of natural language processing (NLP), which is about how computers understand human language, has focused on studying real examples from large collections of text, called "corpora."	Rhetorical Structure
297	Generalized Probabilistic LR Parsing Of Natural Language (Corpora) With Unification-Based Grammars We describe work toward the construction of a very wide-coverage probabilistic parsing system for natural language (NL), based on LR parsing techniques. The system is intended to rank the large number of syntactic analyses produced by NL grammars according to the frequency of occurrence of the individual rules deployed in each analysis. We discuss a fully automatic procedure for constructing an LR parse table from a unification-based grammar formalism, and consider the suitability of alternative LALR(1) parse table construction methods for large grammars. The parse table is used as the basis for two parsers; a user-driven interactive system that provides a computationally tractable and labor-efficient method of supervised training of the statistical information required to drive the probabilistic parser. The latter is constructed by associating probabilities with the LR parse table directly. This technique is superior to parsers based on probabilistic lexical tagging or probabilistic context-free grammar because it allows for a more context-dependent probabilistic language model, as well as use of a more linguistically adequate grammar formalism. We compare the performance of an optimized variant of Tomita's (1987) generalized LR parsing algorithm to an (efficiently indexed and optimized) chart parser. We report promising results of a pilot study training on 150 noun definitions from the Longman Dictionary of Contemporary English (LDOCE) and retesting on these plus a further 55 definitions. Finally, we discuss limitations of the current system and possible extensions to deal with lexical (syntactic and semantic)frequency of occurrence. Our work on statistical parsing uses an adapted version of the system which is able to process tagged input, ignoring the words in order to parse sequences of tags. Our statistical parser is an extension of the ANLT grammar development system.	Generalized Probabilistic LR Parsing Of Natural Language (Corpora) With Unification-Based Grammars We describe work toward building a very broad probabilistic system to analyze natural language (NL) using LR parsing techniques. The system aims to rank many sentence structures made by NL grammars based on how often each rule is used. We talk about an automatic method to create an LR parse table from a unification-based grammar style and consider other LALR(1) table methods for big grammars. The parse table is used for two parsers: an interactive system that helps train the statistical data needed for the probabilistic parser in a way that's easy to manage and doesn't require much manual effort. The second parser is made by directly linking probabilities to the LR parse table. This method is better than those using probabilistic word tagging or simple grammar rules because it considers more context and uses a grammar style that's more accurate for language. We compare the performance of a refined version of Tomita's (1987) generalized LR parsing method with a well-organized chart parser. We share positive results from a small study using 150 noun definitions from the Longman Dictionary of Contemporary English (LDOCE) for training, plus testing on these and another 55 definitions. Finally, we talk about the limits of the current system and potential improvements to handle how often words appear in terms of their meaning and grammar. Our statistical parsing uses a modified system that can work with tagged input, focusing on sequences of tags instead of the actual words. Our statistical parser builds on the ANLT grammar development system.	Using Large
299	A Program For Aligning Sentences In Bilingual Corpora Researchers in both machine translation (e.g., Brown et al. 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann 1990) have recently become interested in studying bilingual corpora, bodies of text such as the Canadian Hansards (parliamentary proceedings), which are available in multiple languages (such as French and English). One useful step is to align the sentences, that is, to identify correspondences between sentences in one language and sentences in the other language. This paper will describe a method and a program (align) for aligning sentences based on a simple statistical model of character lengths. The program uses the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences. A probabilistic score is assigned to each proposed correspondence of sen tences, based on the scaled difference of lengths of the two sentences (in characters) and the variance of this difference. This probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentences. It is remarkable that such a simple approach works as well as it does. An evaluation was performed based on a trilingual corpus of economic reports issued by the Union Bank of Switzerland (UBS) in English, French, and German. The method correctly aligned all but 4% of the sentences. Moreover, it is possible to extract a large subcorpus that has a much smaller error rate. By selecting the best-scoring 80% of the alignments, the error rate is reduced from 4% to 0.7%. There were more errors on the English-French subcorpus than on the English-German subcorpus, showing that error rates will depend on the corpus considered; however, both were small enough to hope that the method will be useful for many language pairs. To further research on bilingual corpora, a much larger sample of Canadian Hansards (approximately 90 million words, half in English and and half in French) has been aligned with the align program and will be available through the Data Collection Initiative of the Association for Computational Linguistics (ACL/DCI). In addition, in order to facilitate replication of the align program, an appendix is provided with detailed c-code of the more difficult core of the align program. We present a hybrid approach, and the basic hypothesis is that longer sentences in one language tend to be translated into longer sentences in the other language, and shorter sentences tend to be translated into shorter sentences. We propose a dynamic programming algorithm for the sentence-level alignment of translations that exploited two facts: the length of translated sentences roughly corresponds to the length of the original sentences and the sequence of sentences in translated text largely corresponds to the original order of sentences.	A Program For Aligning Sentences In Bilingual Corpora Researchers in both machine translation (e.g., Brown et al. 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann 1990) have recently become interested in studying bilingual corpora, which are collections of texts like the Canadian Hansards (parliamentary proceedings) available in multiple languages (such as French and English). One helpful step is to align the sentences, meaning to match sentences in one language with sentences in the other language. This paper will describe a method and a program (align) for matching sentences based on a simple statistical model of character lengths. The program uses the idea that longer sentences in one language usually translate to longer sentences in the other language, and shorter sentences translate to shorter sentences. A probabilistic score, which is a likelihood measure, is given to each proposed match of sentences, based on the difference in lengths of the two sentences (in characters) and how much this difference varies. This score is used in a method called dynamic programming to find the best possible match of sentences. It is surprising that such a simple method works so well. An evaluation was done using a trilingual set of economic reports from the Union Bank of Switzerland (UBS) in English, French, and German. The method correctly matched all but 4% of the sentences. Furthermore, it's possible to select a large part of the text with even fewer errors. By choosing the best-scoring 80% of the matches, the error rate drops from 4% to 0.7%. There were more errors in the English-French text than in the English-German text, indicating that error rates depend on the text considered; however, both were low enough to suggest that the method will be useful for many language pairs. To advance research on bilingual corpora, a much larger set of Canadian Hansards (about 90 million words, half in English and half in French) has been matched with the align program and will be available through the Data Collection Initiative of the Association for Computational Linguistics (ACL/DCI). Additionally, to make it easy to replicate the align program, an appendix is provided with detailed c-code of the more complex core of the align program. We present a combined approach, and the main idea is that longer sentences in one language are usually translated into longer sentences in the other language, and shorter sentences into shorter sentences. We suggest a dynamic programming algorithm for matching translations at the sentence level that uses two ideas: the length of translated sentences closely matches the length of the original sentences, and the order of sentences in translated text mostly matches the original order of sentences.	statistical
300	Structural Ambiguity And Lexical Relations We propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus. This suggests that a distributional approach can provide an approximate solution to parsing problems that, in the worst case, call for complex reasoning. We are the first to show that a corpus-based approach to PP attachment ambiguity resolution can lead to good results. We propose one of the earliest corpus-based approaches to prepositional phrase attachment used lexical preference by computing co-occurrence frequencies (lexical associations) of verbs and nouns with prepositions. We used a partial parser to extract (v, n, p) tuples from a corpus, where p is the preposition whose attachment is ambiguous between the verb v and the noun n.	Structural Ambiguity And Lexical Relations We suggest that many unclear cases where prepositional phrases (like "in," "on," "at") could be attached to different parts of a sentence can be solved by looking at how often these prepositions appear with either verbs (action words) or nouns (things or people) based on patterns found in a large collection of text that a computer has already analyzed. This means that by looking at how words are commonly used together, we can find a simple way to solve sentence structure problems that usually require complicated thinking. We are the first to show that using a large collection of text to solve these prepositional phrase attachment issues can give good results. We were among the first to use a text-based method to figure out where prepositional phrases belong in a sentence by checking how often certain verbs and nouns appear with specific prepositions. We used a tool that partially analyzes sentences to pull out groups of verbs, nouns, and prepositions from a large collection of text, where the preposition could be connected to either the verb or the noun.	studying
301	Text-Translation Alignment We present an algorithm for aligning texts with their translations that is based only on internal evidence. The relaxation process rests on a notion of which word in one text corresponds to which word in the other text that is essentially based on the similarity of their distributions. It exploits a partial alignment of the word level to induce a maximum likelihood alignment of the sentence level, which is in turn used, in the next iteration, to refine the word level estimate. The algorithm appears to converge to the correct sentence alignment in only a few iterations. Our morphology algorithm is applied for splitting potential suffixes and prefixes and for obtaining the normalised word forms.	Text-Translation Alignment We introduce a method for matching texts with their translations using only information from the texts themselves. The process involves figuring out which words in one text match words in the other text based mainly on how often they appear. It uses a partial word match to create the best possible sentence match, which then helps improve the word match in the next round. The method seems to quickly find the correct sentence matches after just a few rounds. Our word structure method is used for breaking down possible word endings and beginnings and for getting the basic word forms.	structure problems
302	Retrieving Collocations From Text: Xtract Natural languages are full of collocations, recurrent combinations of words that co-occur more often than expected by chance and that correspond to arbitrary word usages. Recent work in lexicography indicates that collocations are pervasive in English; apparently, they are common in all types of writing, including both technical and nontechnical genres. Several approaches have been proposed to retrieve various types of collocations from the analysis of large samples of textual data. These techniques automatically produce large numbers of collocations along with statistical figures intended to reflect the relevance of the associations. However, none of these techniques provides functional information along with the collocation. Also, the results produced often contained improper word associations reflecting some spurious aspect of the training corpus that did not stand for true collocations. In this paper, we describe a set of techniques based on statistical methods for retrieving and identifying collocations from large textual corpora. These techniques produce a wide range of collocations and are based on some original filtering methods that allow the production of richer and higher-precision output. These techniques have been implemented and resulted in a lexicographic tool, Xtract. The techniques are described and some results are presented on a 10 million-word corpus of stock market news reports. A lexicographic evaluation of Xtract as a collocation retrieval tool has been made, and the estimated precision of Xtract is 80 %. We develop Xtract, a term extraction system. We propose a statistical model by measuring the spread of the distribution of co occurring pairs of words with higher strength. In terms of practical MWE identification systems, we propose a well known approach that uses a set of techniques based on statistical methods, calculated from word frequencies, to identify MWEs in corpora.	Retrieving Collocations From Text: Xtract Natural languages are full of collocations, which are pairs or groups of words that frequently appear together more often than by random chance and show specific word usage. Recent work in the study of dictionaries shows that collocations are common in English and appear in all kinds of writing, whether technical or not. Several methods have been suggested to find these word combinations by analyzing large amounts of text. These methods automatically find many collocations and use numbers to show how important these word pairings are. However, these methods do not provide useful information about the collocations themselves. Also, the results sometimes include incorrect word pairings influenced by irrelevant details in the training text, which are not true collocations. In this paper, we explain techniques that use statistical methods to find and identify collocations from large text collections. These techniques can find a wide variety of collocations and use special filtering methods to produce better and more accurate results. These techniques have been used to create a dictionary tool, Xtract. The techniques are explained and some results are shown using a 10 million-word collection of stock market news reports. An evaluation of Xtract as a tool to find collocations has been done, and its accuracy is estimated to be 80%. We developed Xtract, a system to extract terms. We suggest a statistical model that measures how often pairs of words appear together with strong connections. For finding multi-word expressions (MWEs), we suggest a well-known approach that uses statistical methods based on how often words appear together to find MWEs in text collections.	beginnings
303	From Grammar To Lexicon: Unsupervised Learning Of Lexical Syntax Imagine a language that is completely unfamiliar; the only means of studying it are an ordinary grammar book and a very large corpus of text. No dictionary is available. How can easily recognized, surface grammatical facts be used to extract from a corpus as much syntactic information as possible about individual words? This paper describes an approach based on two principles. First, rely on local morpho-syntactic cues to structure rather than trying to parse entire sentences. Second, treat these cues as probabilistic rather than absolute indicators of syntactic structure. Apply inferential statistics to the data collected using the cues, rather than drawing a categorical conclusion from a single occurrence of a cue. The effectiveness of this approach for inferring the syntactic frames of verbs is supported by experiments on an English corpus using a program called Lerner. Lerner starts out with no knowledge of content words--it bootstraps from determiners, auxiliaries, modals, prepositions, pronouns, complementizers, coordinating conjunctions, and punctuation. Our study is focused on large-scaled automatic acquisition of subcategorization frames (SCF).	From Grammar To Lexicon: Unsupervised Learning Of Lexical Syntax Imagine a language that you know nothing about; the only way to study it is with a basic grammar book and a huge collection of text. There is no dictionary. How can you use easily noticed grammar rules to gather as much information as possible about how individual words are used in sentences from this collection of text? This paper describes a method based on two ideas. First, use small grammar hints to structure sentences instead of trying to understand whole sentences at once. Second, treat these hints as likely clues rather than definite rules of sentence structure. Use statistical methods to analyze the data collected from these hints, rather than making a firm conclusion from just one example. The success of this method in figuring out how verbs fit into sentences is shown by experiments on English text using a program called Lerner. Lerner starts with no knowledge of the main words--it begins with basic words like determiners (e.g., the, a), helping verbs (e.g., is, have), modals (e.g., can, will), prepositions (e.g., in, on), pronouns (e.g., he, she), words that join clauses (e.g., that, because), coordinating conjunctions (e.g., and, but), and punctuation marks. Our study is focused on automatically gathering large amounts of information about sentence structures (subcategorization frames).	explained
304	The Mathematics Of Statistical Machine Translation: Parameter Estimation We describe a series of five statistical models of the translation process and give algorithms, for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus. Our model for Statistical machine translation (SMT) is focused on word to word translation and was based on the noisy channel approach.	The Mathematics Of Statistical Machine Translation: Parameter Estimation We explain five different statistical models (ways to understand and use numbers) for translating languages and provide methods (step-by-step plans) for finding the parameters (important numbers) in these models using pairs of sentences that are translations of each other. We introduce the idea of matching words one-by-one in these sentence pairs. For any given pair of sentences, each model gives a probability (chance) for every possible word match. We have a method to find the most likely word matches. Even though this method is not perfect, it does a good job of showing how words relate in the sentence pairs. We have a lot of data in French and English from the Canadian Parliament's records. Therefore, we focus only on these two languages, but we believe our methods, which don't rely heavily on language rules, could work well for other language pairs too. We also believe that because our methods don't depend much on specific language rules, it's fair to say that word-by-word matches can be found in any large enough collection of texts in two languages. Our approach to Statistical Machine Translation (SMT) focuses on translating word by word and is based on the noisy channel model (a method used to understand how information can get messed up as it travels).	grammar
305	Building A Large Annotated Corpus Of English: The Penn Treebank	Building A Large Annotated Corpus Of English: The Penn Treebank The Penn Treebank is a big collection of English sentences that have been organized and labeled to help computers understand human language better. It includes lots of sample sentences where each word is tagged with its grammatical role, like noun or verb, making it useful for developing language processing tools.	matches
306	Lexical Semantic Techniques For Corpus Analysis In this paper we outline a research program for computational linguistics, making extensive use of text corpora. We demonstrate how a semantic framework for lexical knowledge can suggest richer relationships among words in text beyond that of simple co-occurrence. The work suggests how linguistic phenomena such as metonymy and polysemy might be exploitable for semantic tagging of lexical items. Unlike with purely statistical collocational analyses, the framework of a semantic theory allows the automatic construction of predictions about deeper semantic relationships among words appearing in collocational systems. We illustrate the approach for the acquisition of lexical information for several classes of nominals, and how such techniques can fine-tune the lexical structures acquired from an initial seeding of a machine-readable dictionary. In addition to conventional lexical semantic relations, we show how information concerning lexical presuppositions and preference relations can also be acquired from corpora, when analyzed with the appropriate semantic tools. Finally, we discuss the potential that corpus studies have for enriching the data set for theoretical linguistic research, as well as helping to confirm or disconfirm linguistic hypotheses. we present an interesting framework for the acquisition of semantic relations from corpora not only relying on statistics, but guided by theoretical lexicon principles. We show how statistical techniques, such as mutual information measures can contribute to automatically acquire lexical information regarding the link between a noun and a predicate. We use generalized syntactic patterns for extracting qualia structures from a partially parsed corpus.	Lexical Semantic Techniques For Corpus Analysis In this paper, we outline a research plan for computational linguistics, which involves using large collections of text. We show how a system for understanding word meanings can reveal deeper connections between words in text, not just words that often appear together. The work explains how language features like metonymy (using a related word) and polysemy (words with multiple meanings) can be used to tag words with their meanings. Unlike basic statistical analyses of word pairings, our meaning-based approach can predict complex relationships between words that often appear together. We demonstrate this method for gathering information about different types of nouns and how it can improve the word structures obtained from a computer-readable dictionary. Besides traditional word meaning connections, we show how information about word assumptions and preferences can also be gathered from text collections when using the right tools. Finally, we discuss how studying large text collections can provide valuable data for language research and help confirm or challenge language theories. We introduce an interesting method for finding word associations from text collections, not just using statistics but also guided by theoretical principles. We explain how statistical tools, like mutual information (a measure of word connection), can help automatically gather information about the link between a noun and a verb. We use general sentence patterns to extract detailed meaning structures from a partly analyzed text collection.	Building
307	Coping With Ambiguity And Unknown Words Through Probabilistic Models From spring 1990 through fall 1991, we performed a battery of small experiments to test the effectiveness of supplementing knowledge-based techniques with probabilistic models. This paper reports our experiments in predicting parts of speech of highly ambiguous words, predicting the intended interpretation of an utterance when more than one interpretation satisfies all known syntactic and semantic constraints, and learning case frame information for verbs from example uses. From these experiments, we are convinced that probabilistic models based on annotated corpora can effectively reduce the ambiguity in processing text and can be used to acquire lexical information from a corpus, by supplementing knowledge-based techniques. Based on the results of those experiments, we have constructed a new natural language system (PLUM)for extracting data from text, e.g., newswire text. Our model incorporates the treatment of unknown words within the probability model.	Coping With Ambiguity And Unknown Words Through Probabilistic Models From spring 1990 through fall 1991, we conducted several small tests to see how well adding probability-based methods to our existing knowledge-based techniques would work. This paper shares our tests focused on predicting the part of speech (like noun, verb, etc.) of words that can have multiple meanings, figuring out the correct meaning of a sentence when it can be understood in different ways, and learning how verbs are used through examples. From these tests, we believe that probability-based methods using labeled text examples can help clear up confusion when processing text and can be used to gather word information from a large collection of text, along with current techniques. Based on what we learned, we created a new computer system called PLUM that helps pick out information from text, such as news articles. Our model also deals with words we haven't seen before using the same probability method.	outline
308	Empirical Studies On The Disambiguation Of Cue Phrases Cue phrases are linguistic expressions such as now and well that function as explicit indicators of the structure of a discourse. For example, now may signal the beginning of a subtopic or a return to a previous topic, while well may mark subsequent material as a response to prior material, or as an explanatory comment. However, while cue phrases may convey discourse structure, each also has one or more alternate uses. While incidentally may be used sententially as an adverbial, for example, the discourse use initiates a digression. Although distinguishing discourse and sentential uses of cue phrases is critical to the interpretation and generation of discourse, the question of how speakers and hearers accomplish this disambiguation is rarely addressed. This paper reports results of empirical studies on discourse and sentential uses of cue phrases, in which both text-based and prosodic features were examined for disambiguating power. Based on these studies, it is proposed that discourse versus sentential usage may be distinguished by intonational features, specifically, pitch accent and prosodic phrasing. A prosodic model that characterizes these distinctions is identified. This model is associated with features identifiable from text analysis, including orthography and part of speech, to permit the application of the results of the prosodic analysis to the generation of appropriate intonational features for discourse and sentential uses of cue phrases in synthetic speech. In the literature, there is still no consistent definition for discourse markers. We find that into national phrasing and pitch accent play a role in disambiguating cue phrases, and hence in helping determine discourse structure.	Empirical Studies On The Disambiguation Of Cue Phrases Cue phrases are words or expressions like "now" and "well" that help show how a conversation or text is organized. For example, "now" might mean starting a new topic or going back to an old one, while "well" could show a reply or explanation. However, these phrases can have different meanings. For instance, "incidentally" can be used as an adverb to add extra information, but in conversation, it might start a side note. Understanding the difference between these uses is important for understanding and creating conversations, but it’s not often studied. This paper shares results from studies on how these phrases are used in conversation and in sentences, looking at text and sound features to see how they help understand meaning. The studies suggest that changes in voice tone, like pitch and rhythm, help tell the difference between the uses. A model describing these voice changes is identified. This model is linked with text features like spelling and grammar, so the findings can be used to create the right voice tones for using cue phrases in computer-generated speech. There is no clear definition for discourse markers in research. We find that voice tone and pitch help clarify the meaning of cue phrases, aiding in understanding conversation structure.	learning
309	Tagging English Text With A Probabilistic Model In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence. The main novelty of these experiments is the use of untagged text in the training of the model. We have used a simple triclass Marlcov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided. Two approaches in particular are compared and combined: using text that has been tagged by hand and computing relative frequency counts, using text without tags and training the model as a hidden Markov process, according to a Maximum Likelihood principle. Experiments show that the best training is obtained by using as much tagged text as possible. They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy. In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available. we attempted to improve HMM POS tagging by expectation maximization with unlabeled data. we introduced the still standard procedure of using a bigram Hidden Markov Model (HMM) trained via Expectation Maximization. In the context of POS tagging, we introduce a method that he calls maximum likelihood tagging.	Tagging English Text With A Probabilistic Model In this paper, we talk about some tests using a probabilistic model, which is a method that uses chances or probabilities, to tag English text. Tagging means giving each word the right label, like noun or verb, based on the sentence it is in. What is new about these tests is using text that hasn't been tagged before to train the model. We used a simple three-part Markov model, which is a tool that predicts based on past information, and we are trying to find the best way to figure out the model's settings based on different types and amounts of training data. We compare and combine two main methods: using text that has been manually tagged and counting how often tags appear, and using text that hasn't been tagged and training the model in a hidden way, like a mystery, using a method called Maximum Likelihood, which is about finding the most likely explanation. Tests show the best results come from using as much tagged text as possible. They also show that training with Maximum Likelihood, a common way to set up hidden Markov models, doesn't always make tagging more accurate. In fact, it usually makes it worse unless there is only a small amount of tagged text. We tried to make HMM, a type of model for tagging parts of speech (POS), better by using unlabeled data with a method called expectation maximization, which is a way of adjusting guesses to improve them. We introduced a common method now that uses a bigram HMM, which looks at pairs of words, trained with expectation maximization. In POS tagging, we introduced a method called maximum likelihood tagging, which is about picking the most likely label for each word.	different
311	Regular Models Of Phonological Rule Systems This paper presents a set of mathematical and computational tools for manipulating and reasoning about regular languages and regular relations and argues that they provide a solid basis for computational phonology. It shows in detail how this framework applies to ordered sets of context-sensitive rewriting rules and also to grammars in Koskenniemi's two-level formalism. This analysis provides a common representation of phonological constraints that supports efficient generation and recognition by a single simple interpreter. we provide an algorithm for compilation into transducers. we describe a general method representing a replacement procedure as finite-state transduction.	Regular Models Of Phonological Rule Systems This paper introduces a set of mathematical and computer-based tools for working with and understanding regular languages (patterns of sounds) and regular relations (connections between sound patterns). It argues that these tools are a strong foundation for computer-based phonology (study of sound patterns). It explains in detail how this approach works with ordered sets of context-sensitive rewriting rules (rules that change sounds based on their surroundings) and also with grammars in Koskenniemi's two-level formalism (a method for analyzing languages). This study provides a shared way of showing phonological constraints (limits on sound patterns) that allows for easy generation and understanding by one simple interpreter (program). We provide a step-by-step method for converting into transducers (tools that transform input into output). We describe a general method for showing a replacement process as finite-state transduction (a simple and predictable change process).	speakers
312	A Syntactic Analysis Method Of Long Japanese Sentences Based On The Detection Of Conjunctive Structures This paper presents a syntactic analysis method that first detects conjunctive structures in a sentence by checking parallelism of two series of words and then analyzes the dependency structure of the sentence with the help of the information about the conjunctive structures. Analysis of long sentences is one of the most difficult problems in natural language processing. The main reason for this difficulty is the structural ambiguity that is common for conjunctive structures that appear in long sentences. Human beings can recognize conjunctive structures because of a certain, but sometimes subtle, similarity that exists between conjuncts. Therefore, we have developed an algorithm for calculating a similarity measure between two arbitrary series of words from the left and the right of a conjunction and selecting the two most similar series of words that can reasonably be considered as composing a conjunctive structure. This is realized using a dynamic programming technique. A long sentence can be reduced into a shorter form by recognizing conjunctive structures. Consequently, the total dependency structure of a sentence can be obtained by relatively simple head-dependent rules. A serious problem concerning conjunctive structures, besides the ambiguity of their scopes, is the ellipsis of some of their components. Through our dependency analysis process, we can find the ellipses and recover the omitted components. We report the results of analyzing 150 Japanese sentences to illustrate the effectiveness of this method. we propose a method to detect conjunctive structures by calculating similarity scores between two sequences of bunsetsus. we propose a similarity-based method to resolve both of the two tasks for Japanese. we propose a Japanese parsing method that included coordinate structure detection.	A Syntactic Analysis Method Of Long Japanese Sentences Based On The Detection Of Conjunctive Structures This paper explains a way to analyze sentence structure by first finding conjunctions (words like 'and', 'or') in a sentence by looking for patterns in two groups of words. Then, it uses this information to understand how the parts of the sentence connect to each other. Analyzing long sentences is very challenging in processing languages with computers. This is mainly because it's hard to figure out the sentence structure where conjunctions are present. People can understand conjunctions because of certain, sometimes subtle, similarities between the parts connected by conjunctions. To address this, we created a method to compare two groups of words on either side of a conjunction and find the most similar ones to identify the conjunction structure. This is done with a technique called dynamic programming, which is a way to solve problems by breaking them into simpler steps. By identifying conjunctions, we can simplify long sentences into shorter ones. As a result, we can understand the whole sentence structure using simple rules about which words depend on others. One major challenge with conjunctions, apart from the confusion about what they connect, is that sometimes parts of the sentence are left out. Our process can find these missing parts and fill them in. We show the results of examining 150 Japanese sentences to demonstrate how well this method works. We suggest a way to find conjunctions by comparing scores of similarity between two sets of 'bunsetsus' (units of words in Japanese). We offer a way based on similarity to solve both tasks for Japanese. We propose a Japanese sentence analysis method that includes finding coordinated structures.	program
313	An Algorithm For Pronominal Anaphora Resolution This paper presents an algorithm for identifying the noun phrase antecedents of third person pronouns and lexical anaphors (reflexives and reciprocals). The algorithm applies to the syntactic representations generated by McCord's Slot Grammar parser and relies on salience measures derived from syntactic structure and a simple dynamic model of attentional state. Like the parser, the algorithm is implemented in Prolog. The authors have tested it extensively on computer manual texts and conducted a blind test on manual text containing 360 pronoun occurrences. The algorithm successfully identifies the antecedent of the pronoun for 86% of these pronoun occurrences. The relative contributions of the algorithm's components to its overall success rate in this blind test are examined. Experiments were conducted with an enhancement of the algorithm that contributes statistically modelled information concerning semantic and real-world relations to the algorithm's decision procedure. Interestingly, this enhancement only marginally improves the algorithm's performance (by 2%). The algorithm is compared with other approaches to anaphora resolution that have been proposed in the literature. In particular, the search procedure of Hobbs' algorithm was implemented in the Slot Grammar framework and applied to the sentences in the blind test set. The authors" algorithm achieves a higher rate of success (4%) than Hobbs' algorithm. The relation of the algorithm to the centering approach is discussed, as well as to models of anaphora resolution that invoke a variety of informational factors in ranking antecedent candidates. In the heuristic salience-based algorithm for pronoun resolution, we introduce a procedure for identifying anaphorically linked NP as a cluster for which a global salience value is computed as the sum of the salience values of its elements. we describe an algorithm for pronominal anaphora resolution that achieves a high rate of correct analyses (85%).	An Algorithm For Pronominal Anaphora Resolution This paper presents a method for identifying the nouns that pronouns refer to. The method works with sentence structures created by McCord's Slot Grammar parser and uses importance measures based on sentence structure and a simple model of where attention is focused. The method, like the parser, is created using Prolog, a programming language. The authors tested it on computer manuals and did a test on 360 pronoun uses. The method correctly found the noun for the pronoun 86% of the time. The authors looked at how different parts of the method contributed to its success. They also tested an improved version of the method that used data about meaning and real-world connections, but it only improved results by 2%. The method was compared to other ways of solving pronoun references mentioned in research. Specifically, they tried another method, Hobbs' algorithm, within the Slot Grammar system on the test sentences. Their method was 4% more successful than Hobbs' method. The paper also discusses how their method relates to the centering theory, which considers various information factors for choosing the right noun. In their approach, they group related nouns and calculate an overall importance score by adding up the scores of each noun. They describe a method for resolving pronouns that correctly analyzes 85% of cases.	connect
314	Word Sense Disambiguation Using A Second Language Monolingual Corpus This paper presents a new approach for resolving lexical ambiguities in one language using statistical data from a monolingual corpus of another language. This approach exploits the differences between mappings of words to senses in different languages. The paper concentrates on the problem of target word selection in machine translation, for which the approach is directly applicable. The presented algorithm identifies syntactic relations between words, using a source language parser, and maps the alternative interpretations of these relations to the target language, using a bilingual lexicon. The preferred senses are then selected according to statistics on lexical relations in the target language. The selection is based on a statistical model and on a constraint propagation algorithm, which simultaneously handles all ambiguities in the sentence. The method was evaluated using three sets of Hebrew and German examples and was found to be very useful for disambiguation. The paper includes a detailed comparative analysis of statistical sense disambiguation methods. we propose an approach to WSD using monolingual corpora,a bilingual lexicon and a parser for the source language.	Word Sense Disambiguation Using A Second Language Monolingual Corpus This paper introduces a new way to clarify word meanings in one language by using statistics from a collection of text in another language where only one language is used. This method uses the differences in how words match up with meanings in different languages. The paper focuses on choosing the right word in machine translation, where this method can be directly applied. The method described finds connections between words using a tool that analyzes the grammar of the source language and matches these connections to another language using a dictionary that includes two languages. The best meanings are then chosen based on statistics about word relationships in the target language. This choice is made using a statistical model and a method that deals with all unclear meanings in a sentence at the same time. The method was tested with examples in Hebrew and German and was found to be very helpful for understanding word meanings. The paper also includes a detailed comparison of methods that use statistics to figure out word meanings. We suggest using a collection of text in one language, a dictionary for two languages, and a grammar tool for the source language to understand word meanings.	sentences
315	Machine Translation Divergences: A Formal Description And Proposed Solution There are many cases in which the natural translation of one language into another results in a very different form than that of the original. The existence of translation divergences (i.e., crosslinguistic distinctions) makes the straightforward transfer from source structures into target structures impractical. Many existing translation systems have mechanisms for handling divergent structures but do not provide a general procedure that takes advantage of the systematic relation between lexical-semantic structure and syntactic structure. This paper demonstrates that a systematic solution to the divergence problem can be derived from the formalization of two types of information: (1) the linguistically grounded classes upon which lexical-semantic divergences are based; and (2) the techniques by which lexical-semantic divergences are resolved. This formalization is advantageous in that it facilitates the design and implementation of the system, allows one to make an evaluation of the status of the system, and provides a basis for proving certain important properties about the system. We categorize sources of syntactic divergence between languages.	Machine Translation Divergences: A Formal Description And Proposed Solution. There are many times when translating from one language to another changes the form a lot compared to the original. The differences in how languages work (called translation divergences) mean that simply copying from one language to another doesn't work well. Many translation systems have ways to handle these differences but don't have a general method to make use of the connection between word meanings and sentence structure. This paper shows that a systematic solution to these differences can be created by formalizing two types of information: (1) the language-based groups that cause differences in word meanings; and (2) the methods used to solve these differences. This formalization is helpful because it makes designing and building the system easier, helps evaluate how well the system is working, and provides a way to prove some important features of the system. We identify the reasons why sentence structures differ between languages.	unclear meanings
316	An Efficient Probabilistic Context-Free Parsing Algorithm That Computes Prefix Probabilities We describe an extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities of successive prefixes being generated by the grammar; b) probabilities of substrings being generated by the nonterminals, including the entire string being generated by the grammar; c) most likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities. Probabilities (a) and (b) are computed incrementally in a single left-to-right pass over the input. Our algorithm compares favorably to standard bottom-up parsing methods for SCFGs in that it works efficiently on sparse grammars by making use of Earley's top-down control structure. It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm. Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs. An Earley chart is used for keeping track of all derivations that are consistent with the input.	An Efficient Probabilistic Context-Free Parsing Algorithm That Computes Prefix Probabilities We explain how we improved Earley's parser, a tool for analyzing sentences, to work with stochastic (random) context-free grammars. This improved parser can calculate these things when given a grammar and a sentence: a) the chances of different sentence beginnings being created by the grammar; b) the chances of parts of the sentence being created by the grammar's building blocks, including the whole sentence; c) the most likely explanation (Viterbi parse) of how the sentence was created; d) how often each rule in the grammar is used, which is needed to update rule chances. Chances (a) and (b) are calculated step-by-step in one pass from left to right over the sentence. Our method is better than the usual step-by-step methods for working with grammars because it works well with grammars that don't have many rules by using Earley's top-down method. It can handle any grammar rule format without needing to change it to a special form, and it calculates everything from (a) to (d) using one method. Also, the method can be easily adjusted to handle sentences with some parts marked and to find possible explanations and their chances for incorrect sentences. An Earley chart, a tool for keeping track of sentence explanations, is used to keep track of all possible ways the sentence could have been created.	systematic solution
317	Centering: A Framework For Modeling The Local Coherence Of Discourse This paper concerns relationships among focus of attention, choice of referring expression, and perceived coherence of utterances within a discourse segment. It presents a framework and initial theory of centering intended to model the local component of attentional state. The paper examines interactions between local coherence and choice of referring expressions; it argues that differences in coherence correspond in part to the inference demands made by different types of referring expressions, given a particular attentional state. It demonstrates that the attentional state properties modeled by centering can account for these differences. Our centering model uses a ranking of discourse entities realized in particular sentence sand computes transitions between adjacent sentences to provide insight in the felicity of texts. Our centering theory postulates strong links between the center of attention in comprehension of adjacent sentences and syntactic position and form of reference. Our centering Theory is an entity-based theory of local coherence, which claims that certain entities mentioned in an utterance are more central than others and that this property constrains a speaker's use of certain referring expressions. Our centering Theory is an influential framework for modelling entity coherence in computational linguistics in the last two decades.	Centering: A Framework For Modeling The Local Coherence Of Discourse This paper is about how focus of attention, choice of words to refer to something, and how smooth and logical sentences are within a part of a conversation relate to each other. It introduces a basic idea called centering to explain how attention works locally. The paper looks at how the smoothness of conversation and the choice of words to refer to something interact; it suggests that how smooth a conversation seems depends partly on the effort needed to understand different kinds of referring words, given a particular focus. It shows that the qualities of attention explained by centering can explain these differences. Our centering model ranks things mentioned in sentences and calculates changes between sentences to help understand how clear and well-structured texts are. Our centering theory suggests strong connections between what people focus on when understanding sentences and the way sentences are structured and refer to things. Our centering theory focuses on how certain things mentioned in a sentence are more important than others, which affects how a speaker uses certain words to refer to things. Our centering theory has been a key idea for understanding how we keep things connected in language studies over the past twenty years.	sentence explanations
318	Transformation-Based-Error-Driven Learning And Natural Language Processing: A Case Study In Part-Of-Speech Tagging Recently, there has been a rebirth of empiricism in the field of natural language processing. Manual encoding of linguistic information is being challenged by automated corpus-based learning as a method of providing a natural language processing system with linguistic knowledge. Although corpus-based approaches have been successful in many different areas of natural language processing, it is often the case that these methods capture the linguistic information they are modelling indirectly in large opaque tables of statistics. This can make it difficult to analyze, understand and improve the ability of these approaches to model underlying linguistic behavior. In this paper, we will describe a simple rule-based approach to automated learning of linguistic knowledge. This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance. We present a detailed case study of this learning method applied to part-of-speech tagging. We outline a transformation-based learned which learns guessing rules from a pre-tagged training corpus. We propose non-sequential transformation-based learning. We introduce a symbolic machine learning method, a class sequence example Transformation-based learning.	Transformation-Based-Error-Driven Learning And Natural Language Processing: A Case Study In Part-Of-Speech Tagging Recently, there has been a renewed focus on using real-world data in the field of natural language processing (NLP). Instead of manually programming language rules, people are using computers to learn from large collections of text (called a corpus) to help NLP systems understand language. While these methods have worked well in many areas, they often store language information in complex and confusing statistics tables. This can make it hard to understand and improve how these methods work with language. In this paper, we will explain a simple method that uses rules to help computers learn language information automatically. This method has been successful in several tasks by capturing information in a more understandable and straightforward way without losing effectiveness. We provide a detailed example of this learning method used in part-of-speech tagging, which is identifying the role of each word in a sentence, like noun or verb. We explain a system that learns guessing rules from existing tagged text data. We suggest a new way of applying this learning method that doesn’t rely on processing text in a set order. We introduce a symbolic machine learning method, which is a type of learning that focuses on clear and logical steps, using a method called transformation-based learning.	something
319	Translating Collocations For Bilingual Lexicons: A Statistical Approach Collocations are notoriously difficult for non-native speakers to translate, primarily because they are opaque and cannot be translated on a word-by-word basis. We describe a program named Champollion which, given a pair of parallel corpora in two different languages and a list of collocations in one of them, automatically produces their translations. Our goal is to provide a tool for compiling bilingual lexical information above the word level in multiple languages, for different domains. The algorithm we use is based on statistical methods and produces p-word translations of n-word collocations in which n and p need not be the same. For example, Champollion translates make ... decision, employment equity, and stock market into prendre ... decision, equite en matiere d'emploi, and bourse respectively. Testing Champollion on three years' worth of the Hansards corpus yielded the French translations of 300 collocations for each year, evaluated at 73% accuracy on average. In this paper, we describe the statistical measures used, the algorithm, and the implementation of Champollion, presenting our results and evaluation. The relationship between pointwise Mutual Information and the Dice coefficient is discussed in this work. We propose a corpus-based method to extract bilingual lexicons. We propose a statistical association measure of the Dice coefficient to deal with the problem of collocation translation.	Translating Collocations For Bilingual Lexicons: A Statistical Approach Collocations are groups of words that often go together, and they are tough for non-native speakers to translate because they don't make sense if you translate them one word at a time. We talk about a program called Champollion, which takes texts in two different languages and a list of these word groups in one language to automatically find their translations. Our aim is to create a tool that helps build bilingual dictionaries for groups of words, not just single words, in many languages and fields. The method we use relies on statistical techniques and creates translations where the number of words in each language doesn't have to match. For instance, Champollion translates phrases like make ... decision, employment equity, and stock market into the French equivalents prendre ... decision, equite en matiere d'emploi, and bourse. When tested on three years of the Hansards text collection, it found 300 French translations of these word groups each year, with an average accuracy of 73%. In this paper, we explain the statistical techniques, the method, and how Champollion works, along with our findings and evaluation. The connection between a statistical measure called pointwise Mutual Information and another called the Dice coefficient is discussed. We suggest a method based on text collections to create bilingual word lists. We also suggest using the Dice coefficient to help solve the problem of translating these word groups.	Speech Tagging
320	A Maximum Entropy Approach To Natural Language Processing The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing. We propose a gain-informed selection method. .	A Maximum Entropy Approach To Natural Language Processing The concept of maximum entropy, which is an idea about evenly spreading out information, has been around for a long time, even back to old times. But only recently have computers become strong enough to use this idea widely to solve real-world problems, like guessing and identifying patterns. In this paper, we explain a way to create statistical models (which are like mathematical representations of data) using maximum entropy. We show a method that helps build these models automatically and explain how to do this efficiently, using some examples from natural language processing (how computers understand human language). We suggest a method for choosing the best options by considering what we gain.	bilingual dictionaries
321	Assessing Agreement On Classification Tasks: The Kappa Statistic Currently, computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics, none of which are easily interpretable or comparable to each other. Meanwhile, researchers in content analysis have already experienced the same difficulties and come up with a solution in the kappa statistic. We discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science, and argue that we would be better off as afield adopting techniques from content analysis. Our method, kappa statistic, is used extensively in empirical studies of discourse (Carletta, 1996).	Assessing Agreement On Classification Tasks: The Kappa Statistic Currently, experts in computer language and the study of thinking who focus on conversation say their personal opinions are consistent using various statistics, which are hard to understand or compare with each other. At the same time, people studying content analysis have faced similar challenges and have found a solution with the kappa statistic. We talk about the problems with current methods for checking reliability in conversation studies in computer language and thinking science, and suggest that this field should use methods from content analysis. Our method, the kappa statistic, is widely used in real-world studies of conversation (Carletta, 1996).	helps build
322	A Stochastic Finite-State Word-Segmentation Algorithm For Chinese The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words. For languages like English one can assume, to a first approximation, that word boundaries are given by whitespace or punctuation. In various Asian languages, including Chinese, on the other hand, whitespace is never used to delimit words, so one must resort to lexical information to "reconstruct" the word-boundary information. In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer. The model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and--since the primary intended application of this model is to text-to-speech synthesis--provides pronunciations for these words. We evaluate the system's performance by comparing its segmentation 'judgments' with the judgments of a pool of human segmenters, and the system is shown to perform quite well. We built a word uni gram model using the Viterbi re-estimation whose initial estimates were derived from the frequencies in the corpus of the strings of each word in the lexicon. We proposed a method to estimate a set of initial word frequencies without segmenting the corpus.	A Stochastic Finite-State Word-Segmentation Algorithm For Chinese The first step in analyzing text for any Natural Language Processing (NLP) task usually involves breaking the input text into individual words, called tokenization. For languages like English, you can generally assume that spaces or punctuation marks show where words end. However, in many Asian languages like Chinese, spaces aren’t used to separate words. Instead, you have to use word information to figure out where words begin and end. In this paper, we introduce a model that uses random processes within a fixed set of rules (stochastic finite-state model) to break down Chinese text into words found in the dictionary and new words formed by different processes. The main part of this model is a tool called the weighted finite-state transducer. Since this model is mainly designed to help turn written text into spoken words (text-to-speech synthesis), it also provides how to pronounce these words. We test how well the system works by comparing its way of separating words with how a group of people do it, and it shows good results. We created a basic word model using a method called Viterbi re-estimation, which started with guesses based on how often each word appears in a collection of texts. We also suggested a way to guess initial word frequencies without needing to split the text collection into separate words first.	experts
323	The Reliability Of A Dialogue Structure Coding Scheme This paper describes the reliability of a dialogue structure coding scheme based on utterance function, game structure, and higher-level transaction structure that has been applied to a corpus of spontaneous task-oriented spoken dialogues. We computed agreement on a coarse segmentation level that was constructed on the top of finer segments, by determining how well coders agreed on where the coarse segments started, and, for agreed starts, by computing how coders agreed on where coarse segments ended.	The Reliability Of A Dialogue Structure Coding Scheme This paper explains how reliable a system is for organizing conversations based on what is being said, the flow of interaction, and the overall conversation pattern. This system was used on a collection of casual, goal-focused spoken conversations. We measured how much the people coding the conversations agreed on breaking them into bigger parts that were built from smaller pieces. We checked how well they agreed on where these bigger parts began and ended.	generally assume
324	TextTiling: Segmenting Text Into Multi-Paragraph Subtopic Passages TextTiling is a technique for subdividing texts into multi-paragraph units that represent passages, or subtopics. The discourse cues for identifying major subtopic shifts are patterns of lexical co-occurrence and distribution. The algorithm is fully implemented and is shown to produce segmentation that corresponds well to human judgments of the subtopic boundaries of 12 texts. Multi-paragraph subtopic segmentation should be useful for many text analysis tasks, including information retrieval and summarization. We compute chance agreement in terms of the probability that coders would say that a segment boundary exists (segt), and the probability that they would not (unsegt).	TextTiling: Segmenting Text Into Multi-Paragraph Subtopic Passages TextTiling is a method for breaking down texts into sections with several paragraphs that show different topics or ideas. The clues for finding big changes in topics are patterns of words that appear together and how they are spread out. The method is completely developed and has been shown to divide text in a way that matches how people naturally identify topic changes in 12 different texts. Breaking text into multi-paragraph sections should help with tasks like analyzing text, finding information, and making summaries. We calculate how often people agree by chance on whether they think a boundary between segments exists (segt) or not (unsegt).	focused
325	Discourse Segmentation By Human And Automated Means The need to model the relation between discourse structure and linguistic features of utterances is almost universally acknowledged in the literature on discourse. However, there is only weak consensus on what the units of discourse structure are, or the criteria for recognizing and generating them. We present quantitative results of a two-part study using a corpus of spontaneous, narrative monologues. The first part of our paper presents a method for empirically validating multiutterance units referred to as discourse segments. We report highly significant results of segmentations performed by naive subjects, where a commonsense notion of speaker intention is the segmentation criterion. In the second part of our study, data abstracted from the subjects' segmentations serve as a target for evaluating two sets of algorithms that use utterance features to perform segmentation. On the first algorithm set, we evaluate and compare the correlation of discourse segmentation with three types of linguistic cues (referential noun phrases, cue words, and pauses). We then develop a second set using two methods: error analysis and machine learning. Testing the new algorithms on a new data set shows that when multiple sources of linguistic knowledge are used concurrently, algorithm performance improves. We describe an experiment where seven untrained annotators were asked to find discourse segments in a corpus of transcribed narratives about a movie.	Discourse Segmentation By Human And Automated Means The need to understand how the structure of conversations relates to language features is widely accepted. However, there's little agreement on what these conversation units are or how to identify them. We share detailed findings from a two-part study using a collection of natural, storytelling speeches. The first part explains a method to test conversation units called discourse segments. We found strong results from people without specialized training, using common sense to decide where segments begin and end. In the second part, we use the data from these people to test two sets of computer programs that look at speech features to find segments. For the first set, we check how well these programs match with three language signals (like names, connecting words, and pauses). We then create a second set using two techniques: error checking and teaching computers to learn. Testing these new programs on fresh data shows they work better when they use different types of language knowledge at the same time. We also conducted an experiment where seven people without training were asked to find conversation segments in written stories about a movie.	analyzing
354	Chunking With Support Vector Machines We apply Support Vector Machines (SVMs) to identify English base phrases (chunks). SVMs are known to achieve high generalization performance even with input data of high dimensional feature spaces. Furthermore, by the Kernel principle, SVMs can carry out training with smaller computational overhead independent of their dimensionality. We apply weighted voting of 8 SVMs-based systems trained with distinct chunk representations. Experimental results show that our approach achieves higher accuracy than previous approaches. In this paper, we develop an SVMs-based chunking tool YamCha.	Chunking With Support Vector Machines We use Support Vector Machines (SVMs) to find basic parts of English sentences, called chunks. SVMs are good at handling complex data and still make accurate predictions. Thanks to something called the Kernel principle, SVMs can learn effectively without needing a lot of computer power, even with complex data. We use a method where 8 different SVM systems, each trained with different ways of identifying chunks, vote on the results. Tests show that our method is more accurate than older ones. In this paper, we create a tool called YamCha that uses SVMs for chunking.	written
326	Finite-State Transducers In Language And Speech Processing Finite-state machines have been used in various domains of natural language processing. We consider here the use of a type of transducer that supports very efficient programs: sequential transducers. We recall classical theorems and give new ones characterizing sequential string-tostring transducers. Transducers that output weights also play an important role in language and speech processing. We give a specific study of string-to-weight transducers, including algorithms for determinizing and minimizing these transducers very efficiently, and characterizations of the transducers admitting determinization and the corresponding algorithms. Some applications of these algorithms in speech recognition are described and illustrated. Application of cascades of weighted string transducers (WSTs) has been well-studied in this work.	Finite-State Transducers In Language And Speech Processing Finite-state machines, which are simple computational models, have been used in many areas of natural language processing, which involves understanding and generating human language. We focus on using a specific type of machine called sequential transducers, which are known for running very efficient programs or tasks. We review well-known rules and introduce new ones that describe how these sequential machines transform one string of characters into another. Machines that also produce numerical values, or weights, are important for processing language and speech. We examine string-to-weight machines in detail, including methods for making these machines run more efficiently by simplifying them, and rules for identifying which machines can be simplified in this way. We also explain how these methods are used in speech recognition, which is the technology that allows computers to understand spoken words, with examples. The use of a series of weighted string machines, known as weighted string transducers, is thoroughly explored in this study.	specialized
327	Stochastic Inversion Transduction Grammars And Bilingual Parsing Of Parallel Corpora We introduce (1) a novel stochastic inversion transduction grammar formalism for bilingual language modeling of sentence-pairs, and (2) the concept of bilingual parsing with a variety of parallel corpus analysis applications. Aside from the bilingual orientation, three major features distinguish the formalism from the finite-state transducers more traditionally found in computational linguistics: it skips directly to a context-free rather than finite-state base, it permits a minimal extra degree of ordering flexibility, and its probabilistic formulation admits an efficient maximum-likelihood bilingual parsing algorithm. A convenient normal form is shown to exist. Analysis of the formalism's expressiveness suggests that it is particularly well suited to modeling ordering shifts between languages, balancing needed flexibility against complexity constraints. We discuss a number of examples of how stochastic inversion transduction grammars bring bilingual constraints to bear upon problematic corpus analysis tasks such as segmentation, bracketing, phrasal alignment, and parsing. We use an inside-outside type of training algorithm to learn statistical context free transduction. Our Bilingual Bracketing is one of the bilingual shallow parsing approaches studied for Chinese-English word alignment. We introduce a polynomial-time solution for the alignment problem based on synchronous binary trees.	Stochastic Inversion Transduction Grammars And Bilingual Parsing Of Parallel Corpora We introduce (1) a new way of using stochastic inversion transduction grammar, which is a method that uses randomness to model pairs of sentences in two languages, and (2) the idea of analyzing bilingual text with various uses in studying texts written in two languages at the same time. Unlike traditional methods, which often use simpler models called finite-state transducers, our method has three main features: it starts with a more advanced model called context-free, it allows some flexibility in how sentences are ordered, and it uses probabilities to allow a more effective way to analyze bilingual text. We show that there is a standard way to use this method. Our analysis suggests that this method is especially good at handling the way languages can change order, providing the right balance between flexibility and complexity. We provide several examples of how using this method helps solve difficult problems in analyzing bilingual texts, like breaking down text into parts, aligning phrases, and parsing, which means understanding the structure. We use a specific type of training called an inside-outside algorithm to learn how to translate sentences in a structured way. Our method of Bilingual Bracketing is one of the ways studied to align words between Chinese and English, which is a type of shallow parsing that doesn't go into too much detail. We offer a solution that works within a reasonable time frame for aligning text based on matching structures called synchronous binary trees.	State Transducers
328	Automatic Rule Induction For Unknown-Word Guessing Words unknown to the lexicon present a substantial problem to NLP modules that rely on morphosyntactic information, such as part-of-speech taggers or syntactic parsers. In this paper we present a technique for fully automatic acquisition of rules that guess possible part-of-speech tags for unknown words using their starting and ending segments. The learning is performed from a general-purpose lexicon and word frequencies collected from a raw corpus. Three complimentary sets of word-guessing rules are statistically induced: prefix morphological rules, suffix morphological rules and ending-guessing rules. Using the proposed technique, unknown-word-guessing rule sets were induced and integrated into a stochastic tagger and a rule-based tagger, which were then applied to texts with unknown words. Our model, LTPOS, performs both sentence identification and POS tagging. Our ltpos is a statistical combined part-of-speech (POS) tagger and sentence boundary disambiguation module. Our ltpos is a statistical combined part-of-speech (POS) tagger and sentence boundary disambiguation module.	Automatic Rule Induction For Unknown-Word Guessing Words unknown to the dictionary create a big problem for Natural Language Processing (NLP) tools, like those that identify parts of speech (e.g., noun, verb) or analyze sentence structure. In this paper, we show a method to automatically create rules to guess the possible parts of speech for unknown words by looking at the beginning and end parts of the words. This learning comes from a general dictionary and word counts gathered from a collection of text. Three types of word-guessing rules are created using statistics: rules based on the start of the word (prefix), the end of the word (suffix), and just the ending. Using this method, sets of rules for guessing unknown words were created and added to a tagger that uses chance (stochastic tagger) and one that uses set rules, which were then used on texts with unknown words. Our model, LTPOS, does both identifying sentences and tagging parts of speech. Our LTPOS is a statistical tool that combines tagging parts of speech and figuring out where sentences start and end.	Bracketing
329	Stochastic Attribute-Value Grammars Probabilistic analogues of regular and context-free grammars are well known in computational linguistics, and currently the subject of intensive research. To date, however, no satisfactory probabilistic analogue of attribute-value grammars has been proposed: previous attempts have failed to define an adequate parameter-estimation algorithm. In the present paper, I define stochastic attribute-value grammars and give an algorithm for computing the maximum-likelihood estimate of their parameters. The estimation algorithm is adapted from Della Pietra, Della Pietra, and Lafferty (1995). To estimate model parameters, it is necessary to compute the expectations of certain functions under random fields. In the application discussed by Della Pietra, Della Pietra, and Lafferty (representing English orthographic constraints), Gibbs sampling can be used to estimate the needed expectations. The fact that attribute-value grammars generate constrained languages makes Gibbs sampling inapplicable, but I show that sampling can be done using the more general Metropolis-Hastings algorithm. We proposes a Markov Random Field or log linear model for SUBGs.	Stochastic Attribute-Value Grammars Probabilistic analogues (similar versions using probability) of regular and context-free grammars (rules for arranging words) are well known in computational linguistics (study of language using computers) and are being heavily researched. However, so far, no satisfactory probabilistic version of attribute-value grammars (a type of grammar involving pairs of characteristics) has been created because previous attempts couldn't define a good method for estimating parameters (important numbers in a model). In this paper, I explain stochastic (random) attribute-value grammars and provide a method for finding the best estimation of their parameters. The method for estimation is based on work by Della Pietra, Della Pietra, and Lafferty (1995). To estimate model parameters, it is necessary to figure out the expected values of certain functions in random settings. In the study by Della Pietra, Della Pietra, and Lafferty (about English spelling rules), Gibbs sampling (a statistical method) is used to calculate these expected values. Because attribute-value grammars create restricted languages, Gibbs sampling doesn't work, but I show that sampling can be done using a broader method called the Metropolis-Hastings algorithm (a technique for generating samples). We propose a Markov Random Field or log-linear model for SUBGs (a type of structured data).	structure
330	Introduction To The Special Issue On Word Sense Disambiguation: The State Of The Art We present a very concise survey of the history of ideas used in word sense disambiguation. In general, the various WSD approaches of the past can be divided into two types, i.e., data and knowledge-based approaches. We argue that word sense ambiguity is a central problem for many established HLT applications (for example Machine Translation, Information Extraction and Information Retrieval).	Introduction To The Special Issue On Word Sense Disambiguation: The State Of The Art We present a short overview of the history of ideas used in word sense disambiguation, which is figuring out which meaning of a word is being used. In general, the different methods for figuring out word meanings from the past can be divided into two types: those based on data and those based on knowledge. We believe that understanding word meanings is a major issue for many well-known human language technology (HLT) applications, like translating languages by computer, pulling specific information from texts, and finding information.	Stochastic
331	Using Corpus Statistics And WordNet Relations For Sense Identification Corpus-based approaches to word sense identification have flexibility and generality but suffer from a knowledge acquisition bottleneck. We show how knowledge-based techniques can be used to open the bottleneck by automatically locating training corpora. We describe a statistical classifier that combines topical context with local cues to identity a word sense. The classifier is used to disambiguate a noun, a verb, and an adjective. A knowledge base in the form of WordNet's lexical relations is used to automatically locate training examples in a general text corpus. Test results are compared with those from manually tagged training examples. We present a method to obtain sense-tagged examples using monosemous relatives.	Using Corpus Statistics And WordNet Relations For Sense Identification Corpus-based methods for figuring out the meaning of a word are flexible and can be used in many situations, but they have a problem with getting enough knowledge. We show how methods that rely on existing knowledge can help solve this problem by automatically finding useful text collections. We explain a tool that uses statistics to combine the general topic and specific hints to identify what a word means. This tool is used to figure out the meaning of a noun, a verb, and an adjective. A database like WordNet, which shows how words are related, is used to automatically find examples in a general collection of texts. We compare test results with results from examples that were tagged by hand. We introduce a way to get examples with specific meanings using words that only have one meaning in a certain context.	meanings
332	A Corpus-Based Investigation Of Definite Description Use We present the results of a study of the use of definite descriptions in written texts aimed at assessing the feasibility of annotating corpora with information about definite description interpretation. We ran two experiments, in which subjects were asked to classify the uses of definite descriptions in a corpus of 33 newspaper articles, containing a total of l,412 definite descriptions. We measured the agreement among annotators about the classes assigned to definite descriptions, as well as the agreement about the antecedent assigned to those definites that the annotators classified as being related to an antecedent in the text. The most interesting result of this study from a corpus annotation perspective was the rather low agreement (K = 0.63) that we obtained using versions of Hawkins's and Prince's classification schemes; better results (K = 0.76) were obtained using the simplified scheme proposed by Fraurud that includes only two classes, firstmention and subsequent-mention. The agreement about antecedents was also not complete. These findings raise questions concerning the strategy of evaluating systems for definite description interpretation by comparing their results with a standardized annotation. From a linguistic point of view, the most interesting observations were the great number of discourse-new definites in our corpus (in one of our experiments, about 50% of the definites in the collection were classified as discourse-new, 30% as anaphoric, and 18% as associative/bridging) and the presence of definites that did not seem to require a complete disambiguation. We propose an annotation scheme, which is a product of a corpus based analysis of definite description (DD) use showing that more than 50% of the DDs in their corpus are discourse new or unfamiliar.	A Corpus-Based Investigation Of Definite Description Use We present the results of a study on how definite descriptions (phrases like "the book" that refer to something specific) are used in written texts. We wanted to see if it was possible to label large collections of text (called corpora) with information about how these descriptions are understood. We conducted two experiments where people were asked to identify different uses of these descriptions in a collection of 33 newspaper articles, containing 1,412 definite descriptions. We checked how much the participants agreed on the categories they assigned to the definite descriptions and how much they agreed on what previous part of the text (antecedent) these descriptions referred to. The most interesting discovery was that the agreement among participants was quite low (K = 0.63) when using complex classification methods by Hawkins and Prince. However, a simpler method suggested by Fraurud, which uses only two categories - first mention and subsequent mention - got better agreement (K = 0.76). Agreement on what the descriptions referred to was also not complete. These findings suggest that it's problematic to judge systems for understanding definite descriptions by comparing them to a standard set of labels. From a language perspective, we noticed many new, unfamiliar definite descriptions in our text collection (in one experiment, around 50% were new, 30% referred back to something mentioned earlier, and 18% were related or connected in another way). Some descriptions didn't need full clarification. We suggest a labeling method based on this study showing that more than 50% of definite descriptions in the text are new or unfamiliar.	examples
333	Generalizing Case Frames Using A Thesaurus And The MDL Principle A new method for automatically acquiring case frame patterns from large corpora is proposed. In particular, the problem of generalizing values of a case frame slot for a verb is viewed as that of estimating a conditional probability distribution over a partition of words, and a new generalization method based on the Minimum Description Length (MDL) principle is proposed. In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as "cuts" in the thesaurus tree, thus reducing the generalization problem to that of estimating a "tree cut model" of the thesaurus tree. An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL. Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity. Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods. We use a minimum description length-based algorithm to find an optimal tree cut over WordNet for each classification problem, finding improvements over both lexical association (Hindle and Rooth, 1993) and conceptual association, and equaling the transformation-based results. We propose a model in which the appropriate cut c is selected according to the MinimumDescription Length principle; this principle explicitly accounts for the trade-off between generalisation and accuracy by minimising a sum of model description length and data description length.	Generalizing Case Frames Using A Thesaurus And The MDL Principle A new method for automatically finding patterns in language structures, called case frames, from large collections of text is introduced. The challenge of simplifying the values in a case frame for a verb is seen as figuring out the likelihood of different words being used, using a new method based on the Minimum Description Length (MDL) principle, which is a way to simplify information. To make this process quicker, the method uses a thesaurus and focuses only on sections that already exist in the thesaurus, making it easier by focusing on a specific part, known as a "tree cut model," of the thesaurus. A fast algorithm is provided, which is guaranteed to find the best tree cut model for the given word usage data, according to MDL. The patterns found by this method were used to solve confusion about where prepositional phrases (like "in the park") should go in a sentence. Tests show that this new method is better or at least as good as other current methods. We use an MDL-based algorithm to find the best tree cut over WordNet, a large thesaurus, for each problem, showing improvements over previous methods that focused on word connections (Hindle and Rooth, 1993) and idea connections, matching the best transformation-based results. We propose a model where the best cut is chosen based on MDL, which carefully balances between being general and being accurate by minimizing the total length of the model and the data descriptions.	referred
334	New Figures Of Merit For Best-First Probabilistic Chart Parsing Best-first parsing methods for natural language try to parse efficiently by considering the most likely constituents first. Some figure of merit is needed by which to compare the likelihood of constituents, and the choice of this figure has a substantial impact on the efficiency of the parser. While several parsers described in the literature have used such techniques, there is little published data on their efficacy, much less attempts to judge their relative merits. We propose and evaluate several figures of merit for best-first parsing, and we identify an easily computable figure of merit that provides excellent performance on various measures and two different grammars. We present best-first parsing with Figures of Merit that allows conditioning of the heuristic function on statistics of the input string.	New Figures Of Merit For Best-First Probabilistic Chart Parsing Best-first parsing methods for natural language aim to analyze sentences efficiently by looking at the most probable parts first. A specific way to measure how likely these parts are is needed, which greatly affects how well the parser works. Although some sentence analyzers mentioned in studies have used these methods, there isn't much information published about how well they actually work, and even fewer efforts to compare their effectiveness. We suggest and test several ways to measure effectiveness for best-first parsing and identify a method that is easy to calculate and works very well across different tests and grammar rules. We introduce best-first parsing with these effectiveness measures that allow adapting the decision-making process based on patterns in the input sentence.	focusing
335	Generating Natural Language Summaries From Multiple On-Line Sources We present a methodology for summarization of news about current events in the form of briefings that include appropriate background (historical) information. The system that we developed, SUMMONS, uses the output of systems developed for the DARPA Message Understanding Conferences to generate summaries of multiple documents on the same or related events, presenting similarities and differences, contradictions, and generalizations among sources of information. We describe the various components of the system, showing how information from multiple articles is combined, organized into a paragraph, and finally, realized as English sentences. A feature of our work is the extraction of descriptions of entities such as people and places for reuse to enhance a briefing. We combine work in information extraction and natural language processing.	Generating Natural Language Summaries From Multiple On-Line Sources We present a method for creating short summaries of news about current events, which also include important background (historical) information. Our system, called SUMMONS, uses results from systems made for the DARPA Message Understanding Conferences to create summaries of several documents about the same or related events. It shows similarities and differences, disagreements, and general ideas among different sources of information. We explain the different parts of the system, showing how information from various articles is put together, organized into a paragraph, and then turned into English sentences. A special part of our work is pulling out details about people and places to use again, making the summaries better. We combine work in getting information and processing natural language.	measure
336	Machine Transliteration It is challenging to translate names and technical terms across languages with different alphabets and sound inventories. These items are commonly transliterated, i.e., replaced with approximate phonetic equivalents. For example, "computer" in English comes out as "konpyuutaa" in Japanese. Translating such items from Japanese back to English is even more challenging, and of practical interest, as transliterated items make up the bulk of text phrases not found in bilingual dictionaries. We describe and evaluate a method for performing backwards transliterations by machine. This method uses a generative model, incorporating several distinct stages in the transliteration process. We proposed a Japanese-English transliteration method based on the mapping probability between English and Japanese katakana sounds.	Machine Transliteration It is difficult to translate names and technical terms between languages that use different alphabets and sounds. These terms are often transliterated, meaning they are replaced with similar-sounding letters. For example, "computer" in English is written as "konpyuutaa" in Japanese. Translating these back from Japanese to English is even harder and important because these transliterated terms are often not found in bilingual dictionaries. We explain and test a method for doing reverse transliterations using a machine. This method uses a step-by-step model that includes different parts of the transliteration process. We suggested a Japanese-English transliteration method based on the chances of matching sounds between English and Japanese katakana.	called SUMMONS
337	PCFG Models Of Linguistic Tree Representations The kinds of tree representations used in a treebank corpus can have a dramatic effect on performance of a parser based on the PCFG estimated from that corpus, causing the estimated likelihood of a tree to differ substantially from its frequency in the training corpus. This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8 %, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today. This performance variation comes about because any PCFG, and hence the corpus of trees from which the PCFG is induced, embodies independence assumptions about the distribution of words and phrases. The particular independence assumptions implicit in a tree representation can be studied theoretically and investigated empirically by means of a tree transformation / detransformation process. We annotate each node by its parent category in a tree, and gets significant improvements compared with the original PCFGs on the Penn Treebank.	PCFG Models Of Linguistic Tree Representations The types of tree structures used in a collection of annotated text, called a treebank, can greatly impact how well a computer program that predicts sentence structure, called a parser, works. This is because the predicted likelihood of a tree structure might be very different from how often it actually appears in the training data. This paper highlights that the Penn II treebank, a specific set of tree structures, is expected to affect parser performance in this way. It also describes a straightforward way of renaming parts of the tree that can boost the parser's accuracy and ability to retrieve information by about 8%, which is roughly half the gap between a basic PCFG model and the top-performing parsers available today. These changes in performance happen because any PCFG, which stands for Probabilistic Context-Free Grammar, assumes certain things about how words and phrases are distributed. The specific assumptions about independence in a tree structure can be explored theoretically and tested practically by changing and then reversing changes in the tree. We add information to each part of the tree about its parent category, leading to significant improvements compared to the original PCFGs on the Penn Treebank.	dictionaries
338	Bitext Maps And Alignment Via Pattern Recognition Texts that are available in two languages (bitexts) are becoming more and more plentiful, both in private data warehouses and on publicly accessible sites on the World Wide Web. As with other kinds of data, the value of bitexts largely depends on the efficacy of the available data mining tools. The first step in extracting useful information from bitexts is to find corresponding words and/or text segment boundaries in their two halves (bitext maps). This article advances the state of the art of bitext mapping by formulating the problem in terms of pattern recognition. From this point of view, the success of a bitext mapping algorithm hinges on how well it performs three tasks: signal generation, noise filtering, and search. The Smooth Injective Map Recognizer (SIMR) algorithm presented here integrates innovative approaches to each of these tasks. Objective evaluation has shown that SIMR's accuracy is consistently high for language pairs as diverse as French/English and Korean/English. If necessary, S IMR's bitext maps can be efficiently converted into segment alignments using the Geometric Segment Alignment (GSA) algorithm, which is also presented here. SIMR has produced bitext maps for over 200 megabytes of French-English bitexts. GSA has converted these maps into alignments. Both the maps and the alignments are available from the Linguistic Data Consortium. We normalize LCS by dividing the length of the longest common subsequence by the length of the longer string and called it longest common subsequence ratio (LCSR).	Bitext Maps And Alignment Via Pattern Recognition Texts that are available in two languages (bitexts) are becoming more and more abundant, both in private data storage areas and on publicly accessible websites on the internet. Like other types of data, the value of bitexts largely depends on how effective the available data analysis tools are. The first step in getting useful information from bitexts is to find matching words or sections in their two parts (bitext maps). This article improves the current methods of bitext mapping by seeing the problem as one of pattern recognition, which is identifying patterns or regularities in the data. From this perspective, the success of a bitext mapping method depends on how well it does three things: creating signals, removing unwanted data (noise), and finding information. The Smooth Injective Map Recognizer (SIMR) method introduced here combines new techniques for each of these tasks. Testing has shown that SIMR's accuracy is consistently high for language pairs as different as French/English and Korean/English. If needed, SIMR's bitext maps can be easily turned into section alignments using the Geometric Segment Alignment (GSA) method, which is also explained here. SIMR has created bitext maps for over 200 megabytes of French-English texts. GSA has changed these maps into alignments. Both the maps and the alignments are available from the Linguistic Data Consortium, which is a group that provides language data. We standardize the Longest Common Subsequence (LCS) by dividing the length of the longest common sequence of characters by the length of the longer text and call it the longest common subsequence ratio (LCSR).	which stands
339	Supertagging: An Approach To Almost Parsing In this paper, we have proposed novel methods for robust parsing that integrate the flexibility of linguistically motivated lexical descriptions with the robustness of statistical techniques. Our thesis is that the computation of linguistic structure can be localized iflexical items are associated with rich descriptions (supertags) that impose complex constraints in a local context. The supertags are designed such that only those elements on which the lexical item imposes constraints appear within a given supertag. Further, each lexical item is associated with as many supertags as the number of different syntactic contexts in which the lexical item can appear. This makes the number of different descriptions for each lexical item much larger than when the descriptions are less complex, thus increasing the local ambiguity for a parser. But this local ambiguity can be resolved by using statistical distributions of supertag co-occurrences collected from a corpus of parses. We have explored these ideas in the context of the Lexicalized Tree-Adjoining Grammar (LTAG) framework. The supertags in LTAG combine both phrase structure information and dependency information in a single representation. Supertag disambiguation results in a representation that is effectively a parse (an almost parse), and the parser need "only" combine the individual supertags. This method of parsing can also be used to parse sentence fragments such as in spoken utterances where the disambiguated supertag sequence may not combine into a single structure. We indicate that, correct disambiguation with supertagging, i.e., assignment of lexical entries before parsing, enable effective LTAG (Lexicalized Tree-Adjoining Grammar) parsing.	Supertagging: An Approach To Almost Parsing In this paper, we have proposed new methods for strong parsing that combine flexible word descriptions based on language rules with strong statistical methods. Our main idea is that understanding sentence structure can be simplified if words are linked to detailed descriptions (supertags) that set specific rules in a small area around them. The supertags are made so that only the parts affected by the word's rules appear in each supertag. Each word can have as many supertags as the different sentence structures it can fit into. This means there are many more descriptions for each word compared to simpler descriptions, increasing confusion for a computer analyzing the sentence. But this confusion can be sorted out by using patterns of how supertags appear together, collected from a large set of analyzed sentences. We have tested these ideas in the Lexicalized Tree-Adjoining Grammar (LTAG) system. In LTAG, supertags combine both sentence structure and word relationships in one form. Solving the supertag puzzle gives a result that is almost a full sentence analysis, and the computer program needs to just put the supertags together. This parsing method can also be used for parts of sentences, like in spoken language, where the supertag sequence might not fit into one structure. We show that correctly figuring out the supertags, which means choosing word entries before analyzing, allows effective LTAG parsing.	storage areas
340	Functional Centering Grounding Referential Coherence In Information Structure Considering empirical evidence from a free-word-order language (German) we propose a revision of the principles guiding the ordering of discourse entities in the forward-looking center list within the centering model. We claim that grammatical role criteria should be replaced by criteria that reflect the functional information structure of the utterances. These new criteria are based on the distinction between hearer-old and hearer-new discourse entities. We demonstrate that such a functional model of centering can be successfully applied to the analysis of several forms of referential text phenomena, viz. pronominal, nominal, and functional anaphora. Our methodological and empirical claims are substantiated by two evaluation studies. In the first one, we compare success rates for the resolution of pronominal anaphora that result from a grammatical role-driven centering algorithm and from a functional centering algorithm. The second study deals with a new cost-based evaluation methodology for the assessment of centering data, one which can be directly derived from and justified by the cognitive load premises of the centering model. we introduce Functional Centering, a variant of Centering Theory which utilizes information status distinctions between hearer-old and hearer-new entities.	Functional Centering Grounding Referential Coherence In Information Structure Considering real-life evidence from German, a language where word order is flexible, we suggest changing the rules that decide how topics are ordered in the centering model. We believe that instead of using grammatical roles, we should use rules that show the important information structure of sentences. These new rules focus on whether the topics are already known or new to the listener. We show that this functional approach can be used to understand different ways topics are referred to in text, such as using pronouns or names. Our ideas are supported by two studies. The first study compares how well two different methods solve pronoun references: one based on grammar roles and the other on our functional approach. The second study introduces a new way to evaluate centering data based on cost, which is explained by the mental effort ideas in the centering model. We present Functional Centering, a version of Centering Theory that uses the difference between what the listener already knows and what is new to them.	computer program
383	Automatically Labeling Semantic Classes Systems that automatically discover semantic classes have emerged in part to address the limitations of broad-coverage lexical resources such as WordNet and Cyc. The current state of the art discovers many semantic classes but fails to label their concepts. We propose an algorithm labeling semantic classes and for leveraging them to extract is-a relationships using a top-down approach. The relationships automatically learned in our system include appositions, nominal subjects, such as relationships, and like relationships. Our syntactical co-occurrence approach has worst-case time complexity O (n2k), where n is the number of words in the corpus and k is the feature space. Given a collection of news articles that is both cleaner and smaller than Web document collections, a syntactic parser is applied to document sentences in order to identify and exploit syntactic dependencies for the purpose of selecting candidate class labels.	Automatically Labeling Semantic Classes Systems that automatically find groups of similar meaning words have been created partly to fix the limits of big word databases like WordNet and Cyc. The best current methods can find many groups but struggle to name their ideas. We suggest a method to name these groups and use them to find "is-a" relationships (like saying a dog is an animal) using a top-down method. The relationships our system learns on its own include appositions (phrases that rename a noun next to them), nominal subjects (the main noun in a sentence), and connections like "such as" and "like." Our method, which looks at how words appear together, has a worst-case time complexity of O(n2k), where "n" is the number of words in the text, and "k" is the number of features we look at. When we use a set of news articles that are cleaner and smaller compared to collections from the internet, we use a syntactic parser (a tool to analyze sentence structure) on the sentences to find and use sentence dependencies to help pick possible labels for the groups.	clear explanation
341	Semiring Parsing We synthesize work on parsing algorithms, deductive parsing, and the theory of algebra applied to formal languages into a general system for describing parsers. Each parser performs abstract computations using the operations of a semiring. The system allows a single, simple representation to be used for describing parsers that compute recognition, derivation forests, Viterbi, n-best, inside values, and other values, simply by substituting the operations of different semirings. We also show how to use the same representation, interpreted differently, to compute outside values. The system can be used to describe a wide variety of parsers, including Earley's algorithm, tree adjoining grammar parsing, Graham Harrison Ruzzo parsing, and prefix value computation. We show how a parsing logic can be combined with various semirings to compute different kinds of information about the input. We augment such logic programs with semiring weights, giving an algebraic explanation for the intuitive connections among classes of algorithms with the same logical structure.	Semiring Parsing We combine ideas from parsing methods, logical parsing, and the use of math in formal languages to create a system that describes parsers. Each parser does basic calculations using a structure called a semiring. This system lets us use a simple method to describe different parsers by changing the semiring operations to get recognition, derivation trees, Viterbi (best paths), top results, internal values, and more. We also explain how to find external values using the same method but in a different way. This system can describe many kinds of parsers, like Earley's method, tree joining grammar parsing, a method by Graham Harrison Ruzzo, and prefix value calculation. We explain how to mix parsing rules with different semirings to find various details about the input. We add semiring weights to these rules, providing a math-based reason for why similar algorithm classes are connected.	first study
342	Decoding Complexity In Word-Replacement Translation Models Statistical machine translation is a relatively new approach to the long-standing problem of translating human languages by computer. Current statistical techniques uncover translation rules from bilingual training texts and use those rules to translate new texts. The general architecture is the source-channel model: an English string is statistically generated (source), then statistically transformed into French (channel). In order to translate (or "decode") a French string, we look for the most likely English source. We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence. We trace this complexity to factors not present in other decoding problems. we proved that the Exact Decoding problem is NP-Hard when the language model is a bigram model. we show that the decoding problem for SMT as well as some bilingual tiling problems are NP-complete, so no efficient algorithm exists in the general case.	Decoding Complexity In Word-Replacement Translation Models Statistical machine translation is a new way to solve the old problem of computers translating human languages. Current methods find translation rules from texts in two languages and use these rules to translate new texts. The main structure is the source-channel model: an English sentence is created using statistics (source) and then changed into French using statistics (channel). To translate a French sentence, we find the most likely English version. We show that for the simplest statistical models, this problem is very hard to solve (NP-complete), meaning it likely takes a lot of time as the sentence gets longer. We find this complexity comes from factors not in other translation problems. We proved that the Exact Decoding problem is very hard (NP-Hard) when using a bigram model, which looks at pairs of words. We show that the problem of translating in statistical machine translation (SMT) and some bilingual matching problems are NP-complete, meaning there is no quick way to solve them in general.	prefix value
343	The Penn Discourse TreeBank 2.0. We present the second version of the Penn Discourse Treebank, PDTB-2.0, describing its lexically-grounded annotations of discourse relations and their two abstract object arguments over the 1 million word Wall Street Journal corpus. We describe all aspects of the annotation, including (a) the argument structure of discourse relations, (b) the sense annotation of the relations, and (c) the attribution of discourse relations and each of their arguments. We list the differences between PDTB-1.0 and PDTB-2.0. We present representative statistics for several aspects of the annotation in the corpus. we present The Penn Discourse Treebank (PDTB) ,such a corpus which provides a discourse-level annotation on top of the Penn Treebank, following a predicate argument approach (Webber, 2004).	The Penn Discourse TreeBank 2.0. We introduce the second version of the Penn Discourse Treebank, PDTB-2.0, explaining its word-based notes on discourse connections (ways sentences and parts of text relate to each other) and their two abstract object arguments (the ideas or topics they connect) using the 1 million word Wall Street Journal text collection. We explain all parts of the notes, including (a) the argument structure of discourse relations (how these connections are built), (b) the sense annotation of the relations (labeling the meaning or function of connections), and (c) the attribution of discourse relations and each of their arguments (who is responsible for these connections). We list the differences between PDTB-1.0 and PDTB-2.0. We show sample numbers for different parts of the notes in the text collection. We present The Penn Discourse Treebank (PDTB), which is a collection that adds a layer of discourse-level notes on top of the Penn Treebank, using a predicate argument approach (a method focusing on the main action and its participants).	complexity comes
344	A Model-Theoretic Coreference Scoring Scheme This note describes a scoring scheme for the coreference task in MUC6. It improves on the original approach l by: (1) grounding the scoring scheme in terms of a model ; (2) producing more intuitive recall and precision scores ; and (3) not requiring explicit computation of the transitive closure of coreference. The principal conceptual difference is that we have moved from a syntactic scoring model based on following coreference links to an approach defined by the model theory of those links. In brief, the scheme operates by comparing the equivalence classes defined by the links in the key and the response, rather than the links themselves (thus, this is only well defined for identity links, at the moment). These classes are of course the models of the IDENT equivalence relation, and this strategy is preferable for a number of reasons, one being that the scores are independent of the particular links used to encode the equivalence relation. The scores themselves are obtained by determining the minimal perturbations to the response that are required to transform its corresponding equivalence classes into those of the key. Specifically, the recall (respectively precision) error terms are found by calculating the least number of links that need to be added to the respons e (respectively the key) in order to have the classes align. Although at first blush this seems combinatorially explosive, due to references to minimal spanning subsets of the equivalence relation, it turns out it can be accomplished with a very simple counting scheme. we introduce the link-based MUC evaluation metric for the MUC-6 and MUC 7 co reference tasks.	A Model-Theoretic Coreference Scoring Scheme This note explains a scoring method for the coreference task in MUC6. It makes the original method better by: (1) basing the scoring on a model; (2) giving more understandable recall and precision scores; and (3) not needing to calculate all possible connections for coreference. The main change is that we switched from a syntax-based scoring model, which followed coreference links, to a method based on the model theory of these links. In simple terms, the method works by comparing groups of equivalent items defined by the links in the key and the response, instead of the links themselves (so, this is currently only defined for identity links). These groups are the models of the IDENT equivalence relation, and this approach is better for several reasons, one being that the scores don't depend on the specific links used to show the equivalence relation. The scores are found by figuring out the smallest changes needed in the response to make its groups of equivalent items match those of the key. Specifically, the recall (or precision) errors are found by counting the smallest number of links needed to be added to the response (or the key) to make the groups match. Although this seems very complex at first, because of references to minimal spanning subsets of the equivalence relation, it turns out it can be done with a simple counting method. We introduce the link-based MUC evaluation metric for the MUC-6 and MUC-7 coreference tasks.	collection
345	MITRE: Description Of The Alembic System Used For MUC-6 As with several other veteran Muc participants, MITRE'S Alembic system has undergone a major transformation in the past two years. The genesis of this transformation occurred during a dinner conversation at the last Muc conference, MUC-5. At that time, several of us reluctantly admitted that our major impediment towards improved performance was reliance on then-standard linguistic models of syntax. We knew we would need an alternative to traditional linguistic grammars, even to the somewhat non-traditional categorial pseudo-parser we had in place at the time. The problem was, which alternative? The answer came in the form of rule sequences, an approach Eric Brill originally laid out in his work on part-of-speech tagging [5, 7]. Rule sequences now underlie all the major processing steps in Alembic: part-of-speech tagging, syntactic analysis, inference, and even some of the set-fill processing in the Template Elemen t task (TE). We have found this approach to provide almost an embarrassment of advantages, speed and accuracy being the most externally visible benefits. In addition, most of our rule sequence processors are trainable, typically from small samples. The rules acquired in this way also have the characteristic that they allow one to readily mix hand-crafted and machine-learned elements. We have exploited this opportunity to apply both machine-learned and hand-crafted rules extensively, choosing in some instances to run sequences that were primarily machine-learned, and in other cases to run sequences that were entirely crafted by hand. Our typical machine learning approaches for English NE are transformation-based learning.	MITRE: Description Of The Alembic System Used For MUC-6 As with several other experienced MUC participants, MITRE's Alembic system has gone through big changes in the last two years. This change started during a dinner talk at the last MUC conference, MUC-5. Back then, some of us admitted that our main problem in getting better results was depending on the usual language rules for sentence structure. We realized we needed a different way than the usual language rules, even the somewhat unique fake-parser we had at the time. But the question was, what should we use instead? The answer was rule sequences, an idea Eric Brill explained in his work on identifying parts of speech [5, 7]. Rule sequences now form the basis for all the main processing steps in Alembic: identifying parts of speech, understanding sentence structure, making logical connections, and even some of the processing in the Template Element task (TE). We found this method gives many benefits, with speed and accuracy being the most obvious. Also, most of our rule sequence processors can be trained, usually from small examples. The rules created this way allow us to easily mix rules made by people and those learned by machines. We have used this chance to apply both machine-learned and hand-made rules a lot, sometimes using mostly machine-learned sequences, and other times using sequences made entirely by hand. Our usual machine learning methods for English Named Entity (NE) are transformation-based learning, which is a method of teaching computers by changing rules.	identity links
346	Transformation Based Learning In The Fast Lane Transformation-based learning has been successfully employed to solve many natural language processing problems. It achieves state-of-the-art performance on many natural language processing tasks and does not overtrain easily. However, it does have a serious drawback: the training time is often intorelably long, especially on the large corpora which are often used in NLP. In this paper, we present a novel and realistic method for speeding up the training time of a transformation-based learner without sacrificing performance. The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems: a standard transformation-based learner, and the ICA system (Hepple, 2000). The results of these experiments show that our system is able to achieve a significant improvement in training time while still achieving the same performance as a standard transformation-based learner. This is a valuable contribution to systems and algorithms which utilize transformation-based learning at any part of the execution. we propose fnTBL toolkit, which implements several optimizations in rule learning to drastically speed up the time needed for training.	Transformation Based Learning In The Fast Lane Transformation-based learning, a method used for solving language problems, has been very successful. It performs very well on tasks related to understanding language and doesn't get worse with too much training. But, it has a big issue: it takes a very long time to learn, especially with large text data often used in language processing. In this paper, we introduce a new and practical way to speed up the learning time for this method without losing effectiveness. We compare the time and performance of our improved method with two other systems: a regular transformation-based learner and the ICA system (a method from Hepple, 2000). Our tests show that our method can learn much faster while still working as well as the regular method. This is important for systems and processes that use transformation-based learning at any step. We suggest using the fnTBL toolkit, which includes several improvements to make the learning process much quicker.	Alembic system
347	Text And Knowledge Mining For Coreference Resolution Traditionally coreference is resolved by satisfying a combination of salience, syntactic, semantic and discourse constraints. The acquisition of such knowledge is time-consuming, difficult and error-prone. Therefore, we present a knowledge minimalist methodology of mining coreference rules from annotated text corpora. Semantic consistency evidence, which is a form of knowledge required by coreference, is easily retrieved from WordNet. Additional consistency knowledge is discovered by a meta-bootstrapping algorithm applied to unlabeled texts. We use paths through Wordnet, using not only synonym and is-a relations, but also parts, morphological derivations, gloss texts and polysemy, which are weighted with a measure based on the relation types and number of path elements. The path patterns in WordNet are utilized to compute the semantic consistency between NPs.	Text And Knowledge Mining For Coreference Resolution Traditionally, coreference, which is determining when different words refer to the same thing, is resolved by meeting a mix of importance, sentence structure, meaning, and conversation rules. Learning this knowledge takes a lot of time, is hard, and can have mistakes. So, we introduce a simple method to find coreference rules from labeled text collections. Evidence of meaning consistency, which is a type of knowledge needed for coreference, can be easily obtained from WordNet, a large database of words and their meanings. More consistency knowledge is found using a smart algorithm applied to texts without labels. We use connections through WordNet, using not just synonyms and "is-a" relationships, but also parts, word forms, explanation texts, and words with multiple meanings, which are given importance based on the type of relation and how many steps are in the connection. The connection patterns in WordNet are used to calculate the meaning consistency between Noun Phrases (NPs).	improved
348	A Decision Tree Of Bigrams Is An Accurate Predictor Of Word Sense This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby. This approach is evaluated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise. It is more accurate than the average results reported for 30 of 36 words, and is more accurate than the best results for 19 of 36 words. We compare decision trees, decision stumps and a Naive Bayesian classifier to show that bigrams are very useful in identifying the intended sense of a word.	A Decision Tree Of Bigrams Is An Accurate Predictor Of Word Sense This paper describes a method to figure out the meaning of a word that can have more than one meaning (word sense disambiguation) by using a decision tree, which makes choices based on pairs of words (bigrams) that appear close to the word in question. This method was tested using a collection of texts where the meanings of words were already identified from an event in 1998 called SENSEVAL, which focused on understanding word meanings. The method was more accurate than the average results for 30 out of 36 words tested and even better than the best results for 19 out of those 36 words. We looked at how decision trees, simple decision stumps (a basic form of decision tree), and a Naive Bayesian classifier (a simple way to make predictions) perform, showing that bigrams are very helpful in figuring out what a word means.	multiple
349	Edit Detection And Parsing For Transcribed Speech We present a simple architecture for parsing transcribed speech in which an edited-word detector first removes such words from the sentence string, and then a standard statistical parser trained on transcribed speech parses the remaining words. The edit detector achieves a misclassification rate on edited words of 2.2%. (The NULL-model, which marks everything as not edited, has an error rate of 5.9%.) To evaluate our parsing results we introduce a new evaluation metric, the purpose of which is to make evaluation of a parse tree relatively indifferent to the exact tree position of EDITED nodes. By this metric the parser achieves 85.3% precision and 86.5% recall. Our work in statistically parsing conversational speech has examined the performance of a parser that removes edit regions in an earlier step.	Edit Detection And Parsing For Transcribed Speech We introduce an easy method for understanding transcribed speech where an edit detector first takes out unnecessary words from the sentence, and then a normal statistical parser, which is specially trained for this kind of speech, analyzes the remaining words. The edit detector makes mistakes with only 2.2% of the edited words. (A simple model that assumes everything is unchanged has a 5.9% error rate.) To check how well our parser works, we use a new way to measure it that doesn't care much about the exact position of the edited parts. Using this measure, the parser is 85.3% accurate and 86.5% complete in its analysis. Our study of understanding spoken conversations has looked at how well a parser works when it removes unnecessary parts early on.	decision
350	Multipath Translation Lexicon Induction Via Bridge Languages This paper presents a method for inducing translation lexicons based on transduction models of cognate pairs via bridge languages. Bilingual lexicons within languages families are induced using probabilistic string edit distance models. Translation lexicons for arbitrary distant language pairs are then generated by a combination of these intra-family translation models and one or more cross-family on-line dictionaries. Up to 95% exact match accuracy is achieved on the target vocabulary (30-68% of inter-family test pairs). Thus substantial portions of translation lexicons can be generated accurately for languages where no bilingual dictionary or parallel corpora may exist. We present a method for inducing translation lexicons based on transduction modules of cognate pairs via bridge languages. We present a method for inducing translation lexicons based on transduction models of cognate pairs via bridge languages.	Multipath Translation Lexicon Induction Via Bridge Languages This paper introduces a way to create translation word lists using models that transform similar word pairs through intermediate languages. Bilingual word lists within language families are created using models that predict changes in word spelling. Translation word lists for languages that are very different from each other are made by combining these models with online dictionaries that work across different language families. This approach achieves up to 95% accuracy when matching exact words in the target language (covering 30-68% of test pairs from different language families). This means that large parts of translation word lists can be created accurately for languages that don't have a bilingual dictionary or similar text resources. We explain a method for creating translation word lists using transformation models of similar word pairs through bridge languages.	Detection
351	A Probabilistic Earley Parser As A Psycholinguistic Model In human sentence processing, cognitive load can be defined many ways. This report considers a definition of cognitive load in terms of the total probability of structural options that have been disconfirmed at some point in a sentence: the surprisal of word wi given its prefix w0...i−1 on a phrase-structural language model. These loads can be efficiently calculated using a probabilistic Earley parser (Stolcke, 1995) which is interpreted as generating predictions about reading time on a word-by-word basis. Under grammatical assumptions supported by corpusfrequency data, the operation of Stolcke’s probabilistic Earley parser correctly predicts processing phenomena associated with garden path structural ambiguity and with the subject/object relative asymmetry. Since the introduction of a parser-based calculation for surprisal, statistical techniques have been become common as models of reading difficulty and linguistic complexity.	A Probabilistic Earley Parser As A Psycholinguistic Model In human sentence processing, cognitive load, or mental effort, can be defined in many ways. This report looks at defining cognitive load based on the total likelihood of sentence structures that have been ruled out as you read: the surprise of reading a word wi given the words before it w0...i−1 in a language structure model. These loads can be calculated effectively using a probabilistic Earley parser (Stolcke, 1995), which helps predict how long it takes to read each word. Based on grammar rules and data about word usage, Stolcke’s probabilistic Earley parser accurately predicts issues like garden path sentences (sentences that lead you to an unexpected meaning) and differences in understanding subject and object in sentences. Since the idea of using a parser to measure surprise, statistical methods have become common to model how hard reading is and the complexity of language.	Multipath
352	Applying Co-Training Methods To Statistical Parsing We propose a novel Co-Training method for statistical parsing. The algorithm takes as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text. The algorithm iteratively labels the entire data set with parse trees. Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data. Our co-training a mostly unsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other.	Applying Co-Training Methods To Statistical Parsing We introduce a new Co-Training method for statistical parsing, which is a way to break down sentences to understand their structure. The method starts with a small set of 9695 sentences that have been marked with their grammatical structure, a list of possible word structures from the training set, and a large amount of text without labels. The method repeatedly assigns grammatical structures to the entire set of data. From testing with the Wall Street Journal text, we demonstrate that training a statistical parser using both labeled and unlabeled data performs much better than using only labeled data. Our co-training method mostly runs without human help by using two or more parsers to assign labels to training examples for each other.	probabilistic Earley
353	Knowledge-Free Induction Of Inflectional Morphologies We propose an algorithm to automatically induce the morphology of inflectional languages using only text corpora and no human input. Our algorithm combines cues from orthography, semantics, and syntactic distributions to induce morphological relationships in German, Dutch, and English. Using CELEX as a gold standard for evaluation, we show our algorithm to be an improvement over any knowledge-free algorithm yet proposed. We use latent semantic analysis to find prefixes, suffixes and circumfixes in German, Dutch and English.	Knowledge-Free Induction Of Inflectional Morphologies We suggest a method to automatically figure out the word forms (morphology) of languages that change word forms (inflectional languages) by using only large collections of written text (text corpora) without any help from humans. Our method uses hints from how words are spelled (orthography), what they mean (semantics), and how they are used in sentences (syntactic distributions) to find relationships in word forms in German, Dutch, and English. By using CELEX, a trusted database, as a standard for checking accuracy, we show that our method is better than any other method that doesn't use prior knowledge. We use a technique called latent semantic analysis to identify beginnings (prefixes), endings (suffixes), and combined forms (circumfixes) in German, Dutch, and English.	labeled
429	Extracting Parallel Sentences from Comparable Corpora using Document Level Alignment The quality of a statistical machine translation (SMT) system is heavily dependent upon the amount of parallel sentences used in training. In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). One resource not yet thoroughly explored is Wikipedia, an online encyclopedia containing linked articles in many languages. We advance the state of the art in parallel sentence extraction by modeling the document level alignment, motivated by the observation that parallel sentence pairs are often found in close proximity. We also include features which make use of the additional annotation given by Wikipedia, and features using an automatically induced lexicon model. Results for both accuracy in sentence extraction and downstream improvement in an SMT system are presented. We report significant improvements mining parallel Wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating a richer set of features and cross-sentence dependencies within a Conditional Random Fields (CRFs) model.	Extracting Parallel Sentences from Comparable Corpora using Document Level Alignment The quality of a statistical machine translation (SMT) system, which is a computer program that translates languages, depends a lot on how many matching sentences, called parallel sentences, are used during its training. Recently, different methods have been created to find these matching sentences from data that aren’t exactly matching, like news articles from the same time or similar web pages. One source that hasn't been fully explored is Wikipedia, which is an online encyclopedia with articles connected in many languages. We improve how we find parallel sentences by looking at entire documents, based on the idea that matching sentences are often close to each other. We also use extra information provided by Wikipedia and a tool that automatically creates a dictionary. We show results that prove both the accuracy of finding matching sentences and how it helps improve the translation system. We found big improvements by searching Wikipedia articles for matching sentences using better hints, more detailed information, and understanding how sentences relate to each other within a Conditional Random Fields (CRFs) model, which is a complex statistical tool.	about linking
355	Inducing Multilingual POS Taggers And NP Bracketers Via Robust Projection Across Aligned Corpora This paper investigates the potential for projecting linguistic annotations including part-of-speech tags and base noun phrase bracketings from one language to another via automatically word-aligned parallel corpora. First, experiments assess the accuracy of unmodified direct transfer of tags and brackets from the source language English to the target languages French and Chinese, both for noisy machine-aligned sentences and for clean hand-aligned sentences. Performance is then substantially boosted over both of these baselines by using training techniques optimized for very noisy data, yielding 94-96% core French part-of-speech tag accuracy and 90% French bracketing F-measure for stand-alone monolingual tools trained without the need for any human-annotated data in the given language. We induce a part-of-speech tagger for French and base noun phrase detectors for French and Chinese via transfer from English resources. We are the first to propose the use of parallel texts to bootstrap the creation of taggers.	Inducing Multilingual POS Taggers And NP Bracketers Via Robust Projection Across Aligned Corpora This paper explores how to transfer language labels like part-of-speech tags (labels that tell the role of words in a sentence) and base noun phrase bracketings (groupings of words that form basic noun phrases) from one language to another using texts that are aligned word-for-word in two languages. Initially, tests measure how accurate it is to directly copy these labels and groupings from English to French and Chinese, using both rough machine-aligned texts and accurate human-aligned texts. Then, by applying techniques that work well with messy data, accuracy significantly improves, reaching 94-96% accuracy for French part-of-speech tags and 90% accuracy for French groupings, even when tools are trained without any human help in the target language. We create a tool to label French words and detect basic noun phrases in both French and Chinese by transferring them from English tools. We are the first to suggest using parallel texts (texts in two languages that match each other) to start developing these labeling tools.	computer power
356	Learning To Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment We address the text-to-text generation problem of sentence-level paraphrasing — a phenomenon distinct from and more difficult than word- or phrase-level paraphrasing. Our approach applies multiple-sequence alignment to sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patterns represented by word lattice pairs and automatically determines how to apply these patterns to rewrite new sentences. The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems. We propose to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. We construct lattices over paraphrases using an iterative pairwise multiple sequence alignment (MSA) algorithm. We propose a multi-sequence alignment algorithm that takes structurally similar sentences and builds a compact lattice representation that encodes local variations. We present an approach for generating sentence level paraphrases, learning structurally similar patterns of expression from data and identifying paraphrasing pairs among them using a comparable corpus.	Learning To Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment We tackle the challenge of changing sentences into different words with the same meaning, which is harder than changing just words or phrases. Our method uses a technique called multiple-sequence alignment on sentences from similar but unmarked text collections, learning sets of patterns for changing sentences and figuring out how to use these patterns to rewrite new sentences. Our tests show that our system creates accurate sentence changes, doing better than basic systems. We suggest using multiple-sequence alignment (MSA) for traditional sentence changes. We create networks over sentence changes using a step-by-step pairwise multiple sequence alignment (MSA) method. We propose a multi-sequence alignment method that takes sentences with similar structures and builds a simple network representation that shows small differences. We introduce a method for creating sentence-level changes by learning similar ways of expression from data and finding pairs to change using a similar text collection.	start developing
357	Inducing History Representations For Broad Coverage Statistical Parsing We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser. The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge. Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions. Of the previous work on using neural net works for parsing natural language, the most empirically successful has been our work using Simple Synchrony Networks. We test the effect of larger input vocabulary on SSN performance by changing the frequency cut-off that selects the input tag-word pairs.	Inducing History Representations For Broad Coverage Statistical Parsing We introduce a method using neural networks to create representations (models) of parse histories (records of how sentences are broken down) and use these models to calculate the chances needed by a specific type of parser (a tool that breaks down sentences). This parser performs well, achieving a score of 89.1% (measured by a standard called F-measure) on a well-known dataset, the Penn Treebank. This score is just 0.6% less than the best parser available, even though it uses fewer words and less language knowledge. The key to our success is using gentle guidance based on structure to create the parse history models, and not assuming things are independent when they are not. Among previous studies using neural networks for understanding natural language, our approach with Simple Synchrony Networks (SSN) has been the most successful based on experiments. We also examine how having a larger set of words affects SSN performance by adjusting the number of times a word must appear to be included in the input.	sentences
358	A* Parsing: Fast Exact Viterbi Parse Selection We present an extension of the classic A* search procedure to tabular PCFG parsing. The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions. We discuss various estimates and give efficient algorithms for com- puting them. On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of precomputation, reduces the work to less than 5%. Unlike best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation. Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time. We describe admissible heuristics and an A*framework for parsing.	A* Parsing: Fast Exact Viterbi Parse Selection We introduce a way to improve the classic A* search method for parsing sentences using PCFG (probabilistic context-free grammar). A* search helps speed up finding the best sentence structure by guessing how likely different parts of the sentence are to complete correctly. We explain different ways to make these guesses and provide quick methods to calculate them. For average-length sentences from the Penn Treebank (a collection of English sentences used for language research), our best guessing method cuts the amount of work down to less than 3% of what would be needed if we checked every possibility. A simpler method, needing less than a minute to set up, reduces the work to less than 5%. Unlike other methods that might only find a close answer quickly, A* is guaranteed to find the most accurate sentence structure. Our parser, which is easier to build than other complex methods, works well with many ways of controlling the parsing process and takes a predictable amount of time even in the worst cases. We talk about allowed guessing methods and an A* system for parsing sentences.	specific
359	Statistical Phrase-Based Translation We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems. We propose STIR, as a pre-ordering step in a state of-the-art phrase-based translation system from English to Japanese.	Statistical Phrase-Based Translation We propose a new method for translating languages using phrases, along with a new process for decoding, which allows us to test and compare different methods for phrase-based translation that were suggested before. In our study, we perform many tests to understand and explain why methods using phrases translate better than those using single words. Our practical results, which apply to all languages we tested, show that the best performance is achieved with simple techniques: guessing phrase translations based on aligning words and giving importance to the meaning of phrases. Surprisingly, learning phrases longer than three words and using very accurate word matching doesn't greatly improve performance. Focusing only on phrases based on grammar rules makes our systems perform worse. We introduce STIR, a step to rearrange words, in a top-performing system that translates from English to Japanese.	Viterbi
360	Automatic Evaluation Of Summaries Using N-Gram Co-Occurrence Statistics Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries. The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results. We are the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives. We propose ROUGE, a semi automatic approach,  which is primarily based on n gram co-occurrence between automatic and human summaries.	Automatic Evaluation Of Summaries Using N-Gram Co-Occurrence Statistics Following the recent use by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct a detailed study of a similar idea for evaluating summaries. The results show that automatic evaluation using unigram (single words) matches between summary pairs surprisingly correlates well with human evaluations, based on various statistical measures; while directly using the BLEU evaluation method doesn't always give good results. We are the first to systematically point out problems with the large-scale DUC evaluation (a specific evaluation method) and to look for solutions by finding more reliable automatic alternatives. We propose ROUGE, a partly automatic method, which mainly focuses on matching sequences of words (n-grams) between automated and human summaries.	translation
361	Multitext Grammars And Synchronous Parsers Multitext Grammars (MTGs) generate arbitrarily many parallel texts via production rules of arbitrary length. Both ordinary MTGs and their bilexical subclass admit relatively efficient parsers. Yet, MTGs are more expressive than other synchronous formalisms for which parsers have been described in the literature. The combination of greater expressive power and relatively low cost of inference makes MTGs an attractive foundation for practical models of translational equivalence. We present algorithms for synchronous parsing with more complex grammars, discussing how to parse grammars with greater than binary branching and lexicalization of synchronous grammars. We discuss the applicability of the so-called hook trick for parsing bilexical multi text grammars.	Multitext Grammars And Synchronous Parsers Multitext Grammars (MTGs) create many parallel texts using flexible rules. Both standard MTGs and a special type called bilexical MTGs have relatively fast parsers. However, MTGs can express more complex ideas than other systems, for which parsers have been mentioned in research papers. Because MTGs are both powerful and efficient, they are a good choice for models that find equivalent meanings in translations. We introduce methods for parsing more complex grammars, focusing on how to handle grammars with more than two branches and those with words that have specific meanings. We also explore using a method known as the hook trick to parse bilexical multitext grammars.	similar
362	COGEX: A Logic Prover For Question Answering Recent TREC results have demonstrated the need for deeper text understanding methods. This paper introduces the idea of automated reasoning applied to question answering and shows the feasibility of integrating a logic prover into a Question Answering system. The approach is to transform questions and answer passages into logic representations. World knowledge axioms as well as linguistic axioms are supplied to the prover which renders a deep understanding of the relationship between question text and answer text. Moreover, the trace of the proofs provide answer justifications. The results show that the prover boosts the performance of the QA system on TREC questions by 30%. COGEX uses its logic prover to extract lexical relationships between the question and its candidate answers.	COGEX: A Logic Prover For Question Answering Recent TREC results have shown that we need better ways to understand text deeply. This paper introduces the idea of using automated reasoning, a computer method of solving problems, for answering questions and shows it's possible to add a logic prover, a program that checks logic, into a Question Answering system. The method is to change questions and the parts of text with answers into logic formats. General knowledge rules and language rules are given to the logic prover to help it understand the connection between the question and answer text. Plus, the steps of the logic process give reasons for the answers. The results indicate that the logic prover improves the Question Answering system's performance on TREC questions by 30%. COGEX uses its logic prover to find word connections between the question and possible answers.	Parsers
363	Syntax-Based Alignment Of Multiple Translations: Extracting Paraphrases And Generating New Sentences We describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets. These FSAs are good representations of paraphrases. They can be used to extract lexical and syntactic paraphrase pairs and to generate new, unseen sentences that express the same meaning as the sentences in the input sets. Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations. We describe a syntax-based algorithm that builds word lattices from parallel translations which can be used to generate new para phrases. We propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences.	Syntax-Based Alignment Of Multiple Translations: Extracting Paraphrases And Generating New Sentences We explain a method that uses sentence structure to automatically create word patterns (called Finite State Automata) from groups of translations that mean the same thing. These word patterns are useful for finding different ways to say the same thing (paraphrases) and can also create new sentences that have the same meaning as those in the original groups. Our method can also check if different ways of saying something are correct, which helps in assessing how good translations are. We explain a method that creates word patterns from similar translations to form new paraphrases. We suggest a method to match groups of similar sentences by focusing on their structure.	program
384	Accurate Information Extraction From Research Papers Using Conditional Random Fields With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance. This paper employs Conditional Random Fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers. The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration. This paper makes an empirical exploration of several factors, including variations on Gaussian, exponential and hyperbolic-L1 priors for improved regularization, and several classes of features and Markov order. On a standard benchmark data set, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares even more favorably against HMMs. CORA consists of two collections: a set of research paper headers annotated for entities such as title, author, and institution; and a collection of references annotated with BibTeX fields such as journal, year, and publisher.	Accurate Information Extraction From Research Papers Using Conditional Random Fields With the growing use of research paper search tools like CiteSeer for finding literature and making hiring choices, it's crucial that these systems are very accurate. This paper uses a method called Conditional Random Fields (CRFs) to pull out common details from the headers and references of research papers. While the basic idea of CRFs is well-known, figuring out the best way to use them with real-life data needs more study. This paper looks into different factors, such as changes to mathematical methods like Gaussian, exponential, and hyperbolic-L1 priors for better model regulation, and different types of features and Markov order, which is a way to consider the order of events. On a standard test data set, we reached the best performance ever, reducing the error in average F1 score by 36%, and word error rate by 78% compared to the previous best results using another method called SVM. Our accuracy is even better when compared to HMMs. CORA includes two sets: one with research paper headers marked for details like title, author, and institution, and another set of references marked with BibTeX details like journal, year, and publisher.	phrases
364	Statistical Sentence Condensation Using Ambiguity Packing And Stochastic Disambiguation Methods For Lexical-Functional Grammar We present an application of ambiguity pack- ing and stochastic disambiguation techniques for Lexical-Functional Grammars (LFG) to the domain of sentence condensation. Our system incorporates a linguistic parser/generator for LFG, a transfer component for parse reduction operating on packed parse forests, and a maximum-entropy model for stochastic output selection. Furthermore, we propose the use of standard parser evaluation methods for automatically evaluating the summarization quality of sentence condensation systems. An experimental evaluation of summarization quality shows a close correlation between the automatic parse-based evaluation and a manual evaluation of generated strings. Overall summarization quality of the proposed system is state-of-the-art, with guaranteed grammaticality of the system output due to the use of a constraint-based parser/generator. We present a discriminative sentence compressor over the output of an LFG parser that is a packed representation of possible compressions. We apply linguistically rich LFG grammars to a sentence compression system.	Statistical Sentence Condensation Using Ambiguity Packing And Stochastic Disambiguation Methods For Lexical-Functional Grammar We introduce a way to make sentences shorter using techniques to handle multiple meanings and choosing the best option, specifically for a type of grammar called Lexical-Functional Grammar (LFG). Our system uses a language processor for LFG, a tool to simplify sentence structures efficiently, and a model that selects the best output based on probability. We also suggest using common methods for evaluating parsers to automatically check how well the system summarizes sentences. Testing shows that our automatic evaluation closely matches human judgment on the quality of summaries. The system creates high-quality summaries and ensures correct grammar because it uses a precise language processor. We have developed a tool to choose the best sentence shortening from multiple options given by an LFG parser. This tool uses detailed LFG grammar rules to improve a system that makes sentences shorter.	different
365	Shallow Parsing With Conditional Random Fields Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods. We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model. Improved training methods based on modern optimization algorithms were critical in achieving these results. We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models CRFs have been applied with impressive empirical results to the tasks of noun phrase chunking in this work.	Shallow Parsing With Conditional Random Fields Conditional random fields (a type of computer model that helps label parts of a sequence) are better than older models like Hidden Markov Models (HMMs) and other methods that label each part separately. In language processing tasks like shallow parsing (finding simple sentence parts), there's been a lot of focus, with many tests and comparisons among different methods. We show how to train a conditional random field to do as well as any known noun-phrase chunking method for the CoNLL task and even better than any single model. New training methods using recent improvement techniques were key to these great results. We compare different models and training methods thoroughly, confirming and enhancing past findings on shallow parsing and training for maximum-entropy models (another type of computer model). CRFs have been used with great success for breaking sentences into noun phrases in this work.	sentences
366	Sentence Level Discourse Parsing Using Syntactic And Lexical Information We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees. The models use syntactic and lexical features. A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8% over a state-of-the-art decision-based discourse parser. A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance. within Rhetorical Structure Theory (RST), we have developed two probabilistic models for identifying clausal elementary discourse units and generating discourse trees at the sentence level. We introduce a statistical discourse segmenter, which is trained on RST DT to label words with boundary or no-boundary labels.	Sentence Level Discourse Parsing Using Syntactic And Lexical Information We introduce two probabilistic models (models that use probability to make decisions) that can be used to identify basic parts of text and build sentence-level structure trees. The models use syntactic (relating to sentence structure) and lexical (relating to words) features. A discourse parsing algorithm (a method for analyzing sentences) that uses these models creates sentence structure trees with an 18.8% improvement over a leading decision-based sentence analyzer. Tests show that our sentence analysis model is advanced enough to produce sentence trees with accuracy similar to human performance. Within Rhetorical Structure Theory (a way to analyze text structure), we have developed two probabilistic models for identifying basic sentence parts and generating sentence trees. We introduce a statistical tool for breaking down sentences, which is trained on RST DT (a database of sentence structures) to label words with either boundary (end of segment) or no-boundary labels.	older models
367	Feature-Rich Part-Of-Speech Tagging With A Cyclic Dependency Network We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result We present a supervised conditional Markov Model part-of-speech tagger (CMM) which exploited information coming from both left and right contexts.	Feature-Rich Part-Of-Speech Tagging With A Cyclic Dependency Network We introduce a new tool that labels words in a sentence as nouns, verbs, etc., using these ideas: (i) it looks at both the tags before and after a word using a special network method, (ii) it uses a wide range of information about words, including looking at several words together, (iii) it effectively uses prior knowledge in advanced statistical models, and (iv) it closely examines features of unfamiliar words. By combining these ideas, our tool is very accurate, correctly labeling 97.24% of words in a well-known language dataset, improving by 4.4% over the previous best tool that learned automatically. We present a supervised (trained with examples) model that labels words by using information from both the left and right side of a word.	advanced enough
368	Factored Language Models And Generalized Parallel Backoff We introduce factored language models (FLMs) and generalized parallel backoff (GPB). An FLM represents words as bundles of features (e.g. , morphological classes, stems, data-driven clusters, etc.), and induces a probability model covering sequences of bundles rather than just words. GPB extends standard backoff to general conditional probability tables where variables might be heterogeneous types, where no obvious natural (temporal) backoff order exists, and where multiple dynamic backoff strategies are allowed. These methodologies were implemented during the JHU 2002 workshop as extensions to the SRI language modeling toolkit. This paper provides initial perplexity results on both CallHome Arabic and on Penn Treebank Wall Street Journal articles. Significantly, FLMs with GPB can produce bigrams with significantly lower perplexity, sometimes lower than highly-optimized baseline trigrams. In a multi-pass speech recognition context, where bigrams are used to create first-pass bigram lattices or N-best lists, these results are highly relevant. We show that factored language models are able to outperform standard n-gram techniques in terms of perplexity. A factored language model (FLM) is based on a representation of words as feature vectors and can utilize a variety of additional information sources in addition to words, such as part-of-speech (POS) information, morphological information, or semantic features, in a unified and principled framework.	Factored Language Models And Generalized Parallel Backoff We introduce factored language models (FLMs) and generalized parallel backoff (GPB). An FLM uses words as collections of features (like word parts, word roots, or groups of similar words) and creates a probability model for sequences of these features instead of just individual words. GPB improves the usual method of handling missing data by allowing for more flexible ways to predict the likelihood of events, even when variables are of different types or there's no clear order to follow. These methods were added to the SRI language modeling toolkit during the JHU 2002 workshop. This paper shows initial results on how well the models predict, tested on CallHome Arabic and Penn Treebank Wall Street Journal articles. Importantly, FLMs with GPB can create pairs of words (bigrams) that are easier to predict than the more complex word groups (trigrams) that are usually very accurate. In multi-step speech recognition, where these word pairs are used to make first guesses or shortlists, these results are very important. We show that factored language models can do better than standard methods in predicting word sequences. A factored language model (FLM) involves representing words as groups of features and can incorporate various additional information such as part-of-speech tags, word structure details, or word meanings in a comprehensive way.	combining
369	Precision And Recall Of Machine Translation Machine translation can be evaluated using precision, recall, and the F-measure. These standard measures have significantly higher correlation with human judgments than recently proposed alternatives. More importantly, the standard measures have an intuitive interpretation, which can facilitate insights into how MT systems might be improved. The relevant software is publicly available. We formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty.	Precision And Recall Of Machine Translation Machine translation can be judged using precision (how accurate the translated words are), recall (how many correct words are captured), and the F-measure (a balance between precision and recall). These usual ways of measuring have a much stronger connection with what humans think than some new methods. More importantly, these usual ways are easy to understand, which can help us think of ways to make machine translation systems better. The necessary software can be found for free by anyone. We create a way to measure translation quality using precision and recall directly, instead of using precision and a penalty for short translations.	toolkit
370	A Statistical Model For Multilingual Entity Detection And Tracking Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks. In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text. Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features. In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers. The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages.	A Statistical Model For Multilingual Entity Detection And Tracking Entity detection and tracking is a relatively new task in understanding language. In this paper, we introduce a statistical method that doesn't depend on any specific language to find and follow names, descriptions, and pronouns referring to things or people in any kind of text, then group them together for each logical item mentioned. Both the mention detection model and the new entity tracking model can use any type of feature, allowing them to incorporate a wide range of word-based, grammatical, and meaning-related features. Additionally, the mention detection model importantly uses information from different systems that recognize named entities. We tested this method in several experiments using Arabic, Chinese, and English texts; a system based on our method and entered into the latest Automatic Content Extraction (ACE) evaluation achieved top-level results in all three languages tested.	software
371	Speed And Accuracy In Shallow And Deep Stochastic Parsing This paper reports some experiments that compare the accuracy and performance of two stochastic parsing systems. The currently popular Collins parser is a shallow parser whose output contains more detailed semantically relevant information than other such parsers. The XLE parser is a deep-parsing system that couples a Lexical Functional Grammar to a log-linear disambiguation component and provides much richer representations theory. We measured the accuracy of both systems against a gold standard of the PARC 700 dependency bank, and also measured their processing times. We report high parsing speeds for a deep parsing system which uses an LFG grammar: 1.9 sentences per second for 560 sentences from section 23 of the Penn Treebank.	Speed And Accuracy In Shallow And Deep Stochastic Parsing This paper discusses some tests that compare how accurate and fast two different sentence analyzing systems are. The well-known Collins parser is a "shallow parser," meaning it analyzes sentences in a simpler way but gives more useful information about the meaning than other similar systems. The XLE parser is a "deep-parsing system," which means it analyzes sentences in a more detailed way. It uses something called Lexical Functional Grammar (LFG) and a special part to help decide between different meanings, giving a much deeper understanding. We checked how accurate both systems were by comparing them to a set standard from the PARC 700 dependency bank and also timed how long they took to process. We found that the deep parsing system using LFG is quite fast, processing about 1.9 sentences every second for 560 sentences from a specific part of the Penn Treebank.	mention detection
372	Training Tree Transducers Many probabilistic models for natural language are now written in terms of hierarchical tree structure. Tree-based modeling still lacks many of the standard tools taken for granted in (finite-state) string-based modeling. The theory of tree transducer automata provides a possible framework to draw on, as it has been worked out in an extensive literature. We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-to-tree and tree-to-string transducers. We define training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. We describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers.	Training Tree Transducers Many probabilistic models for natural language are now written using hierarchical tree structures, which means they organize information in a branching way, like a family tree. However, tree-based modeling still lacks many of the standard tools that are commonly used in string-based modeling, which deals with sequences of symbols. The theory of tree transducer automata, which are mathematical machines that transform trees, offers a possible framework to use, as it has been detailed extensively in scientific writings. We explain why using tree transducers for natural language is beneficial and tackle the challenge of training probabilistic tree-to-tree and tree-to-string transducers, which are systems that convert one tree structure into another or into a sequence of symbols. We define methods for training and decoding, which means interpreting the data, for both generalized tree-to-tree and tree-to-string transducers.	different sentence
373	Catching The Drift: Probabilistic Content Models With Applications To Generation And Summarization We consider the problem of modeling the content structure of texts within a specific domain, in terms of the topics the texts address and the order in which these topics appear. We first present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods. We proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations.	Catching The Drift: Probabilistic Content Models With Applications To Generation And Summarization We look at how to understand the structure of text content in a specific area, focusing on what topics the texts talk about and the order these topics show up. We start by introducing an effective way that doesn't rely on much existing knowledge to learn content models from documents that haven't been annotated or labeled, using a new version of methods for Hidden Markov Models (a statistical model that represents systems with hidden states). Next, we use our approach for two related tasks: arranging information in the right order and picking out key pieces to summarize. Our tests demonstrate that adding content models to these tasks greatly improves the results compared to methods suggested before. We created a domain-specific HMM (Hidden Markov Model) to understand how topics change in a text, where topics are shown as hidden states (invisible factors) and sentences are the visible parts.	natural
374	The Web As A Baseline: Evaluating The Performance Of Unsupervised Web-Based Models For A Range Of NLP Tasks Previous work demonstrated that web counts can be used to approximate bigram frequencies, and thus should be useful for a wide variety of NLP tasks. So far, only two generation tasks (candidate selection for machine translation and confusion-set disambiguation) have been tested using web-scale data sets. The present paper investigates if these results generalize to tasks covering both syntax and semantics, both generation and analysis, and a larger range of n-grams. For the majority of tasks, we find that simple, unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a large corpus. However, in most cases, web-based models fail to outperform more sophisticated state-of-the-art models trained on small corpora. We argue that web-based models should therefore be used as a baseline for, rather than an alternative to, standard models. Our web-based unsupervised model classifies noun noun instances based on Lauer's list of 8 prepositions and uses the web as the training corpus.	The Web As A Baseline: Evaluating The Performance Of Unsupervised Web-Based Models For A Range Of NLP Tasks Previous work showed that counting words on the web can help guess how often pairs of words appear together, which can be useful for many language processing tasks. Until now, only two tasks (choosing words for machine translation and telling apart confusing word sets) have been tested with large web data. This paper looks into whether these findings apply to tasks involving both sentence structure and meaning, creating and understanding language, and using a wider range of word groups. For most tasks, we find that simple models that don't need training data do better when word group frequencies come from the web rather than a large text collection. However, in most cases, web-based models do not do better than the best advanced models trained on smaller text collections. We suggest that web-based models should be used as a starting point or comparison, not as a replacement for usual models. Our web-based model, which doesn't need training data, classifies pairs of nouns based on Lauer's list of 8 linking words and uses the web as the learning material.	introducing
375	Evaluating Content Selection In Summarization: The Pyramid Method We present an empirically grounded method for evaluating content selection in summarization. It incorporates the idea that no single best model summary for a collection of documents exists. Our method quantifies the relative importance of facts to be conveyed. We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference. We propose a manual evaluation method that was based on the idea that there is no single best model summary for a collection of documents.	Evaluating Content Selection In Summarization: The Pyramid Method We present a method based on real-world evidence for checking what information is chosen in summarization. It includes the idea that there's no one perfect summary for a group of documents. Our method measures how important each piece of information is. We believe it is trustworthy, can predict outcomes, and helps identify issues, making it much better than the human evaluation method currently used in the Document Understanding Conference. We suggest a manual way to evaluate, based on the idea that no one summary is the best for a bunch of documents.	linking words
376	A Smorgasbord Of Features For Statistical Machine Translation We describe a methodology for rapid experimentation in statistical machine translation which we use to add a large number of features to a baseline system exploiting features from a wide range of levels of syntactic representation. Feature values were combined in a log-linear model to select the highest scoring candidate translation from an n-best list. Feature weights were optimized directly against the BLEU evaluation metric on held-out data. We present results for a small selection of features at each level of syntactic representation. At the 2003 Johns Hopkins summer workshop on statistical machine translation, a large number of features were tested to discover which ones could improve a state-of-the-art translation system, and the only feature that produced a 'truly significant improvement' was the Model 1 score. The effects of integrating syntactic structure into a state-of-the-art statistical machine translation system are investigated.	A Smorgasbord Of Features For Statistical Machine Translation We explain a method for quick testing in statistical machine translation, which we use to add many new features to a basic system by using features from different levels of sentence structure. Feature values were combined in a mathematical model (log-linear model) to pick the best translation from a list of options (n-best list). The importance of each feature was adjusted to improve results based on the BLEU evaluation metric, which measures translation quality, using a separate set of data. We show results for a few features at each level of sentence structure. At the 2003 Johns Hopkins summer workshop on statistical machine translation, many features were tested to see which ones could make a top-quality translation system better, and the only feature that made a 'truly significant improvement' was the Model 1 score. The impact of adding sentence structure into a top-quality statistical machine translation system is studied.	documents
381	Improvements In Phrase-Based Statistical Machine Translation In statistical machine translation, the currently best performing systems are based in some way on phrases or word groups. We describe the baseline phrase-based translation system and various refinements. We describe a highly efficient monotone search algorithm with a complexity linear in the input sentence length. We present translation results for three tasks: Verb-mobil, Xerox and the Canadian Hansards. For the Xerox task, it takes less than 7 seconds to translate the whole test set consisting of more than 10K words. The translation results for the Xerox and Canadian Hansards task are very promising. The system even outperforms the alignment template system. In our approach smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table.	Improvements In Phrase-Based Statistical Machine Translation In statistical machine translation, the best systems currently use phrases or groups of words. We explain the basic phrase-based translation system and various improvements. We describe a very fast monotone search method that works in a straightforward way with the length of the sentence. We show translation results for three projects: Verb-mobil, Xerox, and the Canadian Hansards. For the Xerox project, it takes less than 7 seconds to translate all the test data, which has over 10,000 words. The translation results for the Xerox and Canadian Hansards projects are very encouraging. The system even does better than the alignment template system. In our method, improved phrase probabilities are created from word-pair probabilities and combined in a log-linear model, which is a mathematical model, with an unadjusted phrase-table.	learning
377	Minimum Bayes-Risk Decoding For Statistical Machine Translation We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation. This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance. We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences. We report the performance of the MBR decoders on a Chinese-to-English translation task. Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions. The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system's translations relative to the model's distribution over possible translations. In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an N best list of translations produced by a first pass decoder.	Minimum Bayes-Risk Decoding For Statistical Machine Translation We introduce a method called Minimum Bayes-Risk (MBR) decoding for machine translation, which uses statistics to reduce the chances of translation mistakes based on how well the translation performs. We explain a range of ways to measure translation quality that use different levels of language information, like basic word strings, how words align between languages, and the grammatical structure of sentences in both the original and translated languages. We tested the MBR approach on translating Chinese to English. Our findings indicate that MBR decoding can help improve translation performance by focusing on specific quality measures. The MBR method works by aiming to make the system's translations as similar as possible to the best possible translations according to the model. In statistical machine translation (SMT), MBR decoding helps reduce mistakes in a single translation system by rearranging a list of the top translation options generated by an initial translation step.	Machine
378	Discriminative Reranking For Machine Translation This paper describes the application of discriminative reranking techniques to the problem of machine translation. For each sentence in the source language, we obtain from a baseline statistical machine translation system, a ranked n-best list of candidate translations in the target language. We introduce two novel perceptron-inspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric. We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-the-art performance in machine translation. We compare different algorithms for tuning the log-linear weights in a re-ranking framework and achieve results comparable to the standard minimum error rate training. We present approaches to re-rank the output of the decoder using syntactic information.	Discriminative Reranking For Machine Translation This paper talks about using special reranking methods to improve machine translation. For each sentence in the original language, we get a list of possible translations from a basic translation system, ranked by quality. We introduce two new reranking methods, inspired by a simple learning model, that make translations better than the basic system, as measured by the BLEU score, which checks translation quality. We show test results from a Chinese-English translation challenge in 2003. We also explain our methods in theory and show through tests that our methods perform as well as the best available in machine translation. We compare different methods for adjusting weights in a reranking setup and get results similar to a popular training method. We show ways to improve translation results using sentence structure information.	translation
379	A Language Modeling Approach To Predicting Reading Difficulty We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling. We derive a measure based on an extension of multinomial naive Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage. The resulting classifier is not specific to any particular subject and can be trained with relatively little labeled data. We perform predictions for individual Web pages in English and compare our performance to widely-used semantic variables from traditional readability measures. We show that with minimal changes, the classifier may be retrained for use with French Web documents. For both English and French, the classifier maintains consistently good correlation with labeled grade level (0.63 to 0.79) across all test sets. Some traditional semantic variables such as type-token ratio gave the best performance on commercial calibrated test passages, while our language modeling approach gave better accuracy for Web documents and very short passages (less than 10 words). We use a smoothed unigram language model to predict the grade reading levels of web page documents and short passages.	A Language Modeling Approach To Predicting Reading Difficulty We show a new way to solve the problem of figuring out how hard a text is to read by using language models (tools that predict words) statistically. We create a method based on a simple classification technique called multinomial naive Bayes that uses several language models to guess the likely school grade level for a piece of text. This tool can work with any subject and doesn't need a lot of labeled examples to learn. We test it by predicting the difficulty of individual web pages in English and compare our results to popular traditional methods that use word meaning. We demonstrate that with small adjustments, the tool can be retrained to work on French web pages. For both English and French, the tool accurately matches the expected grade level (between 0.63 and 0.79 on a scale of 0 to 1) across all tests. Some traditional methods, like counting unique words (type-token ratio), worked best on standard test texts, but our approach was more accurate for web pages and very short texts (under 10 words). We use a simple method called a smoothed unigram language model to estimate how difficult web pages and short texts are to read.	translations
380	Shallow Semantic Parsing Using Support Vector Machines In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others. Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers. We show performance improvements through a number of new features and measure their ability to generalize to a new test set drawn from the AQUAINT corpus. We first build a parse tree, and syntactic constituents are then labeled by feeding hand-built features extracted from the parse tree to a machine learning system.	Shallow Semantic Parsing Using Support Vector Machines In this paper, we suggest a machine learning method for shallow semantic parsing, adding to the research done by Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others. Our method uses Support Vector Machines (a type of machine learning model) which we demonstrate provides better results than older models. We show these improvements by using new features (specific characteristics or data points) and test their effectiveness with a new set of data from the AQUAINT corpus (a collection of text data). We start by creating a parse tree (a structure that shows the grammatical parts of a sentence), and then we label these parts using specific features that we manually extract and input into a machine learning system.	accurately matches
382	What's In A Translation Rule? We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data. We describe how to learn hundreds of millions of tree transformation rules from a parsed, aligned Chinese/English corpus, and we describe probability estimators for those rules. Our translation rules are learned from word-aligned bilingual texts whose source side has been parsed by using a syntactic parser.	What's In A Translation Rule? We suggest a theory that provides a clear explanation for how words match up in texts that are available in two languages. Using this theory, we bring in a simple method to find the smallest group of language-changing rules from these word matches that make sense of how humans translate languages. We explain how to discover many transformation rules from a structured and matched Chinese/English text, and how to estimate the likelihood of these rules being correct. Our translation rules are learned from bilingual texts where the original language side has been analyzed for grammar structure using a special tool.	template
385	Name Tagging With Word Clusters And Discriminative Training We present a technique for augmenting annotated training data with hierarchical word clusters that are automatically derived from a large unannotated corpus. Cluster membership is encoded in features that are incorporated in a discriminatively trained tagging model. Active learning is used to select training examples. We evaluate the technique for named-entity tagging. Compared with a state-of-the-art HMM-based name finder, the presented technique requires only 13% as much annotated data to achieve the same level of performance. Given a large annotated training set of 1,000,000 words, the technique achieves a 25% reduction in error over the state-of-the-art HMM trained on the same material. We use prefixes of the Brown cluster hierarchy to produce clusterings of varying granularity. We use the Brown algorithm for clustering (Brown et al 1992).	Name Tagging With Word Clusters And Discriminative Training We present a method to improve training data by adding groups of similar words (word clusters) that are automatically created from a large collection of text that hasn't been labeled. These word groups are used as features in a model trained to identify names in the text. We use active learning, which means we pick the most useful examples for training. We test this method for finding names in text. Compared to a top-performing method that uses Hidden Markov Models (HMM), our method needs only 13% of the labeled data to perform just as well. With a large set of 1,000,000 labeled words, our method cuts errors by 25% compared to the HMM method trained on the same data. We use parts of a system called the Brown cluster hierarchy to make these word groups at different detail levels. We apply the Brown algorithm (an earlier method from 1992) for creating these clusters.	mathematical methods
386	WordNet::Similarity - Measuring The Relatedness Of Concepts WordNet::Similarity is a freely available software package that makes it possible to measure the semantic similarity and relatedness between a pair of concepts (or synsets). It provides six measures of similarity, and three measures of relatedness, all of which are based on the lexical database WordNet. These measures are implemented as Perl modules which take as input two concepts, and return a numeric value that represents the degree to which they are similar or related. Extensive research concerning the integration of semantic knowledge into NLP for the English language has been arguably fostered by the emergence of WordNet::Similarity package. The measures vary from simple edge-counting to attempt to factor in peculiarities of the network structure by considering link direction, relative path, and density, such as vector, lesk, hso, lch, wup, path, res, lin and jcn.	WordNet::Similarity - Measuring The Relatedness Of Concepts WordNet::Similarity is a free software package that lets you measure how similar or related two ideas (or synsets) are in meaning. It offers six ways to measure similarity and three ways to measure relatedness, all based on the WordNet word database. These measurements are done through Perl modules, which take two concepts and give back a number showing how similar or related they are. A lot of research on adding meaning-based knowledge to Natural Language Processing (NLP) for English has likely grown because of the WordNet::Similarity package. The methods range from simple counting of links to considering special features of the network structure like link direction, relative path, and density, and include vector, lesk, hso, lch, wup, path, res, lin, and jcn.	algorithm
387	Morphological Analysis For Statistical Machine Translation We present a novel morphological analysis technique which induces a morphological and syntactic symmetry between two languages with highly asymmetrical morphological structures to improve statistical machine translation qualities. The technique pre-supposes fine-grained segmentation of a word in the morphologically rich language into the sequence of prefix(es)-stem-suffix(es) and part-of-speech tagging of the parallel corpus. The algorithm identifies morphemes to be merged or deleted in the morphologically rich language to induce the desired morphological and syntactic symmetry. The technique improves Arabic-to-English translation qualities significantly when applied to IBM Model 1 and Phrase Translation Models trained on the training corpus size ranging from 3,500 to 3.3 million sentence pairs. We show that determiner segmentation and deletion in Arabic sentences in an Arabic-to-English translation system improves sentence alignment, thus leading to improved overall translation quality.	Morphological Analysis For Statistical Machine Translation We present a new technique for analyzing word forms that creates a balance between two languages, even if one has more complex word structures. This helps improve machine translation, which is the process of computers translating text. The technique requires breaking down words from the more complex language into parts like prefixes (beginning of a word), stems (main part of a word), and suffixes (end of a word), and also labeling parts of speech (like noun or verb) in both languages being compared. The method finds parts of words that can be combined or removed in the more complex language to create a balance between the languages. This approach makes translations from Arabic to English much better when using certain computer models that learn from a large collection of sentence pairs, from as few as 3,500 to as many as 3.3 million. We demonstrate that breaking down and removing some small words in Arabic sentences helps match them better with English sentences, which improves the overall translation quality.	Language
388	A Unigram Orientation Model For Statistical Machine Translation In this paper, we present a unigram segmentation model for statistical machine translation where the segmentation units are blocks: pairs of phrases without internal structure. The segmentation model uses a novel orientation component to handle swapping of neighbor blocks. During training, we collect block unigram counts with orientation: we count how often a block occurs to the left or to the right of some predecessor block. The orientation model is shown to improve translation performance over two models: 1) no block re-ordering is used, and 2) the block swapping is controlled only by a language model. We show experimental results on a standard Arabic-English translation task. This work introduces lexical features for distortion modeling.	A Unigram Orientation Model For Statistical Machine Translation In this paper, we introduce a unigram segmentation model for statistical machine translation where the segmentation units are blocks: pairs of phrases without any internal structure. The segmentation model uses a new orientation part to manage the swapping (changing places) of neighboring blocks. During training, we count how often a block appears to the left or right of another block. The orientation model is proven to improve translation performance over two models: 1) no block re-ordering is used, and 2) block swapping is controlled only by a language model (a system that understands how language is usually structured). We show test results on a typical Arabic-English translation task. This work introduces word-level features for modeling distortion (changes in order).	technique requires
389	Automatic Tagging Of Arabic Text: From Raw Text To Base Phrase Chunks To date, there are no fully automated systems addressing the community's need for fundamental language processing tools for Arabic text. In this paper, we present a Support Vector Machine (SVM) based approach to automatically tokenize (segmenting off clitics), part-of- speech (POS) tag and annotate base phrases (BPs) in Arabic text. We adapt highly accurate tools that have been developed for English text and apply them to Arabic text. Khoja (2001) first introduced a tagger for Arabic, which has 131 tags, but this work has collapsed the tag set to simplify tagging. We describe a part-of-speech tagger based on support vector machines that is trained on tokenized data (clitics are separate tokens), reporting a tagging accuracy of 95.5%.	Automatic Tagging Of Arabic Text: From Raw Text To Base Phrase Chunks To date, there are no completely automatic systems that meet the community's need for basic tools to process Arabic text. In this paper, we introduce a method using Support Vector Machine (SVM), a type of computer algorithm, to automatically break down text into smaller parts (separating short words attached to others), identify parts of speech (like nouns and verbs), and label simple word groups in Arabic text. We modify very accurate tools originally made for English text and use them for Arabic text. Khoja (2001) first created a tool for marking parts of speech in Arabic, which had 131 labels, but this work has reduced the number of labels to make the process easier. We explain a tool for identifying parts of speech based on support vector machines that is trained on separated data (small words attached to others are separate), showing a 95.5% accuracy in tagging.	language
390	Improved Statistical Machine Translation Using Paraphrases Parallel corpora are crucial for training SMT systems. However, for many language pairs they are available only in very limited quantities. For these language pairs a huge portion of phrases encountered at run-time will be unknown. We show how techniques from paraphrasing can be used to deal with these otherwise unknown source language phrases. Our results show that augmenting a state-of-the-art SMT system with paraphrases leads to significantly improved coverage and translation quality. For a training corpus with 10,000 sentence pairs we increase the coverage of unique test set unigrams from 48% to 90%, with more than half of the newly covered items accurately translated, as opposed to none in current approaches. We propose a novel method which substitutes a paraphrase for an unknown source word or phrase in the input sentence and then proceeds to use the translation of that paraphrase in the production of the target-language result.	Improved Statistical Machine Translation Using Paraphrases Parallel corpora, which are collections of text in two languages, are essential for teaching Statistical Machine Translation (SMT) systems. However, for many pairs of languages, these collections are available only in very small amounts. For these language pairs, many phrases that appear during translation will be unfamiliar or unknown. We show how using techniques from paraphrasing, which involve rewording phrases, can help handle these unknown phrases from the original language. Our results demonstrate that adding paraphrases to a top-performing SMT system significantly improves both the range of phrases it can translate and the quality of the translation. For a training set of 10,000 sentence pairs, we improve the coverage of unique words from a test set from 48% to 90%, with more than half of the newly translated words being accurately translated, compared to none with current methods. We introduce a new method that replaces an unknown word or phrase from the original language with a paraphrase, and then uses the translation of that paraphrase to create the final translated sentence.	marking
391	Learning To Recognize Features Of Valid Textual Entailments This paper advocates a new architecture for textual inference in which finding a good alignment is separated from evaluating entailment. Current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text, using a locally decomposable matching score. We argue that there are significant weaknesses in this approach, including flawed assumptions of monotonicity and locality. Instead we propose a pipelined approach where alignment is followed by a classification step, in which we extract features representing high-level characteristics of the entailment problem, and pass the resulting feature vector to a statistical classifier trained on development data. We report results on data from the 2005 Pascal RTE Challenge which surpass previously reported results for alignment-based systems. We emphasize that there is more to inferential validity than close lexical or structural correspondence: negations, models, non-factive and implicative verbs, and other linguistic constructs can affect validity in ways hard to capture in alignment.	Learning To Recognize Features Of Valid Textual Entailments This paper suggests a new way to understand text by first matching the text and then checking if one text logically follows from the other. Current methods try to solve this by finding the best match between the question and the text, using a score that breaks down into parts. We believe this method has problems, like wrongly assuming that the relationship is straightforward and local. Instead, we suggest a step-by-step method where matching is followed by a sorting step. Here, we identify key features that show important aspects of the problem and use these features in a computer program trained on sample data. We show results on a test from the 2005 Pascal RTE Challenge that are better than previous methods focused on matching. We highlight that understanding if a text logically follows involves more than just matching words or structures: things like negations (opposite statements), models, verbs that aren't factual or suggest something, and other language elements can change whether a text is valid in ways that are hard to capture just by matching.	replaces
392	Named Entity Transliteration And Discovery From Multilingual Comparable Corpora Named Entity recognition (NER) is an important part of many natural language processing tasks. Most current approaches employ machine learning techniques and require supervised data. However, many languages lack such resources. This paper presents an algorithm to automatically discover Named Entities (NEs) in a resource free language, given a bilingual corpora in which it is weakly temporally aligned with a resource rich language. We observe that NEs have similar time distributions across such corpora, and that they are often transliterated, and develop an algorithm that exploits both iteratively. The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration. We evaluate the algorithm on an English-Russian corpus, and show high level of NEs discovery in Russian. Character unigrams and bigrams are used as features to learn a discriminative transliteration model and time series similarity was combined with the transliteration similarity model.	Named Entity Transliteration And Discovery From Multilingual Comparable Corpora Named Entity recognition (NER) is a key part of many tasks that involve understanding human language. Most current methods use computer programs that learn from examples, but many languages don't have enough examples to learn from. This paper introduces a method to find Named Entities (NEs), like names of people or places, in languages that don't have many examples, using texts in two languages that are loosely matched over time with a language that has many examples. We noticed that NEs often appear around the same times in these texts and are often written similarly, so we created a method that uses these patterns repeatedly. The method uses a new way to measure how often NEs appear over time and a way to match NEs that doesn't need many examples. We tested the method on a set of English and Russian texts and showed that it found many NEs in Russian. We used single letters and pairs of letters to create a model that recognizes patterns in how NEs are written and combined this with how similarly they appear over time.	Entailments
393	Alignment By Agreement We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions. We use discriminative SMT on large training data, training 1.5 million features on 67,000 sentences.	Alignment By Agreement We introduce a method that doesn't need labeled data to match words in two languages. It involves training two basic models together to ensure they agree with each other while also fitting the data well. This approach improves accuracy by 32% compared to the usual method where models are trained separately and then combined. Additionally, using a straightforward and fast pair of HMM (Hidden Markov Model) aligners improves accuracy by 29% over the traditional IBM model 4. We apply a selective translation method on a large amount of data, training with 1.5 million features on 67,000 sentences.	similarly
394	Effective Self-Training For Parsing We present a simple, but surprisingly effective, method of self-training a two-phase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. Our improved model achieves an f-score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon. We presented a very effective method for self-training a two-stage parsing system consisting of a first-stage generative lexicalized parser and a second-stage discriminative reranker. Self-training can suffer from over-fitting, in which errors in the original model are repeated and amplified in the new model.	Effective Self-Training For Parsing We present a simple, but surprisingly effective, way to train a two-step system that analyzes sentences using easily available data that hasn't been labeled. We show that it's possible to improve sentence analysis using a technique called bootstrapping, which involves using initial results to improve further analysis, especially when a reranker (a tool that helps decide the best analysis) is used. Our improved model scores 92.1%, which is 1.1% better (or 12% fewer mistakes) than the best previous result for analyzing Wall Street Journal texts. Lastly, we share some insights to help understand why this method works. We introduced a very effective way to train a two-step system for analyzing sentences, where the first step uses a specific type of parser (a tool that breaks down sentences), and the second step uses a reranker to improve results. Self-training can sometimes make the same mistakes worse in the new model as in the original model.	improves
395	Exploiting Semantic Role Labeling WordNet And Wikipedia For Coreference Resolution In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns. We show that including predicate argument pairs as features improved the performance of a coreference resolver. We train a model for classifying whether two mentions are co-referring or not. We suggest to mine semantic relatedness from Wikipedia, which can deal with the data sparseness problem suffered by using WordNet.	Exploiting Semantic Role Labeling WordNet And Wikipedia For Coreference Resolution In this paper, we present an improvement of a computer-based system that finds coreferences, which means determining when different words refer to the same thing. This system uses clues (features) taken from different sources of meaning. These clues come from WordNet (a dictionary-like tool) and Wikipedia, and also involve understanding roles in sentences. We show that these clues help improve how well the system identifies different groups of words, like pronouns (he, she) and common nouns (cat, table). We also show that using pairs of related actions and things in sentences as clues makes the system better. We teach a computer model to decide if two words are referring to the same thing or not. We suggest getting related meanings from Wikipedia, which helps solve the problem of not having enough data when using WordNet alone.	method works
396	Synchronous Binarization For Machine Translation Systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine translation output, but are often very computationally intensive. The complexity is exponential in the size of individual grammar rules due to arbitrary re-orderings between the two languages, and rules extracted from parallel corpora can be quite large. We devise a linear-time algorithm for factoring syntactic re-orderings by binarizing synchronous rules when possible and show that the resulting rule set significantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system. Synchronous binarization simultaneously binarizes both source and target-sides of a synchronous rule, making sure of contiguous spans on both sides whenever possible.	Synchronous Binarization For Machine Translation Systems based on linked grammar rules and tree converters promise to improve the quality of statistical machine translation output, but often require a lot of computing power. The difficulty increases dramatically with the size of individual grammar rules because of the random rearrangements between the two languages, and rules taken from matching text samples can be quite large. We create a simple and quick method for organizing the rearrangements by breaking down the linked rules into smaller parts when possible and show that the new rule set greatly improves the speed and accuracy of a top syntax-based machine translation system. Synchronous binarization breaks down both the original and translated sides of a linked rule at the same time, ensuring that related sections remain together on both sides whenever possible.	different
397	Preemptive Information Extraction Using Unrestricted Relation Discovery We are trying to extend the boundary of Information Extraction (IE) systems. Existing IE systems require a lot of time and human effort to tune for a new scenario. Preemptive Information Extraction is an attempt to automatically create all feasible IE systems in advance without human intervention. We propose a technique called Unrestricted Relation Discovery that discovers all possible relations from texts and presents them as tables. We present a preliminary system that obtains reasonably good results. We apply NER, coreference resolution and parsing to a corpus of newspaper articles to extract two-place relations between NEs. We rely further on supervised methods, defining features over a full syntactic parse, and exploit multiple descriptions of the same event in newswire to identify useful relations. Preemptive IE is a paradigm that first groups documents based on pairwise vector clustering, then applies additional clustering to group entities based on document clusters.	Preemptive Information Extraction Using Unrestricted Relation Discovery We are trying to expand what Information Extraction (IE) systems can do. Current IE systems need a lot of time and human work to adjust to new situations. Preemptive Information Extraction aims to automatically set up all possible IE systems ahead of time without needing people to help. We suggest a method called Unrestricted Relation Discovery that finds all possible connections from texts and shows them as tables. We show an early version of a system that gets fairly good results. We use Named Entity Recognition (NER), coreference resolution (tracking mentions of the same thing in text), and parsing (analyzing sentence structure) to a collection of newspaper articles to find two-part connections between Named Entities (NEs). We also use supervised methods (techniques that learn from examples), defining features using a detailed sentence analysis, and use multiple descriptions of the same story in news articles to find useful connections. Preemptive IE is a method that first groups documents using pairwise vector clustering (a way to organize data) and then uses more clustering to group entities based on document groups.	individual grammar
398	Prototype-Driven Learning For Sequence Models We investigate prototype-driven learning for primarily unsupervised sequence modeling. Prior knowledge is specified declaratively, by providing a few canonical examples of each target annotation label. This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model. On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work. For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints. We also compare to semi-supervised learning and discuss the system's error trends. Prototype-driven learning (PDL) optimizes the joint marginal likelihood of data labeled with prototype input features for each label. We ask the user to suggest a few prototypes (examples) for each class and use those as features.	Prototype-Driven Learning For Sequence Models We explore a method called prototype-driven learning for mostly unsupervised sequence modeling, which involves predicting sequences of data without much guidance. Instead of detailed instructions, we use a few clear examples of what each label should look like. This minimal example information is spread across a larger set of data by looking at similarities in the data using a specific type of model. In tasks like identifying parts of speech (categories like noun, verb, etc.) in English and Chinese, and extracting information, these example-based features help reduce errors significantly compared to other methods and do better than past attempts. For instance, we can correctly identify parts of speech in English 80.5% of the time using just three examples of each category without relying on a dictionary. We also look at how this compares to methods that use some guidance and talk about where the system tends to make mistakes. Prototype-driven learning (PDL) improves how well we can predict data by focusing on the likelihood of data labeled with these example features. We ask users to give a few example cases for each category and use those examples as key features.	features
399	Learning For Semantic Parsing With Statistical Machine Translation We present a novel statistical approach to semantic parsing, WASP, for constructing a complete, formal meaning representation of a sentence. A semantic parser is learned given a set of sentences annotated with their correct meaning representations. The main innovation of WASP is its use of state-of-the-art statistical machine translation techniques. A word alignment model is used for lexical acquisition, and the parsing model itself can be seen as a syntax-based translation model. We show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order. We use the maximum-entropy model, which defines a conditional probability distribution over derivations given an observed NL sentence.	Learning For Semantic Parsing With Statistical Machine Translation We introduce a new statistical method called WASP for understanding the meaning of sentences in a complete and formal way. A semantic parser, which is a tool to understand sentence meanings, is trained using sentences that have been marked with their correct meanings. The key new idea in WASP is using advanced statistical methods from machine translation (turning one language into another). A word alignment model helps in learning the meaning of words, and the parsing model itself works like a translation model that focuses on sentence structure. We demonstrate that WASP is both accurate and thorough compared to other methods that need similar levels of guidance, and it handles different task difficulties and word arrangements better. We utilize the maximum-entropy model, which is a way to calculate the likelihood of different interpretations based on a given normal language sentence.	features
400	Paraphrasing For Automatic Evaluation This paper studies the impact of paraphrases on the accuracy of automatic evaluation. Given a reference sentence and a machine-generated sentence, we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference. We apply our paraphrasing method in the context of machine translation evaluation. Our experiments show that the use of a paraphrased synthetic reference refines the accuracy of automatic evaluation. We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation. We show that creating synthetic reference sentences by substituting synonyms from Wordnet into the original reference sentences can increase the number of exact word matches with an MT system's output and yield significant improvements in correlations of BLEU (Papineni et al., 2002) scores with human judgments of translation adequacy.	Paraphrasing For Automatic Evaluation This paper looks at how changing the words in sentences (paraphrasing) affects the accuracy of automatic evaluation. We try to find a way to reword a reference sentence so it sounds more like a machine-created sentence than the original version. We use this rewording method to help evaluate machine translations. Our tests show that using these reworded reference sentences makes automatic evaluations more accurate. We also noticed that if humans think the automatic rewording is good, it helps the evaluation process. We demonstrate that making new reference sentences by swapping words with their synonyms (similar words) from a tool called Wordnet helps match more words exactly with a machine translation output, which improves the accuracy of BLEU scores, a measure used to judge how good translations are.	another
401	Arabic Preprocessing Schemes For Statistical Machine Translation In this paper, we study the effect of different word-level preprocessing decisions for Arabic on SMT quality. Our results show that given large amounts of training data, splitting off only proclitics performs best. However, for small amounts of training data, it is best to apply English-like tokenization using part-of-speech tags, and sophisticated morphological analysis and disambiguation. Moreover, choosing the appropriate preprocessing produces a significant increase in BLEU score if there is a change in genre between training and test data. We show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size.	Arabic Preprocessing Schemes For Statistical Machine Translation In this paper, we look at how different ways of preparing Arabic words affect the quality of translating it into another language using computers. Our findings show that when we have a lot of training examples, it's best to only separate small parts of words (proclitics). But when we have only a few examples, it's better to break down the text like English using grammar tags and detailed word analysis to understand meanings better. Also, choosing the right way to prepare the text makes a big improvement in translation quality, especially if the topics of the training and test data are different. We demonstrate that different ways of breaking down words lead to better results, but the improvements get smaller as the amount of training data increases.	rewording
402	OntoNotes: The 90% Solution We describe the OntoNotes methodology and its result, a large multilingual richly-annotated corpus constructed at 90% interannotator agreement. An initial portion (300K words of English newswire and 250K words of Chinese newswire) will be made available to the community during 2007. Ontonotes includes a wide array of data sources like broadcast news, news wire, magazine, web text, etc. In the OntoNotes project (Hovy et al., 2006), annotators use small-scale corpus analysis to create sense inventories derived by grouping together WordNet senses, with the procedure restricted to maintain 90% inter-annotator agreement.	OntoNotes: The 90% Solution We explain the OntoNotes method and its result, which is a big collection of texts in different languages that has been carefully labeled and built with 90% agreement among people doing the labeling. A first part (300,000 words of English news articles and 250,000 words of Chinese news articles) will be shared with the public in 2007. OntoNotes includes many types of data sources like TV news, news articles, magazines, online text, etc. In the OntoNotes project, people who label the data (called annotators) use small-scale text analysis to create lists of word meanings by grouping similar meanings from WordNet (a dictionary of word meanings), making sure that 90% of them agree on the labels.	computers
403	Parser Combination By Reparsing We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers. We apply this idea to dependency and constituent parsing, generating results that surpass state-of-the-art accuracy levels for individual parsers. We introduce a threshold for the constituent count and search for the tree with the largest count number from all the possible constituent combinations. We combine five parsers to obtain a score of 92.1, whereas the best single parser obtains a score of 91.0.	Parser Combination By Reparsing We present a new method for combining parsers (tools that analyze sentence structure) by analyzing sentences again after they have been processed by multiple parsers. We use this method for two types of sentence analysis: dependency (how words relate) and constituent parsing (breaking sentences into parts), achieving better accuracy than any single parser alone. We set a limit on the number of sentence parts and look for the arrangement with the most parts from all possible combinations. By combining five parsers, we achieve a score of 92.1, compared to the best single parser score of 91.0.	carefully labeled
404	First-Order Probabilistic Models for Coreference Resolution Traditional noun phrase coreference resolution systems represent features only of pairs of noun phrases. In this paper, we propose a machine learning method that enables features over sets of noun phrases, resulting in a first-order probabilistic model for coreference. We outline a set of approximations that make this approach practical, and apply our method to the ACE coreference dataset, achieving a 45% error reduction over a comparable method that only considers features of pairs of noun phrases. This result demonstrates an example of how a first-order logic representation can be incorporated into a probabilistic model and scaled efficiently. We present a system which uses an online learning approach to train a classifier to judge whether two entities are coreferential or not. We introduce a first-order probabilistic model which implements features over sets of mentions and thus operates directly on entities.	First-Order Probabilistic Models for Coreference Resolution Traditional noun phrase coreference resolution systems only look at features of pairs of noun phrases. In this paper, we suggest a machine learning method that looks at features over groups of noun phrases, leading to a first-order probabilistic model for coreference. We describe some shortcuts that make this method practical and use it on the ACE coreference dataset, reducing errors by 45% compared to a similar method that only looks at pairs of noun phrases. This result shows how using first-order logic (a way to structure reasoning) in a probabilistic model can work well and be efficient. We present a system that uses online learning (a way to continuously train a machine) to teach a classifier (a tool that decides) whether two entities (things or people) refer to the same thing or not. We introduce a first-order probabilistic model that considers features over groups of mentions, so it works directly on entities.	achieving better
405	Bayesian Inference for PCFGs via Markov Chain Monte Carlo This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar. We describe Gibbs samplers for Bayesian inference of PCFG rule probabilities. We introduce adaptor grammars, a monolingual grammar formalism whereby a non-terminal rewrites in a single step as a complete subtree.	Bayesian Inference for PCFGs via Markov Chain Monte Carlo This paper introduces two MCMC methods for Bayesian inference (a way to make predictions) of probabilistic context-free grammars (PCFGs are rules for sentence structure) from basic word strings. These methods offer an alternative to the maximum-likelihood estimation (a way to find the best explanation for data) using the Inside-Outside algorithm. We show these methods by estimating a simple grammar that explains the word structure of the Bantu language Sesotho. This proves that with the right starting assumptions, Bayesian methods can figure out language rules even when maximum-likelihood methods, like the Inside-Outside algorithm, only create simple and unhelpful grammar rules. We explain how to use Gibbs samplers (a specific technique for Bayesian inference) to determine the chances of PCFG rules. We also introduce adaptor grammars, a tool for single-language grammar that allows a non-terminal part (a larger sentence part) to be rewritten all at once as a whole tree of phrases.	order logic
406	Lexicalized Markov Grammars for Sentence Compression We present a sentence compression system based on synchronous context-free grammars (SCFG), following the successful noisy-channel approach of (Knight and Marcu, 2000). We define a head-driven Markovization formulation of SCFG deletion rules, which allows us to lexicalize probabilities of constituent deletions. We also use a robust approach for tree-to-tree alignment between arbitrary document-abstract parallel corpora, which lets us train lexicalized models with much more data than previous approaches relying exclusively on scarcely available document-compression corpora. Finally, we evaluate different Markovized models, and find that our selected best model is one that exploits head-modifier bilexicalization to accurately distinguish adjuncts from complements, and that produces sentences that were judged more grammatical than those generated by previous work.	Lexicalized Markov Grammars for Sentence Compression We introduce a system that shortens sentences using a method called synchronous context-free grammars (SCFG), inspired by a successful approach from 2000. We create a new way to apply SCFG rules that cut out parts of the sentence, which helps us predict more accurately when these parts should be removed. We also use a strong method to match sentence structures in different documents, which allows us to train our models with a lot more data than older methods that relied on limited resources. Finally, we test different models and find that our best model uses a technique to clearly tell apart extra information from main parts of the sentence, and it creates sentences that are considered more grammatically correct than those from previous methods.	chances
419	Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars One of the reasons nonparametric Bayesian inference is attracting attention in computational linguistics is because it provides a principled way of learning the units of generalization together with their probabilities. Adaptor grammars are a framework for defining a variety of hierarchical nonparametric Bayesian models. This paper investigates some of the choices that arise in formulating adaptor grammars and associated inference procedures, and shows that they can have a dramatic impact on performance in an unsupervised word segmentation task. With appropriate adaptor grammars and inference procedures we achieve an 87% word token f-score on the standard Brent version of the Bernstein-Ratner corpus, which is an error reduction of over 35% over the best previously reported results for this corpus. We find that the adaptor grammar with syllable structure phontactic constraints and three levels of collocational structure yields the highest word segmentation token f-score.	Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars One of the reasons nonparametric Bayesian inference, which is a method for making predictions without assuming a fixed number of parameters, is gaining attention in computational linguistics (the study of language using computers) is because it offers a systematic way to learn patterns and their likelihoods. Adaptor grammars are a method for creating different types of complex Bayesian models without fixed parameters. This paper looks into the different decisions made when creating adaptor grammars and how these decisions affect the methods used to make predictions, showing that they can greatly influence how well we can separate words in a text without guidance. By using the right adaptor grammars and prediction methods, we achieve an 87% accuracy in identifying words in the well-known Brent version of the Bernstein-Ratner collection of language data, which reduces errors by over 35% compared to the best results reported before for this data set. We discover that using an adaptor grammar that includes rules for syllable structure and three levels of how words commonly appear together results in the highest accuracy for identifying words.	methods
407	Combining Outputs from Multiple Machine Translation Systems Currently there are several approaches to machine translation (MT) based on different paradigms; e.g., phrasal, hierarchical and syntax-based. These three approaches yield similar translation accuracy despite using fairly different levels of linguistic knowledge. The availability of such a variety of systems has led to a growing interest toward finding better translations by combining outputs from multiple systems. This paper describes three different approaches to MT system combination. These combination methods operate on sentence, phrase and word level exploiting information from N-best lists, system scores and target-to-source phrase alignments. The word-level combination provides the most robust gains but the best results on the development test sets (NIST MT05 and the newsgroup portion of GALE 2006 dry-run) were achieved by combining all three methods. We use minimum Translation Error Rate (TER) (Snover et al, 2006) alignment to build the confusion network. We collect source-to-target correspondences from the input systems, create a new translation option table using only these phrases, and re-decode the source sentence to generate better translations.	Combining Outputs from Multiple Machine Translation Systems Currently, there are several methods for machine translation (MT), which means converting text from one language to another, based on different styles; for example, phrasal (using phrases), hierarchical (using a layered approach), and syntax-based (using sentence structure). These three methods give similar translation quality even though they use different amounts of language knowledge. Having different systems has sparked interest in finding better translations by mixing results from different systems. This paper explains three ways to combine MT systems. These methods work on sentence, phrase, and word levels, using information from lists of top translations (N-best lists), system scores, and matching phrases from the target language back to the source language. The word-level combination gives the most reliable improvements, but the best results on test sets (NIST MT05 and the newsgroup part of GALE 2006 trial) were achieved by using all three methods together. We use a method called minimum Translation Error Rate (TER) alignment (a way to measure translation accuracy) to build a confusion network (a way to organize possible translations). We gather connections from the source language to the target language from the input systems, create a new list of translation options using only these phrases, and re-process the source sentence to create better translations.	Sentence
408	Joint Determination of Anaphoricity and Coreference Resolution using Integer Programming Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution. In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. This joint ILP formulation provides f-score improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets. By using joint inference for anaphoricity and coreference, we avoid cascade-induced errors without the need to separately optimize the threshold.	Joint Determination of Anaphoricity and Coreference Resolution using Integer Programming Standard pairwise coreference resolution systems, which connect words in a text that refer to the same thing, often make mistakes because they try to identify anaphora (words like "he" or "it" that refer back to something mentioned earlier) as part of this connection process. In this paper, we suggest a method using integer linear programming (ILP), which is a math approach, to solve coreference problems by considering anaphoricity and coreference together, so each helps the other for final decisions. This combined ILP method improves the accuracy score by 3.7-5.3% compared to a basic coreference system on the ACE datasets (a set of data used for testing). By working on anaphoricity and coreference together, we avoid mistakes caused by doing things in steps, without having to separately adjust limits.	measure translation
409	Multiple Aspect Ranking Using the Good Grief Algorithm We address the problem of analyzing multiple related opinions in a text. For instance, in a restaurant review such opinions may include food, ambience and service. We formulate this task as a multiple aspect ranking problem, where the goal is to produce a set of numerical scores, one for each aspect. We present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks. This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast. We prove that our agreement-based joint model is more expressive than individual ranking models. Our empirical results further confirm the strength of the model: the algorithm provides significant improvement over both individual rankers and a state-of-the-art joint ranking model. We combine an agreement model based on contrastive RST relations with a local aspect (or target) model to make a more informed overall decision for sentiment classification.	Multiple Aspect Ranking Using the Good Grief Algorithm We tackle the challenge of examining several connected opinions in a text. For example, in a restaurant review, these opinions might be about the food, atmosphere, and service. We treat this task as a multi-part ranking issue, aiming to assign a number score to each part. We introduce a method that learns how to rank each part by considering how the rankings relate to each other. This method helps predict individual rankings by looking at relationships between opinions, like when they agree or differ. We show that our model, which focuses on agreement, is more detailed than models that rank each part separately. Our test results further show that our method is better: it improves greatly over both individual rankings and the latest joint ranking model. We combine an agreement model based on contrasting relationships with a local model focused on specific parts to make a smarter overall decision for understanding opinions.	doing things
410	Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme Conversion Letter-to-phoneme conversion generally requires aligned training data of letters and phonemes. Typically, the alignments are limited to one-to-one alignments. We present a novel technique of training with many-to-many alignments. A letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with fixed lists. We also apply an HMM method in conjunction with a local classification model to predict a global phoneme sequence given a word. The many-to-many alignments result in significant improvements over the traditional one-to-one approach. Our system achieves state-of-the-art performance on several languages and data sets. The M2M-aligner is based on the expectation maximization (EM) algorithm. M2M-aligner is a many-to-many (M-M) alignment algorithm based on EM that allows for mapping of multiple letters to multiple phonemes.	Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme Conversion Letter-to-phoneme conversion usually needs training data where letters and sounds are matched up. Normally, these matches are one-to-one, meaning one letter to one sound. We introduce a new method that allows multiple letters to be matched to multiple sounds. A system predicts pairs of letters and sounds automatically without needing predefined lists. We also use a special method called Hidden Markov Models (HMM) along with a local prediction model to predict the entire set of sounds for a word. This new approach of matching many-to-many shows much better results than the traditional one-to-one matching. Our system performs as well as or better than any other current systems for several languages and data. The M2M-aligner uses a method called the expectation maximization (EM) algorithm, which helps in efficiently matching multiple letters to multiple sounds.	rankings
411	Improved Inference for Unlexicalized Parsing We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs. First, we present a novel coarse-to-fine method in which a grammar's own hierarchical projections are used for incremental pruning, including a method for efficiently computing projections of a grammar without a treebank. In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy. Second, we compare various inference procedures for state-split PCFGs from the standpoint of risk minimization, paying particular attention to their practical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning. An approach found to be effective for coarse-to-fine parsing is to use likelihood-based hierarchical EM training.	Improved Inference for Unlexicalized Parsing We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs. First, we introduce a new method that simplifies parsing by using a grammar's own structure step-by-step, allowing us to cut down unnecessary parts efficiently without needing a large collection of sentence examples (treebank). In our tests, this method speeds up parsing significantly without making it less accurate. Second, we look at different ways to make predictions using state-split PCFGs, focusing on minimizing mistakes and considering their real-world benefits and drawbacks. Lastly, we share experiments in multiple languages showing that this method of splitting states in a hierarchy is both quick and precise, even without adjustments for specific languages. A successful technique for simplifying the parsing process is using a training method based on probability (likelihood-based hierarchical EM training).	Markov Models
412	ISP: Learning Inferential Selectional Preferences Semantic inference is a key component for advanced natural language understanding. However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering. This paper presents ISP, a collection of methods for automatically learning admissible argument values to which an inference rule can be applied, which we call inferential selectional preferences, and methods for filtering out incorrect inferences. We evaluate ISP and present empirical evidence of its effectiveness. Context-sensitive extensions of DIRT focus on making DIRT rules context-sensitive by attaching appropriate semantic classes to the X and Y slots of an inference rule. We build a set of semantic classes using WordNet in one case and CBC clustering algorithm in the other; for each rule, we use the overlap of the fillers found in the input corpus as an indicator of the correct semantic classes. We augment each relation with its selectional preferences, i.e. fine-grained entity types of two arguments, to handle polysemy.	ISP: Learning Inferential Selectional Preferences Semantic inference helps computers understand language better. But the current methods for finding these inference rules, which help computers understand if one statement logically follows from another, haven't worked well in tasks like understanding if two texts mean the same thing or answering questions. This paper introduces ISP, a set of methods to automatically learn which values are suitable for inference rules, called inferential selectional preferences, and ways to remove incorrect conclusions. We test ISP and show evidence that it works well. Context-sensitive extensions of DIRT aim to make DIRT rules adaptable to different situations by adding the right meaning categories to the X and Y parts of these rules. We create groups of meanings using WordNet, a dictionary-like database, and CBC clustering, a method to group similar things. For each rule, we look at how much the examples from the input match to find the right meaning categories. We add specific types of entities to each relationship to understand different meanings of words better.	unnecessary parts
413	TextRunner: Open Information Extraction on the Web Traditional information extraction systems have focused on satisfying precise, narrow, pre-specified requests from small, homogeneous corpora. In contrast, the TEXTRUNNER system demonstrates a new kind of information extraction, called Open Information Extraction (OIE), in which the system makes a single, data-driven pass over the entire corpus and extracts a large set of relational tuples, without requiring any human input. (Banko et al., 2007) TEXTRUNNER is a fully-implemented, highly scalable example of OIE. TEXTRUNNER's extractions are indexed, allowing a fast query mechanism. Our first public demonstration of the TEXTRUNNER system shows the results of performing OIE on a set of 117 million web pages. It demonstrates the power of TEXTRUNNER in terms of the raw number of facts it has extracted, as well as its precision using our novel assessment mechanism. And it shows the ability to automatically determine synonymous relations and objects using large sets of extractions. We have built a faster user interface for querying the results. We provide an online demo of TextRunner.	TextRunner: Open Information Extraction on the Web Traditional information extraction systems have focused on satisfying specific, narrow, pre-planned requests from small, similar collections of text. In contrast, the TEXTRUNNER system shows a new kind of information extraction, called Open Information Extraction (OIE), where the system goes through the entire text collection once and pulls out a large set of relational data pairs, without needing any human help. (Banko et al., 2007) TEXTRUNNER is a fully working, very scalable example of OIE. TEXTRUNNER's extractions are indexed, allowing a fast way to search. Our first public demonstration of the TEXTRUNNER system shows the results of performing OIE on a set of 117 million web pages. It shows the power of TEXTRUNNER by the large number of facts it has collected, as well as its accuracy using our new assessment method. And it shows the ability to automatically find similar relations and objects using large sets of extractions. We have built a faster user interface for searching the results. We provide an online demo of TextRunner.	WordNet
414	A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches This paper presents and compares WordNet-based and distributional similarity approaches. The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets. Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses. We derive a WordNet-based measure using PageRank and combined it with several corpus based vector space models using SVMs. Examining the relations between the words in each pair, we further split this dataset into similar pairs (WS-sim) and related pairs (WS-rel), where the former contains synonyms, antonyms, identical words and hyponyms/hypernyms and the latter capture other word relations.	A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches This paper explains and compares two methods: WordNet-based, which uses a large database of words and their meanings, and distributional similarity, which looks at how words are used in texts. The paper talks about what each method does well and not so well when it comes to figuring out how similar or related words are, and suggests using both together. Each of our methods, when used alone, gives the best results in their category on the RG and WordSim353 word tests, and when we combine them using a supervised method (where a model learns from data), it gives the best results ever recorded on all tests. Lastly, we are the first to explore using these methods for comparing words in different languages, showing that they can be easily adjusted for this and only lose a little accuracy. We create a WordNet-based measure using PageRank (a method to rank web pages) and mix it with several text-based models using SVMs (a type of machine learning model). By looking at the connections between words in each pair, we further divide this dataset into similar pairs (WS-sim) and related pairs (WS-rel), where the first group includes words that mean the same, opposite, are exactly the same, or are more or less specific, and the second group includes other types of word connections.	objects
415	Shared Logistic Normal Distributions for Soft Parameter Tying in Unsupervised Grammar Induction We present a family of priors over probabilistic grammar weights, called the shared logistic normal distribution. This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar. We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus. We see our largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing.	Shared Logistic Normal Distributions for Soft Parameter Tying in Unsupervised Grammar Induction We present a set of guidelines for probabilistic grammar weights called the shared logistic normal distribution. This set builds on the partitioned logistic normal distribution, allowing linked variability between the chances of different outcomes in the probabilistic grammar, offering a new way to include prior knowledge about an unknown grammar. We explain a method called a variational EM algorithm for learning a probabilistic grammar using these guidelines. We then test unsupervised dependency grammar learning and demonstrate significant improvements with our model for learning in one language and across languages using a non-parallel, multilingual dataset. Our biggest improvements come from linking parameters for various broad categories of parts-of-speech within one language, with only moderate gains by allowing influence between languages on top of the within-language sharing.	several
416	Improving Unsupervised Dependency Parsing with Richer Contexts and Smoothing Unsupervised grammar induction models tend to employ relatively simple models of syntax when compared to their supervised counterparts. Traditionally, the unsupervised models have been kept simple due to tractability and data sparsity concerns. In this paper, we introduce basic valence frames and lexical information into an unsupervised dependency grammar inducer and show how this additional information can be leveraged via smoothing. Our model produces state-of-the-art results on the task of unsupervised grammar induction, improving over the best previous work by almost 10 percentage points. We use the lexical values with the frequency more than 100 and defining tied probabilistic context free grammar (PCFG) and Dirichlet priors, the accuracy is improved. We also implement a sort of parameter tying for the E-DMV through a learning a back off distribution on child probabilities.	Improving Unsupervised Dependency Parsing with Richer Contexts and Smoothing Unsupervised grammar induction models, which try to understand sentence structure without labeled examples, usually use simpler methods compared to models that have been trained with labeled examples. These unsupervised models are kept simple because they need to handle complex calculations and often have limited data. In this paper, we add basic patterns and word information to a model that tries to figure out sentence structures without guidance and show how this extra information can be used effectively by a technique called smoothing, which helps deal with gaps in data. Our model achieves the best results so far for figuring out grammar without guidance, improving previous best results by nearly 10%. We use words that appear more than 100 times and define a grammar model using tied probability (a system that shares parameters) and a method called Dirichlet priors, which helps improve accuracy. We also apply a technique to connect parameters for the E-DMV (a specific model) by learning a backup method for estimating the probabilities of parts of the sentence.	dataset
417	11001 New Features for Statistical Machine Translation We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase-based translation system and our syntax-based translation system. On a large-scale Chinese-English translation task, we obtain statistically significant improvements of +1.5 Bleu and +1.1 Bleu, respectively. We analyze the impact of the new features and the performance of the learning algorithm. We only use 100 most frequent words for word context feature. We introduce the features for an SCFG model for Chinese/English translation are of two types: The first type explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-side terminals.	11001 New Features for Statistical Machine Translation We use an improved algorithm called the Margin Infused Relaxed Algorithm by Crammer and others to add many new tools to two machine translation systems: the Hiero system, which uses a hierarchy of phrases, and our system that uses syntax, which is the arrangement of words. On a large Chinese-English translation task, we see clear improvements in quality, with scores increasing by +1.5 and +1.1 on the Bleu scale, which measures translation accuracy. We look at how these new tools affect the systems and how well the learning algorithm works. We use only the 100 most common words for understanding word context. We add features to a model called SCFG for Chinese/English translation, which are in two categories: The first type specifically reduces overestimates of rule counts, or rules with poor connections, bad changes, or unwanted additions of words on the target side.	connect parameters
418	Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages We introduce a novel precedence reordering approach based on a dependency parser to statistical machine translation systems. Similar to other preprocessing reordering approaches, our method can efficiently incorporate linguistic knowledge into SMT systems without increasing the complexity of decoding. For a set of five subject-object-verb (SOV) order languages, we show significant improvements in BLEU scores when translating from English, compared to other reordering approaches, in state-of-the-art phrase-based SMT systems. We show that translation between subject-verb-object (English) and subject-object-verb (Pashto) languages can be improved by reordering the source side of the parallel data. On Web text, we report significant improvements applying one set of hand-crafted rules to translation from English to each of five SOV languages: Korean, Japanese, Hindi, Urdu and Turkish.	Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages We introduce a new way to rearrange words using a tool called a dependency parser, to help computer systems translate languages better. Like other methods that change word order before translation, our method can use language rules without making the translation process more complicated. For five languages that typically use the order subject-object-verb (SOV), we show big improvements in a measure called BLEU scores when translating from English, compared to other methods, in advanced translation systems that use phrases. We demonstrate that translation between languages like English, which uses subject-verb-object (SVO), and languages like Pashto, which uses SOV, can be improved by changing the word order on the English side of the texts used for translation. On online texts, we show big improvements by using a set of carefully designed rules to translate from English to each of five SOV languages: Korean, Japanese, Hindi, Urdu, and Turkish.	additions
420	Joint Parsing and Named Entity Recognition For many language technology applications, such as question answering, the overall system runs several independent processors over the data (such as a named entity recognizer, a coreference system, and a parser). This easily results in inconsistent annotations, which are harmful to the performance of the aggregate system. We begin to address this problem with a joint model of parsing and named entity recognition, based on a discriminative feature-based constituency parser. Our model produces a consistent output, where the named entity spans do not conflict with the phrasal spans of the parse tree. The joint representation also allows the information from each type of annotation to improve performance on the other, and, in experiments with the OntoNotes corpus, we found improvements of up to 1.36% absolute F1 for parsing, and up to 9.0% F1 for named entity recognition. While performing named entity recognition jointly with constituency parsing shows improvement in performance on both tasks, the only aspect of the sytnax which is leveraged by the NER component is the location of noun phrases.	Joint Parsing and Named Entity Recognition For many applications that use language technology, like question answering systems, the overall system uses several separate tools to analyze data (like a tool to identify names of people or places, a tool to link related words, and a tool to understand sentence structure). This can lead to mixed-up results, which can reduce how well the whole system works. We start solving this issue with a combined model that does both parsing (understanding sentence structure) and named entity recognition (identifying names of things), using a specific type of tool that focuses on recognizing parts of sentences. Our model creates results that match well together, so the identified names don't clash with parts of the sentence structure. This combined approach also means that the information from one type of analysis helps improve the other. In tests with a specific dataset called the OntoNotes corpus, we saw improvements of up to 1.36% in one measure called F1 for sentence structure analysis, and up to 9.0% F1 for identifying names. While doing named entity recognition and analyzing sentence structure together improves both tasks, the only part of sentence structure that helps the name identification tool is finding noun phrases (groups of words that act like a noun).	Adaptor
421	Exploring Content Models for Multi-Document Summarization We present an exploration of generative probabilistic models for multi-document summarization. Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way. Our final model, HIERSUM, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions. At the task of producing generic DUC-style summaries, HIERSUM yields state-of-the-art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al. (2007)'s state-of-the-art discriminative system. We also explore HIERSUM's capacity to produce multiple 'topical summaries' in order to facilitate content discovery and navigation. In TOPICSUM, each word is generated by a single topic which can be a corpus-wide background distribution over common words, a distribution of document-specific words or a distribution of the core content of a given cluster. We build a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt.	Exploring Content Models for Multi-Document Summarization We explore different methods to create summaries from multiple documents using models that predict the likelihood of words appearing together (generative probabilistic models). Starting with a basic model that counts how often words appear (Nenkova and Vanderwende, 2005), we develop a series of models that add more detail to how we understand the content of a group of documents, and we see improvements in summary quality measured by a standard called ROUGE. Our final model, HIERSUM, uses a layered approach similar to hierarchical LDA (Latent Dirichlet Allocation) (Blei et al., 2004), which organizes topics and their related words in a tree-like structure. In creating general summaries like those for DUC (a summarization competition), HIERSUM achieves top-level performance and is rated better by users compared to another leading method by Toutanova et al. (2007). We also look at how HIERSUM can create several summaries focused on different topics to help users find and explore content. In TOPICSUM, we generate each word based on one topic, which could be a general set of common words, words specific to a document, or the main ideas of a specific group. We create a summarization system using topic models, learning about both broad topics for documents and more specific topics within them.	identify names
422	Using a maximum entropy model to build segmentation lattices for MT Recent work has shown that translating segmentation lattices (lattices that encode alternative ways of breaking the input to an MT system into words), rather than text in any particular segmentation, improves translation quality of languages whose orthography does not mark morpheme boundaries. However, much of this work has relied on multiple segmenters that perform differently on the same input to generate sufficiently diverse source segmentation lattices. In this work, we describe a maximum entropy model of compound word splitting that relies on a few general features that can be used to generate segmentation lattices for most languages with productive compounding. Using a model optimized for German translation, we present results showing significant improvements in translation quality in German-English, Hungarian-English, and Turkish-English translation over state-of-the-art baselines. We find that unigram weights are an effective feature in our segmentation model.	Using a maximum entropy model to build segmentation lattices for MT Recent work has shown that translating segmentation lattices (which are structures that show different ways to split the input into words for a machine translation system) instead of just using one specific way of splitting the text, makes translations better for languages where writing doesn't clearly show where word parts begin and end. However, a lot of this work has depended on using several different tools to split words, which work differently on the same text, to create a variety of ways to split the source text. In this study, we explain a maximum entropy model (a type of mathematical model) for splitting compound words that uses some general rules to create segmentation lattices for many languages with complex word formations. Using a model specifically tuned for translating German, we show that there are significant improvements in the quality of translations from German to English, Hungarian to English, and Turkish to English compared to the best available methods. We find that using unigram weights (a simple counting method for words) is a useful feature in our word splitting model.	create several
423	Semantic Roles for SMT: A Hybrid Two-Pass Model We present results on a novel hybrid semantic SMT model that incorporates the strengths of both semantic role labeling and phrase-based statistical machine translation. The approach avoids major complexity limitations via a two-pass architecture. The first pass is performed using a conventional phrase-based SMT model. The second pass is performed by a re-ordering strategy guided by shallow semantic parsers that produce both semantic frame and role labels. Evaluation on a Wall Street Journal newswire genre test set showed the hybrid model to yield an improvement of roughly half a point in BLEU score over a strong pure phrase-based SMT baseline – to our knowledge, the first successful application of semantic role labeling to SMT. We perform semantic role labeling on translation output and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation.	Semantic Roles for SMT: A Hybrid Two-Pass Model We share results on a new mixed method for translation that combines the best parts of understanding sentence roles and translating with statistics. This method avoids being too complicated by using two steps. The first step uses a regular translation model that works with phrases. The second step changes the order of the words, guided by simple tools that understand sentence roles and structures. Testing on a Wall Street Journal news set showed that this method improves the translation score by about half a point compared to a strong regular phrase-based translation, marking the first time understanding sentence roles has been successfully used in translation. We analyze the roles of words in the translated sentence and change their order to better match the meaning between the original and translated sentences.	significant improvements
424	Multi-Prototype Vector-Space Models of Word Meaning Current vector-space models of lexical semantics create a single “prototype” vector to represent the meaning of a word. However, due to lexical ambiguity, encoding word meaning with a single vector is problematic. This paper presents a method that uses clustering to produce multiple “sense-specific” vectors for each word. This approach provides a context-dependent vector representation of word meaning that naturally accommodates homonymy and polysemy. Experimental comparisons to human judgements of semantic similarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models. We introduce a multi-prototype VSM where word sense discrimination is first applied by clustering contexts, and then prototypes are built using the contexts of the sense-labeled words.	Multi-Prototype Vector-Space Models of Word Meaning Current models that use vector-space, a type of mathematical model, create one "prototype" or main representation to show what a word means. But because words can have more than one meaning (lexical ambiguity), using just one representation can be a problem. This paper shows a method that groups word meanings into different "sense-specific" representations, meaning each word can have several meanings based on the group it belongs to. This method gives a meaning to words that changes based on the situation, making it easier to handle words that sound the same but have different meanings (homonymy) and words with multiple meanings (polysemy). Tests comparing this method to how humans judge word meanings, both alone and in sentences, show it works better than older methods that use only one main representation or examples. We introduce a new multi-prototype model where we first group the situations where words are used, and then create main representations using these grouped word situations.	Journal
425	Using Mostly Native Data to Correct Errors in Learners&rsquo; Writing We present results from a range of experiments on article and preposition error correction for non-native speakers of English. We first compare a language model and error-specific classifiers (all trained on large English corpora) with respect to their performance in error detection and correction. We then combine the language model and the classifiers in a meta-classification approach by combining evidence from the classifiers and the language model as input features to the meta-classifier. The meta-classifier in turn is trained on error-annotated learner data, optimizing the error detection and correction performance on this domain. The meta-classification approach results in substantial gains over the classifier-only and language-model-only scenario. Since the meta-classifier requires error-annotated data for training, we investigate how much training data is needed to improve results over the baseline of not using a meta-classifier. All evaluations are conducted on a large error-annotated corpus of learner English. We remove sentences from the data where some other error appears immediately next to a preposition or determiner error.	Using Mostly Native Data to Correct Errors in Learners’ Writing We present results from various tests on fixing mistakes with articles (like "a" or "the") and prepositions (like "in" or "on") made by people learning English. First, we compare a language model (a computer system that understands language) and special tools designed to find and fix these mistakes. We see how well each one works. Then, we mix the language model and these tools in a combined approach, using information from both to make better decisions. This combined system is trained using examples of learner mistakes, to improve its ability to find and fix errors. This combined approach works much better than using just the language model or the tools alone. Since the combined system needs learner mistakes to learn from, we check how much of this training data is needed to see improvements over not using this combined system. We test everything on a large set of learner English that has been marked for errors. We take out sentences where other mistakes are right next to an article or preposition mistake.	examples
426	Unsupervised Modeling of Twitter Conversations We propose the first unsupervised approach to the problem of modeling dialogue acts in an open domain. Trained on a corpus of noisy Twitter conversations, our method discovers dialogue acts by clustering raw utterances. Because it accounts for the sequential behaviour of these acts, the learned model can provide insight into the shape of communication in a new medium. We address the challenge of evaluating the emergent model with a qualitative visualization and an intrinsic conversation ordering task. This work is inspired by a corpus of 1.3 million Twitter conversations, which will be made publicly available. This huge amount of data, available only because Twitter blurs the line between chatting and publishing, highlights the need to be able to adapt quickly to a new medium. We propose an evaluation based on rank correlation coefficient, which measures the degree of similarity between any two orderings over sequential data. We limit our dataset by choosing Twitter dialogues containing 3 to 6 posts (utterances), making it tractable to enumerate all permutations. We propose a probabilistic model to discover dialogue acts in Twitter conversations and to classify tweets in a conversation according to those acts. Under the block HMM, messages in a conversation flow according to a Markov process, where the words of messages are generated according to language models associated with a state in a hidden Markov model.	Unsupervised Modeling of Twitter Conversations We introduce the first method that doesn't need pre-labeled data to understand the roles of messages (dialogue acts) in open discussions. Our technique, trained on messy Twitter conversations, groups similar messages together to find out their roles. By considering the order of these roles, the model helps us understand how people communicate in this new way. We tackle the problem of checking how well this model works by using a visual representation and a task that orders conversations. This study is based on a large collection of 1.3 million Twitter chats, which we will share with the public. This large amount of data, possible because Twitter mixes chatting with posting publicly, shows the importance of adapting to new ways of communication. We suggest an evaluation method that uses a rank correlation coefficient, which checks how similar two arrangements of data are. We narrow our data by selecting Twitter chats with 3 to 6 messages, making it easier to list all possible arrangements. We present a method that uses probability to find out the roles of messages in Twitter chats and to categorize tweets in a chat based on those roles. In the block HMM (Hidden Markov Model), messages in a chat follow a sequence determined by a Markov process, where the words in messages are created based on language patterns linked to a state in the hidden Markov model.	people learning
427	For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia We report on work in progress on extracting lexical simplifications (e.g., “collaborate” → “work together”), focusing on utilizing edit histories in Simple English Wikipedia for this task. We consider two main approaches: (1) deriving simplification probabilities via an edit model that accounts for a mixture of different operations, and (2) using metadata to focus on edits that are more likely to be simplification operations. We find our methods to outperform a reasonable baseline and yield many high-quality lexical simplifications not included in an independently-created manually prepared list. We learn lexical simplifications without taking syntactic context into account.	For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia We are working on a method to find simpler versions of words (like changing “collaborate” to “work together”) by looking at changes made in Simple English Wikipedia. We are trying two main methods: (1) calculating how likely a word change is a simplification by looking at different kinds of edits, and (2) focusing on changes that are probably simplifications by using extra information about the edits. Our methods work better than a basic approach and find many good simple word changes that aren’t on a list made by people. We find simpler words without considering the sentence structure.	Hidden Markov
428	Coreference Resolution in a Modular Entity-Centered Model Coreference resolution is governed by syntactic, semantic, and discourse constraints. We present a generative, model-based approach in which each of these factors is modularly encapsulated and learned in a primarily unsupervised manner. Our semantic representation first hypothesizes an underlying set of latent entity types, which generate specific entities that in turn render individual mentions. By sharing lexical statistics at the level of abstract entity types, our model is able to substantially reduce semantic compatibility errors, resulting in the best results to date on the complete end-to-end coreference task.	Coreference Resolution in a Modular Entity-Centered Model Coreference resolution, which is about linking words that refer to the same thing, is controlled by grammar rules, meaning-based rules, and conversation context. We present a new approach that uses a model where each of these rules is separately handled and learned mostly without direct instruction. Our model guesses some hidden categories of entity types that create specific entities, which then connect to individual mentions or references. By sharing word information at the level of these general entity categories, our model reduces mistakes related to meaning, leading to the best results so far in fully connecting all related references in text.	simplicity
430	An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing We present a novel deterministic dependency parsing algorithm that attempts to create the easiest arcs in the dependency structure first in a non-directional manner. Traditional deterministic parsing algorithms are based on a shift-reduce framework: they traverse the sentence from left-to-right and, at each step, perform one of a possible set of actions, until a complete tree is built. A drawback of this approach is that it is extremely local: while decisions can be based on complex structures on the left, they can look only at a few words to the right. In contrast, our algorithm builds a dependency tree by iteratively selecting the best pair of neighbours to connect at each parsing step. This allows incorporation of features from already built structures both to the left and to the right of the attachment point. The parser learns both the attachment preferences and the order in which they should be performed. The result is a deterministic, best-first, O(nlogn) parser, which is significantly more accurate than best-first transition based parsers, and nears the performance of globally optimized parsing models. We observe that parsing time is dominated by feature extraction and score calculation.	An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing We introduce a new straightforward method for dependency parsing, focusing on creating the simplest connections first, without following a specific direction. Traditional methods use a shift-reduce strategy: they move through a sentence from left to right, choosing from a set of actions until they form a complete tree. A downside of this method is that it focuses too much on the immediate next word or words: while it can use complex structures on the left side, it only considers a few words ahead on the right. Our method, on the other hand, builds the tree by repeatedly choosing the best pair of neighboring words to connect at each step. This means it can use information from both sides of the connection point that has already been created. The parser learns both where to attach words and the order in which to do it. The result is a straightforward, best-first parser that works in O(nlogn) time, which is much more accurate than other similar methods and almost as good as the most advanced models. We find that most of the parsing time is spent on finding and calculating the features.	Conditional
431	The viability of web-derived polarity lexicons We examine the viability of building large polarity lexicons semi-automatically from the web. We begin by describing a graph propagation framework inspired by previous work on constructing polarity lexicons from lexical graphs (Kim and Hovy, 2004; Hu and Liu, 2004; Esuli and Sabastiani, 2009; BlairGoldensohn et al., 2008; Rao and Ravichandran, 2009). We then apply this technique to build an English lexicon that is significantly larger than those previously studied. Crucially, this web-derived lexicon does not require WordNet, part-of-speech taggers, or other language-dependent resources typical of sentiment analysis systems. As a result, the lexicon is not limited to specific word classes – e.g., adjectives that occur in WordNet – and in fact contains slang, misspellings, multiword expressions, etc. We evaluate a lexicon derived from English documents, both qualitatively and quantitatively, and show that it provides superior performance to previously studied lexicons, including one derived from WordNet. We construct a graph where the nodes are 20 million candidate words or phrases, selected using a set of heuristics including frequency and mutual information of word boundaries.	The viability of web-derived polarity lexicons We look at how possible it is to create large lists of positive or negative words (polarity lexicons) from the internet, doing it partly by machine. We start by explaining a method that spreads information through a network of words, based on past research on making these lists from word connections (like Kim and Hovy, 2004; Hu and Liu, 2004; Esuli and Sabastiani, 2009; Blair-Goldensohn et al., 2008; Rao and Ravichandran, 2009). We use this method to create a much larger English word list than those studied before. Importantly, this list made from the internet doesn’t need special language tools like WordNet (a database of English words), tools that identify parts of speech (like nouns or verbs), or other resources usually used in systems that analyze opinions and feelings. This means the list isn't just limited to certain types of words, like adjectives in WordNet, and actually includes slang, typos, phrases with multiple words, etc. We test this list made from English texts both by looking at its quality and by measuring its performance, showing that it's better than lists studied before, even one made from WordNet. We make a network where each point is one of 20 million possible words or phrases, chosen using rules based on how often words appear and information about where words start and end.	parsing
432	Batch Tuning Strategies for Statistical Machine Translation There has been a proliferation of recent work on SMT tuning algorithms capable of handling larger feature sets than the traditional MERT approach. We analyze a number of these algorithms in terms of their sentence-level loss functions, which motivates several new approaches, including a Structured SVM. We perform empirical comparisons of eight different tuning strategies, including MERT, in a variety of settings. Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online, and consistently outperforms other options.	Batch Tuning Strategies for Statistical Machine Translation There has been a lot of recent work on improving SMT (Statistical Machine Translation) tuning methods that can manage more features than the old MERT (Minimum Error Rate Training) method. We look at some of these methods based on how they handle errors in sentences, which leads us to try out new methods, like a Structured SVM (Support Vector Machine). We compare eight different tuning methods, including MERT, in different situations. One important finding is that a straightforward and effective group version of MIRA (Margin Infused Relaxed Algorithm) works just as well as learning one-by-one and often does better than other methods.	identify parts
433	Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure. While previous work has focused primarily on English, we extend these results to other languages along two dimensions. First, we show that these results hold true for a number of languages across families. Second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction. Specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%. When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%.	Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure It has been shown that using word groupings (word cluster features) from large collections of text without labels (unlabeled corpora) can greatly improve the way we predict language patterns. Previously, research mostly focused on English, but we are now showing these benefits for other languages in two ways. First, we demonstrate that these benefits apply to many different languages. Second, more excitingly, we introduce a method (algorithm) to create language groupings (clusters) that work across different languages and show that using these groupings makes predicting language patterns across languages much better. Specifically, we show that by adding these clusters to systems that directly transfer language tools, the error rate of tools that understand sentence structure (delexicalized dependency parsers), trained on English and used for other languages, can be reduced by up to 13%. When we use the same approach for tools that recognize names of people, places, etc. (named-entity recognizers), we see improvements of up to 26%.	features
434	Better Evaluation for Grammatical Error Correction We present a novel method for evaluating grammatical error correction. The core of our method, which we call MaxMatch (M2), is an algorithm for efficiently computing the sequence of phrase-level edits between a source sentence and a system hypothesis that achieves the highest overlap with the gold-standard annotation. This optimal edit sequence is subsequently scored using F1 measure. We test our M2 scorer on the Helping Our Own (HOO) shared task data and show that our method results in more accurate evaluation for grammatical error correction. We propose an alternative evaluation scheme which operates in terms of tokens rather than character offsets.	Better Evaluation for Grammatical Error Correction We introduce a new way to judge how well grammatical errors are fixed. Our main tool, called MaxMatch (M2), is a step-by-step process that quickly figures out the best changes needed to match a corrected sentence to the original, correct version. This best sequence of changes is then graded using a method called F1 measure, which is a way to balance accuracy and completeness. We try out our M2 tool on a specific set of data for fixing grammar errors, called the Helping Our Own (HOO) task, and find that it gives a more precise evaluation of how well errors are corrected. We suggest a different way to evaluate, focusing on small parts of words (tokens) instead of the positions of individual characters.	dependency
435	Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters We consider the problem of part-of-speech tagging for informal, online conversational text. We systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy. With these features, our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks; Twitter tagging is improved from 90% to 93% accuracy (more than 3% absolute). Qualitative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre. Additionally, we contribute the first POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines. Tagging software, annotation guidelines, and large-scale word clusters are available at: http://www.ark.cs.cmu.edu/TweetNLP This paper describes release 0.3 of the CMU Twitter Part-of-Speech Tagger and annotated data.	Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters We look at the challenge of identifying parts of speech (like nouns, verbs, etc.) in casual online chat. We test using big groups of similar words found without manual labeling and new word details to make tagging more accurate. Using these methods, our system achieves top results in identifying parts of speech on both Twitter and IRC. For Twitter, accuracy improves from 90% to 93%, which is over a 3% increase. By studying these groups of words, we learn more about language and how it's used in these types of conversations. Plus, we create the first set of rules for marking parts of speech in this kind of text and provide a new collection of English tweets marked following these rules. The tagging software, rules, and big groups of words can be found at: http://www.ark.cs.cmu.edu/TweetNLP This paper explains version 0.3 of the CMU Twitter Part-of-Speech Tagger and the marked data.	Better Evaluation
436	Linguistic Regularities in Continuous Space Word Representations Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, "King - Man + Woman" results in a vector very close to "Queen". We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems. We reach top accuracy on the syntactic subset (an syn) with a CBOW predict model.	Linguistic Regularities in Continuous Space Word Representations Continuous space language models have recently shown excellent results in different tasks. In this paper, we look at word representations in a special way that are automatically learned by the first layer of the model. We discover that these representations are very good at understanding grammar and meaning patterns in language, with each pattern having a specific way of showing the difference between words. This lets us reason with words based on these differences. For example, the difference between male and female is learned automatically, so "King - Man + Woman" gives a result very close to "Queen". We show that these word representations understand grammar patterns using grammar analogy questions (included in this paper) and can correctly answer almost 40% of the questions. We also show that these word representations understand meaning patterns by using the method to answer questions from a test called SemEval-2012 Task 2. Surprisingly, this method does better than the best systems used before. We achieve top accuracy on the grammar-focused part (an syn) with a model called CBOW predict.	collection
437	Robust Temporal Processing Of News We introduce an annotation scheme for temporal expressions, and describe a method for resolving temporal expressions in print and broadcast news. The system, which is based on both hand-crafted and machine-learnt rules, achieves an 83.2% accuracy (F-measure) against hand-annotated data. Some initial steps towards tagging event chronologies are also described. The main part of the system is a temporal expression tagger that employs finite state transducers based on hand-written rules. We work on news and introduce an annotation scheme for temporal expressions, and a method for using explicit temporal expressions to assign activity times to the entirety of an article. We attribute over half the errors of our baseline method to propagation of an incorrect event time to neighboring events.	Robust Temporal Processing Of News We present a system for marking time-related phrases in news, and explain how we identify these time phrases in newspapers and TV news. The system uses both rules written by humans and those learned by computers, and it is correct 83.2% of the time when compared to manually checked data. We also talk about the first steps in marking the order of events. The main tool in the system is a tagger for time phrases that uses a series of steps based on rules written by humans. We focus on news and explain a system for marking time phrases and a method to use these phrases to set time frames for the whole article. More than half of the mistakes in our basic method are due to spreading a wrong event time to nearby events.	Regularities
438	Rule Writing Or Annotation: Cost-Efficient Resource Usage For Base Noun Phrase Chunking This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker: human rule writing and active learning using interactive realtime human annotation. Several novel variations on active learning are investigated, and underlying cost models for cross-modal machine learning comparison are presented and explored. Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment. We use an ensemble based on bagging and partitioning for active learning for base NP chunking.	Rule Writing Or Annotation: Cost-Efficient Resource Usage For Base Noun Phrase Chunking This paper compares two methods for creating a tool that identifies base noun phrases: writing rules by hand and using active learning, which involves real-time human feedback. Several new methods of active learning are tested, and the costs of each method are compared. Results show that using active learning is more efficient and effective than writing rules by hand, even with a similar amount of human effort. We use a mix of techniques, like bagging (combining results from multiple models) and partitioning (dividing the data), for active learning in identifying base noun phrases.	phrases
439	Minimally Supervised Morphological Analysis By Multimodal Alignment This paper presents a corpus-based algorithm capable of inducing inflectional morphological analyses of both regular and highly irregular forms (such as brought→bring) from distributional patterns in large monolingual text with no direct supervision. The algorithm combines four original alignment models based on relative corpus frequency, contextual similarity, weighted string similarity and incrementally retrained inflectional transduction probabilities. Starting with no paired <inflection,root> examples for training and no prior seeding of legal morphological transformations, accuracy of the induced analyses of 3888 past-tense test cases in English exceeds 99.2% for the set, with currently over 80% accuracy on the most highly irregular forms and 99.7% accuracy on forms exhibiting non-concatenative suffixation. We obtain outstanding results at inducing English past tense after beginning with a list of the open class roots in the language, a table of a language's inflectional parts of speech, and the canonical suffixes for each part of speech. We propose an algorithm that extracts morphological rules relating roots and inflected forms of verbs (but the algorithm can be extended to other morphological relations). The supervised morphological learner presented in this paper models lemmatization as a word-final stem change plus a suffix taken from a (possibly empty) list of potential suffixes.	Minimally Supervised Morphological Analysis By Multimodal Alignment This paper introduces a method that uses a collection of written texts to figure out how words change form, like turning "brought" into "bring," by analyzing patterns in a lot of single-language text without direct guidance. The method uses four unique ways to line up text based on how often words appear, how similar their context is, how similar their spelling is, and probabilities that are updated over time on how words change form. It starts without any examples pairing changed words with their base form and without any pre-set rules on how words can change form. The method achieves over 99.2% accuracy in analyzing 3888 past-tense examples in English, more than 80% accuracy on the hardest-to-change forms, and 99.7% accuracy on forms that change by adding parts to the end of words. We get excellent results in figuring out the past tense in English after starting with a list of basic word forms, a table showing how words change depending on their part of speech, and standard endings for each part of speech. We suggest a method that finds rules for how base words and their changed forms relate, especially for verbs, but it can also be extended to other word changes. The method described here treats word change as altering the end of the word and adding an ending from a possible list of endings, which might be empty.	combining results
440	An Improved Error Model For Noisy Channel Spelling Correction The noisy channel model has been applied to a wide range of problems, including spelling correction. These models consist of two components: a source model and a channel model. Very little research has gone into improving the channel model for spelling correction. This paper describes a new channel model for spelling correction, based on generic string to string edits. Using this model gives significant performance improvements compared to previously proposed models. We present an improved error model for noisy channel spelling correction that goes beyond single insertions, deletions, substitutions, and transpositions. We show that adding a source language model increases the accuracy significantly. We characterise the error model by computing the product of operation probabilities on slice-by-slice string edits. We introduce a model that worked on character sequences, not only on character level, and was conditioned on where in the word the sequences occurred.	An Improved Error Model For Noisy Channel Spelling Correction The noisy channel model, which helps correct spelling mistakes, has been used in many areas. These models have two parts: a source model and a channel model. Not much research has focused on making the channel model better for spelling correction. This paper talks about a new channel model for fixing spelling mistakes using general changes from one string to another. This new model works much better than older models. We offer an improved error model that fixes more than just simple errors like adding, removing, changing, or swapping letters. We show that including a source language model, which understands the language better, makes the corrections much more accurate. We describe the error model by calculating the likelihood of different changes happening step-by-step. We present a model that looks at sequences of letters, not just single letters, and considers where these sequences appear in a word.	examples pairing
441	Headline Generation Based On Statistical Translation Extractive summarization techniques cannot generate document summaries shorter than a single sentence, something that is often required. An ideal summarization system would understand each document and generate an appropriate summary directly from the results of that understanding. A more practical approach to this problem results in the use of an approximation: viewing summarization as a problem analogous to statistical machine translation. The issue then becomes one of generating a target document in a more concise language from a source document in a more verbose language. This paper presents results on experiments using this approach, in which statistical models of the term selection and term ordering are jointly applied to produce summaries in a style learned from a training corpus. We approximate the length distribution with a Gaussian. We draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization.	Headline Generation Based On Statistical Translation Extractive summarization techniques can't create document summaries shorter than one sentence, which is often needed. An ideal summarization system would understand each document and make a suitable summary directly from that understanding. A more practical way to solve this problem is to treat summarization like statistical machine translation, which is a method that translates text using mathematical models. The challenge then is to create a short version of a document from a longer one. This paper shows results of experiments using this approach, where statistical models are used to choose and arrange words to make summaries in a style learned from a set of training examples. We estimate the length of summaries using a bell curve (Gaussian). We take ideas from Machine Translation and create headlines using statistical models for picking content and forming sentences.	Channel
442	Improved Statistical Alignment Models In this paper, we present and compare various single-word based alignment models for statistical machine translation. We discuss the five IBM alignment models, the Hidden-Markov alignment model, smoothing techniques and various modifications. We present different methods to combine alignments. As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies. The Alignment Error Rate (AER) measures the fraction of links by which the automatic alignment differs from the reference alignment.	Improved Statistical Alignment Models In this paper, we present and compare different models that match single words for computer-based language translation. We talk about five IBM models for matching, a Hidden-Markov model, methods to smooth out data, and some changes to these models. We explain different ways to join these matches together. To judge how well these models work, we compare the final Viterbi match to a match made by a person. We show that models that consider the order of words and how many words are matched at once give much better results than simpler models like IBM-1 or IBM-2, which don't consider order. The Alignment Error Rate (AER) checks how much the computer-made match is different from the match made by a person.	experiments
443	Statistical Parsing With An Automatically-Extracted Tree Adjoining Grammar We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance. We find that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG. We extract a stochastic tree-insertion grammar or STIG from the WSJ, obtaining 86.6% LP and 86.9% LR for sentences of 40 words.	Statistical Parsing With An Automatically-Extracted Tree Adjoining Grammar We talk about the benefits of using a special type of grammar called lexicalized tree-adjoining grammar (LTAG) instead of another grammar called lexicalized PCFG for understanding sentence structures using statistics. We explain how we create a probabilistic (based on chance) LTAG model using a resource called the Penn Treebank and check how well it works in understanding sentences. We find that our method of creating this model is better than a previous method (by Hwa, 1998) and gives similar results to the lexicalized PCFG. We create a type of grammar called stochastic tree-insertion grammar (STIG) from a news source database called WSJ, achieving 86.6% accuracy for parts of sentences (LP) and 86.9% accuracy for complete sentences (LR) when the sentences are 40 words long.	these models
490	An Improved Extraction Pattern Representation Model For Automatic IE Pattern Acquisition Several approaches have been described for the automatic unsupervised acquisition of patterns for information extraction. Each approach is based on a particular model for the patterns to be acquired, such as a predicate-argument structure or a dependency chain. The effect of these alternative models has not been previously studied. In this paper, we compare the prior models and introduce a new model, the Subtree model, based on arbitrary subtrees of dependency trees. We describe a discovery procedure for this model and demonstrate experimentally an improvement in recall using Subtree patterns. Our method consists of three phases to learn extraction patterns from the source documents for a scenario specified by the user. We use frequent dependency subtrees as measured by TF*IDF to identify named entities and IE patterns important for a given domain. We also propose representations for IE patterns which extends the SVO representation.	An Improved Extraction Pattern Representation Model For Automatic IE Pattern Acquisition Several methods have been explained for automatically finding patterns to pull out information without human guidance. Each method uses a specific way of looking at patterns, like how actions and their details are structured, or how words depend on each other. The impact of these different ways has not been looked at before. In this paper, we compare previous models and present a new model called the Subtree model, which is based on any part of dependency trees. We explain a way to find these patterns and show through experiments that this model improves recall, meaning it can pull out more relevant information. Our method involves three steps to learn how to extract patterns from the source documents for a situation given by the user. We use common parts of dependency trees, identified using TF*IDF (a way to find important words) to spot named entities (specific names or terms) and key patterns for a certain area. We also suggest new ways to represent these patterns that expand on the SVO (Subject-Verb-Object) format.	training
444	Automatic Labeling Of Semantic Roles We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data. We split the task into two sequential subtasks: first, argument recognition decides for each instance whether it bears a semantic role or not; then, argument labelling assigns a label to instances recognised as role-bearers. We present a system that uses completely syntactic features to classify the Frame Elements in a sentence in the FrameNet database.	Automatic Labeling Of Semantic Roles We introduce a system that figures out the meaning-based relationships, or roles, that parts of a sentence have within a specific context. We use different word-based and sentence structure features taken from sentence diagrams to create statistical models from manually labeled example data. The task is divided into two steps: first, argument recognition checks if a part of the sentence has a meaning-based role; then, argument labeling gives a name to the parts that do have roles. We describe a system that uses only sentence structure features to sort the Frame Elements (parts of a sentence) in a database called FrameNet.	Extracted
445	The Structure And Performance Of An Open-Domain Question Answering System This paper presents the architecture, operation and results obtained with the LASSO Question Answering system developed in the Natural Language Processing Laboratory at SMU. To find answers, the system relies on a combination of syntactic and semantic techniques. The search for the answer is based on a novel form of indexing called paragraph indexing. A score of 55.5% for short answers and 64.5% for long answers was achieved at the TREC-8 competition. We transform a natural language question into an IR query. We select as keywords all named entities that were recognized as proper nouns.	The Structure And Performance Of An Open-Domain Question Answering System This paper explains the design, how it works, and the results from using the LASSO Question Answering system created at the Natural Language Processing Laboratory at SMU. To find answers, the system uses a mix of sentence structure and meaning-based methods. It searches for answers using a new way of sorting information called paragraph indexing. In the TREC-8 competition, it scored 55.5% for short answers and 64.5% for long answers. We change a question in everyday language into a search query. We pick out keywords from all named entities, which are names of people, places, or organizations.	database
446	Scaling To Very Very Large Corpora For Natural Language Disambiguation The amount of readily available on-line text has reached hundreds of billions of words and continues to grow. Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less. In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used. We are fortunate that for this particular application, correctly labeled training data is free. Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost. We suggest that the development of very large training corpora may be more effective for progress in empirical Natural Language Processing than improving methods that use existing smaller training corpora. We show that even using a very simple algorithm, the results continue to improve log-linearly with more training data, even out to a billion words.	Scaling To Very Very Large Corpora For Natural Language Disambiguation The amount of easily accessible online text has reached hundreds of billions of words and keeps growing. However, for most main tasks in natural language processing (understanding), algorithms are still being improved, tested, and compared after being trained on collections of text (corpora) that usually have only one million words or less. In this paper, we assess how well different learning methods perform on a typical task of figuring out the correct meaning of words in context, called confusion set disambiguation, when trained on much larger amounts of labeled data than previously used. We are lucky that for this specific application, correctly labeled training data is free. Since this is not always the case, we look at ways to make the most of very large text collections when labeled data costs money. We propose that creating very large training text collections may be more beneficial for advancing practical Natural Language Processing than just improving methods that use the existing smaller collections. We demonstrate that even with a very basic algorithm, the results keep getting better in a predictable way as more training data is used, extending even to a billion words.	answers
447	Extracting Paraphrases From A Parallel Corpus While paraphrasing is critical both for interpretation and generation of natural language, current systems use manual or semi-automatic methods to collect paraphrases. We present an unsupervised learning algorithm for identification of paraphrases from a corpus of multiple English translations of the same source text. Our approach yields phrasal and single word lexical paraphrases as well as syntactic paraphrases. We incorporate part-of-speech information and other morphosyntactic clues into our co-training algorithm.	Extracting Paraphrases From A Parallel Corpus While changing the wording but keeping the meaning is important for understanding and creating natural language, current systems rely on manual or partly automatic methods to gather these reworded phrases. We introduce a fully automated learning method to find paraphrases from a collection of different English translations of the same original text. Our method produces both short phrases and single word rewordings, as well as changes in sentence structure. We include information about word types and other clues about word structure and order into our shared learning method.	larger amounts
448	Immediate-Head Parsing For Language Models We present two language models based upon an “immediate-head” parser — our name for a parser that conditions all events below a constituent c upon the head of c. While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology. The perplexity for both of these models significantly improve upon the trigram model base-line as well as the best previous grammar-based language model. For the better of our two models these improvements are 24% and 14% respectively. We also suggest that improvement of the underlying parser should significantly improve the model’s perplexity and that even in the near term there is a lot of potential for improvement in immediate-head language models. The model presented identifies both syntactic structural and lexical dependencies that aid in language modeling. These contexts include syntactic structure such as parent and grandparent category labels as well as lexical items such as the head of the parent or the head of a sibling constituent.	Immediate-Head Parsing For Language Models We introduce two language models based on an "immediate-head" parser—our term for a parser that uses the main word (head) of a sentence part (constituent) to predict what happens below it. Although the most precise statistical parsers use this type, no previous language model based on grammar uses it. The difficulty level (perplexity) for both of these models is much better compared to the basic trigram model and the best earlier grammar-based language model. For our better model, these improvements are 24% and 14% respectively. We also propose that enhancing the main parser should greatly improve the model's difficulty level and even in the short term, there is a lot of room for improvement in immediate-head language models. The model identifies both sentence structure (syntactic) and word choice (lexical) relationships that help in understanding language. These contexts include sentence structure like parent and grandparent category labels as well as words like the main word (head) of the parent or a sibling part.	current
496	Exploiting Parallel Texts For Word Sense Disambiguation: An Empirical Study A central problem of word sense disambiguation (WSD) is the lack of manually sense-tagged data required for supervised learning. In this paper, we evaluate an approach to automatically acquire sense-tagged training data from English-Chinese parallel corpora, which are then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task. Our investigation reveals that this method of acquiring sense-tagged data is promising. On a subset of the most difficult SENSEVAL-2 nouns, the accuracy difference between the two approaches is only 14.0%, and the difference could narrow further to 6.5% if we disregard the advantage that manually sense-tagged data have in their sense coverage. Our analysis also highlights the importance of the issue of domain dependence in evaluating WSD programs. We address word sense disambiguation by manually annotating WordNet senses with their translation in the target language (Chinese), and then automatically extracting labeled examples for word sense disambiguation by applying the IBM Models to a bilingual corpus. When several senses of an English word are translated by the same Chinese word, we can collapse these senses to obtain a coarser-grained, lumped sense inventory.	Exploiting Parallel Texts For Word Sense Disambiguation: An Empirical Study A main issue with word sense disambiguation (WSD) is not having enough manually labeled examples to teach the computer. In this paper, we look at a way to automatically get labeled examples from English-Chinese text pairs, which we then use to figure out the meanings of nouns in a specific English task called SENSEVAL-2. Our research shows that this method of getting labeled examples is promising. For the hardest SENSEVAL-2 nouns, the success rate difference between the two methods is only 14.0%, and it could get smaller to 6.5% if we ignore the advantage of manually labeled examples having more detailed meanings. Our study also points out how important it is to consider the topic area when testing WSD programs. We solve word sense disambiguation by manually linking WordNet meanings with their translation in Chinese, then automatically pulling out examples using IBM Models from a two-language text collection. When different meanings of an English word are translated into the same Chinese word, we can combine these meanings into a bigger, shared meaning group.	suggest
449	An Algebra For Semantic Construction In Constraint-Based Grammars We develop a framework for formalizing semantic construction within grammars expressed in typed feature structure logics, including HPSG. The approach provides an alternative to the lambda calculus; it maintains much of the desirable flexibility of unification- based approaches to composition, while constraining the allowable operations in order to capture basic generalizations and improve maintainability. The semantic interpretations are expressed using Minimal Recursion Semantics (MRS), which provides the means to represent interpretations with a flat, underspecified semantics using terms of the predicate calculus and generalized quantifiers. An MRS consists of a bag of labeled elementary predicates and their arguments, a list of scoping constraints, and a pair of relations that provide a hook into the representation - a label, which must outscope all the handles, and an index.	An Algebra For Semantic Construction In Constraint-Based Grammars We create a system to build meanings in grammars (rules for structuring sentences) using typed feature structure logics, such as HPSG (Head-driven Phrase Structure Grammar). This method offers an alternative to the lambda calculus; it keeps the useful adaptability of unification-based methods (combining parts smoothly) but limits operations to capture basic patterns and make it easier to manage. The meanings are shown using Minimal Recursion Semantics (MRS), which helps to represent meanings simply and broadly using terms from basic logic and general quantifiers (words that express quantity like "all" or "some"). An MRS includes a collection of basic labeled predicates (statements) and their arguments (details), a list of rules for how they relate, and two connections for linking the representation: a label (a marker that must be more important than all others) and an index (a reference point).	Parsing
450	Methods For The Qualitative Evaluation Of Lexical Association Measures This paper presents methods for a qualitative, unbiased comparison of lexical association measures and the results we have obtained for adjective-noun pairs and preposition-noun-verb triples extracted from German corpora. In our approach, we compare the entire list of candidates, sorted according to the particular measures, to a reference set of manually identified "true positives". We also show how estimates for the very large number of hapaxlegomena and double occurrences can be inferred from random samples. We extract German PP-verb combinations from a chunk-parsed version of the Frankfurter Rundschau Corpus. We use four collocation measures: Point-wise mutual information (PMI); T-Score; log-likelihood; and the raw frequency of N1 N2 in the corpus. The t-test measure, which, of all standard measures, yields the best results in general-language collocation extraction studies.	Methods For The Qualitative Evaluation Of Lexical Association Measures This paper shows methods to fairly compare how well words are associated with each other, looking at adjective-noun pairs and preposition-noun-verb triples from German texts. We compare a full list of word combinations, ranked by different methods, to a set of correct examples identified by people. We explain how to estimate the large numbers of words that appear only once (hapaxlegomena) or twice by using random samples. We gather German preposition-verb pairs from a processed version of the Frankfurter Rundschau Corpus. We use four methods to measure word pairings: Point-wise mutual information (PMI) which measures how often things occur together compared to separately; T-Score which helps find unusual word pairings; log-likelihood which shows how likely it is the words occur together; and the raw count of how often two words appear together in the text. The t-test measure, which is one of the standard methods, gives the best results in studies that look at usual word pairings in general language.	details
451	Fast Decoding And Optimal Decoding For Machine Translation A good decoding algorithm is critical to the success of any statistical machine translation system. The decoder’s job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them). Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions. In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem. We compare translations obtained by a multi-stack decoder and a greedy hill-climbing algorithm against those produced by an optimal integer programming decoder that treats decoding as a variant of the traveling-salesman problem.	Fast Decoding And Optimal Decoding For Machine Translation A good decoding algorithm is very important for the success of any statistical machine translation system. The decoder's task is to find the translation that is most likely correct based on a set of previously learned rules (and a method for combining them). Since there are so many possible translations, typical decoding methods can only look at a small part of them, which means they might miss good solutions. In this paper, we compare the speed and quality of translations from a traditional stack-based decoding method with two new decoders: a fast greedy decoder and a slow but perfect decoder that sees decoding as a math problem to solve. We compare translations from a multi-stack decoder and a greedy hill-climbing method against those from a perfect decoder that treats decoding like a version of the traveling-salesman problem, a well-known math puzzle.	pairings
452	A Statistical Model For Domain-Independent Text Segmentation We propose a statistical method that finds the maximum-probability segmentation of a given text. This method does not require training data because it estimates probabilities from the given text. Therefore, it can be applied to any text in any domain. An experiment showed that the method is more accurate than or at least as accurate as a state-of-the-art text segmentation system. We model the problem of TS as a problem of finding the minimum cost path in a graph and therefore adopts a dynamic programming algorithm. We introduce one of the first probabilistic approaches using Dynamic Programming (DP) called U00.	A Statistical Model For Domain-Independent Text Segmentation We suggest a statistical way to divide text into parts that are most likely to be correct. This method doesn't need prior data to learn from because it calculates chances directly from the text itself. So, it can work on any text, no matter the subject. A test showed that this method is either more precise or just as precise as the latest advanced text segmentation system. We think of text segmentation (TS) as finding the cheapest path in a network and use a step-by-step calculation method called dynamic programming. We present one of the first methods that use chances and dynamic programming, named U00.	translation
457	A Generative Constituent-Context Model For Improved Grammar Induction We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts. Parameter search with EM produces higher quality analyses than previously exhibited by unsupervised systems, giving the best published unsupervised parsing results on the ATIS corpus. Experiments on Penn treebank sentences of comparable length show an even higher F1 of 71% on non-trivial brackets. We compare distributionally induced and actual part-of-speech tags as input data, and examine extensions to the basic model. We discuss errors made by the system, compare the system to previous models, and discuss upper bounds, lower bounds, and stability for this task. We induce parts-of-speech from the full WSJ tree bank together with additional WSJ newswire.	A Generative Constituent-Context Model For Improved Grammar Induction We introduce a model that helps computers understand the structure of language without needing labeled examples. It works by looking at parts of sentences and their surroundings. Using a method called EM to adjust the model's settings, we achieve better results than other methods, especially when tested on the ATIS dataset. When we tried it on another set of sentences similar in length, we got an F1 score of 71%, which measures how well the model finds sentence parts. We compare different ways of labeling words and look at ways to improve the model. We talk about mistakes the system makes, how it stacks up against older models, and discuss its potential and limitations. We also identify word types from a large set of Wall Street Journal articles.	method called
453	A Syntax-Based Statistical Translation Model We present a syntax-based statistical translation model. Our model transforms a source-language parse tree into a target-language string by applying stochastic operations at each node. These operations capture linguistic differences such as word order and case marking. Model parameters are estimated in polynomial time using an EM algorithm. The model produces word alignments that are better than those produced by IBM Model 5. We use a parser in the target language to train probabilities on a set of 609 operations that transform a target parse tree into a source string. We present an algorithm for estimating probabilistic parameters for a model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures.	A Syntax-Based Statistical Translation Model We present a syntax-based statistical translation model. Our model changes a source-language parse tree (a structure that shows how a sentence is put together) into a target-language string by using random operations at each part of the tree. These operations deal with language differences like word order (how words are arranged) and case marking (how words change depending on their role in a sentence). Model parameters are estimated in polynomial time using an EM algorithm (a method to find the best estimates for the model). The model creates word alignments (matching words from one language to another) that are better than those by IBM Model 5. We use a parser in the target language to train probabilities on a set of 609 operations that change a target parse tree into a source string. We present an algorithm for estimating probabilistic parameters for a model which shows translation as a series of re-ordering steps over parts of nodes in a syntactic tree, using automatic parser output for the starting tree structures.	precise
454	Parameter Estimation For Probabilistic Finite-State Transducers Weighted finite-state transducers suffer from the lack of a training algorithm. Training is even harder for transducers that have been assembled via finite-state operations such as composition, minimization, union, concatenation, and closure, as this yields tricky parameter tying. We formulate a “parameterized FST” paradigm and give training algorithms for it, including a general bookkeeping trick (“expectation semirings”) that cleanly and efficiently computes expectations and gradients. We use finite-state operations such as composition, which do combine weights entirely within the expectation semiring before their result is passed to the forward-backward algorithm. We claim that parsing under an expectation semiring is equivalent to the Inside-Outside algorithm for PCFGs. We give a general EM algorithm for parameter estimation in probabilistic finite-state transducers. We describe the expectation semiring for parameter learning.	Parameter Estimation For Probabilistic Finite-State Transducers Weighted finite-state transducers, which are mathematical models used to process sequences, don't have a clear way to be trained or taught. It's even more challenging to train them when they are created using complex methods like combining, simplifying, joining, and repeating, as this makes it difficult to manage the settings or parameters. We propose a new way to handle these transducers, called "parameterized FST," and provide methods to teach them, including a clever method ("expectation semirings") that helps calculate important values and changes in an organized and effective way. We use methods that combine different parts of the transducer, working within the "expectation semiring" system, before using another method called the forward-backward algorithm. We suggest that using the expectation semiring is similar to a known method (Inside-Outside algorithm) used for another type of grammar models (PCFGs). We present a general method (EM algorithm) for figuring out the best settings for these probabilistic finite-state transducers. We explain how the expectation semiring helps with learning the parameters.	algorithm
455	Learning Surface Text Patterns For A Question Answering System In this paper we explore the power of surface text patterns for open-domain question answering systems. In order to obtain an optimal set of patterns, we have developed a method for learning such patterns automatically. A tagged corpus is built from the Internet in a bootstrapping process by providing a few hand-crafted examples of each question type to Altavista. Patterns are then automatically extracted from the returned documents and standardized. We calculate the precision of each pattern, and the average precision for each question type. These patterns are then applied to find answers to new questions. Using the TREC-10 question set, we report results for two cases: answers determined from the TREC-10 corpus and from the web. We present an alternatve ontology for type preference and describe a method for using this alternative ontology to extract particular answers using surface text patterns.	Learning Surface Text Patterns For A Question Answering System In this paper, we look into how useful simple text patterns are for systems that answer questions on any topic. To find the best set of patterns, we've created a way to learn these patterns automatically. We create a tagged collection of text from the Internet by starting with a few examples made by hand for each question type and using Altavista, a search engine. Patterns are then automatically taken from the search results and made uniform. We measure how accurate each pattern is and find the average accuracy for each question type. These patterns are then used to find answers to new questions. Using the TREC-10 question set, we share results for two situations: answers found in the TREC-10 text collection and from the web. We offer a different way to classify question types and explain a method for using this classification to find specific answers using simple text patterns.	known method
456	Improving Machine Learning Approaches To Coreference Resolution We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC-6 and MUC-7 coreference resolution data sets - F-measures of 70.4 and 63.4, respectively. Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge. In the testing phase, we used the best-first clustering. We expand the feature set of Soon et al (2001) from 12 to 53 features. We propose a rule-induction system with rule pruning.	Improving Machine Learning Approaches To Coreference Resolution We present a system that identifies when different noun phrases (like names or things) refer to the same person or object. This system builds on the work of Soon et al. (2001) and, as far as we know, it gives the best results so far on two standard tests for this task, called MUC-6 and MUC-7, with scores of 70.4 and 63.4. We improved the system in two main ways: by making changes not related to language in how the system learns and by greatly increasing the number of features, or characteristics, it uses to include more advanced language understanding. During testing, we used a method called best-first clustering, which groups similar items together. We increased the number of features used by Soon et al. (2001) from 12 to 53. We suggest a system that creates rules and then simplifies them by removing unnecessary ones.	patterns
458	A Simple Pattern-Matching Algorithm For Recovering Empty Nodes And Their Antecedents This paper describes a simple pattern matching algorithm for recovering empty nodes and identifying their co-indexed antecedents in phrase structure trees that do not contain this information. The patterns are minimal connected tree fragments containing an empty node and all other nodes co-indexed with it. This paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure, which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a gold standard corpus. Evaluating the algorithm on the output of Charniak’s parser (Charniak, 2000) and the Penn treebank (Marcus et al., 1993) shows that the pattern matching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity. We propose an algorithm that is able to find long-distance dependencies, as a post processing step, after parsing. While Charniak's parser does not generate empty category information, we have developed an algorithm that extracts patterns from the Treebank which can be used to insert empty categories into the parser's output. It is the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees.	A Simple Pattern-Matching Algorithm For Recovering Empty Nodes And Their Antecedents This paper explains an easy method for finding missing parts, called empty nodes, in sentence structures and identifying the parts they relate to, called antecedents, even when the sentence structures don't show this information. The method uses small connected pieces of the sentence structure that include the empty node and all related parts. The paper also suggests a way to evaluate how well different methods find these empty nodes, without focusing too much on specific sentence details, allowing comparison with a high-quality example set. Testing the method using Charniak’s parser (Charniak, 2000) and the Penn treebank (Marcus et al., 1993) reveals that the simple method works surprisingly well on the most common types of empty nodes. We suggest a method that can identify distant connections between parts of a sentence after the initial analysis is done. Although Charniak's parser doesn't recognize empty parts, we've created a method to find patterns in the Treebank to add these missing parts to the parser's results. It's the first method to find long-distance connections after the main analysis by using a simple pattern-finding method on basic sentence structures.	identify
459	Pronunciation Modeling For Improved Spelling Correction This paper presents a method for incorporating word pronunciation information in a noisy channel model for spelling correction. The proposed method builds an explicit error model for word pronunciations. By modeling pronunciation similarities between words we achieve a substantial performance improvement over the previous best performing models for spelling correction. We consider a pronunciation variation model to generate multiple pronunciations for each canonical pronunciation in a pronouncing dictionary. We extend Brill and Moore (2000) to consider edits over both letter sequences and sequences of phones in the pronunciations of the word and misspelling. We use the noisy channel model approach to determine the types and weights of edit operations. Since a spelling correction model needs to rank candidate words rather than candidate pronunciations, we derive an error model that determines the probability that a word w was spelled as the non-word r based on their pronunciations.	Pronunciation Modeling For Improved Spelling Correction This paper presents a method for adding word pronunciation information to a model that helps correct spelling mistakes. The proposed method creates a clear error model for how words sound. By understanding the similarities in how words are pronounced, we greatly improve the performance over the best models used before for correcting spelling. We use a pronunciation variation model to create multiple ways of pronouncing each standard pronunciation in a pronunciation dictionary. We expand on the work by Brill and Moore (2000) to consider changes in both letter sequences and sequences of sounds (phones) in the pronunciations of the word and its misspelling. We use the noisy channel model approach to figure out the types and importance (weights) of editing actions. Since a spelling correction model needs to rank candidate words instead of candidate pronunciations, we create an error model that calculates the chance that a word w was spelled as the non-word r based on how they sound.	called antecedents
460	GATE: A Framework And Graphical Development Environment For Robust NLP Tools And Applications In this paper we present GATE, a framework and graphical development environment which enables users to develop and deploy language engineering components and resources in a robust fashion. The GATE architecture has enabled us not only to develop a number of successful applications for various language processing tasks (such as Information Extraction), but also to build and annotate corpora and carry out evaluations on the applications generated. The framework can be used to develop applications and resources in multiple languages, based on its thorough Unicode support. We include the ANNIE IE system in the standard GATE distribution for text tokenization, sentence splitting and part-of-speech tagging. We propose mechanisms to help heterogeneous linguistic modules to communicate through a common XML interface.	GATE: A Framework And Graphical Development Environment For Robust NLP Tools And Applications In this paper, we introduce GATE, a system and visual tool that helps users create and use language processing tools and resources in a reliable way. The GATE design allows us to make various successful tools for language tasks (like Information Extraction), build and label text collections (corpora), and test the tools we create. The system can create tools and resources in many languages because it supports Unicode, which is a standard for text encoding. We provide the ANNIE IE system with GATE for breaking down text, splitting sentences, and tagging parts of speech (identifying words as nouns, verbs, etc.). We suggest ways to help different language tools work together using a shared XML format (a standard way to organize data).	correction
461	The Necessity Of Parsing For Predicate Argument Recognition Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time. Statistical systems have been trained to automatically label semantic roles from the output of statistical parsers on unannotated text. In this paper, we quantify the effect of parser accuracy on these systems' performance, and examine the question of whether a flatter "chunked" representation of the input can be as effective for the purposes of semantic role identification. We note that this deep syntax feature plays an important role in connecting semantic role with surface grammatical function. We experiment with the set of features: Pred HW, Arg HW, Phrase Type, Position, Path, Voice.	The Necessity Of Parsing For Predicate Argument Recognition Broad-coverage text collections marked with information on semantic roles (roles in a sentence) are now available. Statistical systems have been trained to automatically label these roles using the results from statistical parsers (tools that analyze sentence structure) on unmarked text. In this paper, we measure how parser accuracy affects these systems' performance and explore whether a simpler "chunked" (grouped) version of the input can be just as effective for identifying semantic roles. We note that this deep syntax feature (detailed sentence structure) is important for linking semantic roles with surface grammatical functions (basic sentence parts). We test with a set of features: Pred HW (main word of the predicate), Arg HW (main word of the argument), Phrase Type, Position, Path, Voice.	create tools
462	An Unsupervised Method For Word Sense Tagging Using Parallel Corpora We present an unsupervised method for word sense disambiguation that exploits translation correspondences in parallel corpora. The technique takes advantage of the fact that cross-language lexicalizations of the same concept tend to be consistent, preserving some core element of its semantics, and yet also variable, reflecting differing translator preferences and the influence of context. Working with parallel corpora introduces an extra complication for evaluation, since it is difficult to find a corpus that is both sense tagged and parallel with another language; therefore we use pseudo-translations, created by machine translation systems, in order to make possible the evaluation of the approach against a standard test set. The results demonstrate that word-level translation correspondences are a valuable source of information for sense disambiguation. We present an unsupervised approach to WSD that exploits translational correspondences in parallel corpora that are artificially created by applying commercial MT systems on a sense-tagged English corpus. Cross-language tagging is the goal and we present a method for word sense tagging both the source and target texts of parallel bilingual corpora with the WordNet sense inventory.	An Unsupervised Method For Word Sense Tagging Using Parallel Corpora We introduce an unsupervised way, meaning it doesn't need human guidance, to figure out the meaning of words by using translations in parallel texts. This method uses the idea that translating the same idea into different languages usually keeps the main meaning but can change based on translator choices and context. Using parallel texts (same content in two languages) can be tricky to evaluate because it's hard to find texts that are both tagged for meaning and translated into another language. So, we use fake translations made by machine translation systems to test our method with a standard set of examples. The findings show that translating words is a useful tool for understanding their meanings. We introduce a method that doesn't need human guidance to figure out word meanings by using translations in parallel texts created by using commercial machine translation systems on a set of English texts that have been tagged for meaning. Our goal is to tag words in both the original and translated texts with their meanings using the WordNet sense inventory, which is a database of word meanings.	syntax feature
463	New Ranking Algorithms For Parsing And Tagging: Kernels Over Discrete Structures And The Voted Perceptron This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the “all subtrees” (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data. Convolution kernels are used to implicitly define a tree substructure space. The tree kernel is proposed for syntactic parsing reranking. Tree kernels evaluate the similarity between two trees in terms of their overlap, generally measured as the number of common substructures.	New Ranking Algorithms For Parsing And Tagging: Kernels Over Discrete Structures And The Voted Perceptron This paper introduces new methods for teaching computers to understand human language, using a simple learning method called the perceptron algorithm. We explain how these methods can be used effectively with very large representations of sentence structures, like the "all subtrees" method (a way to represent all parts of a sentence tree) described by Bod in 1998, or a method that keeps track of all small sentence parts. We provide test results showing big improvements in two tasks: understanding Wall Street Journal text, and finding specific names or terms in web data. Convolution kernels (a tool to analyze data) are used to define the space of tree parts without making them explicitly. The tree kernel is suggested for improving the order of possible sentence structures. Tree kernels compare how similar two sentence trees are by looking at how many common parts they share.	meanings
464	Parsing The Wall Street Journal Using A Lexical-Functional Grammar And Discriminative Estimation Techniques We present a stochastic parsing system consisting of a Lexical-Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model. We report on the results of applying this system to parsing the UPenn Wall Street Journal (WSJ) treebank. The model combines full and partial parsing techniques to reach full grammar coverage on unseen data. The treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models. Disambiguation performance is evaluated by measuring matches of predicate-argument relations on two distinct test sets. On a gold standard of manually annotated f-structures for a subset of the WSJ treebank, this evaluation reaches 79% F-score. An evaluation on a gold standard of dependency relations for Brown corpus data achieves 76% F-score. We describe a discriminative LFG parsing model that is trained on standard (syntax only) tree bank annotations by treating each tree as a full LFG analysis with an observed c-structure and hidden f-structure. XLE selects the most probable analysis from the potentially large candidate set by means of a stochastic disambiguation component based on a log-linear probability model that works on the packed representations.	Parsing The Wall Street Journal Using A Lexical-Functional Grammar And Discriminative Estimation Techniques We present a system for analyzing sentences that uses a Lexical-Functional Grammar (LFG), which is a set of language rules, a parser that follows these rules, and a method to choose the best interpretation based on probability. We discuss how this system was used to analyze sentences from the UPenn Wall Street Journal (WSJ) database. The system uses methods to fully and partially understand sentences to cover all grammar rules for new data. The WSJ database, which has some labeled sentence data, helps in training the system using statistical methods to improve accuracy. We evaluate how well the system removes confusion by checking if it correctly identifies relationships between actions and things in sentences on two different test sets. For a subset of WSJ data with manually checked sentence structures, the system achieves a 79% F-score, which measures accuracy. For Brown corpus data, which is another set of sentence examples, it scores 76% F-score. We explain a model that learns to understand sentences using standard sentence examples by considering each sentence as having a visible structure and a hidden meaning structure. XLE, a tool within the system, picks the most likely interpretation from many possibilities using a probabilistic method that relies on a mathematical model to handle complex sentence data.	analyze
465	Discriminative Training And Maximum Entropy Models For Statistical Machine Translation We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine translation system is significantly improved using this approach.	Discriminative Training And Maximum Entropy Models For Statistical Machine Translation We present a framework for statistical machine translation (changing text from one language to another using statistics) of natural languages based on direct maximum entropy models (a way to predict things based on given information), which includes the commonly used source-channel approach as a special case. All sources of information are treated as feature functions (specific rules or patterns), which depend on the source language sentence, the target language sentence, and possible hidden variables (unknown factors that might affect the outcomes). This approach allows a basic machine translation system to be improved easily by adding new feature functions. We show that a basic statistical machine translation system is significantly improved using this approach.	manually
466	A Decoder For Syntax-Based Statistical MT This paper describes a decoding algorithm for a syntax-based translation model (Yamada and Knight, 2001). The model has been extended to incorporate phrasal translations as presented here. In contrast to a conventional word-to-word statistical model, a decoder for the syntax-based model builds up an English parse tree given a sentence in a foreign language. As the model size becomes huge in a practical setting, and the decoder considers multiple syntactic structures for each word alignment, several pruning techniques are necessary. We tested our decoder in a Chinese-to-English translation system, and obtained better results than IBM Model 4. We also discuss issues concerning the relation between this decoder and a language model. We propose a syntax-based decoder that restrict word reordering based on reordering operations on syntactic parse-trees of the input sentence.	A Decoder For Syntax-Based Statistical MT This paper explains a decoding method for a syntax-based translation model, originally by Yamada and Knight in 2001. This model has been improved to include group translations, as shown here. Unlike a traditional word-for-word statistical model, a decoder for this syntax-based model creates an English sentence structure (parse tree) from a sentence in another language. As the model becomes very large in real-world use and the decoder looks at different word arrangements, several trimming methods are needed. We tested our decoder in a Chinese-to-English translation system and got better outcomes than IBM Model 4. We also talk about the connection between this decoder and a language model, which helps predict the next word in a sentence. We suggest a syntax-based decoder that limits changing the order of words by using changes on the sentence's structure.	target language
467	Bleu: A Method For Automatic Evaluation Of Machine Translation Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. BLEU is a system for automatic evaluation of machine translation. BLEU is based on measuring string level similarity between the reference translation and translation hypothesis.	Bleu: A Method For Automatic Evaluation Of Machine Translation Human evaluations of machine translation take a lot of time and cost a lot. Human evaluations can take months to complete and require human effort that can't be reused. We suggest a way to automatically evaluate machine translation that's fast, cheap, and works with any language. It matches well with human evaluation and doesn't cost much extra each time you use it. BLEU is a system that automatically checks machine translation. BLEU works by comparing how similar the translated text is to a reference translation.	creates
468	Building Deep Dependency Structures Using A Wide-Coverage CCG Parser This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures. The parser differs from most existing wide-coverage treebank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies. A set of dependency structures used for training and testing the parser is obtained from a treebank of CCG normal-form derivations, which have been derived (semi-) automatically from the Penn Treebank. The parser correctly recovers over 80% of labelled dependencies, and around 90% of unlabelled dependencies. We provide examples showing how heads can fill dependency slots during a derivation, and how long-range dependencies can be recovered through unification of co-indexed head variables. We define predicate argument structure for CCG in terms of the dependencies that hold between words with lexical functor categories and their arguments.	Building Deep Dependency Structures Using A Wide-Coverage CCG Parser This paper talks about a statistical parser, which is like a computer program that analyzes sentences, that uses Combinatory Categorial Grammar (CCG) to create structures showing how words depend on each other. Unlike most other parsers that cover a lot of language, this one can understand complex sentence parts that are far apart, like when words are joined together (coordination), when something is taken out (extraction), when subjects and verbs are switched (raising), and when actions are controlled by other verbs. It also handles the usual nearby word relationships. The parser is trained and tested using a set of these word dependency structures, which come from a collection of CCG sentence analyses that were partially automatically created from the Penn Treebank, a big database of sentences. The parser can correctly figure out over 80% of the labeled word connections and about 90% of the unlabeled ones. We give examples to show how main words can fit into dependency slots while analyzing a sentence, and how far-apart connections can be understood by matching linked main word variables. We explain the structure of how verbs and their arguments relate in CCG using the dependencies between words with special categories and their arguments.	translated
469	Generative Models For Statistical Parsing With Combinatory Categorial Grammar This paper compares a number of generative probability models for a wide-coverage Combinatory Categorial Grammar (CCG) parser. These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations. According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to the figures given by Collins (1999) for a linguistically less expressive grammar. In contrast to Gildea (2001), we find a significant improvement from modeling word-word dependencies. The CCG combinatory rules are encoded as rule instances, together with a number of additional rules which deal with punctuation and type-changing. The dependency features are defined in terms of the local rule instantiations, by adding the heads of the combining categories to the rule instantiation features.	Generative Models For Statistical Parsing With Combinatory Categorial Grammar This paper compares different methods for creating probability models to help a computer understand language using a system called Combinatory Categorial Grammar (CCG). These methods are trained and tested using a set of language examples that are converted from another language system called the Penn Treebank into a form that CCG can use. When checking how well these methods understand relationships between words, our best method got a score of 89.9%, which is similar to results from a simpler language system by Collins (1999). Unlike Gildea (2001), we found that focusing on how words relate to each other significantly improves results. The CCG rules, which help combine words and phrases, are represented by specific examples, along with extra rules for punctuation and changing word types. The features for understanding word relationships are defined by looking at these specific examples and adding important words from the categories being combined.	partially automatically
470	Bootstrapping This paper refines the analysis of co-training, defines and evaluates a new co-training algorithm that has theoretical justification, gives a theoretical justification for the Yarowsky algorithm, and shows that co-training and the Yarowsky algorithm are based on different independence assumptions. We show that the independence assumption can be relaxed, and co-training is still effective under a weaker independence assumption. We refine Dasgupta et al's result by relaxing the view independence assumption with a new constraint. We propose the Greedy Agreement Algorithm, which, based on two independent views of the data, learns two binary classifiers from a set of hand-typed seed rules. We show that if certain independence conditions between the classifier rules are satisfied and the precision of each rule is larger than a threshold T, then the precision of the final classifier is larger than T. We argue that the conditional independence assumption is remarkably strong and is rarely satisfied in real data sets, showing that a weaker independence assumption suffices.	Bootstrapping This paper improves the study of co-training, explains and tests a new co-training method that has a solid reason for its use, explains the reasoning behind the Yarowsky method, and shows that co-training and the Yarowsky method rely on different independence ideas. We demonstrate that the idea of independence can be loosened, and co-training still works well with a less strict independence idea. We improve Dasgupta et al's finding by loosening the requirement of view independence with a new condition. We introduce the Greedy Agreement Algorithm, which uses two separate perspectives of the data to create two simple decision-makers from a set of hand-created starting rules. We demonstrate that if certain independence conditions between these decision-maker rules are met and the accuracy of each rule is higher than a specific level T, then the accuracy of the final decision-maker is higher than T. We claim that the assumption of conditional independence is very strong and is not often found in real data sets, indicating that a less strict independence assumption is enough.	punctuation
471	An Unsupervised Approach To Recognizing Discourse Relations We present an unsupervised approach to recognizing discourse relations of CONTRAST, EXPLANATION-EVIDENCE, CONDITION and ELABORATION that hold between arbitrary spans of texts. We show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases. We use a pattern based approach to extract instances of discourse relations such as Contrast and Elaboration from unlabeled corpora. We propose a method to identify discourse relations between text segments using Naive Bayes classifiers trained on a huge corpus.	An Unsupervised Approach To Recognizing Discourse Relations We present a method that doesn't require pre-labeled data (unsupervised approach) to identify connections between parts of text, like CONTRAST (showing differences), EXPLANATION-EVIDENCE (providing reasons or proof), CONDITION (if-then situations), and ELABORATION (adding details). We demonstrate that classifiers, which are like smart sorting programs, trained on examples automatically gathered from large amounts of text, can tell some of these connections apart with up to 93% accuracy, even if there are no obvious hint words. We use a pattern-based method to find examples of these connections, such as Contrast and Elaboration, from large collections of text that haven't been labeled. We suggest a way to find connections between parts of text using a simple statistical method (Naive Bayes classifiers) trained on a very large set of text.	independence
472	Evaluating Translational Correspondence Using Annotation Projection Recently, statistical machine translation models have begun to take advantage of higher level linguistic structures such as syntactic dependencies. Underlying these models is an assumption about the directness of translational correspondence between sentences in the two languages; however, the extent to which this assumption is valid and useful is not well understood. In this paper, we present an empirical study that quantifies the degree to which syntactic dependencies are preserved when parses are projected directly from English to Chinese. Our results show that although the direct correspondence assumption is often too restrictive, a small set of principled, elementary linguistic transformations can boost the quality of the projected Chinese parses by 76% relative to the unimproved baseline. The dependency projection method DPA (Hwa et al, 2005) based on Direct Correspondence Assumption (Hwa et al, 2002) can be described as: if there is a pair of source words with a dependency relationship, the corresponding aligned words in target sentence can be considered as having the same dependency relationship equivalently. We align the parallel sentences using phrase based statistical MT models and then projected the alignments back to the parse trees.	Evaluating Translational Correspondence Using Annotation Projection Recently, computer programs that translate languages have started using more advanced language structures, like how words depend on each other. These programs assume that sentences in different languages match up directly, but we don't fully understand if this assumption is correct or useful. In this study, we measure how well these word dependencies stay the same when we translate directly from English to Chinese. Our findings indicate that although assuming direct matching is often too strict, making a few basic language adjustments can improve the quality of Chinese translations by 76% compared to no improvements. The method called dependency projection (DPA) based on the Direct Correspondence Assumption means that if two words are connected in one language, their matching words in the other language are assumed to be connected in the same way. We match sentences in both languages using translation models based on phrases and then apply these matches to sentence structure diagrams.	large amounts
473	Translating Named Entities Using Monolingual And Bilingual Resources Named entity phrases are some of the most difficult phrases to translate because new phrases can appear from nowhere, and because many are domain specific, not to be found in bilingual dictionaries. We present a novel algorithm for translating named entity phrases using easily obtainable monolingual and bilingual resources. We report on the application and evaluation of this algorithm in translating Arabic named entities to English. We also compare our results with the results obtained from human translations and a commercial system for the same task. We show that use of outside linguistic resources such as WWW counts of transliteration candidates can greatly boost transliteration accuracy. A spelling-based model is described that directly maps English letter sequences into Arabic letter sequences with associated probability that are trained on a small English/Arabic name list without the need for English pronunciations. The phonetics-based and spelling-based models have been linearly combined into a single transliteration model. We use Web statistics information to validate the translation candidates generated by language model, and obtained the accuracy of 72.6% in Arabic-English OOV word translation.	Translating Named Entities Using Monolingual And Bilingual Resources Named entity phrases are some of the hardest to translate because new ones can appear suddenly, and many are specific to certain areas or topics and aren't in bilingual dictionaries. We introduce a new method for translating these phrases using easily available resources in one language and two languages. We tested this method on translating Arabic named entities into English. We also compared our results with human translations and a commercial translation system doing the same job. We show that using extra language resources like online counts of possible spellings can significantly improve translation accuracy. We describe a model based on spelling that directly changes English letter sequences into Arabic ones, with a probability that is learned from a small list of English and Arabic names, without needing English pronunciations. The models based on sounds (phonetics) and spelling have been combined into one translation model. We use information from the Web to check the translation options created by the language model, achieving an accuracy of 72.6% in translating Arabic words not found in English dictionaries (OOV words).	languages using
474	Thumbs Up Or Thumbs Down? Semantic Orientation Applied To Unsupervised Classification Of Reviews This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., “subtle nuances”) and a negative semantic orientation when it has bad associations (e.g., “very cavalier”). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “excellent” minus the mutual information between the given phrase and the word “poor”. A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84% for automobile reviews to 66% for movie reviews. We describe a way to automatically build a lexicon based on looking at co-occurrences of words with other words whose sentiment is known.	Thumbs Up Or Thumbs Down? Semantic Orientation Applied To Unsupervised Classification Of Reviews This paper introduces a simple method that doesn't need human supervision to decide if reviews are good (thumbs up) or bad (thumbs down). The decision is made by looking at the average meaning of phrases with describing words (adjectives or adverbs) in the review. A phrase is seen as positive if it has good meanings (like “subtle nuances”) and negative if it has bad meanings (like “very careless”). The paper explains that to find out if a phrase is positive or negative, they compare how often a phrase appears with the word “excellent” versus the word “poor”. A review is marked as good if the average meaning of its phrases is positive. This method correctly identifies whether a review is good or bad 74% of the time when tested on 410 reviews from different areas like cars, banks, movies, and travel spots. The success rate is highest at 84% for car reviews and lowest at 66% for movie reviews. We explain how to automatically create a word list by checking which words often appear together with words known to have certain feelings or opinions.	directly changes
475	Named Entity Recognition Using An HMM-Based Chunk Tagger This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities. Through the HMM, our system is able to apply and integrate four types of internal and external evidences: 1) simple deterministic internal feature of the words, such as capitalization and digitalization; 2) internal semantic feature of important triggers; 3) internal gazetteer feature; 4) external macro context feature. In this way, the NER problem can be resolved effectively. Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6% and 94.1% respectively. It shows that the performance is significantly better than reported by any other machine-learning system. Moreover, the performance is even consistently better than those based on handcrafted rules. Our named entity recognition system recognizes various types of MUC-style named entities such as organization, location, person, date, time, money and percentage.	Named Entity Recognition Using An HMM-Based Chunk Tagger This paper suggests using a Hidden Markov Model (HMM) and a special tool called an HMM-based chunk tagger to create a system that can find and classify names, times, and numbers. With the HMM, our system can use and combine four types of clues: 1) simple internal word features like if a word starts with a capital letter or is a number; 2) important word meanings that act as triggers; 3) lists of names and terms (gazetteer feature); 4) the larger context around the words. This approach helps solve the named entity recognition problem effectively. When tested on specific English tasks, our system achieved scores of 96.6% and 94.1%, which are very high. This means our system works better than any other machine-learning system reported so far. It even performs better than systems that use detailed human-made rules. Our system can identify different types of named entities like organizations, locations, people, dates, times, money, and percentages.	movie reviews
476	Ranking Algorithms For Named Entity Extraction: Boosting And The Voted Perceptron This paper describes algorithms which rerank the top N hypotheses from a maximum-entropy tagger, the application being the recovery of named-entity boundaries in a corpus of web data. The first approach uses a boosting algorithm for ranking problems. The second approach uses the voted perceptron algorithm. Both algorithms give comparable, significant improvements over the maximum-entropy baseline. The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples. We describe a mapping from words to word types which groups words with similar orthographic forms into classes.	Ranking Algorithms For Named Entity Extraction: Boosting And The Voted Perceptron This paper talks about methods that reorder the best N guesses from a tool that labels parts of text, specifically for finding named entities (like names and places) in web data. The first method uses a boosting algorithm, which is a technique to make predictions better. The second method uses the voted perceptron algorithm, which is another technique for improving predictions. Both methods show similar and important improvements compared to the original labeling tool. The voted perceptron method can be much quicker to train, but it might take more time to process test examples. We explain a way to group words with similar letter patterns into categories.	Based Chunk
477	Offline Strategies For Online Question Answering: Answering Questions Before They Are Asked Recent work in Question Answering has focused on web-based systems that extract answers using simple lexico-syntactic patterns. We present an alternative strategy in which patterns are used to extract highly precise relational information offline, creating a data repository that is used to efficiently answer questions. We evaluate our strategy on a challenging subset of questions, i.e. “Who is …” questions, against a state of the art web-based Question Answering system. Results indicate that the extracted relations answer 25% more questions correctly and do so three orders of magnitude faster than the state of the art system. We use part of speech patterns to extract a subset of hyponym relations involving proper nouns. The precision of the extracted information can be improved significantly by using machine learning methods to filter out noise.	Offline Strategies For Online Question Answering: Answering Questions Before They Are Asked Recent work in Question Answering has focused on web-based systems that find answers using simple word and grammar patterns. We present a different strategy where patterns are used to gather very accurate relational information ahead of time, creating a data collection that is used to quickly answer questions. We test our strategy on a difficult group of questions, like "Who is …" questions, against a top web-based Question Answering system. Results show that the gathered relations correctly answer 25% more questions and do so a thousand times faster than the top system. We use word type patterns to find a group of "type of" relations involving specific names. The accuracy of the gathered information can be greatly improved by using machine learning methods to remove irrelevant data.	similar letter
478	Using Predicate-Argument Structures For Information Extraction In this paper we present a novel, customizable IE paradigm that takes advantage of predicate-argument structures. We also introduce a new way of automatically identifying predicate argument structures, which is central to our IE paradigm. It is based on: (1) an extended set of features; and (2) inductive decision tree learning. The experimental results prove our claim that accurate predicate-argument structures enable high quality IE results. We apply semantic parsing to capture the predicate-argument sentence structure.	Using Predicate-Argument Structures For Information Extraction In this paper, we present a new, adaptable method for Information Extraction (IE) that uses predicate-argument structures, which are sentence patterns showing relationships between actions and entities. We also introduce a new automatic method to identify these patterns, which is crucial for our IE approach. It is based on: (1) a larger set of characteristics; and (2) using decision tree learning, a method that helps computers make decisions. The test results support our claim that precise predicate-argument patterns lead to high-quality IE results. We use semantic parsing, a technique to understand the meaning of sentences, to identify the predicate-argument structure in sentences.	questions
479	A Noisy-Channel Approach To Question Answering We introduce a probabilistic noisy-channel model for question answering and we show how it can be exploited in the context of an end-to-end QA system. Our noisy-channel system outperforms a state-of-the-art rule-based QA system that uses similar resources. We also show that the model we propose is flexible enough to accommodate within one mathematical framework many QA-specific resources and techniques, which range from the exploitation of WordNet, structured, and semi-structured databases to reasoning, and paraphrasing. We have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. Another form of combining strategies for advanced QA is proposed: (1) a knowledge-based Q/A implementation based on syntactic/semantic processing is combined using a maximum-entropy framework with (2) a statistical noisy-channel algorithm for Q/A and (3) a pattern-based approach that learn from Web data.	A Noisy-Channel Approach To Question Answering We introduce a method that uses probability, called a noisy-channel model, for answering questions and show how it can be used in a complete question-answering system. Our noisy-channel system works better than an advanced rule-based question-answering system that uses similar information. We also show that our model is flexible enough to include many resources and techniques specific to question answering, such as using WordNet (a large database of words), structured and semi-structured databases, reasoning, and rephrasing. We have created a noisy-channel model for question answering that shows how a sentence with an answer can be changed into the question through a series of random steps. We also suggest another way to combine strategies for advanced question answering: (1) a system based on understanding sentence structure and meaning is combined using a method that finds the most likely outcome with (2) a statistical noisy-channel method for question answering and (3) a pattern-based method that learns from data on the Web.	results
480	Fast Methods For Kernel-Based Text Analysis Kernel-based learning (e.g., Support Vector Machines) has been successfully applied to many hard problems in Natural Language Processing (NLP). In NLP, although feature combinations are crucial to improving performance, they are heuristically selected. Kernel methods change this situation. The merit of the kernel methods is that effective feature combination is implicitly expanded without loss of generality and increasing the computational costs. Kernel-based text analysis shows an excellent performance in terms in accuracy; however, these methods are usually too slow to apply to large-scale text analysis. In this paper, we extend a Basket Mining algorithm to convert a kernel-based classifier into a simple and fast linear classifier. Experimental results on English BaseNP Chunking, Japanese Word Segmentation and Japanese Dependency Parsing show that our new classifiers are about 30 to 300 times faster than the standard kernel-based classifiers. We propose polynomial kernel inverted (PKI). PKI - Inverted Indexing, stores for each feature the support vectors in which it appears. The PKE approach uses a basket mining approach to prune many features from the expansion. An extension of the PrefixSpan algorithm (Pei et al, 2001) is used to efficiently mine the features in a low degree polynomial kernel space.	Fast Methods For Kernel-Based Text Analysis Kernel-based learning, like Support Vector Machines, has been used successfully for difficult tasks in Natural Language Processing (NLP). In NLP, choosing the right combinations of features (important characteristics) is essential for better performance, but they are often chosen based on trial and error. Kernel methods improve this by automatically combining features effectively without making the process more complicated or slow. Kernel-based text analysis is very accurate, but it's usually too slow for analyzing large amounts of text. In this paper, we modify a Basket Mining algorithm to turn a kernel-based classifier into a simple and fast linear classifier. Tests on English BaseNP Chunking, Japanese Word Segmentation, and Japanese Dependency Parsing show our new classifiers are about 30 to 300 times faster than the usual kernel-based classifiers. We introduce polynomial kernel inverted (PKI). PKI - Inverted Indexing, keeps track of each feature and the important data points where it appears. The PKE approach uses a basket mining method to remove unnecessary features from the expansion. We use a version of the PrefixSpan algorithm to quickly find features in a simple polynomial kernel space.	reasoning
481	Clustering Polysemic Subcategorization Frame Distributions Semantically Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data. We describe a new approach which involves clustering subcategorization frame (SCF) distributions using the Information Bottleneck and nearest neighbour methods. In contrast to previous work, we particularly focus on clustering polysemic verbs. A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters, offering us a good insight into the potential and limitations of semantically classifying undisambiguated SCF data. We evaluate hard clusterings based on a gold standard with multiple classes per verb. We create a test set by considering verb polysemy on the basis of Levin's classes and the LCS database (Dorr, 1997). Prepositional phrases (pp) are parameterized for two frequent sub categorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb.	Clustering Polysemic Subcategorization Frame Distributions Semantically Previous research has shown that grouping similar things together (clustering) helps in finding groups of verbs with similar meanings from data that hasn't been sorted by meaning. We explain a new method that groups patterns of how verbs are used with their different structures (subcategorization frame distributions) using two techniques: Information Bottleneck and nearest neighbour methods. Unlike past studies, we focus on grouping verbs that have multiple meanings (polysemic verbs). We introduce a new way to evaluate, which considers how having multiple meanings affects these groups, giving us a better understanding of how well we can sort these unsorted verb structures by meaning. We test our groups (hard clusterings) against a standard that includes multiple categories for each verb. We create a test sample by looking at how verbs have multiple meanings based on Levin's classes and the LCS database (Dorr, 1997). Prepositional phrases (pp) are set up for two common verb structures (NP and NP PP), and we use the raw counts of these structures, without filtering, to represent a verb.	better performance
482	Reliable Measures For Aligning Japanese-English News Articles And Sentences We have aligned Japanese and English news articles and sentences to make a large parallel corpus. We first used a method based on cross-language information retrieval (CLIR) to align the Japanese and English articles and then used a method based on dynamic programming (DP) matching to align the Japanese and English sentences in these articles. However, the results included many incorrect alignments. To remove these, we propose two measures (scores) that evaluate the validity of alignments. The measure for article alignment uses similarities in sentences aligned by DP matching and that for sentence alignment uses similarities in articles aligned by CLIR. They enhance each other to improve the accuracy of alignment. Using these measures, we have successfully constructed a large-scale article and sentence alignment corpus available to the public. We build an automatically sentence aligned Japanese/English Yomiuri newspaper corpus consisting of 180K sentence pairs. We use the BM25 similarity measure.	Reliable Measures For Aligning Japanese-English News Articles And Sentences We have matched Japanese and English news articles and sentences to create a large collection of aligned text (parallel corpus). First, we used a method that helps find information across languages (cross-language information retrieval, CLIR) to match the articles, and then we used a method called dynamic programming (DP) matching to align the sentences within those articles. However, there were many mistakes in the matches. To fix this, we suggest two ways (scores) to check if the matches are correct. To align articles, we compare how similar the sentences matched by DP are, and to align sentences, we compare how similar the articles matched by CLIR are. These methods help each other to make the matches more accurate. With these methods, we've created a large collection of matched articles and sentences that anyone can use. We have automatically created a matched Japanese/English Yomiuri newspaper collection with 180,000 pairs of sentences. We use a similarity measure called BM25.	grouping
483	Loosely Tree-Based Alignment For Machine Translation We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length. This is done by adding a new subtree cloning operation to either tree-to-string or tree-to-tree alignment algorithms. We found that parallel trees over-constrained the alignment problem, and achieved better results with a tree-to-string model than with a tree-to-tree model using two trees. We train a system on parallel constituent trees from the Korean-English Treebank, evaluating agreement with hand-annotated word alignments. The "clone" operation allows words to be aligned even in cases of radically mismatched trees, at a cost in the probability of the alignment.	Loosely Tree-Based Alignment For Machine Translation We improve a translation model that rearranges parts of sentence structures (syntactic trees) so that it can also handle alignments that don't fit the original structure, while keeping the process manageable and not too complex. This is done by adding a new method called subtree cloning to the existing algorithms that align parts of one tree to a string of words or another tree. We discovered that using two trees made the alignment too strict, and we got better results when aligning a tree to a string than when aligning two trees. We trained a system using pairs of sentence structures from the Korean-English Treebank, checking how well it matched with alignments done by people. The "clone" method helps align words even when the structures are very different, but it decreases the likelihood of the alignment being accurate.	Sentences
484	A Probability Model To Improve Word Alignment Word alignment plays a crucial role in statistical machine translation. Word-aligned corpora have been found to be an excellent source of translation-related knowledge. We present a statistical model for computing the probability of an alignment given a sentence pair. This model allows easy integration of context-specific features. Our experiments show that this model can be an effective tool for improving an existing word alignment. We propose a direct alignment formulation and argue that it would be straightforward to estimate the parameters given a supervised alignment corpus. We use the phrasal cohesion of a dependency tree as a constraint on a beam search aligner.	A Probability Model To Improve Word Alignment Word alignment is very important in statistical machine translation, which is a method for automatically converting text from one language to another. Collections of text that are aligned word-by-word have been found to be a great source of knowledge for translation. We introduce a statistical model, which is a mathematical way, to calculate the likelihood (probability) of how words align in a pair of sentences. This model makes it easy to include features that are specific to the context, or situation. Our tests indicate that this model can effectively help improve current word alignment. We suggest a direct way to align words and claim it would be easy to figure out the necessary parameters using a supervised alignment corpus, which is a collection of texts with already aligned words. We use the idea that phrases stay together in a dependency tree, which shows how words are connected, as a rule for a beam search aligner, a method for finding the best alignment.	structure
485	Probabilistic Parsing For German Using Sister-Head Dependencies We present a probabilistic parsing model for German trained on the Negra treebank. We observe that existing lexicalized parsing models using head-head dependencies, while successful for English, fail to outperform an unlexicalized baseline model for German. Learning curves show that this effect is not due to lack of training data. We propose an alternative model that uses sister-head dependencies instead of head-head dependencies. This model outperforms the baseline, achieving a labeled precision and recall of up to 74%. This indicates that sister-head dependencies are more appropriate for treebanks with very flat structures such as Negra. We show that full lexicalization does not carry over across different tree-banks for other languages, annotations or domains. We show that the task of assigning correct grammatical functions is harder than mere constituent-based parsing.	Probabilistic Parsing For German Using Sister-Head Dependencies We present a model that predicts sentence structure in German using probabilities, trained on a data set called the Negra treebank. We find that current models, which focus on word-to-word connections, work well for English but don't do better than basic models for German. Learning curves, which show how performance improves with more data, reveal that this issue isn't due to a lack of data. We suggest a different model that uses connections between a word and its "sister" words, rather than just direct word-to-word connections. This new model does better than the basic ones, reaching up to 74% accuracy in labeling and finding the right sentence parts. This suggests that sister-word connections work better for data sets with simple structures like Negra. We demonstrate that fully focusing on word-to-word connections doesn't work well across different languages, types of data, or subject areas. We also show that figuring out the correct grammatical roles is harder than just understanding the basic structure of sentences.	Improve
486	A Comparative Study On Reordering Constraints In Statistical Machine Translation In statistical machine translation, the generation of a translation hypothesis is computationally expensive. If arbitrary word-reorderings are permitted, the search problem is NP-hard. On the other hand, if we restrict the possible word-reorderings in an appropriate way, we obtain a polynomial-time search algorithm. In this paper, we compare two different reordering constraints, namely the ITG constraints and the IBM constraints. This comparison includes a theoretical discussion on the permitted number of reorderings for each of these constraints. We show a connection between the ITG constraints and the since 1870 known Schroder numbers. We evaluate these constraints on two tasks: the Verbmobil task and the Canadian Hansards task. The evaluation consists of two parts: First, we check how many of the Viterbi alignments of the training corpus satisfy each of these constraints. Second, we restrict the search to each of these constraints and compare the resulting translation hypotheses. The experiments will show that the baseline ITG constraints are not sufficient on the Canadian Hansards task. Therefore, we present an extension to the ITG constraints. These extended ITG constraints increase the alignment coverage from about 87% to 96%. We show that ITG constraints yield significantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). We introduce a normal form ITG which avoids over-counting.	A Comparative Study On Reordering Constraints In Statistical Machine Translation In statistical machine translation, creating a possible translation is very demanding for computers. If we allow words to be rearranged in any order, the search becomes extremely difficult to solve. However, if we limit how words can be rearranged in a smart way, we can find a solution faster. In this paper, we compare two different ways to limit word rearranging, called ITG constraints and IBM constraints. We discuss how many ways words can be rearranged with each method. We show a link between ITG constraints and Schroder numbers, a mathematical concept known since 1870. We test these methods on two tasks: the Verbmobil task and the Canadian Hansards task. The testing has two parts: First, we check how many of the best word matches in the training data fit each method. Second, we limit the search to each method and compare the translations we get. The tests will show that the basic ITG constraints aren't enough for the Canadian Hansards task. So, we offer an improvement to the ITG constraints. These improved ITG constraints raise the alignment match from about 87% to 96%. We show that ITG constraints provide much better matches than those used in IBM translation methods for both German-English (Verbmobil data) and French-English (Canadian Hansards data). We introduce a standard form of ITG that prevents counting errors.	right sentence
487	Minimum Error Rate Training In Statistical Machine Translation Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria. A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text. In this paper, we analyze various training criteria which directly optimize translation quality. These training criteria make use of recently proposed automatic evaluation metrics. We describe a new algorithm for efficient training an unsmoothed error count. We show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure. In our model, feature weights are tuned with Minimum Error Rate Training (MERT) to maximize BLEU.	Minimum Error Rate Training In Statistical Machine Translation Often, the training process for statistical machine translation models relies on maximum likelihood, which is a method of estimating the best-fit parameters, or similar methods. A common issue with this method is that it doesn't closely relate to how well the translation works on new, unseen text. In this paper, we examine different training methods that aim to directly improve how good the translation is. These methods use new automatic ways to evaluate translations. We explain a new method for effectively training without using an adjusted error count. We demonstrate that much better results can often be achieved if we use the final evaluation measure directly as part of the training process. In our model, the importance of different features is adjusted using Minimum Error Rate Training (MERT) to increase the BLEU score, which is a way to measure translation quality.	Canadian
488	A Machine Learning Approach To Pronoun Resolution In Spoken Dialogue We apply a decision tree based approach to pronoun resolution in spoken dialogue. Our system deals with pronouns with NP- and non-NP-antecedents. We present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features. We evaluate the system on twenty Switchboard dialogues and show that it compares well to Byron's (2002) manually tuned system. We aim at finding a subset of the available features with which the resulting coreference classifier yields the best clustering-level accuracy on held-out data.	A Machine Learning Approach To Pronoun Resolution In Spoken Dialogue We use a decision tree method to figure out pronouns in spoken conversations. Our system handles pronouns that refer to both noun phrases (NP) and other types of words. We introduce specific characteristics to help identify pronouns in spoken dialogue and find out which ones work best. We test the system on twenty Switchboard conversations and show that it works as well as Byron's (2002) manually adjusted system. We want to identify a group of the best characteristics that make the system correctly group related words when tested on new data.	better results
489	Coreference Resolution Using Competition Learning Approach In this paper we propose a competition learning approach to coreference resolution. Traditionally, supervised machine learning approaches adopt the single-candidate model. Nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model. By contrast, our approach adopts a twin-candidate learning model. Such a model can present the competition criterion for antecedent candidates reliably, and ensure that the most preferred candidate is selected. Furthermore, our approach applies a candidate filter to reduce the computational cost and data noises during training and resolution. The experimental results on MUC-6 and MUC-7 data set show that our approach can outperform those based on the single-candidate model. We make use of non-anaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and improve the performance by 2.9 and 1.6 to 67.3 and 67.2 in F1-measure on the MUC-6 and MUC-7corpora, respectively.	Coreference Resolution Using Competition Learning Approach In this paper we suggest a new way of using competition learning for coreference resolution, which is a method to identify when words in a text refer to the same thing. Usually, machine learning techniques use a single-candidate model, where they look at one option at a time. However, this method isn't very good at figuring out which option is best. Our method uses a twin-candidate learning model, which means it looks at two options together to better decide which one is preferred. This model helps in reliably choosing the best option. Additionally, we use a candidate filter to make the process faster and reduce errors during training and resolution. Tests on specific data sets, MUC-6 and MUC-7, show that our method works better than the single-candidate model. We use examples of non-anaphors (words that don't refer back to something else) to create a special group of training examples in the twin-candidate model, which improves our results by 2.9 and 1.6, leading to scores of 67.3 and 67.2 in F1-measure, a performance metric, on the MUC-6 and MUC-7 data collections, respectively.	related
491	Improved Source-Channel Models For Chinese Word Segmentation This paper presents a Chinese word segmentation system that uses improved source-channel models of Chinese sentence generation. Chinese words are defined as one of the following four types: lexicon words, morphologically derived words, factoids, and named entities. Our system provides a unified approach to the four fundamental features of word-level Chinese language processing: (1) word segmentation, (2) morphological analysis, (3) factoid detection, and (4) named entity recognition. The performance of the system is evaluated on a manually annotated test set, and is also compared with several state-of-the-art systems, taking into account the fact that the definition of Chinese words often varies from system to system.	Improved Source-Channel Models For Chinese Word Segmentation This paper introduces a system that breaks down Chinese sentences into words using improved models that predict how sentences are formed. Chinese words are categorized into four types: words found in dictionaries, words formed by adding prefixes or suffixes (morphologically derived), basic facts or statements (factoids), and specific names for people, places, or organizations (named entities). Our system offers a single method to handle four key tasks: (1) breaking sentences into words, (2) analyzing word structure, (3) identifying basic facts, and (4) recognizing names. We tested the system using a set of sentences that were manually checked, and we also compared its performance with other top systems, keeping in mind that different systems might define Chinese words differently.	documents
492	Counter-Training In Discovery Of Semantic Patterns This paper presents a method for unsupervised discovery of semantic patterns. Semantic patterns are useful for a variety of text understanding tasks, in particular for locating events in text for information extraction. The method builds upon previously described approaches to iterative unsupervised pattern acquisition. One common characteristic of prior approaches is that the output of the algorithm is a continuous stream of patterns, with gradually degrading precision. Our method differs from the previous pattern acquisition algorithms in that it introduces competition among several scenarios simultaneously. This provides natural stopping criteria for the unsupervised learners, while maintaining good precision levels at termination. We discuss the results of experiments with several scenarios, and examine different aspects of the new procedure. We develop Counter-Trainin for detecting negative rules for a specific domain or a specific class by learning from multiple domains or classes at the same time. We use predicate-argument (SVO) model, which allows subtrees containing only a verb and its direct subject and object as extraction pattern candidates.	Counter-Training In Discovery Of Semantic Patterns This paper introduces a method to find patterns in meaning without using labeled examples. These patterns help in understanding text, like finding events in text for information extraction. The method builds on earlier methods that also find patterns without guidance. Usually, these methods keep producing patterns, but they become less accurate over time. Our method is different because it makes multiple scenarios compete against each other. This helps decide when to stop the process while keeping accuracy high. We talk about experiments with different scenarios and look at various parts of the new method. We created Counter-Training to find incorrect rules in a specific area or category by learning from different areas or categories at once. We use a model called predicate-argument (SVO), which considers only the parts of a sentence with a verb, its subject, and its object as possible pattern candidates.	system offers
493	Language Model Based Arabic Word Segmentation We approximate Arabic’s rich morphology by a model that a word consists of a sequence of morphemes in the pattern prefix*-stem-suffix* (* denotes zero or more occurrences of a morpheme). Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus. The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input. The language model is initially estimated from a small manually segmented corpus of about 110,000 words. To improve the segmentation accuracy, we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus, and re-estimate the model parameters with the expanded vocabulary and training corpus. The resulting Arabic word segmentation system achieves around 97% exact match accuracy on a test corpus containing 28,449 word tokens. We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest. We demonstrate a technique for segmenting Arabic text and use it as a morphological processing step in machine translation.	Language Model Based Arabic Word Segmentation We simplify Arabic's complex word structure by using a model that breaks a word into parts called morphemes, which can include prefixes (beginning parts), stems (main parts), and suffixes (ending parts). Our approach starts with a small set of Arabic words that have been manually broken down and uses this to help an automated process learn from a large set of unbroken Arabic words. The process uses a three-part language model to guess the most likely sequence of word parts. This model is first created from about 110,000 words that have been manually broken down. To make the word breaking more accurate, we use an automated method to find new main word parts from a large collection of 155 million unbroken words and update the model with this new information. The final system for breaking Arabic words achieves about 97% accuracy when tested on a set of 28,449 words. We think this is a top-level result and the method can be used for other languages with complex word forms if there is a small set of manually broken down words available. We show how this method can be used to break down Arabic words as a step in translating languages with machines.	created
494	Accurate Unlexicalized Parsing We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize. We also present a manual symbol refinement method.	Accurate Unlexicalized Parsing We show that a grammar model called an unlexicalized PCFG (Probabilistic Context-Free Grammar without specific word information) can understand sentences more accurately than thought before. This is done by using simple changes based on language rules, which fix wrong assumptions in a standard grammar model. Its accuracy of 86.36% is better than early models that included specific words and almost as good as the best current models. This means that unlexicalized PCFGs could be useful because they are simpler, easier to copy, and easier to understand than more complex models that use specific words. The methods for using them are simpler, more known, less complex, and easier to improve. We also introduce a manual way to improve the symbols used.	translating
495	Is It Harder To Parse Chinese Or The Chinese Treebank? We present a detailed investigation of the challenges posed when applying parsing models developed against English corpora to Chinese. We develop a factored-model statistical parser for the Penn Chinese Treebank, showing the implications of gross statistical differences between WSJ and Chinese Treebanks for the most general methods of parser adaptation. We then provide a detailed analysis of the major sources of statistical parse errors for this corpus, showing their causes and relative frequencies, and show that while some types of errors are due to difficult ambiguities inherent in Chinese grammar, others arise due to treebank annotation practices. We show how each type of error can be addressed with simple, targeted changes to the independence assumptions of the maximum likelihood-estimated PCFG factor of the parsing model, which raises our F1 from 80.7% to 82.6% on our development set, and achieves parse accuracy close to the best published figures for Chinese parsing. We argue that a careful error classification can reveal possible improvements. Noun/verb mis-taggings are a frequent error case for PCFG parsing on PCTB data, compounded in Chinese by the lack of function words and morphology. There are many linguistic differences between Chinese and English, as well as structural differences between their corresponding tree banks, and some of these make it a harder task to parse Chinese.	Is It Harder To Parse Chinese Or The Chinese Treebank? We examine the problems faced when using models made for English to work with Chinese. We create a statistical parsing tool for the Penn Chinese Treebank, highlighting the big statistical differences between the WSJ (Wall Street Journal) and Chinese data collections, which affect how well general parsing methods can be adapted. We analyze the main reasons for errors in this data set, showing their causes and how often they occur. Some errors are because of tricky grammar issues in Chinese, while others happen due to the way the data is organized. We explain how to fix each error type with simple changes to the assumptions in our PCFG (Probabilistic Context-Free Grammar) model, improving our accuracy score from 80.7% to 82.6% on our test data, nearing the best accuracy reported for Chinese parsing. We suggest that sorting errors carefully can lead to better performance. Mistakes in tagging nouns and verbs often occur in our PCFG model with PCTB data, made worse in Chinese by the lack of specific grammar words and word changes. There are many language and structure differences between Chinese and English and their data sets, making it more challenging to process Chinese.	thought before
497	Probabilistic Text Structuring: Experiments With Sentence Ordering Ordering information is a critical task for natural language generation applications. In this paper we propose an approach to information ordering that is particularly suited for text-to-text generation. We describe a model that learns constraints on sentence order from a corpus of domain-specific texts and an algorithm that yields the most likely order among several alternatives. We evaluate the automatically generated orderings against authored texts from our corpus and against human subjects that are asked to mimic the model's task. We also assess the appropriateness of such a model for multi-document summarization. We build a conditional model of words across adjacent sentences, focusing on words in particular semantic roles. We proposed an algorithm that computes the probability of two sentences being adjacent for ordering sentences. As the features, we propose the Cartesian product of content words in adjacent sentences.	Probabilistic Text Structuring: Experiments With Sentence Ordering Ordering information is a crucial task for creating natural language applications. In this paper, we suggest a method for organizing information that works well for generating text from other text. We explain a model that learns rules about the order of sentences from a collection of texts focused on a specific subject and a method that finds the most likely order out of several options. We test the automatically created sentence orders by comparing them to original texts from our collection and to people who try to do the same task as the model. We also check how suitable this model is for summarizing information from multiple documents. We develop a model that looks at words across sentences that are next to each other, especially focusing on words with specific meanings or roles. We propose a method that calculates how likely two sentences are to be next to each other to help order sentences. For features, we suggest using the combination of important words in sentences that are next to each other.	language
498	Discourse Segmentation Of Multi-Party Conversation We present a domain-independent topic segmentation algorithm for multi-party speech. Our feature-based algorithm combines knowledge about content using a text-based algorithm as a feature and about form using linguistic and acoustic cues about topic shifts extracted from speech. This segmentation algorithm uses automatically induced decision rules to combine the different features. The embedded text-based algorithm builds on lexical cohesion and has performance comparable to state-of-the-art algorithms based on lexical information. A significant error reduction is obtained by combining the two knowledge sources. We provide gold standard for thematic segmentations by considering the agreement between at least three human annotations. We propose the lexical chain based unsupervised segmenter (LCSeg) and a supervised segmenter for segmenting meeting transcripts. Our LCseg system is the only word distribution based system evaluated on ICSI meeting data.	Discourse Segmentation Of Multi-Party Conversation We introduce a general method to divide conversations with multiple people into topics. Our method uses features, which are like clues, about the content and form of speech. It combines a text-based system that looks at what is being said and clues from speech sounds to spot changes in topics. This method uses automatically created rules to mix these different clues together. The text-based system looks at how words stick together and works as well as the best current systems that rely on word information. By using both content and form clues, we make fewer mistakes. We ensure high quality by checking if at least three human reviewers agree on the topic divisions. We suggest using a method that looks at word connections without training (LCSeg) and one that uses training for dividing meeting notes into topics. Our LCSeg is the only system that looks at how words are spread out, tested on ICSI meeting data.	several options
499	Automatic Error Detection In The Japanese Learners' English Spoken Data This paper describes a method of detecting grammatical and lexical errors made by Japanese learners of English and other techniques that improve the accuracy of error detection with a limited amount of training data. In this paper, we demonstrate to what extent the proposed methods hold promise by conducting experiments using our learner corpus, which contains information on learners' errors. We use error annotated transcripts of Japanese speakers in an interview-based test of spoken English to train a maximum entropy classifier to recognize 13 different types of grammatical and lexical errors, including errors involving prepositions. In the Japanese Learners of English corpus (Izumi et al., 2003), errors related to verbs are among the most frequent categories. The usage of articles has been found to be the most frequent error class in the JLE (Japanese Learner English) corpus.	Automatic Error Detection In The Japanese Learners' English Spoken Data This paper explains a way to find grammar and word choice mistakes made by Japanese people learning English, and other methods that help make finding these mistakes more accurate even with a small amount of practice data. In this paper, we show how well these methods work by doing tests using our collection of learner mistakes. We use edited transcripts of Japanese speakers in a spoken English test to train a computer program to identify 13 different types of grammar and word choice mistakes, like errors with prepositions (words like "in" or "at"). In the Japanese Learners of English collection of data (Izumi et al., 2003), mistakes with verbs are one of the most common types. Using articles (words like "a," "an," or "the") has been found to be the most common mistake in the JLE (Japanese Learner English) data collection.	content
500	Learning Non-Isomorphic Tree Mappings For Machine Translation Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings. Unlike previous statistical formalisms (limited to isomorphic trees), synchronous TSG allows local distortion of the tree topology. We reformulate it to permit dependency trees, and sketch EM/Viterbi algorithms for alignment, training, and decoding. We argue that if the parse tree of source sentence is provided, decoding (for tree-to-string and tree-to-tree models) can also be cast as a tree-parsing problem. We consider synchronous tree substitution grammar, a formalism that can account for structural mismatches, and is trained discriminatively.	Learning Non-Isomorphic Tree Mappings For Machine Translation Often, someone might want to learn how to convert one tree structure into another, using examples that don't perfectly match or using a mix of trees and simple text. Unlike older methods that only work with matching tree shapes (isomorphic trees), a new method called synchronous TSG allows for changes in the tree structure. We change this method to work with dependency trees, and outline some algorithms (step-by-step procedures) for matching, training, and converting. We suggest that if you have the structure of the original sentence, converting can also be seen as a problem of analyzing the tree structure. We look at synchronous tree substitution grammar, a method that can handle differences in structure and is trained using a method that distinguishes between different options.	articles
501	A TAG-Based Noisy-Channel Model Of Speech Repairs This paper describes a noisy channel model of speech repairs, which can identify and correct repairs in speech transcripts. A syntactic parser is used as the source model, and a novel type of TAG-based transducer is the channel model. The use of TAG is motivated by the intuition that the reparandum is a "rough copy" of the repair. The model is trained and tested on the Switchboard disfluency-annotated corpus. Noisy channel models do well on the disfluency detection task. Although the standard noisy channel model performs well, a log linear re-ranker can be used to increase performance. Our TAG system achieves a high EDIT-F score, largely as a result of its explicit tracking of overlapping words between reparanda and alterations.	A TAG-Based Noisy-Channel Model Of Speech Repairs This paper talks about a system that can find and fix mistakes in spoken words written down. It uses a tool that checks sentence structure as the main model and a new kind of machine (TAG-based transducer) as the second model. This method uses the idea that the mistake is a "rough copy" of the correction. The system is tested with a special set of conversations that have mistakes marked in them. These models are good at finding mistakes. Even though the basic model works well, using a special tool called a log linear re-ranker can make it even better. Our TAG system is very good at fixing errors because it keeps track of words that overlap between mistakes and corrections.	substitution
502	Discriminative Training Of A Neural Network Statistical Parser Discriminative methods have shown significant improvements over traditional generative methods in many machine learning applications, but there has been difficulty in extending them to natural language parsing. One problem is that much of the work on discriminative methods conflates changes to the learning method with changes to the parameterization of the problem. We show how a parser can be trained with a discriminative learning method while still parameterizing the problem according to a generative probability model. We present three methods for training a neural network to estimate the probabilities for a statistical parser, one generative, one discriminative, and one where the probability model is generative but the training criteria is discriminative. The latter model outperforms the previous two, achieving state-of-the-art levels of performance (90.1% F-measure on constituents). We provide a discussion of why generative models are better than models parameterized to estimate the a posteriori probability directly. We use neural networks to induce latent left-corner parser states. We find that discriminative training was too slow, and reports accuracy higher than generative models by discriminatively reranking the output of the generative model.	Discriminative Training Of A Neural Network Statistical Parser Discriminative methods, which focus on distinguishing between different categories, have shown big improvements over older methods in many machine learning areas. However, applying them to understanding human language has been challenging. One issue is that many studies on discriminative methods mix up changes to how the learning is done with changes to how the problem is set up. We demonstrate how a language processor (parser) can be trained using a discriminative method while still organizing the problem with a traditional model that predicts based on past examples. We introduce three methods for training a neural network to guess the chances for a statistical parser: one follows the traditional model, one uses the distinguishing method, and one uses the traditional model for predictions but the distinguishing method for training. This last approach performs the best, reaching top levels of accuracy (90.1% F-measure on parts of sentences). We discuss why traditional models are better than those trying to directly predict the probability after something has occurred. We use neural networks to create hidden states in a type of parser known as left-corner. We find that training with the distinguishing method was too slow, but it reports better accuracy than traditional models by re-evaluating the results of the traditional model using the distinguishing approach.	machine
503	Parsing The WSJ Using CCG And Log-Linear Models This paper describes and evaluates log-linear parsing models for Combinatory Categorial Grammar (CCG). A parallel implementation of the L-BFGS optimisation algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation. We also develop a new efficient parsing algorithm for CCG which maximises expected recall of dependencies. We compare models which use all CCG derivations, including non-standard derivations, with normal-form models. The performances of the two models are comparable and the results are competitive with existing wide-coverage CCG parsers. Our CCG parser is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. Our parsing peformance relies on a super tagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). Our parsing performance provides an indication of how super tagging accuracy corresponds to overall dependency recovery.	Parsing The WSJ Using CCG And Log-Linear Models This paper explains and tests log-linear parsing models, which are methods for understanding sentences, for Combinatory Categorial Grammar (CCG), a type of language structure. It talks about a way to run the L-BFGS optimization algorithm, which helps improve models, on a group of computers called a Beowulf cluster, allowing the use of the complete Penn Treebank, a large language database, for estimation. We also create a new fast method for parsing CCG that aims to capture the most sentence connections correctly. We compare different models that use all CCG sentence structures, even unusual ones, with standard models. Both models perform similarly well and compete with other CCG parsers that work with all kinds of texts. Our CCG parser is very accurate and fast, finding sentence connections with an overall F-score, a measure of accuracy, of over 84% on Wall Street Journal text, and can process up to 50 sentences per second. Our parsing success depends on a super tagger, a tool that labels words with their types, having at least a 97% accuracy for each word and at least 60% accuracy for whole sentences (using 1.5 categories per word). Our parsing performance shows how accurate word labeling relates to how well we can understand sentence connections.	something
504	Incremental Parsing With The Perceptron Algorithm This paper describes an incremental parsing approach where parameters are estimated using a variant of the perceptron algorithm. A beam-search algorithm is used during both training and decoding phases of the method. The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank. We demonstrate that training a perceptron model to combine with the generative model during search provides a 2.1 percent F-measure improvement over the generative model alone, to 88.8 percent. We propose an early update mechanism, where decoding is stopped to update model weights whenever the single gold action falls outside the beam. The best results of our parser (LR=88.4%, LP=89.1% and F=88.8%) are achieved when the parser utilizes the information about the final punctuation and the look-ahead. The early-update strategy is used so as to improve accuracy and speed up the training.	Incremental Parsing With The Perceptron Algorithm This paper explains a step-by-step way to break down sentences using a learning method called the perceptron algorithm. This method uses a technique called beam-search, which helps find the best outcome during both learning and understanding phases. The perceptron method used the same key details as a previous model by Roark, 2001a, and tests show it works just as well at understanding the Penn treebank, a collection of sentence structures. We show that when we train the perceptron model to work together with the previous model during the search, it improves accuracy by 2.1 percent, reaching an 88.8 percent success rate. We suggest a new way to stop and adjust the model when a correct step is not in the current choices. Our parser, which breaks down sentences, works best (with scores of LR=88.4%, LP=89.1%, and F=88.8%) when it uses information about punctuation marks and what comes next in a sentence. This new stopping and adjusting method helps make the model more accurate and quicker to learn.	which helps
505	A Mention-Synchronous Coreference Resolution Algorithm Based On The Bell Tree This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained. We apply beam search at test time, but use a static assignment of antecedents and learns log-linear model using batch learning. We use a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree. To cope with computational complexity, we heuristically search for the most probable partition by performing a beam search through a Bell tree. We show that one can obtain a very high MUC score simply by lumping all mentions together. We applied the ANY predicate to generate cluster-level features for their entity-mention model, which did not perform as well as the mention-pair model.	A Mention-Synchronous Coreference Resolution Algorithm Based On The Bell Tree This paper introduces a new method for coreference resolution, which is figuring out which words in a text refer to the same thing. It uses a Bell tree, a type of structure to organize choices, to explore different options, treating the problem as finding the best path from the start of the tree to the end points. A Maximum Entropy model, a statistical tool, is used to rank these paths. The results of coreference performance on data from the 2002 and 2003 Automatic Content Extraction (ACE) will be shared. We also trained a system using MUC6 data and got good results. During testing, we use beam search, a method to narrow down choices, but stick to a fixed way of matching references and train a model using batch learning, which means training with groups of data at a time. We use a Bell tree to represent different ways to group mentions into entities and trained a model to search this tree. To handle the complexity of calculations, we use a smart way to find the most likely groups by performing a beam search through the Bell tree. We show that you can get a high MUC score, a measure of success, just by putting all mentions together. We used the ANY predicate, a type of condition, to create features for their model that looks at groups of mentions, but it did not work as well as the model that looks at pairs of mentions.	previous
506	A Joint Source-Channel Model For Machine Transliteration Most foreign names are transliterated into Chinese, Japanese or Korean with approximate phonetic equivalents. The transliteration is usually achieved through intermediate phonemic mapping. This paper presents a new framework that allows direct orthographical mapping (DOM) between two different languages, through a joint source-channel model, also called n-gram transliteration model (TM). With the n-gram TM model, we automate the orthographic alignment process to derive the aligned transliteration units from a bilingual dictionary. The n-gram TM under the DOM framework greatly reduces system development effort and provides a quantum leap in improvement in transliteration accuracy over that of other state-of-the-art machine learning algorithms. The modeling framework is validated through several experiments for English-Chinese language pair. We find that English-to-Chinese transliteration without Chinese phonemes outperforms that with Chinese phonemes. Our grapheme-based approach, which treats transliteration as statistical machine translation problem under monotonic constraint, aims to obtain a direct orthographical mapping (DOM) to reduce possible errors introduced in multiple conversions. Phoneme-based approaches are usually not good enough, because name entities have various etymological origins and transliterations are not always decided by pronunciations. Many transliterated words are proper names, whose pronunciation rules may vary depending on the language of origin. Our direct orthographic mapping, making use of individual Chinese graphemes, tends to overcome the problem and model the character choice directly.	A Joint Source-Channel Model For Machine Transliteration Most foreign names are changed into Chinese, Japanese, or Korean using sounds that are close but not exact. This change (transliteration) usually goes through a middle step of sound mapping. This paper presents a new method that lets us directly connect the writing systems of two different languages using a combined source-channel model, also known as an n-gram transliteration model (TM). With the n-gram TM model, we automate the alignment of writing systems to get matched transliteration parts from a dictionary with two languages. The n-gram TM in this new method makes it much easier to develop systems and significantly improves accuracy in transliteration compared to other advanced machine learning methods. This model is tested with several English-Chinese language experiments. We discover that translating English to Chinese without using Chinese sound units is better than using them. Our approach, which treats transliteration like a translation task with a straightforward rule, tries to get a direct writing connection to reduce mistakes from multiple changes. Sound-based methods are often not good enough because names come from different origins, and transliterations are not always based on how they sound. Many translated words are names, and how they sound can change depending on their original language. Our direct writing method using individual Chinese characters helps solve this issue by directly choosing the right characters.	which words
507	A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based On Minimum Cuts Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as "thumbs up" or "thumbs down". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints. We created a movie-review dataset for opinion detection. We argue that subjectivity detection performed prior to the sentiment analysis leads to better results in the latter. We show that sentence level classification can improve document level analysis. In our subjectivity detection method, soft local consistency constraints are created between every sentence in a document and inference is solved using a min-cut algorithm.	A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based On Minimum Cuts Sentiment analysis is about figuring out the opinion or feeling behind a piece of writing, like deciding if a movie review is "thumbs up" or "thumbs down." To figure out if the sentiment is positive or negative, we suggest a new machine-learning method that uses text-categorization (sorting text into categories) techniques, focusing only on the parts of the text that are opinion-based. Finding these parts can be done using efficient methods for finding minimum cuts in graphs (a way to simplify complex data), which helps include connections between sentences. We made a movie-review dataset to detect opinions. We believe that identifying the opinionated parts before analyzing the sentiment gives better results. We demonstrate that analyzing sentences individually can improve the analysis of the whole document. In our method for detecting opinionated parts, we create soft local consistency constraints (gentle rules) between every sentence in a document, and solve it using a min-cut algorithm (a method to separate data efficiently).	enough because
508	Finding Predominant Word Senses In Untagged Text In word sense disambiguation (WSD), the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed. The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of hand-tagged data. Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration. We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically. The acquired predominant senses give a precision of 64% on the nouns of the SENSEVAL-2 English all-words task. This is a very promising result given that our method does not require any hand-tagged text, such as SemCor. Furthermore, we demonstrate that our method discovers appropriate predominant senses for words from two domain-specific corpora. The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account. We restrict the word sense disambiguation task to determining predominant sense in a given domain.	Finding Predominant Word Senses In Untagged Text In figuring out the meaning of words (word sense disambiguation or WSD), a simple rule of picking the most common meaning is very effective because the meanings of a word are often not evenly spread. The issue with using the most common meaning, aside from ignoring the words around it, is that it needs some amount of manually labeled data. While there are some manually labeled text collections available for certain languages, it's expected that how often meanings appear in words, especially common topics, depends on the type and subject of the text. We show how to use a thesaurus (a book of words with synonyms) made from natural text and a WordNet similarity tool to automatically find the main meanings of nouns. The main meanings we find are correct 64% of the time in a test called SENSEVAL-2 for English nouns. This is a very good outcome since our method doesn't need any manually labeled text, like SemCor. Moreover, we show that our method finds the right main meanings for words in two specific subject text collections. The first meaning rule, often used as a basic comparison for trained WSD systems, does better than many systems that do consider the words around. We limit the task of word meaning clarification to finding the main meaning in a specific subject area.	between
509	Long-Distance Dependency Resolution In Automatically Acquired Wide-Coverage PCFG-Based LFG Approximations This paper shows how finite approximations of long distance dependency (LDD) resolution can be obtained automatically for wide-coverage, robust, probabilistic Lexical-Functional Grammar (LFG) resources acquired from treebanks. We extract LFG subcategorisation frames and paths linking LDD reentrancies from f-structures generated automatically for the Penn-II treebank trees and use them in an LDD resolution algorithm to parse new text. Unlike (Collins, 1999; Johnson, 2002), in our approach resolution of LDDs is done at f-structure (attribute-value structure representations of basic predicate-argument or dependency structure) without empty productions, traces and coindexation in CFG parse trees. Currently our best automatically induced grammars achieve 80.97% f-score for f-structures parsing section 23 of the WSJ part of the Penn-II treebank and evaluating against the DCU 1051 and 80.24% against the PARC 700 Dependency Bank (King et al., 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al., 2004). Our f-structure annotation algorithm used for inducing LFG resources from the Penn-II treebank for English uses configurational, categorial, function tag and trace information. We automatically map c-structures to f-structures by assigning grammatical functions to tree nodes based on their phrasal category, the category of the mother node and their position relative to the local head. Our parser automatically annotates input text with c-structure trees and f-structure dependencies, reaching high precision and recall rates.	Long-Distance Dependency Resolution In Automatically Acquired Wide-Coverage PCFG-Based LFG Approximations This paper explains how we can automatically solve long-distance dependency (LDD) problems using a simple method for broad, reliable, probability-based Lexical-Functional Grammar (LFG) resources that we get from treebanks, which are large collections of sentences. We take LFG subcategorisation frames (the structure of verb phrases) and paths connecting LDD links from f-structures (functional structures) that are automatically created for the Penn-II treebank, and use them in a method to analyze new text. Unlike previous methods by Collins and Johnson, our method handles LDDs at the f-structure level (which are like organized maps of sentence structure) without using empty productions, traces, or matching indexes in the context-free grammar (CFG) trees. Our best automatically created grammars currently achieve an 80.97% score for parsing f-structures in a specific section of the Wall Street Journal part of the Penn-II treebank, and 80.24% against another standard called the PARC 700 Dependency Bank, performing as well or slightly better than the best hand-made grammars. Our method for adding f-structures uses information about sentence structure, categories, function tags, and traces from the Penn-II treebank for English. We automatically connect c-structures (constituent structures) to f-structures by giving grammatical roles to parts of the sentence based on their type, the type of their parent part, and their position related to the main word. Our parser works automatically to label input text with c-structure trees and f-structure links, achieving high accuracy and completeness.	comparison
510	A Study On Convolution Kernels For Shallow Statistic Parsing In this paper we have designed and experimented novel convolution kernels for automatic classification of predicate arguments. Their main property is the ability to process structured representations. Support Vector Machines (SVMs), using a combination of such kernels and the flat feature kernel, classify PropBank predicate arguments with accuracy higher than the current argument classification state-of-the-art. Additionally, experiments on FrameNet data have shown that SVMs are appealing for the classification of semantic roles even if the proposed kernels do not produce any improvement. We can label semantic roles by classifying the smallest subtree that includes the predicate with one of its arguments, i.e. the so called PAF structure. Our convolution kernel is characterized by two aspects: the semantic space of the subcategorization structures and the kernel function that measure their similarities.	A Study On Convolution Kernels For Shallow Statistic Parsing In this paper, we created and tested new methods called convolution kernels for automatically identifying predicate arguments, which are parts of sentences that give more information about the action or state described by the verb. These methods are especially good at handling complex data structures. Using Support Vector Machines (SVMs), which are a type of computer algorithm, with these new methods and another method that looks at simpler features, we can more accurately identify parts of sentences than current top methods. Also, tests using FrameNet data, which is another set of language information, showed that SVMs are useful for recognizing different roles in sentences, even if our new methods didn't make it better. We can identify these roles by looking at the smallest part of a sentence structure that includes the main verb and one of its related parts, known as the PAF structure. Our convolution kernel focuses on two things: the meaning of the sentence structures and a function that checks how similar they are.	Automatically Acquired
511	Discovering Relations Among Named Entities From Large Corpora Discovering the significant relations embedded in documents would be very useful not only for information retrieval but also for question answering and summarization. Prior methods for relation discovery, however, needed large annotated corpora which cost a great deal of time and effort. We propose an unsupervised method for relation discovery from large corpora. The key idea is clustering pairs of named entities according to the similarity of context words intervening between the named entities. Our experiments using one year of newspapers reveals not only that the relations among named entities could be detected with high recall and precision, but also that appropriate labels could be automatically provided for the relations. We introduce a fully unsupervised Open IE systems, based on clustering of entity pair contexts to produce clusters of entity pairs that share the same relations. We use large corpora and an Extended Named Entity tagger to find novel relations and their participants.	Discovering Relations Among Named Entities From Large Corpora Discovering important connections hidden in documents would be very helpful not just for finding information but also for answering questions and creating summaries. Previous methods for finding these connections needed large collections of labeled documents, which took a lot of time and effort. We suggest a method that doesn't require labeled data (unsupervised method) to find connections in large collections of text. The main idea is to group pairs of named entities (like people, places, organizations) based on the similarity of the words found between them. Our tests using a year's worth of newspaper articles show that not only can we find connections between named entities accurately, but we can also automatically assign suitable labels to these connections. We introduce a fully unsupervised system called Open IE, which clusters pairs of entities that share the same connections. We use large text collections and a tool called an Extended Named Entity tagger to discover new types of connections and their participants.	simpler features
512	Dependency Tree Kernels For Relation Extraction We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences. Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles. We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a bag-of-words kernel. To compare relations in two instance sentences, we propose to compare the subtrees induced by the relation arguments i.e. computing the node kernel between the two lowest common ancestors (lca) in the dependecy tree. We also use a dependency tree kernel to detect the Named Entity classes in natural language texts.	Dependency Tree Kernels For Relation Extraction We build on earlier work to measure how similar the sentence structures (dependency trees) are. Using this method with a machine learning tool called Support Vector Machine, we identify and categorize connections between pieces of information (entities) in a set of news stories (ACE corpus). We test different features like Wordnet hypernyms (a tool to find word meanings), parts of speech (like nouns, verbs), and types of entities, and discover that our method improves accuracy by 20% compared to just counting word occurrences. To compare connections in two sentences, we suggest looking at the smaller tree structures formed by the relation parts, which means calculating similarities between the shared points (lowest common ancestors) in the sentence structure. We also apply this method to identify types of important names (Named Entity classes) in everyday language.	automatically assign
513	Collective Information Extraction With Relational Markov Networks Most information extraction (IE) systems treat separate potential extractions as independent. However, in many cases, considering influences between different potential extractions could improve overall accuracy. Statistical methods based on undirected graphical models, such as conditional random fields (CRFs), have been shown to be an effective approach to learning accurate IE systems. We present a new IE method that employs Relational Markov Networks (a generalization of CRFs), which can represent arbitrary dependencies between extractions. This allows for "collective information extraction" that exploits the mutual influence between possible extractions. Experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach. We present AImed, a corpus for the evaluation of PPI extraction systems.	Collective Information Extraction With Relational Markov Networks Most information extraction (IE) systems treat each piece of information they find as separate and not connected to others. However, in many cases, thinking about how different pieces of information affect each other could make the system more accurate. Statistical methods using undirected graphical models, like conditional random fields (CRFs), have been shown to work well for creating accurate IE systems. We introduce a new IE method that uses Relational Markov Networks (an advanced form of CRFs), which can show any kind of connection between pieces of information. This allows for "collective information extraction" that takes advantage of the way possible pieces of information can influence each other. Tests on learning to find protein names from scientific text show the benefits of this method. We introduce AImed, a collection of data used to test systems that extract protein-protein interaction information.	ancestors
514	Corpus-Based Induction Of Syntactic Structure: Models Of Dependency And Constituency We present a generative model for the unsupervised learning of dependency structures. We also describe the multiplicative combination of this dependency model with a model of linear constituency. The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data. Our contributions include the generative Dependency Model with Valence (DMV). We argue that consistent syntactic representations are desirable in the evaluation of unsupervised syntactic parsers.	Corpus-Based Induction Of Syntactic Structure: Models Of Dependency And Constituency We introduce a model that can learn dependency structures without needing labeled examples. We also explain how this dependency model can be combined with another model that looks at the order of words. This combined model works better than each model on its own, achieving the best results for learning both dependency and word order without labeled data. We show that the combined model is effective across different languages, using either how words are linked or patterns in the data. Our work includes a new Dependency Model with Valence (DMV), which is a way to learn these structures. We believe that having consistent ways to represent sentence structures is important for evaluating how well these models can learn without examples.	Collective
515	Improving IBM Word Alignment Model 1 We investigate a number of simple methods for improving the word-alignment accuracy of IBM Model 1. We demonstrate reduction in alignment error rate of approximately 30% resulting from (1) giving extra weight to the probability of alignment to the null word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters. A limitation of IBM Model 1 is that each word in the target sentence can be generated by at most one word in the source sentence. We also suggested adding multiple empty words to the target sentence for IBM Model 1. Our method also alleviates another related limitation by enabling translation between contiguous words across the query and documents.	Improving IBM Word Alignment Model 1 We look at some easy ways to make IBM Model 1 better at matching words correctly. We show that we can make mistakes in word matching go down by about 30% by (1) giving more importance to matching with a word that doesn't exist (null word), (2) adjusting probabilities for words that don't appear often, and (3) using a simple guessing method to start or replace the usual EM training for setting up the model. One problem with IBM Model 1 is that each word in the sentence we are translating to can only come from one word in the original sentence. We also suggested adding extra empty words to the sentence we are translating to in IBM Model 1. Our method also helps with another problem by allowing translations between groups of words in both the question and the documents.	order without
516	Multi-Criteria-Based Active Learning For Named Entity Recognition In this paper, we propose a multi-criteria-based active learning approach and effectively apply it to named entity recognition. Active learning targets to minimize the human annotation efforts by selecting examples for labeling. To maximize the contribution of the selected examples, we consider the multiple criteria: informativeness, representativeness and diversity and propose measures to quantify them. More comprehensively, we incorporate all the criteria using two selection strategies, both of which result in less labeling cost than single-criterion-based method. The results of the named entity recognition in both MUC-6 and GENIA show that the labeling cost can be reduced by at least 80% without degrading the performance. We consider Active Learning (AL) for entity recognition based on Support Vector Machines. Our diversity-motivated intra-stratum sampling scheme considers K-diverse neighbors and aims to maximize the training utility of all seeds from a stratum.	Multi-Criteria-Based Active Learning For Named Entity Recognition In this paper, we introduce a method that uses multiple factors to improve active learning and apply it successfully to named entity recognition (a task in computer science to identify names in text like people, places, or organizations). Active learning aims to reduce the amount of human effort needed for labeling data by choosing the most useful examples for labeling. To make sure the chosen examples are helpful, we look at different factors: how informative they are (how much new information they offer), how representative they are (how well they reflect the data set), and how diverse they are (how varied they are), and we come up with ways to measure these factors. We combine all these factors using two methods to choose examples, both of which are cheaper than using just one factor. The results in specific tests, MUC-6 and GENIA, show that we can cut labeling costs by at least 80% without reducing effectiveness. We focus on Active Learning for entity recognition using a technique called Support Vector Machines (a type of machine learning model). Our approach to ensure variety (diversity-motivated intra-stratum sampling) looks at a diverse group of similar examples (K-diverse neighbors) and aims to make the most out of all examples from a group (stratum).	suggested
517	Automatic Evaluation Of Machine Translation Quality Using Longest Common Subsequence And Skip-Bigram Statistics In this paper we describe two new objective automatic evaluation methods for machine translation. The first method is based on longest common subsequence between a candidate translation and a set of reference translations. Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring in-sequence n-grams automatically. The second method relaxes strict n-gram matching to skip-bigram matching. Skip-bigram is any pair of words in their sentence order. Skip-bigram co-occurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency. We experimented with a wide set of metrics, including NIST, WER (Niefen et al, 2000), PER (Tillmann et al, 1997), and variants of ROUGE, BLEU and GTM.	Automatic Evaluation Of Machine Translation Quality Using Longest Common Subsequence And Skip-Bigram Statistics In this paper we explain two new automatic ways to measure how good a machine translation is. The first way uses the longest common subsequence, which compares a translation with several reference translations to find the longest matching word sequences in the same order. This method naturally considers how similar the overall sentence structure is. The second way is less strict and uses skip-bigram matching, which looks at any two words in the correct order but not necessarily next to each other. Skip-bigram statistics measure how often these word pairs occur in both the translation and the reference translations. Our results show that both methods match human opinions well in terms of how well the translation conveys the right meaning (adequacy) and how smoothly it reads (fluency). We tested these methods against many other evaluation measures, including NIST, WER (a measure of word errors), PER (another error measure), and different forms of ROUGE, BLEU, and GTM.	learning
518	Statistical Machine Translation By Parsing In an ordinary syntactic parser, the input is a string, and the grammar ranges over strings. This paper explores generalizations of ordinary parsing algorithms that allow the input to consist of string tuples and/or the grammar to range over string tuples. Such algorithms can infer the synchronous structures hidden in parallel texts. It turns out that these generalized parsers can do most of the work required to train and apply a syntax-aware statistical machine translation system. When a parser's grammar can have fewer dimensions than the parser's input, we call it a synchronizer. We formalize machine translation problem as synchronous parsing based on multi text grammars.	Statistical Machine Translation By Parsing In a typical sentence analyzer, the input is a sequence of words, and the rules apply to sequences of words. This paper looks into expanding typical parsing methods to handle inputs made of multiple sequences of words and/or apply rules to multiple sequences. These methods can discover matching patterns in texts that are translated side by side. It turns out that these expanded parsers can handle most tasks needed to create and use a translation system that understands sentence structure. When the rules of a parser have fewer parts than the input, we call it a synchronizer. We describe the machine translation problem as matching patterns in multiple texts using rules for multiple texts.	automatic
519	Identifying Agreement And Disagreement In Conversational Speech: Use Of Bayesian Networks To Model Pragmatic Dependencies We describe a statistical approach for modeling agreements and disagreements in conversational interaction. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse. We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance. Our approach achieves 86.9% accuracy, a 4.9% increase over previous work. We define an adjacent pair to consist of two parts that are ordered, adjacent, and produced by different speakers. We also achieved an 8% increase in speaker identification. We consider speaker change, segment duration, and adjacency pair sequence dependency features using a dynamic Bayesian network. We suggest that further gains can be achieved by augmenting the feature set.	Identifying Agreement And Disagreement In Conversational Speech: Use Of Bayesian Networks To Model Pragmatic Dependencies We explain a method using statistics to understand when people agree or disagree during conversations. Our method first finds pairs of related speech parts, called adjacency pairs, by using a ranking system that looks at words, how long they are spoken, and the structure of the conversation. This system checks both what has already been said and what's coming next. Then, we decide if a part of the conversation shows agreement or disagreement by looking at these pairs and considering how past agreements or disagreements affect what's being said now. Our method is correct 86.9% of the time, which is 4.9% better than older methods. An adjacent pair is defined as two connected parts spoken in order by different people. We also improved identifying who is speaking by 8%. We analyze changes in speakers, how long parts of the conversation last, and the order of adjacency pairs using a dynamic Bayesian network, which is a type of statistical model. We believe that adding more features can make this method even better.	multiple texts
520	Combining Lexical Syntactic And Semantic Features With Maximum Entropy Models For Information Extraction Extracting semantic relationships between entities is challenging because of a paucity of annotated data and the errors induced by entity detection modules. We employ Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text. Our system obtained competitive results in the Automatic Content Extraction (ACE) evaluation. Here we present our general approach and describe our ACE results. We use two kinds of features: syntactic ones and word based ones, for example, the path of the given pair of NEs in the parse tree and the word n-gram between NEs. We obtain improvement in results when we combine a variety of features. We achieved the F-measure of 52.8 on the 24 relation subtypes in the ACE RDC 2003 corpus.	Combining Lexical Syntactic And Semantic Features With Maximum Entropy Models For Information Extraction Extracting semantic relationships between entities is challenging because there isn't enough labeled data, and mistakes happen when finding entities. We use Maximum Entropy models to mix different word, sentence structure, and meaning-based features from the text. Our system did well in the Automatic Content Extraction (ACE) evaluation. Here, we explain our general method and describe our ACE results. We use two kinds of features: ones based on sentence structure and ones based on words, like the path of the given pair of Named Entities (NEs) in the sentence structure tree and the sequence of words between NEs. We get better results when we use many different features together. We achieved an F-measure score of 52.8 for the 24 relation types in the ACE RDC 2003 dataset.	disagreements
521	A High-Performance Semi-Supervised Learning Method For Text Chunking In machine learning, whether one can build a more accurate classifier by using unlabeled data (semi-supervised learning) is an important issue. Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear. This paper presents a novel semi-supervised method that employs a learning paradigm which we call structural learning. The idea is to find “what good classifiers are like” by learning from thousands of automatically generated auxiliary classification problems on unlabeled data. By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve performance on the target problem. The method produces performance higher than the previous best results on CoNLL’00 syntactic chunking and CoNLL’03 named entity chunking (English and German). We utilize a multi task learner within our semi-supervised algorithm to learn feature representations which are useful across a large number of related tasks. Our structural learning method uses alternating structural optimization (ASO). For both computational and statistical reasons, we follow compute a low-dimensional linear approximation to the pivot predictor space.	A High-Performance Semi-Supervised Learning Method For Text Chunking In machine learning, a key question is whether we can make a more accurate system for classifying data by using data that isn't labeled (this is called semi-supervised learning). Although many semi-supervised methods have been suggested, how well they work for natural language processing (NLP) tasks isn't always certain. This paper introduces a new semi-supervised method using a learning approach we call structural learning. The idea is to understand "what makes a good classifier" by studying thousands of automatically made extra classification problems using unlabeled data. By doing this, the common patterns that help predict outcomes in these problems can be identified and used to perform better on the main problem we want to solve. This method achieves better results than the previous best methods on CoNLL’00 syntactic chunking and CoNLL’03 named entity chunking (in English and German). We use a system that learns from multiple tasks within our semi-supervised algorithm to find features that are helpful for many related tasks. Our structural learning method applies a technique called alternating structural optimization (ASO). For reasons related to both computing speed and statistical analysis, we calculate a simplified version of the space where important predictors are found.	entities
522	Probabilistic CFG With Latent Annotations This paper defines a generative probabilistic model of parse trees, which we call PCFG-LA. This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables. Fine-grained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm. Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared. In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F1, sentences <= 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection. We use a markovized grammar to get a better unannotated parse forest during decoding, but we do not markovize the training data. We right-binarize the tree bank data to construct grammars with only unary and binary productions.	Probabilistic CFG With Latent Annotations This paper describes a method for creating parse trees, which are structures used in understanding sentences, called PCFG-LA. This method improves upon a basic version known as PCFG by adding hidden elements to the symbols used in the trees. Detailed rules for constructing these trees are automatically learned from a collection of example sentences by training the PCFG-LA model using a technique called the EM-algorithm. Since finding an exact solution with PCFG-LA is very complex and difficult (NP-hard), the paper discusses several simplified methods and compares them through experiments. In tests with a well-known set of sentences (Penn WSJ corpus), our model, which was trained automatically, performed quite well with an accuracy of 86.6% for sentences up to 40 words long, similar to another model that required a lot of manual adjustment. We use a special method (markovized grammar) to get better results when interpreting sentence structures but do not apply this method to the training examples. We also change the structure of example sentence data to make it simpler, using only two types of constructions (unary and binary).	Learning
523	Probabilistic Disambiguation Models For Wide-Coverage HPSG Parsing This paper reports the development of log-linear models for the disambiguation in wide-coverage HPSG parsing. The estimation of log-linear models requires high computational cost, especially with wide-coverage grammars. Using techniques to reduce the estimation cost, we trained the models using 20 sections of Penn Treebank. A series of experiments empirically evaluated the estimation techniques, and also examined the performance of the disambiguation models on the parsing of real-world sentences. Our HPSG parser computes deeper analyses, such as predicate argument structures. We also introduce a hybrid model, where the probabilities of the previous model are multiplied by the super tagging probabilities instead of a preliminary probabilistic model, to help the process of estimation by filtering unlikely lexical entries.	Probabilistic Disambiguation Models For Wide-Coverage HPSG Parsing This paper discusses the creation of log-linear models, which are mathematical tools, to help choose the correct meaning in wide-coverage HPSG parsing (a type of language processing). Making these models requires a lot of computer power, especially with large grammar structures. By using methods to make this process less demanding, we trained the models using 20 sections from the Penn Treebank, a large collection of sentences used for training language models. Through a series of tests, we checked how well these methods worked and how effective the models were when analyzing real-world sentences. Our HPSG parser, a tool for analyzing language, can understand complex structures like who is doing what to whom in a sentence. We also present a mixed model where we combine the probabilities from our previous model with probabilities from super tagging (a method to quickly assign parts of speech to words) instead of using an initial probability model, to make it easier to estimate by removing unlikely word choices.	example sentence
524	Online Large-Margin Training Of Dependency Parsers We present an effective training algorithm for linearly-scored dependency parsers that implements online large-margin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efficient parsing techniques for dependency trees (Eisner, 1996). The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language specific enhancements. We have achieved parsers with O(n3) time complexity without the grammar constant. We use the prefix of each word form instead of word form itself as features. Our dependency parser achieves accuracy as good as Charniak (2000) with speed ten times faster than Collins (1997) and four times faster than Charniak (2000).	Online Large-Margin Training Of Dependency Parsers We present an effective training method for parsers that analyze sentence structure, using an online large-margin multi-class approach (a way to make decisions with clear boundaries) based on efficient techniques for dependency trees (a type of sentence structure analysis). The trained parsers perform well in understanding sentence structures for both English and Czech without needing special adjustments for each language. We have developed parsers that work with a time complexity of O(n3), which describes how the processing time increases with more data, without needing extra grammar rules. Instead of using whole words, we use the beginning parts of words as features for analysis. Our parser is as accurate as Charniak's model from 2000 and is ten times faster than Collins' model from 1997 and four times faster than Charniak's model.	requires
525	Pseudo-Projective Dependency Parsing In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures. We show how a data-driven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures. Experiments using data from the Prague Dependency Treebank show that the combined system can handle non-projective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy. This leads to the best reported performance for robust non-projective parsing of Czech. In our pseudo-projective approach, non projective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the non projective links at parse time. We show how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to preprocess training data and post-process parser output, so-called pseudo-projective parsing. For handling non-projective relations, we suggest applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective. We note that since the number of non-projective dependencies is much smaller than the number of projective dependencies, it is not efficient to perform non-projective parsing for all cases.	Pseudo-Projective Dependency Parsing In order to fully use dependency-based syntactic parsing (a way to understand sentence structures), it's best to allow non-projective dependency structures (structures that can have crossings). We show how a data-driven deterministic dependency parser (a tool that builds sentence structures) that usually works with projective structures (no crossings) can be combined with graph transformation techniques (methods to change graphs) to create non-projective structures. Tests using data from the Prague Dependency Treebank (a database of sentence structures) show that this combined system can handle non-projective constructions well enough to significantly improve overall parsing accuracy. This results in the best reported performance for strong non-projective parsing of Czech. In our pseudo-projective approach, non-projective links (connections that cross) are moved up in the tree to make it projective (no crossings), and special labels are used to bring back the non-projective links when needed. We demonstrate how to remove the limit to projective dependency graphs by using graph transformation techniques to prepare training data and adjust the parser's results, known as pseudo-projective parsing. For dealing with non-projective relations, we suggest using a preprocessing step on a dependency parser, which involves lifting non-projective arcs (connections) to their head repeatedly until the tree becomes pseudo-projective. We point out that since there are fewer non-projective dependencies compared to projective ones, it's not efficient to perform non-projective parsing for every case.	language
526	Seeing Stars: Exploiting Class Relationships For Sentiment Categorization With Respect To Rating Scales We address the rating-inference problem, wherein rather than simply decide whether a review is thumbs up or thumbs down, as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five "stars"). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, three stars is intuitively closer to four stars than to one star. We first evaluate human performance at the task. Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem. We created a sentiment-annotated dataset consisting of movie reviews to train a classifier for identifying positive sentences in a full length review.	Seeing Stars: Exploiting Class Relationships For Sentiment Categorization With Respect To Rating Scales We tackle the challenge of figuring out the exact rating a review gives, like one to five stars, instead of just saying if it's positive (thumbs up) or negative (thumbs down). This is more complex because there are different levels of similarity between ratings; for instance, three stars is more like four stars than one star. We start by seeing how well humans can do this task. Next, we use a special method that changes how a rating system works to make sure similar things get similar ratings. We demonstrate that this method can greatly improve results compared to other systems when we use a new way to measure similarity that fits this task. We developed a set of movie reviews with ratings to train a tool that identifies positive sentences in a full review.	projective
527	Extracting Semantic Orientations Of Words Using Spin Model We propose a method for extracting semantic orientations of words: desirable or undesirable. Regarding semantic orientations as spins of electrons, we use the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function. We also propose a criterion for parameter selection on the basis of magnetization. Given only a small number of seed words, the proposed method extracts semantic orientations with high accuracy in the experiments on English lexicon. The result is comparable to the best value ever reported. We build lexical network from not only co-occurrence but other resources including thesaurus. We determine term orientation (for Japanese) according to a spin model, i.e. a physical model of a set of electrons each endowed with one between two possible spin directions, and where electrons propagate their spin direction to neighbouring electrons until the system reaches a stable configuration.	Extracting Semantic Orientations Of Words Using Spin Model We propose a method for identifying if words have positive or negative meanings. We think of these meanings like tiny particles called electrons, and use a simplified calculation method to estimate how likely certain meanings are, instead of using a complex calculation. We also suggest a way to choose the best settings for this calculation based on how these particles behave. With just a few example words, our method can accurately identify word meanings in English tests, and the results are as good as the best ones reported. We create a network of words using not only word pairings but also other sources like a thesaurus. We decide the meaning of words (for Japanese) using a model that imagines electrons, which can point in two directions and influence each other until everything is stable.	special
528	Modeling Local Coherence: An Entity-Based Approach This paper considers the problem of automatic assessment of local coherence. We present a novel entity-based representation of discourse which is inspired by Centering Theory and can be computed automatically from raw text. We view coherence assessment as a ranking learning problem and show that the proposed discourse representation supports the effective learning of a ranking function. Our experiments demonstrate that the induced model achieves significantly higher accuracy than a state-of-the-art coherence model. To further refine the computation of the subsequence distribution, we divide the matrix into a salient matrix and a non-salient matrix. We show that our entity based model is able to distinguish a source text from its permutation accurately. We exploit the use of the distributional and referential information of discourse entities to improve summary coherence.	Modeling Local Coherence: An Entity-Based Approach This paper looks at the problem of automatically checking how logically connected a text is. We introduce a new way to represent text structure that is based on Centering Theory, which can be automatically calculated from plain text. We treat the task of checking coherence as a ranking problem, meaning we try to order or prioritize different parts of the text, and demonstrate that our text structure method helps in effectively learning how to rank these parts. Our tests show that this new model achieves much better accuracy than the best existing model for checking coherence. To improve how we compute the sequence of parts in text, we split the matrix, which is a grid of data, into important and less important parts. We prove that our model, which focuses on entities (people, places, or things in the text), can correctly identify the original order of a text compared to a mixed-up version. We use the patterns and connections of these entities to make summaries clearer and more logical.	estimate
529	Machine Learning For Coreference Resolution: From Local Classification To Global Ranking In this paper, we view coreference resolution as a problem of ranking candidate partitions generated by different coreference systems. We propose a set of partition-based features to learn a ranking model for distinguishing good and bad partitions. Our approach compares favorably to two state-of-the-art coreference systems when evaluated on three standard coreference data sets. We emphasize the global optimization of ranking clusters obtained locally. We cast coreference resolution as a classification task and solve it in two steps: (1) train a classifier to determine whether two mentions are co-referent or not, and (2) use a clustering algorithm to partition the mentions into clusters, based on the pairwise predictions. Our method ranks base models according to their performance on separate tuning set, and then uses the highest-ranked base model for predicting on test documents. Most learning based coreference systems can be defined by four elements: the learning algorithm used to train the coreference classifier, the method of creating training instances for the learner, the feature set used to represent a training or test instance, and the clustering algorithm used to coordinate the coreference classification decisions.	Machine Learning For Coreference Resolution: From Local Classification To Global Ranking In this paper, we treat coreference resolution (figuring out when different words refer to the same thing) as a problem of ranking different groups created by various systems. We suggest using special features of these groups to teach a model how to pick the best ones. Our method performs well compared to two top coreference systems when tested on three common data sets. We focus on improving the overall ranking of groups formed by simpler steps. We approach coreference resolution as a task of identifying and grouping: (1) train a model to see if two words refer to the same thing, and (2) use a method to group these words based on the pairwise results. Our method ranks different base models by how well they do on a separate test set and then uses the best one for final predictions. Most coreference systems that use learning can be broken down into four parts: the algorithm to train the model, how we create examples for training, the features used to describe these examples, and the method to organize the final decisions.	effectively learning
530	Coarse-To-Fine N-Best Parsing And MaxEnt Discriminative Reranking Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000). A discriminative reranker requires a source of candidate parses for each sentence. This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000). This method generates 50-best lists that are of substantially higher quality than previously obtainable. We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less. We use pruning, where edges in a coarse-grained parse forest are pruned to allow full evaluation with fine grained categories. We show accuracy improvements from composed local tree features on top of a lexicalized base parser. To improve performance and robustness, features are pruned so that selected features must distinguish a parse with the highest F1 score in a n-best list, from a parse with a suboptimal F1 score at least five times.	Coarse-To-Fine N-Best Parsing And MaxEnt Discriminative Reranking Discriminative reranking is a technique used to make statistical parsers, which help computers understand sentences, work better (Collins, 2000). A discriminative reranker needs a list of possible sentence interpretations. This paper explains a new and simple way to create sets of the 50 best sentence interpretations using a step-by-step generative parser (Charniak, 2000). This method produces better quality lists of interpretations than before. We used these interpretations as input for a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002), which picks the best interpretation for each sentence, achieving a score of 91.0% on sentences with 100 words or less. We use pruning, which means removing less important parts in a rough initial analysis to allow a detailed analysis. We show improved accuracy using detailed local features added to a basic word-based parser. To make the system work better and be more reliable, features are trimmed so that the chosen features must clearly identify the best interpretation, which has the highest score, from one that is not as good by at least five times.	special features
531	A Hierarchical Phrase-Based Model For Statistical Machine Translation We present a statistical phrase-based translation model that uses hierarchical phrases - phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system. We use the k-best parsing algorithm in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU. We note that whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items. To better leverage syntactic constraint yet still allow non-syntactic translations, we introduce a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. Our hierarchical phrase models for machine translation is an evolution from the traditional word (Brown et al, 1993) and phrase based (Koehn et al, 2003a) models.	A Hierarchical Phrase-Based Model For Statistical Machine Translation We present a statistical phrase-based translation model that uses hierarchical phrases - phrases that contain smaller phrases within them. The model is technically a synchronous context-free grammar, but it is learned from a pair of aligned texts (bitext) without using any grammar rules. This means it shifts to using formal grammar rules like syntax-based translation systems but without relying on language rules. In our tests, using a scoring method called BLEU, the hierarchical phrase-based model shows a 7.5% improvement compared to Pharaoh, which is a top-notch phrase-based system. We use the k-best parsing method in a grammar-based model to find the best feature weights that improve BLEU scores. We note that when combining two parts of a translation, we need to check how smooth they sound together by including the score of any language model features that affect the overall flow of the sentence. To better use grammar rules but still allow for translations that don't follow those rules, we keep a count for each proposed translation and add it whenever it fits perfectly with the grammar rules of the original text. Our hierarchical phrase models for machine translation build on from the traditional word-based models (Brown et al, 1993) and phrase-based models (Koehn et al, 2003a).	interpretation
537	Extracting Relations With Integrated Information Using Kernel Methods Entity relation detection is a form of information extraction that finds predefined relations between pairs of entities in text. This paper describes a relation detection approach that combines clues from different levels of syntactic processing using kernel methods. Information from three different levels of processing is considered: tokenization, sentence parsing and deep dependency analysis. Each source of information is represented by kernel functions. Then composite kernels are developed to integrate and extend individual kernels so that processing errors occurring at one level can be overcome by information from other levels. We present an evaluation of these methods on the 2004 ACE relation detection task, using Support Vector Machines, and show that each level of syntactic processing contributes useful information for this task. When evaluated on the official test data, our approach produced very competitive ACE value scores. We also compare the SVM with KNN on different kernels. We define several feature based composite kernels to integrate diverse features for relation extraction and achieved the F-measure of 70.4 on the 7 relation types of the ACE RDC 2004 corpus. We show that adding local information to deep syntactic information improved IE results. We extract bigram of the words between the two mentions, aiming to provide more order information of the tokens between the two mentions.	Extracting Relations With Integrated Information Using Kernel Methods Entity relation detection is a way to find specific connections between pairs of things or names in a text. This paper explains a method to find these connections by using clues from different levels of sentence structure analysis, called kernel methods. It looks at information from three levels: breaking down text into words (tokenization), understanding sentence structure (sentence parsing), and analyzing word relationships (deep dependency analysis). Each of these information sources is represented by special mathematical tools called kernel functions. Then, these tools are combined to make sure mistakes at one level can be corrected by information from another level. We tested these methods on a task from 2004 called ACE relation detection, using a technique called Support Vector Machines (SVM), and showed that each level of sentence analysis gives helpful information. When tested with official data, our method scored very well. We also compared SVM with another method called KNN using different kernels. We created several combined tools called composite kernels to mix various features for finding relations and achieved a success rate of 70.4% on 7 types of relations in the ACE RDC 2004 dataset. We showed that adding nearby information to detailed sentence analysis improved information extraction results. We also looked at pairs of words between two mentions to give more order information about the words between them.	information
532	Dependency Treelet Translation: Syntactically Informed Phrasal SMT We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation. This method requires a source language dependency parser, target language word segmentation and an unsupervised word alignment component. We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model. We describe an efficient decoder and show that using these tree-based models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser. Our treelet-based SMT system is trained on about 4.6M parallel sentence pairs from diverse sources including bilingual books, dictionaries and web publications. We extend paths to treelets, arbitrary connected subgraphs of dependency structures, and propose a model based on treelet pairs. We demonstrate the success of using fragments of a target language's grammar, treelets, to improve performance in phrasal translation.	Dependency Treelet Translation: Syntactically Informed Phrasal SMT We explain a new way to use computers to translate languages by mixing grammar details from the original language with modern methods of translating phrases. This technique needs a tool that breaks down the sentence structure of the original language, splits words in the target language, and finds word connections without human help. We match sentences that say the same thing in two languages, apply the original language's grammar structure to the target language sentence, pull out small parts of sentences that match, and teach a model to arrange these parts correctly. We talk about a fast program that uses these small sentence parts together with usual translation models to create a strong method that mixes phrase translation with the broad understanding of language from grammar tools. Our system is trained using about 4.6 million pairs of sentences from different places like books in two languages, dictionaries, and online articles. We expand from paths to treelets, which are any connected parts of sentence structures, and suggest a model based on these pairs. We show how using small parts of the target language's grammar, called treelets, helps make phrase translation better.	whenever
533	Supervised And Unsupervised Learning For Sentence Compression In Statistics-Based Summarization - Step One: Sentence Compression, Knight and Marcu (Knight and Marcu, 2000) (K&M) present a noisy-channel model for sentence compression. The main difficulty in using this method is the lack of data; Knight and Marcu use a corpus of 1035 training sentences. More data is not easily available, so in addition to improving the original K&M noisy-channel model, we create unsupervised and semi-supervised models of the task. Finally, we point out problems with modeling the task in this way. They suggest areas for future research. We approximate the rules of compression from a non-parallel corpus (e.g., the Penn Treebank) by considering context-free grammar derivations with matching expansions. We argue that the noisy-channel model is not an appropriate compression model since it uses a source model trained on uncompressed sentences and as a result tends to consider compressed sentences less likely than uncompressed ones. We show that applying handcrafted rules for trimming sentences can improve both content and linguistic quality.	Supervised And Unsupervised Learning For Sentence Compression In Statistics-Based Summarization - Step One: Sentence Compression, Knight and Marcu (Knight and Marcu, 2000) (K&M) present a noisy-channel model for sentence compression. The main challenge in using this method is the lack of data; Knight and Marcu use a collection of 1035 training sentences. More data is not easily available, so besides improving the original K&M noisy-channel model, we create models that don't need labeled data (unsupervised) and models that need only some labeled data (semi-supervised) for the task. Finally, we point out problems with modeling the task in this way. They suggest areas for future research. We estimate the rules of compression from a collection of unrelated sentences (e.g., the Penn Treebank) by considering grammar rules with matching expansions. We argue that the noisy-channel model is not a good fit for compression since it uses a source model trained on full sentences and therefore tends to view compressed sentences as less likely than full ones. We show that applying handmade rules for shortening sentences can improve both the content and the quality of language.	mixing grammar
534	Contrastive Estimation: Training Log-Linear Models On Unlabeled Data Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and named-entity extraction (McCallum and Li, 2003). CRFs are log-linear, allowing the incorporation of arbitrary features into the model. To train on unlabeled data, we require unsupervised estimation methods for log-linear models; few exist. We describe a novel approach, contrastive estimation. We show that the new technique can be intuitively understood as exploiting implicit negative evidence and is computationally efficient. Applied to a sequence labeling problem - POS tagging given a tagging dictionary and unlabeled text - contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features. To define an approximation to the partition function for log-linear estimation, our contrastive estimation uses a technique to generate local neighborhoods of a parses. We generate negative evidence for the contrastive estimation method by moving or removing a word in a sentence. We define coarse grained POS tags on PTB. We initialized with all weights equal to zero (uninformed, deterministic initialization) and performed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data. We attempt to identify negative examples that are semantically similar to the positive ones in order to improve the discriminative power of the classifier. We show that good performance on unsupervised syntax learning is possible even when learning from very small discriminative neighborhoods. The contrastive estimation technique we propose is globally normalized and thus capable of dealing with arbitrary features.	Contrastive Estimation: Training Log-Linear Models On Unlabeled Data Conditional random fields (Lafferty et al., 2001) are very good at tasks where you need to label parts of a sequence, like breaking down sentences into smaller parts (Sha and Pereira, 2003) and finding names of people or places in text (McCallum and Li, 2003). CRFs use a type of math model called log-linear, which allows adding any kind of information to improve performance. To train on data without labels, we need methods that don't rely on pre-labeled examples for these models; there aren't many such methods. We introduce a new method called contrastive estimation. This new method can be easily understood as using hidden clues to improve learning and works quickly. When used for tagging parts of speech (POS tagging) with a dictionary and unlabeled text, contrastive estimation works better than another method called EM, even when the dictionary isn't perfect, and improves by using more information. For estimating log-linear models, contrastive estimation uses a way to create nearby examples to help calculate its math model. We create negative clues by changing or removing a word in a sentence. We define broad categories of POS tags on a dataset called PTB. We start with all weights set to zero (a basic starting point) and choose the best model without using labeled examples by testing on new, unlabeled data. We try to find examples that look wrong but are similar to correct ones to make the tool that sorts them better. We show that it's possible to learn sentence structure without labeled examples, even when using very small sets of similar examples. Our contrastive estimation method is designed to handle any type of information.	original
535	Incorporating Non-Local Information Into Information Extraction Systems By Gibbs Sampling Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks.	Incorporating Non-Local Information Into Information Extraction Systems By Gibbs Sampling Most current statistical natural language processing models use only local features because it allows them to calculate results quickly and effectively, but this limits their ability to understand the overall structure that is common in language use. We demonstrate how to fix this issue using Gibbs sampling, a simple method that performs approximate calculations in complex probability models. By using a process called simulated annealing instead of Viterbi decoding in sequence models such as Hidden Markov Models (HMMs), Conditional Markov Models (CMMs), and Conditional Random Fields (CRFs), it is possible to include overall structure while keeping calculations manageable. We apply this technique to improve an existing CRF-based information extraction system with models that consider long-distance relationships, ensuring consistency in labels and extraction templates. This method leads to an error reduction of up to 9% compared to leading systems on two well-known information extraction tasks.	methods
536	A Semantic Approach To IE Pattern Induction This paper presents a novel algorithm for the acquisition of Information Extraction patterns. The approach makes the assumption that useful patterns will have similar meanings to those already identified as relevant. Patterns are compared using a variation of the standard vector space model in which information from an ontology is used to capture semantic similarity. Evaluation shows this algorithm performs well when compared with a previously reported document-centric approach. We propose a weakly supervised approach to sentence filtering that uses semantic similarity and bootstrapping to acquire IE patterns. We use subject-verb-object triples for the features.	A Semantic Approach To IE Pattern Induction This paper introduces a new method for gathering Information Extraction patterns. The method assumes that useful patterns will have meanings similar to patterns already known to be important. Patterns are compared using a modified version of the standard model, where information from a knowledge system (ontology) is used to measure how similar the meanings are. Testing shows this method works well compared to an earlier method that focused on documents. We suggest a lightly guided method for selecting sentences that uses meaning similarity and a technique called bootstrapping to get IE patterns. We use subject-verb-object groups as the features.	sequence
538	Exploring Various Knowledge In Relation Extraction Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types. We show that it is difficult to extract new effective features to further improve the extraction accuracy. We use a set of flat features (i.e. word, entity type, mention level, overlap, base phrase chunking, dependency tree, parse tree and semantic information).	Exploring Various Knowledge In Relation Extraction Extracting meaningful connections between pieces of information (entities) is difficult. This paper looks at using different types of word meaning, sentence structure, and overall meaning in a method that identifies these connections using a tool called SVM (Support Vector Machine). Our study shows that using basic sentence parts (base phrase chunking) is very helpful for finding these connections and gives most of the improvement in understanding sentence structure, while using detailed sentence structure (full parsing) offers only a little extra help. This means that the most important information for finding connections is simple and can be found by looking at basic sentence parts. We also show how using tools like WordNet (a dictionary of word meanings) and Name Lists can help improve the process. Testing on the ACE dataset shows that using a variety of features effectively allows our system to perform better than the best systems known before on the 24 ACE connection types and performs much better than systems using complex methods by over 20% in accuracy on the 5 ACE connection types. We demonstrate that finding new useful characteristics to further improve accuracy is challenging. We use a collection of straightforward features such as words, types of entities, levels of mention, overlap, basic sentence parts, dependency tree, sentence structure tree, and overall meaning information.	relation detection
539	Log-Linear Models For Word Alignment We present a framework for word alignment based on log-linear models. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible additional variables. Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information. In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features. Our experiments show that log-linear models significantly outperform IBM translation models. We present a log-linear model combining IBM Model 3 trained in both directions with heuristic features which resulted in a 1-to-1 alignment.	Log-Linear Models For Word Alignment We present a system for matching words from different languages using log-linear models, which are mathematical formulas. All the information we use is turned into feature functions, which are like tools that use the original and translated sentences, and sometimes extra details. Log-linear models make it easy to add grammar-related information to the word matching process. In this paper, we use IBM Model 3, which is a method for matching words, along with parts of speech (POS) matching and coverage from a bilingual dictionary as tools. Our tests show that log-linear models work much better than IBM translation models. We introduce a log-linear model that mixes IBM Model 3, trained to match words both ways, with simple rules, resulting in a one-to-one word matching.	connection
540	Stochastic Lexicalized Inversion Transduction Grammar For Alignment We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training. Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences. We present a model in which the nonterminals are lexicalized by English and foreign language word pairs so that the inversions are dependent on lexical information on the left hand side of synchronous rules. We propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. Tic-tac-toe pruning algorithm uses dynamic programming to compute inside and outside scores for a span pair in O (n4).	Stochastic Lexicalized Inversion Transduction Grammar For Alignment We introduce a type of Inversion Transduction Grammar (a method for comparing languages) that uses word-specific probabilities throughout the analysis process, with methods to cut down unnecessary parts for faster training. The alignment, or matching, results are better than a non-word-specific version for short sentences where complete Expectation-Maximization (EM, a statistical method) is possible, but cutting down parts negatively affects longer sentences. We describe a model where language rules are detailed with word pairs from English and another language, making the changes rely on specific word information. We suggest a method called Tic-tac-toe pruning, which relies on the basic probabilities of word pairs both within and outside a section of text. The Tic-tac-toe pruning method uses a step-by-step approach to calculate scores for a section of text in a time-efficient manner (proportional to four times the length of the text, noted as O(n4)).	bilingual
541	Reading Level Assessment Using Support Vector Machines And Statistical Language Models Reading proficiency is a fundamental component of language competency. However, finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers. This task can be addressed with natural language processing technology to assess reading level. Existing measures of reading level are not well suited to this task, but previous work and our own pilot experiments have shown the benefit of using statistical language models. In this paper, we also use support vector machines to combine features from traditional reading level measures, statistical language models, and other language processing tools to produce a better method of assessing reading level. We develop a SVM categoriser combining a classifier based on trigram language models (one for each level of difficulty), some parsing features such as average tree height, and variables traditionally used in readability. We use syntactic features, such as parse tree height or the number of passive sentences, to predict reading grade levels.	Reading Level Assessment Using Support Vector Machines And Statistical Language Models Reading skill is an essential part of being good at a language. However, it's hard for teachers to find suitable texts for learners of foreign or second languages. This problem can be solved using technology that helps understand language to check the reading level. Current ways to measure reading levels are not very effective for this job, but past research and our own small tests show that using statistical language models (ways to predict text patterns) can help. In this paper, we also use support vector machines (a type of computer algorithm) to combine different features from traditional reading level methods, statistical language models, and other language tools to create a better way to evaluate reading levels. We create a system using support vector machines that combines a text classifier based on trigram language models (which looks at three-word combinations for each difficulty level), some features like average tree height from parsing (breaking down sentences into parts), and variables usually used to determine readability. We use sentence structure features, such as the height of parse trees (a visual breakdown of sentence structure) or the number of passive sentences, to estimate reading grade levels.	affects longer
542	Clause Restructuring For Statistical Machine Translation We describe a method for incorporating syntactic information in statistical machine translation systems. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the parse tree, effectively reordering the surface string on the source language side of the translation system. The goal of this step is to recover an underlying word order that is closer to the target language word-order than the original string. The reordering approach is applied as a pre-processing step in both the training and decoding phases of a phrase-based statistical MT system. We describe experiments on translation from German to English, showing an improvement from 25.2% Bleu score for a baseline system to 26.8% Bleu score for the system with reordering, a statistically significant improvement. We present sign test to measure the siginificance of score improvement in BLUE. We note that it is not clear whether the conditions required by bootstrap resampling are met in the case of BLUE, and recommend the sign test instead. We use six hand-crafted reordering rules targeting the placement of verbs, subjects, particles and negation.	Clause Restructuring For Statistical Machine Translation We explain a way to include grammar information in systems that automatically translate languages. The first step is to break down the sentence from the language being translated. The second step is to make changes to the sentence structure, rearranging the words to match the word order of the target language better than the original. This rearranging is done before both learning and translating phases in a system that translates language phrases. We did tests translating German to English, showing an improvement in the translation quality from a 25.2% Bleu score (a quality measure) for a basic system to 26.8% with rearranging, a noticeable improvement. We use a sign test to check if the score improvement is meaningful. We point out it's unsure if the conditions needed for another method, called bootstrap resampling, apply to Bleu scores, so we suggest using the sign test. We use six specific rules to rearrange verbs, subjects, small words, and negative words.	suitable texts
543	Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data. In this paper, we present a syntax-based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar. Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees. We first introduce our approach to inducing such a grammar from parallel corpora. Second, we describe the graphical model for the machine translation task, which can also be viewed as a stochastic tree-to-tree transducer. We introduce a polynomial time decoding algorithm for the model. We evaluate the outputs of our MT system using the NIST and Bleu automatic MT evaluation software. The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality. Our approach requires some assumptions on the level of isomorphism (lexical and/or structural) between two languages. We present a translation model based on Synchronous Dependency Insertion Grammar (SDIG), which handles some of the non-isomorphism but requires both source and target dependency structures.	Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars Syntax-based statistical machine translation (MT) uses statistical models to understand and translate structured language data. In this paper, we introduce a system that uses these statistical methods with a specific type of grammar called probabilistic synchronous dependency insertion grammar. This grammar is a version that works with dependency trees, which show how words in a sentence relate to each other. We explain how we create this grammar using collections of translated texts known as parallel corpora. Next, we describe our method for translation using a visual model that acts like a tree-to-tree converter, changing one language structure into another using probability. We also developed a fast method to decode, or interpret, this model. We tested our translation system with software that automatically checks translation quality, called NIST and Bleu. The results showed our system translates faster and better than a basic system using older methods from IBM. Our method needs some assumptions about how similar the two languages are, either in vocabulary or structure. We propose a translation model based on Synchronous Dependency Insertion Grammar (SDIG), which can handle some differences between languages but still needs the structure of both the original and translated sentences.	information
544	Arabic Tokenization Part-Of-Speech Tagging And Morphological Disambiguation In One Fell Swoop We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including part-of-speech tagging) Arabic words in one process. We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer. We obtain accuracy rates on all tasks in the high nineties. For choosing the best Buckwalter morphological analyzer (BAMA) results, we simply count the number of predicted values for the set of linguistic features in each candidate analysis.	Arabic Tokenization Part-Of-Speech Tagging And Morphological Disambiguation In One Fell Swoop We present a method that uses a tool to break down and label Arabic words with their grammatical roles, all in one step. We train computer programs to recognize specific word features and decide which analysis to choose from the tool's results. Our accuracy for all tasks is over 90%. To select the best results from the Buckwalter morphological analyzer (BAMA), we count how many times each predicted feature appears in the possible analyses.	translation quality
545	Semantic Role Labeling Using Different Syntactic Views Semantic role labeling is the process of annotating the predicate-argument structure in text with semantic labels. In this paper we present a state-of-the-art baseline semantic role labeling system based on Support Vector Machine classifiers. We show improvements on this system by: i) adding new features including features extracted from dependency parses, ii) performing feature selection and calibration and iii) combining parses obtained from semantic parsers trained using different syntactic views. Error analysis of the baseline system showed that approximately half of the argument identification errors resulted from parse errors in which there was no syntactic constituent that aligned with the correct argument. In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements. We combine systems that are based on phrase-structure parsing, dependency parsing, and shallow parsing. We use the Constituent, Predicate, and Predicate-Constituent related features for the kernel, resulting in the best performance. We combine the outputs of multiple parsers to extract reliable syntactic information, which is translated into features for a machine learning experiment in assigning semantic roles.	Semantic Role Labeling Using Different Syntactic Views Semantic role labeling is the task of marking parts of a sentence with labels that show their meaning and role. In this paper, we introduce a top-level system for semantic role labeling using Support Vector Machine classifiers. We enhance this system by: i) adding new details, including information from dependency parses (which show how words in a sentence are related), ii) choosing and adjusting the details carefully, and iii) merging results from different methods of analyzing sentence structure. An error check of the basic system showed that about half of the mistakes in identifying roles came from errors in sentence structure analysis. To fix this, we combined different sentence structure analyses from Minipar and a simplified method with our original system based on Charniak's analysis. All these methods led to better results. We mix systems based on different ways to analyze sentence structure, like phrase-structure, dependency, and shallow parsing. We use features related to parts of the sentence and their roles to improve performance. We combine results from multiple analyzers to gather accurate sentence structure information, which is used in a machine learning test to assign meaning roles.	analyses
546	Joint Learning Improves Semantic Role Labeling Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding. This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between arguments. We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative log-linear models. This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a state-of-the art independent classifier for gold-standard parse trees on PropBank. We introduce a joint approach for SRL and demonstrate that a model that scores the full predicate argument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. We employ decomposition for efficiency in training: that is, the decomposition allows us to train the classification models on a subset of training examples consisting only of those phrases that have a case marker.	Joint Learning Improves Semantic Role Labeling Despite much recent progress in accurately identifying and labeling the roles words play in sentences (semantic role labeling), previous methods mostly used separate tools that worked independently, sometimes combining these with other models that sequence labels using a method called Viterbi decoding. This approach is very different from the linguistic idea that the main parts of a sentence (core argument frame) are connected and depend on each other. We demonstrate how to create a combined model that takes into account these connections, using new features that capture these interactions in advanced log-linear models (a type of mathematical model). This system reduces errors by 22% for all arguments and 32% for core arguments compared to a top independent classifier when using perfect sentence structures from PropBank (a resource for linguistic analysis). We present a combined approach for semantic role labeling (SRL) and show that a model assessing the complete structure of a sentence can significantly reduce errors compared to using separate classifiers for each part of the sentence. To make training efficient, we use a method called decomposition, which means we train the models using only a part of the training examples, specifically those that have a case marker (a word indicating the grammatical function of a noun).	semantic
547	Paraphrasing With Bilingual Parallel Corpora Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrase-based statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments. We define a paraphrasing probability between two phrases based on their translation probability through all possible pivot phrases.	Paraphrasing With Bilingual Parallel Corpora Previous work has used collections of similar texts in the same language to find and create paraphrases, which are different ways of saying the same thing. We show that this can be done using collections of texts in two different languages, a more common resource. By using methods from computer programs that translate languages, we show how you can find paraphrases in one language by using a phrase in another language as a middle step. We define a way to measure how likely something is a paraphrase, which helps organize paraphrases found in two-language text collections using how likely they are to translate, and show how to improve this by considering surrounding words. We test our methods for finding and organizing paraphrases using a set of carefully matched words, and compare the quality with paraphrases found using automatic matching. We define how likely two phrases are paraphrases based on their chance of being translated through all possible middle phrases.	method called
548	Randomized Algorithms And NLP: Using Locality Sensitive Hash Functions For High Speed Noun Clustering In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data. We apply these algorithms to generate noun similarity lists from 70 million pages. We reduce the running time from quadratic to practically linear in the number of elements to be computed. We show that by using the LSH nearest neighbors calculation can be done in O(nd) time. Our method can produce over 70% accuracy in extracting synonyms.	Randomized Algorithms And NLP: Using Locality Sensitive Hash Functions For High Speed Noun Clustering In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data. We apply these algorithms to generate noun similarity lists from 70 million pages. We reduce the running time from quadratic (which grows quickly as data increases) to practically linear (grows slowly with more data) in the number of elements to be computed. We show that by using the LSH (Locality Sensitive Hashing, a method to find similar items) nearest neighbors calculation can be done in O(nd) time (a way to describe how fast an algorithm runs). Our method can produce over 70% accuracy in extracting synonyms (words with similar meanings).	similar texts
549	Using Emoticons To Reduce Dependency In Machine Learning Techniques For Sentiment Classification Sentiment Classification seeks to identify a piece of text according to its author's general feeling toward their subject, be it positive or negative. Traditional machine learning techniques have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the training and test data with respect to topic. This paper demonstrates that match with respect to domain and time is also important, and presents preliminary experiments with training data labeled with emoticons, which has the potential of being independent of domain, topic and time. In sentiment analysis research, we use emoticons in newsgroup articles to extract instances relevant for training polarity classifiers. We find that when authors of an electronic communication use an emotion, they are effectively marking up their own text with an emotional state.	Using Emoticons To Reduce Dependency In Machine Learning Techniques For Sentiment Classification Sentiment Classification tries to figure out if a text shows a positive or negative feeling from the writer towards a topic. Traditional machine learning methods have been used for this and have worked fairly well, but they only work well if the training data (examples used to teach the computer) and test data (new examples to check the computer) are similar in topic. This paper shows that having similar data in terms of subject area and timing is also important. It shares early experiments using training data with emoticons (like smiley faces or frowns), which might not depend on subject, topic, or time. In sentiment analysis research, we use emoticons in online group posts to find examples useful for training emotion detectors. We notice that when people use emoticons in messages, they are actually adding emotional signals to their text.	calculation
550	Multi-Engine Machine Translation Guided By Explicit Word Matching We describe a new approach for synthetically combining the output of several different Machine Translation (MT) engines operating on the same input. The goal is to produce a synthetic combination that surpasses all of the original systems in translation quality. Our approach uses the individual MT engines as "black boxes" and does not require any explicit cooperation from the original MT systems. A decoding algorithm uses explicit word matches, in conjunction with confidence estimates for the various engines and a trigram language model in order to score and rank a collection of sentence hypotheses that are synthetic combinations of words from the various original engines. The highest scoring sentence hypothesis is selected as the final output of our system. Experiments, using several Arabic-to-English systems of similar quality, show a substantial improvement in the quality of the translation output. We propose a heuristic-based matching algorithm which allows non monotonic alignments to align the words between the hypotheses.	Multi-Engine Machine Translation Guided By Explicit Word Matching We describe a new way to combine the outputs of different Machine Translation (MT) systems working on the same text. The aim is to create a combined translation that is better than all the original translations. Our method treats each MT system as a separate unit and doesn't need them to work together. A decoding process uses specific word matches, along with confidence levels for each system and a three-word language model, to evaluate and rank different sentence options that mix words from the original systems. The best sentence option is chosen as the final translation. Tests with several Arabic-to-English systems of similar quality show a big improvement in translation quality. We suggest a method based on rules that allows flexible word matching between the different sentence options.	important
551	Minimum Cut Model For Spoken Lecture Segmentation We consider the task of unsupervised lecture segmentation. We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion. Our approach moves beyond localized comparisons and takes into account long-range cohesion dependencies. Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors. We optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences. Our problem is to find topical boundaries in transcripts of course lectures. We create a corpus of course lectures segmented by four annotators, noting that the annotators operated at different levels of granularity.	Minimum Cut Model For Spoken Lecture Segmentation We are working on dividing lectures into sections without any prior training. We treat this task like solving a puzzle by cutting a "graph" into parts in the best way possible. Our method looks at the big picture instead of just small parts and considers how well everything sticks together over long stretches. Our findings show that this broader view helps us cut the lecture more accurately, even when there are mistakes in the speech-to-text process. We focus on finding the best way to cut based on how similar the sentences are to each other, using a method similar to cosine similarity (a way to measure how alike two things are). Our goal is to identify where new topics start in the written version of the lectures. We collected a set of lectures that were divided into sections by four different people, each doing it in their own detailed way.	combine
552	Bootstrapping Path-Based Pronoun Resolution We present an approach to pronoun resolution based on syntactic paths. Through a simple bootstrapping procedure, we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entities. This path information enables us to handle previously challenging resolution instances, and also robustly addresses traditional syntactic coreference constraints. Highly coreferent paths also allow mining of precise probabilistic gender/number information. We combine statistical knowledge with well known features in a Support Vector Machine pronoun resolution classifier. Significant gains in performance are observed on several datasets. Given an automatically parsed corpus, we extract from each parse tree a dependency path, which is represented as a sequence of nodes and dependency labels connecting a pronoun and a candidate antecedent, and collect statistical information from these paths to determine the likelihood that a pronoun and a candidate antecedent connected by a given path are coreferent. We show that learned gender is the most important feature in their pronoun resolution systems. We achieve achieve state-of-the-art noun gender classification performance, and we make the database of the obtained noun genders available online. We build a statistical model from paths that include the lemma of the intermediate tokens, but replace the end nodes with noun, pronoun, or pronoun-self for nouns, pronouns, and reflexive pronouns, respectively.	Bootstrapping Path-Based Pronoun Resolution We present a method for figuring out what pronouns refer to using sentence structure paths. Through a simple learning process, we determine how likely it is that a pronoun refers to a specific noun based on the path between them in a sentence structure diagram. This path information helps us solve difficult pronoun references and also effectively deals with traditional sentence structure rules for pronouns. Highly connected paths also allow us to gather detailed information about gender and number. We mix statistical knowledge with well-known features in a machine learning tool for resolving pronouns. We see major improvements in performance across several data sets. Using an automatically processed text collection, we take from each sentence structure a path showing relationships, represented as a series of points and labels linking a pronoun and a possible noun it refers to, and gather statistical data from these paths to determine how likely they are linked. We show that learning about gender is the most important part of their pronoun resolution systems. We reach top-level performance in identifying noun gender, and we share the database of the noun genders we found online. We create a statistical model from paths that include the base form of middle words, but replace the end points with noun, pronoun, or a special pronoun type for nouns, pronouns, and reflexive pronouns, respectively.	lecture
553	Discriminative Word Alignment With Conditional Random Fields In this paper we present a novel approach for inducing word alignments from sentence aligned data. We use a Conditional Random Field (CRF), a discriminative model, which is estimated on a small supervised training set. The CRF is conditioned on both the source and target texts, and thus allows for the use of arbitrary and overlapping features over these data. Moreover, the CRF has efficient training and decoding processes which both find globally optimal solutions. We apply this alignment model to both French-English and Romanian-English language pairs. We show how a large number of highly predictive features can be easily incorporated into the CRF, and demonstrate that even with only a few hundred word-aligned training sentences, our model improves over the current state-of-the-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively.	Discriminative Word Alignment With Conditional Random Fields In this paper, we introduce a new way to match words from sentences in two different languages. We use a Conditional Random Field (CRF), which is a special method trained with a small set of examples. The CRF looks at both the original and translated texts, allowing it to use different and overlapping characteristics from these texts. Additionally, the CRF is quick and effective in learning and finding the best solutions. We use this model to match words in both French-English and Romanian-English language pairs. We show that many useful characteristics can easily be added to the CRF, and that even with just a few hundred training sentences, our model is better than the current best methods, with error rates of 5.29 and 25.8 for the two tasks, respectively.	Through
554	Named Entity Transliteration With Comparable Corpora In this paper we investigate Chinese-English name transliteration using comparable corpora, corpora where texts in the two languages deal in some of the same topics - and therefore share references to named entities - but are not translations of each other. We present two distinct methods for transliteration, one approach using phonetic transliteration, and the second using the temporal distribution of candidate pairs. Each of these approaches works quite well, but by combining the approaches one can achieve even better results. We then propose a novel score propagation method that utilizes the co-occurrence of transliteration pairs within document pairs. This propagation method achieves further improvement over the best results from the previous step. We compare names from comparable and contemporaneous English and Chinese texts, scoring matches by training a learning algorithm to compare the phonemic representations of the names in the pair, in addition to taking into account the frequency distribution of the pair over time.	Named Entity Transliteration With Comparable Corpora In this paper, we explore how to change Chinese names into English using collections of texts in both languages that are on similar topics but are not direct translations of each other. We introduce two different methods for changing names: one uses sound-based transliteration and the other looks at how often the name pairs appear together over time. Both methods work well, but combining them gives even better results. We then suggest a new way to improve these results further by using a method that spreads scores based on how often name pairs appear together in related documents. This new method improves the results even more. We compare names from similar and current English and Chinese texts by using a learning program that checks how the sounds of the names match and also considers how often the names appear over time.	special method
560	Reranking And Self-Training For Parser Adaptation Statistical parsers trained and tested on the Penn Wall Street Journal (WSJ) treebank have shown vast improvements over the last 10 years. Much of this improvement, however, is based upon an ever-increasing number of features to be trained on (typically) the WSJ treebank data. This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres. Such worries have merit. The standard "Charniak parser" checks in at a labeled precision-recall f-measure of 89.7% on the Penn WSJ test set, but only 82.9% on the test set from the Brown treebank corpus. This paper should allay these fears. In particular, we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%. Furthermore, use of the self-training techniques described in (McClosky et al., 2006) raise this to 87.8% (an error reduction of 28%) again without any use of labeled Brown data. We successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation.	Reranking And Self-Training For Parser Adaptation Statistical parsers, which are computer programs that analyze sentence structure, have gotten much better over the last 10 years when they are trained and tested using the Penn Wall Street Journal (WSJ) language data set. This improvement is mostly because they are learning from more and more details in the WSJ data. However, there is a worry that these programs might work too well only on this specific data and not as well on other types of writing. These concerns are valid. The commonly used "Charniak parser" scores 89.7% accuracy in understanding sentences in the WSJ test but drops to 82.9% in a different set of sentences from the Brown data set. This paper aims to ease these worries. It shows that by using a technique called reranking, which was explained by Charniak and Johnson in 2005, the parser's performance on the Brown data improves to 85.2%. Additionally, using another method called self-training, mentioned in a 2006 study by McClosky and others, raises the accuracy to 87.8%, which means 28% fewer mistakes, and this is done without using labeled Brown data, which are pre-identified examples. We used self-training, a method that learns from unlabeled or raw data, to improve sentence understanding and saw impressive results when adapting this technique to new types of writing.	collections
555	Extracting Parallel Sub-Sentential Fragments From Non-Parallel Corpora We present a novel method for extracting parallel sub-sentential fragments from comparable, non-parallel bilingual corpora. By analyzing potentially similar sentence pairs using a signal processing-inspired approach, we detect which segments of the source sentence are translated into segments in the target sentence, and which are not. This method enables us to extract useful machine translation training data even from very non-parallel corpora, which contain no parallel sentence pairs. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. We first use the GI ZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. We first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. We use standard information retrieval together with simple word-based translation for cross-lingual information retrieval (CLIR), and extract phrases from the retrieval results using a clean bilingual lexicon and an averaging filter. We perform phrase extraction by combining clean alignment lexica for initial signals with heuristics to smooth alignments for final fragment extraction.	Extracting Parallel Sub-Sentential Fragments From Non-Parallel Corpora We introduce a new way to find matching parts of sentences in texts that are similar but not exactly the same in two different languages. By looking at sentence pairs that might be similar using a method inspired by signal processing, we figure out which parts of the original sentence match parts in the translated sentence and which do not. This allows us to gather useful data for training translation machines, even from texts that don't have any matching sentences. We test the quality of this data by showing it makes a modern translation system work better. We start by using a tool called GIZA++ (with a specific method called grow-diag-final-and) to match words between the original and translated texts, then measure how strongly these words are connected. We first find possible matching sentences from the similar texts and then accurately pick out matching parts of sentences using a special bilingual dictionary. We use standard methods to search for information along with simple word-based translation to find information across languages (CLIR), and we identify phrases from the search results using a clear bilingual dictionary and a method to smooth out the data. We identify phrases by combining clean word matching dictionaries for initial signals with strategies to refine the matches for final extraction of sentence parts.	results
556	Meaningful Clustering Of Senses Helps Boost Word Sense Disambiguation Performance Fine-grained sense distinctions are one of the major obstacles to successful Word Sense Disambiguation. In this paper, we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies, namely the Oxford Dictionary of English. We assess the quality of the mapping and the induced clustering, and evaluate the performance of coarse WSD systems in the Senseval-3 English all-words task. In our coarse-grained task, the sense inventory was first clustered semi-automatically with each cluster representing an equivalence class over senses. We present an automatic approach for mapping between sense inventories; here similarities in gloss definition and structured relations between the two sense inventories are exploited in order to map between WordNet senses and distinctions made within the coarser-grained Oxford English Dictionary. We argue that automatically creating new alignments is difficult because of word ambiguities, different granularities of senses, or language specific conceptualizations.	Meaningful Clustering Of Senses Helps Boost Word Sense Disambiguation Performance Fine-grained sense distinctions, which means very detailed differences between word meanings, are one of the main challenges in successfully telling apart different meanings of a word. In this paper, we present a method for making the WordNet sense inventory, a collection of word meanings, less detailed by connecting it to a manually created dictionary that organizes meanings, specifically the Oxford Dictionary of English. We check how good this connection is and how well the groupings work, and we test how well systems that handle less detailed word meanings perform in the Senseval-3 English all-words task. In our task, we first grouped the sense inventory into clusters, or groups, semi-automatically, with each group representing a set of similar meanings. We present an automatic way to connect different collections of word meanings; here, we use similarities in word explanations and the organized relationships between the two collections of meanings to connect WordNet meanings with the broader categories in the Oxford English Dictionary. We believe that automatically creating new connections is hard because of word ambiguities, which means words can have multiple meanings, different levels of detail in meanings, or ideas that are specific to a language.	strategies
557	Espresso: Leveraging Generic Patterns For Automatically Harvesting Semantic Relations In this paper, we present Espresso, a weakly-supervised, general-purpose, and accurate algorithm for harvesting semantic relations. The main contributions are: i) a method for exploiting generic patterns by filtering incorrect instances using the Web; and ii) a principled measure of pattern and instance reliability enabling the filtering algorithm. We present an empirical comparison of Espresso with various state of the art systems, on different size and genre corpora, on extracting various general and specific relations. Experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision. In the pattern induction step, our system computes a reliability score for each candidate pattern based on the weighted pointwise mutual information, PMI, of the pattern with all instances extracted so far. We induce specific reliable patterns in a bootstrapping manner for entity relation extraction. Our minimally-supervised Espresso algorithm is initialized with a single set that mixes seeds of heterogeneous types, such as leader-panel and oxygen-water, which respectively correspond to the member-of and sub-quantity-of relations in the taxonomy of Keet and Artale (2008).	Espresso: Leveraging Generic Patterns For Automatically Harvesting Semantic Relations In this paper, we introduce Espresso, a computer program that is not heavily supervised, is versatile, and accurately finds connections between meanings. Our main contributions are: i) a way to use broad patterns by removing wrong examples with help from the Internet; and ii) a careful method to judge how trustworthy patterns and examples are, which helps in filtering. We compare Espresso with other top systems on various texts of different sizes and types, to find different general and specific connections. Tests show that using broad patterns greatly improves the system's ability to find connections without much loss in accuracy. In the pattern creation step, our system calculates a trust score for each possible pattern using a method called weighted pointwise mutual information, PMI, which looks at how often the pattern appears with all examples found so far. We create specific trustworthy patterns step-by-step for finding connections between entities. Our program, which doesn't need much supervision, starts with a single set that mixes different types of examples, like leader-panel and oxygen-water, which relate to member-of and part-of relationships as described by Keet and Artale in 2008.	grained sense
558	Correcting ESL Errors Using Phrasal SMT Techniques This paper presents a pilot study of the use of phrasal Statistical Machine Translation (SMT) techniques to identify and correct writing errors made by learners of English as a Second Language (ESL). Using examples of mass noun errors found in the Chinese Learner Error Corpus (CLEC) to guide creation of an engineered training set, we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers. Our system was able to correct 61.81% of mistakes in a set of naturally-occurring examples of mass noun errors found on the World Wide Web, suggesting that efforts to collect alignable corpora of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners. We utilize phrasal Statistical Machine Translation (SMT) techniques to correct ESL writing errors and demonstrate that this data-intensive SMT approach is very promising, but we also point out SMT approach relies on the availability of large amount of training data.	Correcting ESL Errors Using Phrasal SMT Techniques This paper presents an initial study on using phrasal Statistical Machine Translation (SMT) methods to find and fix writing mistakes made by people learning English as a Second Language (ESL). By using examples of errors with mass nouns (words like "information" that don't usually have a plural form) found in the Chinese Learner Error Corpus (CLEC), we created a special training set. We show that using the SMT approach can find mistakes that common proofreading tools for native speakers often miss. Our system was able to fix 61.81% of errors in real-life examples of mass noun mistakes found online, indicating that collecting examples of before and after edits of ESL writings can help develop SMT-based writing tools. These tools can fix many complicated grammar and word choice problems in ESL writing. We use phrasal SMT methods to correct ESL writing errors and show that this approach, which needs a lot of data, is very promising. However, we also note that the SMT approach depends on having a large amount of training data.	specific
559	Efficient Unsupervised Discovery Of Word Categories Using Symmetric Patterns And High Frequency Words We present a novel approach for discovering word categories, sets of words sharing a significant aspect of their meaning. We utilize meta-patterns of high-frequency words and content words in order to discover pattern candidates. Symmetric patterns are then identified using graph-based measures, and word categories are created based on graph clique sets. Our method is the first pattern-based method that requires no corpus annotation or manually provided seed patterns or words. We evaluate our algorithm on very large corpora in two languages, using both human judgments and WordNet-based evaluation. Our fully unsupervised results are superior to previous work that used a POS tagged corpus, and computation time for huge corpora are orders of magnitude faster than previously reported. We show that pairs of words that often appear together in symmetric patterns tend to belong to the same class (that is, they share some notable aspect of their semantics).	Efficient Unsupervised Discovery Of Word Categories Using Symmetric Patterns And High Frequency Words We introduce a new way to find groups of words that share similar meanings. We use common patterns of frequently used words and important words to find potential patterns. Then, we identify balanced patterns using methods based on graphs, and word groups are formed based on connected sets in these graphs. Our method is the first to use patterns without needing any pre-marked text or starting words. We test our method on large text collections in two languages, using both human feedback and checks against WordNet, a language database. Our method, which doesn't need any pre-labeled text, works better than past methods that used labeled texts and is much faster for large text collections. We demonstrate that words that often appear together in balanced patterns usually belong to the same group, meaning they have similar meanings.	mistakes
561	Learning Accurate Compact And Interpretable Tree Annotation We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank. Starting with a simple X-bar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals. In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves an F1 of 90.2% on the Penn Treebank, higher than fully lexicalized systems. We use hierarchical EM training. We show that in the domain of syntactic parsing with probabilistic context-free grammars, automatically induced grammar refinements can outperform sophisticated methods which exploit substantial manually articulated structure. We introduce split-merge-smooth estimation.	Learning Accurate Compact And Interpretable Tree Annotation We introduce an automatic method for marking trees, where basic parts of a structure (nonterminal symbols) are repeatedly split into smaller parts or combined to improve how well the method can predict data from a training set of tree structures. We start with a simple set of rules (X-bar grammar) and learn new rules using smaller parts of the original structure symbols. Unlike past methods, we can split different parts of the structure to varying levels, depending on how complicated the data is. Our learned rules automatically discover the same kinds of language differences that were found in previous work done by hand. However, our rules are more straightforward and much more accurate than past automatic methods. Even though the rules are simple, our best set of rules scores 90.2% on a well-known language test (Penn Treebank), which is better than methods that use full word details. We use a step-by-step improvement method called hierarchical EM training. We show that in the area of analyzing sentence structures using statistical rules (probabilistic context-free grammars), our automatically improved rules can do better than advanced methods that rely on a lot of hand-crafted structure. We introduce a method called split-merge-smooth estimation, which helps improve these rules.	accuracy
562	Maximum Entropy Based Phrase Reordering Model For Statistical Machine Translation We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs). The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext. We present an algorithm to extract all reordering events of neighbor blocks from bilingual data. In our experiments on Chinese-to-English translation, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks. We improve the reordering model for SMT based on the collocated words crossing the neighboring components. We propose a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent. In our maximum entropy-based reordering model, MEBTG, three rules are used to derive the translation of each sub sentence: lexical rule, straight rule and inverted rule.	Maximum Entropy Based Phrase Reordering Model For Statistical Machine Translation We introduce a new way to rearrange phrases for a type of language translation tool called statistical machine translation (SMT). It uses a method called maximum entropy (MaxEnt) to predict how parts of sentences (phrase pairs) should be reordered. This model allows rearranging phrases based on their content and structure using features that it learns automatically from real bilingual texts. We explain a method to find all the ways parts of sentences can be reordered using bilingual data. In our tests with translating Chinese to English, this MaxEnt-based method greatly improved the translation quality, shown by better BLEU scores in specific translation tasks. We enhance how phrases are reordered in SMT by looking at words that appear near each other. We suggest a model for predicting how parts of sentences will change order when creating new sentence structures, based on a grammar system called bracketing transduction grammar (BTG). In our MaxEnt-based model, called MEBTG, three rules help translate parts of sentences: the lexical rule, the straight rule, and the inverted rule.	known language
563	Distortion Models For Statistical Machine Translation In this paper, we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation. We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations. We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used. We also propose a novel metric to measure word order similarity (or difference) between any pair of languages based on word alignments. Our lexicalized distortion model predicts the jump from the last translated word to the next one, with a class for each possible jump length. We find that deterministic word reordering is beyond the scope of optimization and cannot be undone by the decoder.	Distortion Models For Statistical Machine Translation In this paper, we argue that n-gram language models (a type of model that predicts the next word in a sequence) are not enough to manage the rearrangement of words needed for Machine Translation. We propose a new distortion model (a model to improve word order) that can be used with current phrase-based SMT (Statistical Machine Translation) systems to fix these limitations of n-gram language models. We present real-world results in translating Arabic to English that show clear improvements when our model is used. We also suggest a new way to measure how similar or different the word order is between any two languages, using word alignments (connections between words in both languages). Our lexicalized distortion model predicts the jump from the last translated word to the next one, with a category for each possible jump distance. We find that simple, rule-based word rearrangement is too complex to improve and cannot be corrected by the translation system.	lexical
564	Annealing Structural Bias In Multilingual Weighted Grammar Induction We first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004). Next, by annealing the free parameter that controls this bias, we achieve further improvements. We then describe an alternative kind of structural bias, toward "broken" hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement. We relate this approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1-17% (absolute) over CE (and 8-30% over EM), achieving to our knowledge the best results on this task to date. Our method, structural annealing, is a general technique with broad applicability to hidden-structure discovery problems. We penalize the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words. We propose structural annealing (SA), in which a strong bias for local dependency attachments is enforced early in learning, and then gradually relaxed. Our annealing approach tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step.	Annealing Structural Bias In Multilingual Weighted Grammar Induction We first show how focusing on nearby connections (structural locality bias) can enhance the accuracy of advanced models for understanding sentence structures, which are trained using a method called EM (Expectation-Maximization) from examples without labels (Klein and Manning, 2004). Next, by gradually adjusting the control of this bias (annealing the free parameter), we achieve even better results. We then describe another type of structural bias, which involves incomplete structures over parts of sentences ("broken" hypotheses), and show it also leads to improvements. We connect this approach to a method called contrastive estimation (Smith and Eisner, 2005a), apply it to understanding sentence structures in six languages, and show that our new method increases accuracy by 1-17% more than contrastive estimation and 8-30% more than EM, achieving what we believe to be the best results so far. Our method, called structural annealing, is a general technique useful for discovering hidden patterns. We limit the predicted connections between words in a sentence structure task to prevent distant word connections. We suggest structural annealing (SA), which strongly prefers nearby word connections at the start and then loosens this preference over time. Our annealing approach gradually changes the hidden outcomes during the E-step to help learning in the M-step.	Statistical Machine
574	An All-Subtrees Approach To Unsupervised Parsing We investigate generalizations of the all-subtrees "DOP" approach to unsupervised parsing. Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees. We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent. We report state-of-the-art results on English (WSJ), German (NEGRA) and Chinese (CTB) data. To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal, leading to the surprising result that an unsupervised parsing model beats a widely used supervised model (a treebank PCFG). We find that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences ? 40 words, while the binarized version achieves only 64.6% f-score.	An All-Subtrees Approach To Unsupervised Parsing We look into expanding the all-subtrees "DOP" method for unsupervised parsing, which means analyzing the structure of sentences without pre-labeled data. Unsupervised DOP models try all possible ways to break down sentences into parts and then use a large random selection of these parts to figure out the most likely sentence structures. We will examine both a method that uses relative frequencies and a method based on maximum likelihood, which is a statistical approach that ensures reliability. We show top-level results using data in English (WSJ), German (NEGRA), and Chinese (CTB). As far as we know, this is the first study testing the maximum likelihood method for DOP on the Wall Street Journal data, revealing that this unsupervised method performs better than a popular supervised method, which uses pre-labeled data (a treebank PCFG). We find that using a grammar structure that isn't broken into smaller parts scores an average of 72.3% accuracy on WSJ sentences with up to 40 words, while the broken-down version scores only 64.6% accuracy.	without
565	Tree-To-String Alignment Template For Statistical Machine Translation We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models. We perform derivation-level combination for mixing different types of translation rules within one derivation. We also add non-syntactic PBSMT - phrase-based statistical machine translation - phrases into our tree-to-string translation system.	Tree-To-String Alignment Template For Statistical Machine Translation We introduce a new translation model that uses a method called tree-to-string alignment template (TAT). This method shows how a structured source language (parse tree) aligns with a target language (string of words). A TAT can create both direct words (terminals) and placeholders (non-terminals), and it can rearrange words in simple or complex ways. The model is based on language structure (syntax) because TATs are taken automatically from texts that have been aligned by words and have a structured format on the source side. To translate a sentence, we first use a tool to create a structured format (parse tree) from the source language, and then use TATs to change this structure into a string of words in the target language. Our tests show that the TAT-based model is much better than Pharaoh, which is a top-performing tool for phrase-based translation. We combine different types of translation rules within one process to improve results. We also incorporate phrases from non-structured phrase-based statistical machine translation (PBSMT) into our tree-to-string translation system.	gradually changes
566	An Unsupervised Morpheme-Based HMM For Hebrew Morphological Disambiguation Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text. When the word is ambiguous (there are several possible analyses for the word), a disambiguation procedure based on the word context must be applied. This paper deals with morphological disambiguation of the Hebrew language, which combines morphemes into a word in both agglutinative and fusional ways. We present an unsupervised stochastic model - the only resource we use is a morphological analyzer - which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language. We present a text encoding method for languages with affixational morphology in which the knowledge of word formation rules (which are quite restricted in Hebrew) helps in the disambiguation. We adapt HMM algorithms for learning and searching this text representation, in such a way that segmentation and tagging can be learned in parallel in one step. Results on a large scale evaluation indicate that this learning improves disambiguation for complex tag sets. Our method is applicable to other languages with affix morphology. We provide for each word not only the PoS, but also full morphological features, such as Gender, Number, Person, Construct, Tense, and the affixes' properties. We present a lattice-based modification of the BaumWelch algorithm to handle the segmentation ambiguity.	An Unsupervised Morpheme-Based HMM For Hebrew Morphological Disambiguation Morphological disambiguation is the process of picking the correct set of word features for each word in a text. When a word is unclear (it can mean several things), a procedure based on surrounding words must be used to clarify it. This paper focuses on Hebrew language disambiguation, which combines word parts in different ways. We introduce a model that doesn't rely on pre-labeled data - we only use a tool that breaks words into parts - to tackle the issue of limited data because of Hebrew's complex word structure. We describe a way to encode text for languages with complex word parts, using knowledge of Hebrew's word-building rules to help clarify meaning. We modify HMM algorithms to learn and search this text format, allowing for identifying and labeling word parts at the same time. Large evaluations show this learning method enhances clarity for complicated word label sets. Our method can be used for other languages with complex word structures. For each word, we provide not only the part of speech but also full features like gender, number, person, form, tense, and word parts' details. We offer a modified version of the Baum-Welch algorithm to deal with unclear word part separation.	combine
567	Contextual Dependencies In Unsupervised Word Segmentation Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages, and may shed light on how humans learn to segment speech. We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively. The bigram model greatly outperforms the unigram model (and previous probabilistic models), demonstrating the importance of such dependencies for word segmentation. We also show that previous probabilistic models rely crucially on sub-optimal search procedures. We start at a random derivation of the corpus, and at every iteration resample a derivation by amending the current one through local changes made at the node level. We use hierarchical Dirichlet processes (HDP) to induce contextual word models.	Contextual Dependencies In Unsupervised Word Segmentation Developing better ways to break down continuous text into words is important for improving how we process Asian languages, and it can also help us understand how people learn to break down spoken language into words. We suggest two new methods based on Bayesian statistics for word segmentation, which assume two types of word relationships: unigram (single word) and bigram (pair of words) models. The bigram model performs much better than the unigram model (and previous models based on probability), showing how important these word relationships are for breaking text into words. We also show that earlier models based on probability depend heavily on not-so-great search methods. We begin with a random breakdown of the text, and in each step, we adjust this breakdown by making small changes at specific points. We use hierarchical Dirichlet processes (HDP), which is a statistical method, to create models that consider word context.	building
568	A Discriminative Global Training Algorithm For Statistical MT This paper presents a novel training algorithm for a linearly-scored block sequence translation model. The key component is a new procedure to directly optimize the global scoring function used by a SMT decoder. No translation, language, or distortion model probabilities are used as in earlier work on SMT. Therefore our method, which employs less domain specific knowledge, is both simpler and more extensible than previous approaches. Moreover, the training procedure treats the decoder as a black-box, and thus can be used to optimize any decoding scheme. The training algorithm is evaluated on a standard Arabic-English translation task. We use a BLEU oracle decoder for discriminative training of a local reordering model. We use a perceptron style algorithm for training a large number of features. We compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple, local reordering models. We present a procedure to directly optimize the global scoring function used by a phrase based decoder on the accuracy of the translations.	A Discriminative Global Training Algorithm For Statistical MT This paper introduces a new way (training algorithm) to improve a model that translates block sequences (pieces of text) and scores them using a simple formula. The main part is a new method to improve the overall scoring function used by SMT (Statistical Machine Translation) software. Unlike earlier methods, we don't use the chances (probabilities) of translation, language, or changes in order. This makes our method, which uses less specific information, simpler and easier to expand than older methods. Also, the new method treats the decoder (the part that translates) as a black-box, meaning it can work with any translation method. We tested this training on a common task of translating Arabic to English. We use a special technique (BLEU oracle decoder) to train a model that changes the order of words. We use a perceptron style method (a simple algorithm) to train many features. We find high BLEU scores (a measure of translation quality) by using a regular decoder to improve sentence-by-sentence BLEU-4 scores with a simple way to change word order. We show a way to directly improve the overall scoring function used by a phrase-based decoder to make translations more accurate.	great search
569	Machine Learning Of Temporal Relations This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts. To address data sparseness, we used temporal reasoning as an oversampling method to dramatically expand the amount of training data, resulting in predictive accuracy on link labeling as high as 93% using a Maximum Entropy classifier on human annotated data. This method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions. While machine learning approaches attempt to improve classification accuracy through feature engineering, we introduce a temporal reasoning component to greatly expand the training data. We use the links introduced by closure to boost the amount of training data for a tlink classifier.	Machine Learning Of Temporal Relations This paper looks at using machine learning to figure out the order and timing of events in written language. To handle the problem of not having enough data, we used a technique called temporal reasoning to create more training examples, which helped us reach a 93% accuracy in predicting event links using a Maximum Entropy classifier on data marked by humans. This method did well compared to other complex techniques based on human intuition. While traditional machine learning tries to improve accuracy by selecting important features, we added a temporal reasoning element to significantly increase the training data. We used the connections created by closure (a method of linking data) to increase the training data for a tlink classifier (a tool that predicts temporal links).	improve
570	Semi-Supervised Training For Statistical Word Alignment We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus. We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality. If human-aligned data is available, the EMD algorithm provides higher baseline alignments than GIZA++ that have led to better MT performance. We combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences. We pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. We propose an EMD algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional Expectation-Maximization algorithm used in IBM models.	Semi-Supervised Training For Statistical Word Alignment We introduce a semi-supervised method for training in statistical machine translation. This method switches between a traditional step called Expectation Maximization, used on a large set of training data, and a decision-making step targeting better word matching on a small set of data that has been manually matched. We demonstrate that our method not only improves word matching but also results in better quality machine translation. If data aligned by humans is available, the EMD algorithm provides better starting alignments than GIZA++, leading to improved machine translation performance. We mix a model that creates word alignments with a decision-making model trained on a few sentences that are hand-matched. We treat the alignment task like a search in a decision-making space with features from IBM alignment models. We suggest an EMD algorithm for word matching that includes a decision-making step in every round of the traditional Expectation-Maximization process used in IBM models.	connections created
571	Semantic Taxonomy Induction From Heterogenous Evidence We propose a novel algorithm for inducing semantic taxonomies. Previous algorithms for taxonomy induction have typically focused on independent classifiers for discovering new single relationships based on hand-constructed or automatically discovered textual patterns. By contrast, our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy, using knowledge of a word's coordinate terms to help in determining its hypernyms, and vice versa. We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition, where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantic taxonomy (WordNet 2.1). We add 10,000 novel synsets to WordNet 2.1 at 84% precision, a relative error reduction of 70% over a non-joint algorithm using the same component classifiers. Finally, we show that a taxonomy built using our algorithm shows a 23% relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs. We generate tens of thousands of hypernym patterns and combine these with noun clusters to generate high-precision suggestions for unknown noun insertion into WordNet. We use known hypernym / hyponym pairs to generate training data for a machine-learning system, which then learns many lexico-syntactic patterns. We add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy. We use syntactic path patterns as features for supervised hyponymy and synonymy classifiers, whose training examples are derived automatically from WordNet.	Semantic Taxonomy Induction From Heterogenous Evidence We propose a new method for creating semantic taxonomies, which are like family trees for words that show how they are related in meaning. Previous methods focused on finding single connections between words using patterns found in text, either by hand or automatically. Our method is different because it uses information from several sources about different kinds of word relationships to improve the whole structure. It uses knowledge about similar words to figure out what broader categories they belong to, and vice versa. We test our method on the task of finding specific types of nouns, combining predictions about word categories with existing knowledge from a tool called WordNet 2.1. We successfully add 10,000 new word groups to WordNet 2.1 with high accuracy, making 70% fewer mistakes compared to older methods. Our new method improves the accuracy by 23% over WordNet 2.1 when tested on a separate set of word category examples. We create many patterns to find broader categories for nouns and use these to suggest where new words might fit in WordNet. We use known word pairs to teach a computer system to recognize patterns in language. We add new words by choosing those that best fit the patterns we have, based on the evidence available. We use specific sentence structures as clues for teaching the computer to recognize when words are similar or belong to a larger category, using examples from WordNet.	Maximization process
572	Weakly Supervised Named Entity Transliteration And Discovery From Multilingual Comparable Corpora Named Entity recognition (NER) is an important part of many natural language processing tasks. Current approaches often employ machine learning techniques and require supervised data. However, many languages lack such resources. This paper presents an (almost) unsupervised learning algorithm for automatic discovery of Named Entities (NEs) in a resource free language, given a bilingual corpora in which it is weakly temporally aligned with a resource rich language. NEs have similar time distributions across such corpora, and often some of the tokens in a multi-word NE are transliterated. We develop an algorithm that exploits both observations iteratively. The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration. Seeded with a small number of transliteration pairs, our algorithm discovers multi-word NEs, and takes advantage of a dictionary (if one exists) to account for translated or partially translated NEs. We evaluate the algorithm on an English-Russian corpus, and show high level of NEs discovery in Russian. We explore the use of a perceptron-based ranking model for the purpose of finding name transliterations across comparable corpora. Our feature space consists of n-gram pairs from the two languages. We find that the projection of NER tags is easier in comparison to projecting other types of annotations such as POS-tags and BPC. We introduce a Russian data set comprised of temporally aligned news articles.	Weakly Supervised Named Entity Transliteration And Discovery From Multilingual Comparable Corpora Named Entity recognition (NER) is a key part of many tasks in understanding human languages using computers. Current methods often use machine learning, which needs labeled data to learn. But many languages don't have these resources. This paper introduces a nearly unsupervised learning method to automatically find Named Entities (NEs) in languages without resources, using bilingual text collections that are loosely matched over time with a language that has more resources. NEs tend to appear around the same times in these texts, and sometimes parts of multi-word NEs are converted from one script to another. We created a method that takes advantage of these patterns repeatedly. This method uses a new way to measure time similarities and a method to identify script conversion without extra resources. Starting with a few known script conversion examples, our method finds multi-word NEs and uses a dictionary (if available) to handle NEs that are fully or partly translated. We test the method using English and Russian texts, showing it finds many NEs in Russian. We look at using a model that ranks to find name conversions across similar texts in different languages. Our features are pairs of short word sequences (n-grams) from both languages. We discover that transferring NER tags is simpler than transferring other labels like parts of speech (POS) and some other linguistic categories (BPC). We introduce a Russian dataset made of news articles matched over time.	several
573	A Composite Kernel To Extract Relations Between Entities With Both Flat And Structured Features This paper proposes a novel composite kernel for relation extraction. The composite kernel consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples. The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction. Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features. Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction. We use a convolution tree kernel (CTK, Collins and Duffy, 2001) to investigate various structured information for relation extraction and find that the Shortest Path enclosed Tree (SPT) achieves the F-measure of 67.7 on the 7 relation types of the ACE RDC 2004 corpus.	A Composite Kernel To Extract Relations Between Entities With Both Flat And Structured Features This paper introduces a new method called a composite kernel to find connections between entities. The composite kernel is made up of two parts: one part focuses on features related to the entities themselves, and the other part uses a model called a convolution parse tree kernel to understand the grammar structure of examples. The goal of this method is to make use of the advantages of kernel methods to explore different kinds of information for finding relationships. Our research shows that this composite kernel can effectively capture both simple and complex features without needing a lot of extra work to create features. It can also easily be expanded to include more features. Tests on the ACE dataset show that our method performs better than the best methods reported before and does much better than two earlier models using dependency trees for finding relationships. We use a method called a convolution tree kernel (CTK) to look into different structured information for finding relationships and discover that using the Shortest Path enclosed Tree (SPT) achieves an accuracy rate, or F-measure, of 67.7 on the 7 types of relationships in the ACE RDC 2004 dataset.	Comparable
575	Methods For Using Textual Entailment In Open-Domain Question Answering Work on the semantics of questions has argued that the relation between a question and its answer(s) can be cast in terms of logical entailment. In this paper, we demonstrate how computational systems designed to recognize textual entailment can be used to enhance the accuracy of current open-domain automatic question answering (Q/A) systems. In our experiments, we show that when textual entailment information is used to either filter or rank answers returned by a Q/A system, accuracy can be increased by as much as 20% overall. we applied a TE component to rerank candidate answers returned by a retrieval step for the task of Question Answering.	Methods For Using Textual Entailment In Open-Domain Question Answering Work on understanding the meaning of questions has suggested that the link between a question and its answer(s) can be explained using logical entailment, which means if one statement is true, another related statement must also be true. In this paper, we show how computer systems that identify textual entailment, which is understanding if one piece of text logically follows from another, can improve the accuracy of current open-domain automatic question answering (Q/A) systems. In our tests, we demonstrate that using textual entailment information to either sort or filter answers given by a Q/A system can improve accuracy by up to 20% in total. We used a Textual Entailment (TE) component to reorder possible answers returned by a search step for the task of Question Answering.	examine
576	Using String-Kernels For Learning Semantic Parsers We present a new approach for mapping natural language sentences to their formal meaning representations using string-kernel-based classifiers. Our system learns these classifiers for every production in the formal language grammar. Meaning representations for novel natural language sentences are obtained by finding the most probable semantic parse using these string classifiers. Our experiments on two real-world data sets show that this approach compares favorably to other existing systems and is particularly robust to noise. We use word subsequence kernel to compute the similarity between two substrings. Our model, KRISP, takes a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. KRISP (Kernel-based Robust Interpretation for Semantic Parsing) (Kate and Mooney, 2006) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data.	Using String-Kernels For Learning Semantic Parsers We introduce a new method for converting regular sentences into their formal or structured meanings using special tools called string-kernel-based classifiers. Our system learns these tools for each rule in the formal language structure. For new sentences, it figures out the most likely structured meaning using these string tools. Our tests on two real-world examples show this method works well compared to other systems and handles errors well. We use a process that looks at parts of words to find how similar two parts are. Our model, KRISP, builds meaning structures from regular sentences in a layered way. KRISP (Kernel-based Robust Interpretation for Semantic Parsing) (Kate and Mooney, 2006) is a system that learns by example for understanding sentence meanings, using pairs of sentences and their meanings as learning material.	computer systems
577	Scalable Inference And Training Of Context-Rich Syntactic Translation Models Statistical MT has made great progress in the last few years, but current translation models are weak on re-ordering and target language fluency. Syntactic approaches seek to remedy these problems. In this paper, we take the framework for acquiring multi-level syntactic translation rules of (Galley et al., 2004) from aligned tree-string pairs, and present two main extensions of their approach: first, instead of merely computing a single derivation that minimally explains a sentence pair, we construct a large number of derivations that include contextually richer rules, and account for multiple interpretations of unaligned words. Second, we propose probability estimates and a training procedure for weighting these rules. We contrast different approaches on real examples, show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated, and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules. We use xRS formalism to allow for the use of translation rules that have multi-level target tree annotations and discontinuous source language phrases. Our rule composing method composes two or more minimal GHKM or SPMT rules having shared states to form larger rules.	Scalable Inference And Training Of Context-Rich Syntactic Translation Models Statistical Machine Translation (MT) has improved a lot recently, but current translation models struggle with changing word order and making the translated language sound natural. Syntactic (structure-based) methods try to solve these issues. In this paper, we use a method for getting multi-level syntactic translation rules from aligned tree-string pairs (Galley et al., 2004) and introduce two main improvements: first, instead of just finding one simple way to explain a sentence pair, we create many ways that use contextually richer rules and consider different meanings of unaligned words. Second, we suggest ways to estimate probabilities and a training process to prioritize these rules. We compare different methods using real examples, show that our estimates using multiple sentence interpretations prefer word re-orderings that make more sense, and prove that our larger rules improve translation quality by 3.63 BLEU points compared to minimal rules. We use xRS formalism to apply translation rules with multiple levels of target tree annotations and source phrases that aren't continuous. Our method combines two or more basic GHKM or SPMT rules with shared elements to create larger rules.	other systems
578	Empirical Lower Bounds On The Complexity Of Translational Equivalence This paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts. The study found that the complexity of these patterns in every bitext was higher than suggested in the literature. These findings shed new light on why syntactic constraints have not helped to improve statistical translation models, including finite-state phrase-based models, tree-to-string models, and tree-to-tree models. The paper also presents evidence that inversion transduction grammars cannot generate some translational equivalence relations, even in relatively simple real bitexts in syntactically similar languages with rigid word order. Our methodology measures the complexity of word alignment using the number of gaps that are necessary for their synchronous parser which allows discontinuous spans to succeed in parsing. Translational equivalence is a mathematical relation that holds between linguistic expressions with the same meaning. We argue for the necessity of discontinuous spans (i.e., for a formalism beyond Synchronous CFG) in order for synchronous parsing to cover human-annotated word alignment data under the constraint that rules have a rank of no more than two.	Empirical Lower Bounds On The Complexity Of Translational Equivalence This paper describes a study of the patterns of how translations match up between two languages, found in different sets of bilingual texts. The study found that these matching patterns were more complicated than what was previously thought. These findings help explain why certain language rules haven't improved statistical translation methods, like phrase-based models that use limited state changes, and models that convert between different tree structures. The paper also provides evidence that certain grammar systems can't handle some translation matches, even in simple bilingual texts of languages that have similar sentence structures and strict word order. Our method checks how complicated it is to match words by counting the gaps needed for a special parser that can handle breaks in sentence order. Translational equivalence is when language expressions mean the same thing. We argue that allowing for breaks in sentence structure (more advanced than basic Synchronous Context-Free Grammars) is necessary to match human-annotated word pairs, while keeping rules simple with a limit of two parts.	estimates
588	The Second Release Of The RASP System We describe the new release of the RASP (robust accurate statistical parsing) system, designed for syntactic annotation of free text. The new version includes a revised and more semantically-motivated output representation, an enhanced grammar and part-of-speech tagger lexicon, and a more flexible and semi-supervised training method for the structural parse ranking model. We evaluate the released version on the WSJ using a relational evaluation scheme, and describe how the new release allows users to enhance performance using (in-domain) lexical information.	The Second Release Of The RASP System We describe the new version of the RASP system, which is used for adding grammatical structure to text. This new version has an improved way of showing results that considers meaning more, a better list of grammar rules and word types, and a more adaptable way to train the system using some guidance for ranking sentence structures. We test this version on the Wall Street Journal using a method that compares relationships and explain how users can improve results by using specific vocabulary related to their field.	labeled
579	A Hierarchical Bayesian Language Model Based On Pitman-Yor Processes We propose a new hierarchical Bayesian n-gram model of natural languages. Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages. We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney, one of the best smoothing methods for n-gram language models. Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney. We priovide a Bayesian interpretation to smoothing techniques, such as Kneser-Ney and Witten-Bell back-off schemes. Nonparametric Bayesian modeling is able to provide priors that are especially suitable for tasks in NLP. While the Dirichlet process is simply the Pitman Yor process with d= 0, we find that the discount parameter allows for more effective modeling of the long-tailed distributions that are often found in natural language.	A Hierarchical Bayesian Language Model Based On Pitman-Yor Processes We suggest a new structured approach using Bayesian n-gram models to understand natural languages better. Our model uses an advanced version of the usual method called Dirichlet distributions, specifically the Pitman-Yor processes, which create patterns that look more like how words naturally appear in languages. We demonstrate that a simplified version of our hierarchical Pitman-Yor language model matches the precise setup of interpolated Kneser-Ney, known as one of the best ways to smooth out n-gram language models. Tests show that our model achieves better results in measuring unpredictability (cross entropy) than the interpolated Kneser-Ney and is similar to the modified Kneser-Ney. We offer a Bayesian (a method to make predictions using probabilities) explanation for smoothing techniques like Kneser-Ney and Witten-Bell back-off schemes. Nonparametric Bayesian modeling can create starting points (priors) that are particularly helpful for tasks in natural language processing (NLP). While the Dirichlet process is just a simpler form of the Pitman-Yor process with a specific setting (d=0), we find that adjusting the discount parameter helps in better modeling of the long-tailed distributions, which are common in natural language.	provides evidence
580	Word Sense And Subjectivity Subjectivity and meaning are both important properties of language. This paper explores their interaction, and brings empirical evidence in support of the hypotheses that (1) subjectivity is a property that can be associated with word senses, and (2) word sense disambiguation can directly benefit from subjectivity annotations. We study the distinction between objectivity and subjectivity in each different sense of a word, and their empirical effects in the context of sentiment analysis. We provide evidence that word sense labels, together with contextual subjectivity analysis, can be exploited to improve performance in word sense disambiguation. We show that even reliable subjectivity clues have objective senses. We show that subjectivity annotations can be helpful for word sense disambiguation when a word has distinct subjective senses and objective senses. We conduct a study on human annotation of 354 words senses with polarity and report a high inter-annotator agreement. We define subjective expressions as words and phrases being used to express mental and emotional states, such as speculations, evaluations, sentiments, and beliefs.	Word Sense And Subjectivity Subjectivity (personal feelings or opinions) and meaning are both important properties of language. This paper looks at how they interact and provides real-world evidence for the ideas that (1) subjectivity can be linked to different meanings of a word, and (2) figuring out a word's meaning can benefit from knowing if it's subjective or not. We look at the difference between objective (fact-based) and subjective (opinion-based) meanings of a word, and their effects in understanding opinions and emotions in text. We show that knowing if a word sense is subjective can help improve the process of figuring out its correct meaning. We demonstrate that even words with clear subjective meanings can have factual meanings. We show that knowing if a word is subjective or objective can help when a word has both types of meanings. We study how people label 354 word meanings with positive or negative feelings and find that people mostly agree on these labels. We define subjective expressions as words and phrases used to express mental and emotional states, like guesses, judgments, feelings, and beliefs.	create starting
581	A Phrase-Based Statistical Model For SMS Text Normalization Short Messaging Service (SMS) texts behave quite differently from normal written texts and have some very special phenomena. To translate SMS texts, traditional approaches model such irregularities directly in Machine Translation (MT). However, such approaches suffer from customization problem as tremendous effort is required to adapt the language model of the existing translation system to handle SMS text style. We offer an alternative approach to resolve such irregularities by normalizing SMS texts before MT. In this paper, we view the task of SMS normalization as a translation problem from the SMS language to the English language and we propose to adapt a phrase-based statistical MT model for the task. Evaluation by 5-fold cross validation on a parallel SMS normalized corpus of 5000 sentences shows that our method can achieve 0.80702 in BLEU score against the baseline BLEU score 0.6958. Another experiment of translating SMS texts from English to Chinese on a separate SMS text corpus shows that, using SMS normalization as MT preprocessing can largely boost SMS translation performance from 0.1926 to 0.3770 in BLEU score. We also use Phrase-based SMT techniques on character level. We use a phrase-based statistical machine translation model by splitting sentences into their k most probable phrases.	A Phrase-Based Statistical Model For SMS Text Normalization Short Messaging Service (SMS) texts act very differently from regular written texts and have some unique features. To convert SMS texts, traditional methods try to directly address these differences in Machine Translation (MT). However, these methods face a customization problem because it takes a lot of work to change the language model of the current translation system to handle the SMS style. We offer a different solution to fix these differences by adjusting SMS texts before using MT. In this paper, we treat the task of SMS normalization as a translation problem from SMS language to the English language, and we suggest using a phrase-based statistical MT model for this task. Testing with 5-fold cross-validation (a method to check accuracy) on a collection of 5000 SMS sentences shows our method can achieve a BLEU score of 0.80702, compared to the baseline score of 0.6958. Another test of translating SMS texts from English to Chinese on a different SMS text collection shows that using SMS normalization before MT can greatly improve SMS translation performance from a BLEU score of 0.1926 to 0.3770. We also use Phrase-based SMT (Statistical Machine Translation) techniques at the character level. We use a phrase-based statistical machine translation model by dividing sentences into their k most likely phrases.	subjective
582	Evaluating The Accuracy Of An Unlexicalized Statistical Parser On The PARC DepBank We evaluate the accuracy of an unlexicalized statistical parser, trained on 4K treebanked sentences from balanced data and tested on the PARC DepBank. We demonstrate that a parser which is competitive in accuracy (without sacrificing processing speed) can be quickly tuned without reliance on large in-domain manually-constructed treebanks. This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure. The comparison of systems using DepBank is not straightforward, so we extend and validate DepBank and highlight a number of representation and scoring issues for relational evaluation schemes. We show that the system has equivalent accuracy to the PARC XLE parser when the morphosyntactic features in the original DepBank gold standard are taken into account. We provide annotation for internal NP structure. We recommend looking at accuracy figures by dependency type to understand what a parser is good at. We re annotated DepBank using GRs scheme, and used it to evaluate the RASP parser.	Evaluating The Accuracy Of An Unlexicalized Statistical Parser On The PARC DepBank We check how correct an unlexicalized statistical parser is. It's trained on 4,000 sentences from a balanced set of data and tested on the PARC DepBank. We show that a parser can be accurate and fast without needing large, manually-made treebanks from specific fields, making it easier to use in tools that need to understand sentence structures. Comparing systems with DepBank is complicated, so we improved and checked DepBank, pointing out some issues with how results are represented and scored. We show that the system is just as accurate as the PARC XLE parser when considering certain language features in the original DepBank standard. We also provide detailed notes on the internal structure of noun phrases. We suggest looking at accuracy by the type of dependency to see what a parser is good at. We updated DepBank using a GRs scheme and used it to test the RASP parser.	suggest
583	Soft Syntactic Constraints For Word Alignment Through Discriminative Training Word alignment methods can gain valuable guidance by ensuring that their alignments maintain cohesion with respect to the phrases specified by a monolingual dependency tree. However, this hard constraint can also rule out correct alignments, and its utility decreases as alignment models become more complex. We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint. The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser. We use dependency structures as soft constraints to improve word alignment in an ITG framework. We introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment.	Soft Syntactic Constraints For Word Alignment Through Discriminative Training Word alignment methods can improve by making sure their word pairings stay connected with the phrases determined by a single-language dependency tree (a structure showing how words depend on each other). However, strictly following this rule can sometimes ignore correct pairings, and its usefulness lessens as pairing models become more advanced. We use a publicly available structured output Support Vector Machine (SVM) to create a max-margin syntactic aligner, which allows some flexibility in keeping phrases together. This aligner is the first, as far as we know, to use a learning method that focuses on differences to train an ITG bitext parser (a tool for analyzing bilingual text). We use dependency structures as flexible rules to improve word pairings in an ITG framework (a method for managing translations). We add flexible syntactic ITG (Wu, 1997) rules into a model that focuses on differences, and use an ITG parser to limit the search for the best possible word pairing.	features
584	Mildly Non-Projective Dependency Structures Syntactic parsing requires a fine balance between expressivity and complexity, so that naturally occurring structures can be accurately parsed without compromising efficiency. In dependency-based parsing, several constraints have been proposed that restrict the class of permissible structures, such as projectivity, planarity, multi-planarity, well-nestedness, gap degree, and edge degree. While projectivity is generally taken to be too restrictive for natural language syntax, it is not clear which of the other proposals strikes the best balance between expressivity and complexity. In this paper, we review and compare the different constraints theoretically, and provide an experimental evaluation using data from two treebanks, investigating how large a proportion of the structures found in the treebanks are permitted under different constraints. The results indicate that a combination of the well-nestedness constraint and a parametric constraint on discontinuity gives a very good fit with the linguistic data.	Mildly Non-Projective Dependency Structures Syntactic parsing, which is the process of analyzing sentence structure, needs to find a balance between being detailed and being simple enough to work efficiently. Dependency-based parsing is a method that uses rules to limit the types of sentence structures that are allowed, such as projectivity (which means not allowing crossing branches), planarity (flat structure), multi-planarity, well-nestedness (properly ordered), gap degree (distance between elements), and edge degree (connection limit). Although projectivity is often seen as too limiting for natural language, it's unclear which of the other rules works best. In this paper, we review and compare these different rules and test them with data from two collections of tree structures, to see how many structures fit under each rule. The results show that using the well-nestedness rule along with another rule that controls gaps provides a very good match with the language data.	aligner
585	On-Demand Information Extraction At present, adapting an Information Extraction system to new topics is an expensive and slow process, requiring some knowledge engineering for each new topic. We propose a new paradigm of Information Extraction which operates 'on demand' in response to a user's query. On-demand Information Extraction (ODIE) aims to completely eliminate the customization effort. Given a user's query, the system will automatically create patterns to extract salient relations in the text of the topic, and build tables from the extracted information using paraphrase discovery technology. It relies on recent advances in pattern discovery, paraphrase discovery, and extended named entity tagging. We report on experimental results in which the system created useful tables for many topics, demonstrating the feasibility of this approach.	On-Demand Information Extraction At present, adapting an Information Extraction system to new topics is an expensive and slow process, requiring some knowledge engineering (specialized understanding) for each new topic. We propose a new paradigm (model) of Information Extraction which operates 'on demand' in response to a user's query (question). On-demand Information Extraction (ODIE) aims to completely eliminate the customization effort (work needed to adjust it). Given a user's query, the system will automatically create patterns to extract salient (important) relations in the text of the topic, and build tables from the extracted information using paraphrase discovery technology (finding different ways to say the same thing). It relies on recent advances (new improvements) in pattern discovery, paraphrase discovery, and extended named entity tagging (recognizing and classifying names). We report on experimental results in which the system created useful tables for many topics, demonstrating the feasibility (possibility) of this approach.	structures
586	Minimum Risk Annealing For Training Log-Linear Models When training the parameters for a natural language system, one would prefer to minimize 1-best loss (error) on an evaluation set. Since the error surface for many natural language problems is piecewise constant and riddled with local minima, many systems instead optimize log-likelihood, which is conveniently differentiable and convex. We propose training instead to minimize the expected loss, or risk. We define this expectation using a probability distribution over hypotheses that we gradually sharpen (anneal) to focus on the 1-best hypothesis. Besides the linear loss functions used in previous work, we also describe techniques for optimizing nonlinear functions such as precision or the BLEU metric. We present experiments training log-linear combinations of models for dependency parsing and for machine translation. In machine translation, annealed minimum risk training achieves significant improvements in BLEU over standard minimum error training. We also show improvements in labeled dependency parsing. We use a linearization technique to approximate the expectation of log BLEU score. We present a deterministic annealing training procedure, whose objective is to minimize the expected error (together with the entropy regularization technique). We observe test set gains by minimum risk annealing, which incorporates a temperature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface.	Minimum Risk Annealing For Training Log-Linear Models When training the settings for a natural language system, you want to reduce the main mistake on a test group. Since the mistake pattern for many language problems is uneven and full of tricky spots, many systems instead focus on log-likelihood, which is easier to calculate and has a nice, smooth shape. We suggest training to reduce the expected mistake, or risk. We do this by using a probability model over possible answers that we gradually narrow down to focus on the best answer. Besides the simple mistake methods used before, we also explain ways to improve complex methods like precision or the BLEU score, which measures translation quality. We show tests with models for sentence structure analysis and for translating languages. In translating, this new method leads to better BLEU scores than the usual error reduction method. We also see better results in sentence structure analysis. We use a method to roughly estimate the expected log BLEU score. We introduce a planned approach to training that aims to minimize expected mistakes, using a technique to manage unpredictability. We see better test results with this method, which uses a factor to balance how smooth the goal is and how well it shows the real mistake pattern.	useful tables
587	Unsupervised Part-Of-Speech Tagging Employing Efficient Graph Clustering An unsupervised part-of-speech (POS) tagging system that relies on graph clustering methods is described. Unlike in current state-of-the-art approaches, the kind and number of different tags is generated by the method itself. We compute and merge two partitionings of word graphs: one based on context similarity of high frequency words, another on log-likelihood statistics for words of lower frequencies. Using the resulting word clusters as a lexicon, a Viterbi POS tagger is trained, which is refined by a morphological component. The approach is evaluated on three different languages by measuring agreement with existing taggers. We directly compare the tagger output to supervised taggers for English, German and Finnish via information-theoretic measures. We conceptualize a network of words that capture the word co-occurrence patterns. We cluster the most frequent 10,000 words using contexts formed from the most frequent 150-200 words.	Unsupervised Part-Of-Speech Tagging Employing Efficient Graph Clustering An unsupervised system for part-of-speech (POS) tagging, which doesn't require labeled data, uses graph clustering techniques. Unlike current leading methods, it automatically determines the types and number of tags. We create and combine two groupings of word graphs: one based on how often words appear together for common words, and another using statistical measures for less common words. The groups of words formed are used as a dictionary to train a Viterbi POS tagger, which is improved with a word structure analysis part. The method is tested on three languages by checking how well it matches existing taggers. We directly compare the results with taggers that use labeled data for English, German, and Finnish using methods that measure the amount of information. We create a network of words to understand how often words appear together. We group the 10,000 most common words using the contexts formed by the 150-200 most common words.	unpredictability
593	Forest Rescoring: Faster Decoding with Integrated Language Models Efficient decoding has been a fundamental problem in machine translation, especially with an integrated language model which is essential for achieving good translation quality. We develop faster approaches for this problem based on k-best parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the same levels of search error and translation accuracy. We make assumptions about the the amount of reordering the language model can trigger in order to limit exploration. We introduce cube pruning and and its variation, cube growing. We use the forest concept to characterize the search space of decoding with integrated language models.	Forest Rescoring: Faster Decoding with Integrated Language Models Efficient decoding, or quickly turning input text into another language, has been a big challenge in translating languages, especially when using an integrated language model (a tool that helps improve the quality of translations). We create faster methods for this issue using k-best parsing algorithms (techniques to find the best translations) and show their effectiveness in both phrase-based and syntax-based (structure-focused) translation systems. In both cases, our methods significantly speed up the process, often more than ten times faster, compared to the usual beam-search method (a common technique) without losing quality or making more mistakes. We make assumptions about how much rearranging the language model can cause to limit how much we explore different possibilities. We introduce cube pruning and its variation, cube growing (both are techniques to cut down unnecessary options). We use the idea of a forest to describe the range of possibilities in decoding with integrated language models.	Disambiguation
589	Tailoring Word Alignments to Syntactic Machine Translation Extracting tree transducer rules for syntactic MT systems can be hindered by word alignment errors that violate syntactic correspondences. We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model. Our model's predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posterior-based methods of reconciling bidirectional alignments. We refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. We use hard union competitive thresholding. We use a syntax based distance in an HMM word alignment model to favor syntax-friendly alignments.	Tailoring Word Alignments to Syntactic Machine Translation Extracting tree transducer rules for syntactic MT (Machine Translation) systems can be made difficult by word alignment mistakes that disrupt the matching of sentence structures. We suggest a new method for automatically matching words that specifically considers the sentence structure of the target language, while still being as strong and fast as the HMM (Hidden Markov Model) alignment method. Our method's predictions help to get more rules for tree transducers without losing the quality of word matching. We also talk about the effects of different methods that balance word alignments from both languages. We improve the part of the HMM aligner that deals with word order to consider the distance between parts of sentence structures instead of just the order of words. We use a method called hard union competitive thresholding. We apply a distance based on sentence structure in an HMM word alignment model to prefer alignments that fit well with sentence structures.	vocabulary related
590	Transductive learning for statistical machine translation Statistical machine translation systems are usually trained on large amounts of bilingual text and monolingual text in the target language. In this paper we explore the use of transductive semi-supervised methods for the effective use of monolingual data from the source language in order to improve translation quality. We propose several algorithms with this aim, and present the strengths and weaknesses of each one. We present detailed experimental evaluations on the French-English EuroParl data set and on data from the NIST Chinese-English large-data track. We show a significant improvement in translation quality on both tasks.	Transductive learning for statistical machine translation Statistical machine translation systems are usually trained using large collections of two-language texts (bilingual) and single-language texts (monolingual) in the language being translated into (target language). In this paper, we look into using a method called transductive semi-supervised learning, which helps make better use of single-language text from the language being translated from (source language) to improve the quality of translations. We suggest several methods to achieve this and discuss what each method does well and what it doesn’t. We provide detailed tests using French-English texts from the EuroParl data set and Chinese-English texts from the NIST large-data track. We demonstrate a noticeable improvement in translation quality for both sets of tasks.	Alignments
591	Word Sense Disambiguation Improves Statistical Machine Translation Recent research presents conflicting evidence on whether word sense disambiguation (WSD) systems can help to improve the performance of statistical machine translation (MT) systems. In this paper, we successfully integrate a state-of-the-art WSD system into a state-of-the-art hierarchical phrase-based MT system, Hiero. We show for the first time that integrating a WSD system improves the performance of a state-of-the-art statistical MT system on an actual translation task. Furthermore, the improvement is statistically significant. We train a discriminative model for WSD using local but also across-sentence unigram collocations of words in order to refine phrase pair selection dynamically by incorporating scores from the WSD classifier. We use an SVM based classifier for disambiguating word senses which are directly incorporated in the decoder through additional features that are part of the log-linear combination of models.	Word Sense Disambiguation Improves Statistical Machine Translation Recent research shows mixed results on whether systems that help identify the correct meaning of words (WSD systems) can make statistical machine translation (MT) systems better. In this paper, we successfully combine an advanced WSD system with an advanced translation system called Hiero. We demonstrate for the first time that adding a WSD system enhances the performance of a top-level statistical MT system during a real translation task. Moreover, this improvement is clearly significant. We train a model that learns to distinguish word meanings using nearby word patterns within the same sentence and also in nearby sentences. This helps in choosing the right phrase pairs by using scores from the WSD system. We use a classifier method called SVM to figure out word meanings, which is included in the translation process through extra features that are part of a combined model approach.	several
592	Domain Adaptation with Active Learning for Word Sense Disambiguation When a word sense disambiguation (WSD) system is trained on one domain but applied to a different domain, a drop in accuracy is frequently observed. This highlights the importance of domain adaptation for word sense disambiguation. In this paper, we first show that an active learning approach can be successfully used to perform domain adaptation of WSD systems. Then, by using the predominant sense predicted by expectation-maximization (EM) and adopting a count-merging technique, we improve the effectiveness of the original adaptation process achieved by the basic active learning approach. We perform supervised domain adaptation on a manually selected subset of 21 nouns from the DSO corpus. We notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning.	Domain Adaptation with Active Learning for Word Sense Disambiguation When a system that figures out the meaning of words (WSD) is trained for one topic but used for a different topic, it often becomes less accurate. This shows why it is important to adjust WSD systems for different topics (domain adaptation). In this paper, we first demonstrate that using an approach where the system actively selects useful examples (active learning) can help adjust WSD systems for different topics. Then, by using the most common meaning predicted by a method called expectation-maximization (EM) and combining word counts, we make the original adjustment process better than just using active learning alone. We carry out supervised domain adjustment on a carefully chosen group of 21 nouns from the DSO collection. We notably show that noticing changes in the most common meaning, as modeled by topic-based sense expectations (domain sense priors), can enhance the ability to figure out meanings, even after adjustment with active learning.	choosing
594	A Simple Similarity-based Model for Selectional Preferences We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics. Focusing on the task of semantic role labeling, we compute selectional preferences for semantic roles. In evaluations the similarity-based model shows lower error rates than both Resnik's WordNet-based model and the EM-based clustering model, but has coverage problems. We extract the set of seen head words from corpora with semantic role annotation, and use only a single vector space representation. We model the contexts of a word as the distribution of words that co-occur with it. We select a subset of roles in FrameNet (Baker et al, 1998) to test and uses all labeled instances within this subset.	A Simple Similarity-based Model for Selectional Preferences We suggest a new, easy way to automatically figure out selectional preferences, which are choices made by words about what other words they go with, by using data from texts and measuring how similar words are. We focus on identifying roles that words play in sentences and calculate preferences for these roles. In tests, our similarity-based model makes fewer mistakes compared to Resnik's model, which uses a large dictionary called WordNet, and another model based on grouping words, but our model sometimes misses some words. We gather main words from texts that have been tagged with these roles and use just one type of data representation. We consider what other words appear with a word to understand its context. We pick a few roles from a resource called FrameNet to test and use all examples labeled within these roles.	effectiveness
595	Fully Unsupervised Discovery of Concept-Specific Relationships by Web Mining We present a web mining method for discovering and enhancing relationships in which a specified concept (word class) participates. We discover a whole range of relationships focused on the given concept, rather than generic known relationships as in most previous work. Our method is based on clustering patterns that contain concept words and other words related to them. We evaluate the method on three different rich concepts and find that in each case the method generates a broad variety of relationships with good precision. We introduce the use of term frequency patterns for relationship discovery. As a pre-requisite to extracting relations among pairs of classes, our method extracts class instances from unstructured Web documents, by submitting pairs of instances as queries and analyzing the contents of the top 1,000 documents returned by a Web search engine. We propose a method for unsupervised discovery of concept specific relations, requiring initial word seeds.	Fully Unsupervised Discovery of Concept-Specific Relationships by Web Mining We introduce a method using web data to find and improve relationships involving a specific idea or category of words. Instead of looking for general and known links, we find many different connections focused on the specific idea. Our approach involves grouping patterns that have words related to the idea and other connected words. We test this method on three different detailed ideas and find that it consistently identifies a wide range of relationships accurately. We use how often terms appear together to discover these connections. Before finding relationships among pairs of categories, our method identifies examples of these categories from web pages by sending pairs of examples as searches and examining the contents of the top 1,000 results from a search engine. We suggest a method that finds specific connections related to ideas without supervision, starting with some initial example words.	selectional preferences
596	Adding Noun Phrase Structure to the Penn Treebank The Penn Treebank does not annotate within base noun phrases (NPs), committing only to at structures that ignore the complexity of English NPs. This means that tools trained on Treebank data cannot learn the correct internal structure of NPs. This paper details the process of adding gold-standard bracketing within each noun phrase in the Penn Treebank. We then examine the consistency and reliability of our annotations. Finally, we use this resource to determine NP structure using several statistical approaches, thus demonstrating the utility of the corpus. This adds detail to the Penn Treebank that is necessary for many NLP applications. Our annotation scheme inserts NML and JJP brackets to describe the correct NP structure. We use NE tags during the annotation process, as we find that NER based features will be helpful in a statistical model.	Adding Noun Phrase Structure to the Penn Treebank The Penn Treebank does not show details inside simple noun phrases (NPs), only showing flat structures that ignore how complex English NPs can be. This means that tools trained with Treebank data cannot learn the correct details inside NPs. This paper explains the process of adding high-quality brackets inside each noun phrase in the Penn Treebank. We then check how consistent and reliable our added notes are. Finally, we use this resource to figure out NP structure using several methods based on statistics, showing how useful the collection is. This adds important details to the Penn Treebank needed for many natural language processing (NLP) applications. Our way of adding notes includes NML and JJP brackets to show the correct NP structure. We use NE tags during the note-taking process, as we find that features based on recognizing names (NER) will be helpful in a statistical model.	detailed
597	Formalism-Independent Parser Evaluation with CCG and DepBank A key question facing the parsing community is how to compare parsers which use different grammar formalisms and produce different output. Evaluating a parser on the same resource used to create it can lead to non-comparable accuracy scores and an over-optimistic view of parser performance. In this paper we evaluate a CCG parser on DepBank, and demonstrate the difficulties in converting the parser output into DepBank grammatical relations. In addition we present a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy. The CCG parser obtains an F-score of 81.9% on labelled dependencies, against an upper bound of 84.8%. We compare the CCG parser against the RASP parser, outperforming RASP by over 5% overall and on the majority of dependency types. We develop a set of mapping rules from the output of a Combinatorial Categorial grammar parser to the Grammatical Relations (GR) (Carroll et al, 1998). We demonstrate the use of techniques like adaptive super tagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C & C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al, 2009).	Formalism-Independent Parser Evaluation with CCG and DepBank A key question for those studying computer programs that analyze sentence structure is how to compare different programs (parsers) that use various grammar systems and create different outputs. Testing a parser with the same data it was built with can result in misleadingly high accuracy scores and give an overly positive impression of its performance. In this paper, we test a CCG parser with DepBank and show the challenges of changing the parser's results to match DepBank's grammar rules. We also introduce a way to measure how well this conversion works, which sets a limit on how accurate the parsing can be. The CCG parser gets a score of 81.9% for correctly identifying sentence parts, compared to a best possible score of 84.8%. We compare the CCG parser with the RASP parser, finding that CCG performs better by more than 5% overall and in most types of sentence parts. We create a set of rules to change the output from a Combinatorial Categorial Grammar parser to match Grammatical Relations (GR). We show how methods like adaptive super tagging, doing multiple tasks at once (parallelization), and a specific type of algorithm (dynamic-programming chart parsing) help make the C & C parser, a very efficient CCG parser that does well compared to those using other grammar systems.	trained
598	Instance Weighting for Domain Adaptation in NLP Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains. In this paper, we study the domain adaptation problem from the instance weighting perspective. We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains. We then propose a general instance weighting framework for domain adaptation. Our empirical results on three NLP tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective. We weigh training instances based on their similarity to unlabeled target domain data. We find that balanced bootstrapping is more effective in domain adaptation than standard bootstrapping. In our instance weighting, we assign larger weights to transferable instances so that the model trained on the source domain can adapt more effectively to the target domain.	Instance Weighting for Domain Adaptation in NLP Domain adaptation is a key issue in natural language processing (NLP) because there isn't enough labeled data in new areas. In this paper, we look at this problem by focusing on how we can adjust the importance of each example, or instance, from one area to another. We carefully study and describe the problem of adapting from one domain to another by looking at the differences in how data is spread out. We show that there are two main reasons for needing to adapt: the differences in the types of data (instances) and how categories are determined (classification functions) in the starting area (source) and the new area (target). We suggest a broad approach to adjusting the importance of each instance to help with domain adaptation. Our practical results in three NLP tasks show that using more information from the target area by adjusting instance importance works well. We adjust the importance of training examples based on how similar they are to data from the target area that hasn't been labeled. We discover that a balanced method of gradually adding new examples (balanced bootstrapping) works better for adapting to new areas than the regular method (standard bootstrapping). In our approach, we give more importance to examples that can be easily used in the new area, so the model trained in the starting area can adjust more effectively to the new area.	those using
599	Guiding Semi-Supervision with Constraint-Driven Learning Over the last few years, two of the main research directions in machine learning of natural language processing have been the study of semi-supervised learning algorithms as a way to train classifiers when the labeled data is scarce, and the study of ways to exploit knowledge and global information in structured learning tasks. In this paper, we suggest a method for incorporating domain knowledge in semi-supervised learning algorithms. Our novel framework unifies and can exploit several kinds of task specific constraints. The experimental results presented in the information extraction domain demonstrate that applying constraints helps the model to generate better feedback during learning, and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks. We introduce constraint driven learning, CoDL. We use constraints at multiple levels, such as sentence-level constraints to specify field boundaries and global constraints to ensure relation-level consistency.	Guiding Semi-Supervision with Constraint-Driven Learning Over the last few years, two main research areas in machine learning for understanding language have been finding ways to train systems when we don't have much labeled data, and using existing knowledge and information for tasks that need structured learning. In this paper, we propose a method to add specialized knowledge into these learning techniques. Our new approach combines and uses different task-specific rules. The tests we did in the field of extracting information show that using these rules helps the system learn better, allowing it to perform well with much less training data than before. We introduce a method called constraint-driven learning, CoDL. We apply rules at different levels, like sentence-level rules to define field limits and overall rules to make sure relationships are consistent.	because there
600	Supertagged Phrase-Based Statistical Machine Translation Until quite recently, extending Phrase-based Statistical Machine Translation (PBSMT) with syntactic structure caused system performance to deteriorate. In this work we show that incorporating lexical syntactic descriptions in the form of supertags can yield significantly better PBSMT systems. We describe a novel PBSMT model that integrates supertags into the target language model and the target side of the translation model. Two kinds of supertags are employed: those from Lexicalized Tree-Adjoining Grammar and Combinatory Categorial Grammar. Despite the differences between these two approaches, the supertaggers give similar improvements. In addition to supertagging, we also explore the utility of a surface global grammaticality measure based on combinatory operators. We perform various experiments on the Arabic to English NIST 2005 test set addressing issues such as sparseness, scalability and the utility of system subcomponents. Our best result (0.4688 BLEU) improves by 6.1% relative to a state-of-the- art PBSMT model, which compares very favourably with the leading systems on the NIST 2005 task.	Supertagged Phrase-Based Statistical Machine Translation Until recently, making Phrase-based Statistical Machine Translation (PBSMT) include grammar structures made the system work worse. In this work, we show that using detailed grammatical descriptions called supertags can greatly improve PBSMT systems. We describe a new PBSMT model that includes supertags in both the language being translated to and the translation process. We use two types of supertags: one from Lexicalized Tree-Adjoining Grammar and another from Combinatory Categorial Grammar. Even though these two methods are different, they both improve results similarly. Besides using supertags, we also look at a general grammar check based on combination operators. We conduct several tests on translating Arabic to English using the NIST 2005 test set, focusing on problems like data scarcity, system growth, and the usefulness of different system parts. Our best result (0.4688 BLEU score) is 6.1% better than a top PBSMT model, which is very good compared to the best systems in the NIST 2005 task.	extracting information
601	Improved Word-Level System Combination for Machine Translation Recently, confusion network decoding has been applied in machine translation system combination. Due to errors in the hypothesis alignment, decoding may result in ungrammatical combination outputs. This paper describes an improved confusion network based method to combine outputs from multiple MT systems. In this approach, arbitrary features may be added log-linearly into the objective function, thus allowing language model expansion and re-scoring. Also, a novel method to automatically select the hypothesis which other hypotheses are aligned against is proposed. A generic weight tuning algorithm may be used to optimize various automatic evaluation metrics including TER, BLEU and METEOR. The experiments using the 2005 Arabic to English and Chinese to English NIST MT evaluation tasks show significant improvements in BLEU scores compared to earlier confusion network decoding based methods. We use the tercom script (Snover et al, 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another. We propose a multiple confusion network (CN) or super-network framework, where we use each of all individual system results as the backbone to build CNs based on an alignment metric, TER. Each word in the confusion network is associated with a word posterior probability.	Improved Word-Level System Combination for Machine Translation Recently, a method called confusion network decoding has been used to combine outputs from different machine translation systems. However, mistakes in matching sentences can cause incorrect and awkward results. This paper talks about a better method using confusion networks to mix results from multiple translation systems. In this method, different features can be added in a way that improves the translation quality by allowing for more language model testing and scoring. Additionally, a new way to automatically choose which translation to align others with is introduced. A general method for adjusting weights can be used to improve various automatic evaluation scores like TER, BLEU, and METEOR. Tests with translations from Arabic to English and Chinese to English in 2005 show much better BLEU scores than older methods using confusion networks. We use a script called tercom that uses strategies and a method called dynamic programming to find a set of changes needed to transform one sentence into another. We suggest a framework called multiple confusion network or super-network, where each system's results are used as a main structure to create confusion networks based on a matching measure called TER. Each word in this network is linked to a chance of being correct.	using detailed
602	Fast Unsupervised Incremental Parsing This paper describes an incremental parser and an unsupervised learning algorithm for inducing this parser from plain text. The parser uses a representation for syntactic structure similar to dependency links which is well-suited for incremental parsing. In contrast to previous unsupervised parsers, the parser does not use part-of-speech tags and both learning and parsing are local and fast, requiring no explicit clustering or global optimization. The parser is evaluated by converting its output into equivalent bracketing and improves on previously published results for unsupervised parsing from plain text. Our incremental parsing approach uses a novel representation called common cover links, which can be converted to constituent brackets. Though punctuation is usually entirely ignored in unsupervised parsing research, we use phrasal punctuation - punctuation symbols that often mark phrasal boundaries within a sentence.	Fast Unsupervised Incremental Parsing This paper explains a step-by-step parser, which is a tool that figures out sentence structure, and a learning method that teaches this tool by using regular text. The parser uses a way to show sentence structure similar to dependency links, which are connections that show how words relate to each other, and it works well with step-by-step parsing. Unlike older parsers that learn without guidance, this parser doesn’t rely on part-of-speech tags (labels like noun or verb). It learns and works quickly without needing to group words into categories or find the best overall solution. The parser's results are checked by turning them into a format called bracketing, which groups words together, and it performs better than previous methods for understanding text structure without guidance. Our step-by-step method uses a new way called common cover links, which can be changed into group brackets. Even though punctuation is usually ignored in this type of research, we focus on punctuation that often shows the boundaries of phrases within a sentence.	translation
626	A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence. These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance. Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity. Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far. Goldberg and Tsarfaty (2008) concluded that an integrated model of morphological disambiguation and syntactic parsing in Hebrew Treebank parsing improves the results of a pipelined approach. We demonstrate that the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text.	A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing Morphological processes in Semitic languages create words separated by spaces that introduce several different grammar parts into the sentence. These words are often unclear, making it hard for most grammar analyzers to predict the sentence structure ahead of time. We suggest a unified model that handles both breaking down words into parts (morphological segmentation) and clarifying the sentence structure (syntactic disambiguation) without the usual complications. Our model uses a special type of grammar (treebank grammar), a data-based word list (lexicon), and a smart way to deal with unknown words. It works better than previous systems that handled the tasks separately or in parts, reducing errors by 12% compared to the best results so far for Hebrew language processing. Goldberg and Tsarfaty (2008) found that combining word breakdown and sentence structure analysis improves outcomes compared to handling them one after the other. We show that using a method called lattice parsing, which processes both tasks together, is effective for Hebrew text.	supervised latent
603	Structured Models for Fine-to-Coarse Sentiment Analysis In this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Inference in the model is based on standard sequence classification techniques using constrained Viterbi to ensure consistent solutions. The primary advantage of such a model is that it allows classification decisions from one level in the text to influence decisions at another. Experiments show that this method can significantly reduce classification error relative to models trained in isolation. We showed that jointly learning fine-grained (sentence) and coarse-grained (document) sentiment improves predictions at both levels.	Structured Models for Fine-to-Coarse Sentiment Analysis In this paper, we look into a structured model that classifies the sentiment (feelings) of text at different detail levels. The model uses usual sequence classification methods with a restricted Viterbi algorithm to make sure solutions are consistent. The main benefit of this model is that it lets decisions made at one text level affect decisions at another level. Tests show that this method can greatly lower mistakes in classification compared to models that are trained separately. We demonstrated that learning both detailed (sentence) and broad (document) sentiment together improves predictions at both levels.	without
604	Biographies Bollywood Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains. We introduced a multi-domain sentiment dataset.	**Biographies Bollywood Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification** Automatic sentiment classification, which is a way to automatically figure out if people's opinions or feelings are positive or negative, has been studied a lot and used in recent years. But since people express their feelings differently in different areas, it's not realistic to label every possible area we're interested in. We look into how to make sentiment classifiers, which are tools that help us understand emotional tones, work better across different areas, focusing on online reviews for various products. First, we improved a new method called the structural correspondence learning (SCL) algorithm for figuring out sentiment, cutting down mistakes by 30% compared to the original SCL method and by 46% compared to a basic supervised method. Second, we found a way to measure how similar two areas are, which helps predict how well a tool for one area can be used in another. This method can help choose a few areas to label so their tools can work well in many other areas. We also created a dataset that includes opinions from multiple areas.	demonstrated
605	Statistical Machine Translation for Query Expansion in Answer Retrieval We present an approach to query expansion in answer retrieval that uses Statistical Machine Translation (SMT) techniques to bridge the lexical gap between questions and answers. SMT-based query expansion is done by i) using a full-sentence paraphraser to introduce synonyms in context of the entire query, and ii) by translating query terms into answer terms using a full-sentence SMT model trained on question-answer pairs. We evaluate these global, context-aware query expansion techniques on tfidf retrieval from 10 million question-answer pairs extracted from FAQ pages. Experimental results show that SMT-based expansion improves retrieval performance over local expansion and over retrieval without expansion. We demonstrate the advantages of translation-based approach to answer retrieval by utilizing a more complex translation model also trained from a large amount of data extracted from FAQs on the Web.	Statistical Machine Translation for Query Expansion in Answer Retrieval We present a method to make questions better for finding answers by using Statistical Machine Translation (SMT) techniques to help match words between questions and answers. SMT-based query expansion is done by i) using a full-sentence paraphraser to add synonyms that fit the whole question, and ii) by changing question words into answer words using a full-sentence SMT model trained on question-answer pairs. We test these smart query expansion techniques on tfidf retrieval from 10 million question-answer pairs taken from FAQ pages. Test results show that using SMT-based expansion improves how well we find answers compared to using simpler methods or not expanding at all. We show the benefits of using a translation-based method for finding answers by using a more advanced translation model, also trained from a lot of data collected from FAQs on the Web.	another
606	Randomised Language Modelling for Statistical Machine Translation A Bloom filter (BF) is a randomised data structure for set membership queries. Its space requirements are significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability. Here we explore the use of BFs for language modelling in statistical machine translation. We show how a BF containing n-grams can enable us to use much larger corpora and higher-order models complementing a conventional n-gram LM within an SMT system. We also consider (i) how to include approximate frequency information efficiently within a BF and (ii) how to reduce the error rate of these models by first checking for lower-order sub-sequences in candidate n-grams. Our solutions in both cases retain the one-sided error guarantees of the BF while taking advantage of the Zipf-like distribution of word frequencies to reduce the space requirements. We present a scheme for associating static frequency information with a set of n-grams in a BF efficiently.	Randomised Language Modelling for Statistical Machine Translation A Bloom filter (BF) is a special tool that uses random methods to check if something is part of a group. It uses less space than what is usually needed, but it can sometimes say something is there when it isn't (false positives), and we can measure how often this happens. Here, we look at using BFs to help with language modelling in translation done by computers. We explain how a BF that holds word sequences called n-grams can help us use much bigger collections of text and more complex models alongside a regular n-gram language model in a statistical machine translation system. We also look at (i) how to add rough frequency details efficiently into a BF and (ii) how to lower the mistake rate of these models by first checking smaller parts of candidate n-grams. Our methods keep the one-sided error promise of the BF while using how often words appear in a Zipf-like pattern to take up less space. We describe a way to attach fixed frequency details with a group of n-grams in a BF efficiently.	answer words
607	Learning to Extract Relations from the Web using Minimal Supervision We present a new approach to relation extraction that requires only a handful of training examples. Given a few pairs of named entities known to exhibit or not exhibit a particular relation, bags of sentences containing the pairs are extracted from the web. We extend an existing relation extraction method to handle this weaker form of supervision, and present experimental results demonstrating that our approach can reliably extract relations from web documents. We provide a dataset that contains multiple realizations of an entity pair in a target semantic relation, unlike similar datasets from previous work.	Learning to Extract Relations from the Web using Minimal Supervision We present a new method to find connections between pieces of information that only needs a small number of examples to learn from. Given a few pairs of specific names or things that show or don't show a certain connection, groups of sentences with these pairs are taken from the internet. We improve an existing method for finding these connections to work with less guidance, and we show test results proving our method can consistently find connections in online documents. We offer a collection of data that includes many examples of how two entities (like people or things) relate in a specific way, which is different from similar data collections used in past research.	complex models
627	A Tree Sequence Alignment-based Tree-to-Tree Translation Model This paper presents a translation model that is based on tree sequence alignment, where a tree sequence refers to a single sequence of subtrees that covers a phrase. The model leverages on the strengths of both phrase-based and linguistically syntax-based method. It automatically learns aligned tree sequence pairs with mapping probabilities from word-aligned biparsed parallel texts. Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span. This gives our model stronger expressive power than other reported models. Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems. Our method generates all possible tree fragments rooted by each node in the source parse tree or forest, and then matches all the generated tree fragments against the source parts (left hand side) of translation rules to extract the useful rules.	A Tree Sequence Alignment-based Tree-to-Tree Translation Model This paper introduces a translation model that uses tree sequence alignment, which means arranging a sequence of small tree structures to cover a phrase. The model combines the advantages of both phrase-based (using word groups) and syntax-based (using sentence structure) methods. It automatically learns pairs of tree sequences with mapping probabilities from texts that have been parsed (analyzed grammatically) and word-aligned (matching words across languages). Unlike earlier models, it captures non-structured phrases and phrases that are spread out, using linguistically structured features, and also supports rearranging structures at different levels with a wider range. This makes our model more powerful than other models. Tests on a 2005 Chinese-English translation task show that our method performs better than standard systems. Our method creates all possible tree fragments (small parts of a tree structure) starting from each point in the source tree or forest, and then matches these fragments with the source parts of translation rules to find useful rules.	grammar
608	A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation Inspired by previous preprocessing approaches to SMT, this paper proposes a novel, probabilistic approach to reordering which combines the merits of syntax and phrase-based SMT. Given a source sentence and its parse tree, our method generates, by tree operations, an n-best list of reordered inputs, which are then fed to standard phrase-based decoder to produce the optimal translation. Experiments show that, for the NIST MT-05 task of Chinese-to-English translation, the proposal leads to BLEU improvement of 1.56%. We use a maximum entropy system to learn reordering rules for binary trees (i.e., whether to keep or reorder for each node). We model reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation.	A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation Inspired by earlier ways to prepare data for SMT (Statistical Machine Translation), this paper suggests a new method that uses probability to rearrange words. This method combines the strengths of using grammar rules (syntax) and phrases in SMT. Given a sentence and its structure (parse tree), our method changes the order of words using tree operations to create a list of the best possible rearrangements. These are then used by a standard phrase-based translator to find the best translation. Tests show that for the NIST MT-05 task, which involves translating from Chinese to English, this approach improves translation quality by 1.56% as measured by BLEU, a tool that evaluates translation accuracy. We use a system called maximum entropy to learn rules for rearranging binary trees (deciding whether to keep the current order or change it for each part). We apply this system to rearrange parts of the sentence structure using both the visible sentence features and hidden grammatical features to improve Chinese-to-English translation.	examples
609	Machine Translation by Triangulation: Making Effective Use of Multi-Parallel Corpora Current phrase-based SMT systems perform poorly when using small training sets. This is a consequence of unreliable translation estimates and low coverage over source and target phrases. This paper presents a method which alleviates this problem by exploiting multiple translations of the same source phrase. Central to our approach is triangulation, the process of translating from a source to a target language via an intermediate third language. This allows the use of a much wider range of parallel corpora for training, and can be combined with a standard phrase-table using conventional smoothing methods. Experimental results demonstrate BLEU improvements for triangulated models over a standard phrase-based system.	Machine Translation by Triangulation: Making Effective Use of Multi-Parallel Corpora Current phrase-based SMT (Statistical Machine Translation) systems do not work well with small training data sets. This happens because the translation guesses are not reliable and there is not enough variety in the phrases used to translate from one language to another. This paper introduces a way to fix this issue by using many different translations of the same phrase in the original language. The key idea is triangulation, which means translating from one language to another by going through a third language. This method lets us use a larger variety of translation examples for training and can be mixed with a regular phrase list using typical methods to fill in missing data. Tests show that triangulated models improve BLEU scores, which measure translation quality, compared to a regular phrase-based system.	approach improves
610	A fully Bayesian approach to unsupervised part-of-speech tagging Unsupervised learning of linguistic structure is a difficult problem. A common approach is to define a generative model and maximize the probability of the hidden structure given the observed data. Typically, this is done using maximum-likelihood estimation (MLE) of the model parameters. We show using part-of-speech tagging that a fully Bayesian approach can greatly improve performance. Rather than estimating a single set of parameters, the Bayesian approach integrates over all possible parameter values. This difference ensures that the learned structure will have high probability over a range of possible parameters, and permits the use of priors favoring the sparse distributions that are typical of natural language. Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discriminative model (Smith and Eisner, 2005), up to 14 percentage points better than MLE. We find improvements both when training from data alone, and using a tagging dictionary. In our model, the transition, emission and coupling parameters are governed by Dirichlet priors, and a token-level collapsed Gibbs sampler is used for inference.	A fully Bayesian approach to unsupervised part-of-speech tagging Unsupervised learning of language structure is a tough challenge. A common method is to create a model that can generate data and increase the chances of guessing the hidden structure based on what we observe. Usually, this is done by finding the most likely values for the model's settings, called maximum-likelihood estimation (MLE). We demonstrate, using part-of-speech tagging (identifying words as nouns, verbs, etc.), that a fully Bayesian approach can significantly enhance results. Instead of picking one set of settings, the Bayesian method considers all possible settings. This ensures that the learned structure is likely correct across many different settings and allows using 'priors,' which are initial beliefs that favor simpler patterns commonly found in language. Our model is structured like a regular trigram HMM (a model that considers sequences of three words), but its performance is almost as good as the best models available (like the one by Smith and Eisner, 2005), and up to 14% better than using MLE. We see improvements whether we train from data alone or use a dictionary for tagging. In our model, we use Dirichlet priors (a way to set initial beliefs about data) to control how words and their categories are connected, and a technique called a token-level collapsed Gibbs sampler (a method for making educated guesses) is used to draw conclusions.	which measure
611	Guided Learning for Bidirectional Sequence Classification In this paper, we propose guided learning, a new learning framework for bidirectional sequence classification. The tasks of learning the order of inference and training the local classifier are dynamically incorporated into a single Perceptron like learning algorithm. We apply this novel learning algorithm to POS tagging. It obtains an error rate of 2.67% on the standard PTB test set, which represents 3.3% relative error reduction over the previous best result on the same data set, while using fewer features. Our model is competitive to CRF in tagging accuracy but requires much less training time. We develop new algorithms based on the easiet-first strategy and the perceptron algorithm.	Guided Learning for Bidirectional Sequence Classification In this paper, we introduce guided learning, a new way to teach computers to understand sequences in both directions. We combine the tasks of figuring out the order of reasoning and training the part of the computer that makes decisions into one simple learning method, similar to a Perceptron (a basic type of artificial neuron). We use this new learning method for POS tagging, which is labeling parts of a sentence like nouns and verbs. It achieves a 2.67% error rate on the standard PTB test set, meaning it makes fewer mistakes than previous methods on the same data set, and it does this with less information. Our model is as accurate as CRF (a popular method for tagging) but takes much less time to train. We create new methods based on starting with the easiest parts and a simple learning strategy.	commonly
633	EM Can Find Pretty Good HMM POS-Taggers (When Given a Good Start) We address the task of unsupervised POS tagging. We demonstrate that good results can be obtained using the robust EM-HMM learner when provided with good initial conditions, even with incomplete dictionaries. We present a family of algorithms to compute effective initial estimations p(t|w). We test the method on the task of full morphological disambiguation in Hebrew achieving an error reduction of 25% over a strong uniform distribution baseline. We also test the same method on the standard WSJ unsupervised POS tagging task and obtain results competitive with recent state-of-the-art methods, while using simple and efficient learning methods. We use linguistic considerations for choosing a good starting point for the EM algorithm. We note that fixing noisy dictionaries by hand is actually quite feasible, and suggest that effort should focus on exploiting human knowledge rather than just algorithmic improvements.	EM Can Find Pretty Good HMM POS-Taggers (When Given a Good Start) We tackle the challenge of unsupervised POS (Part of Speech) tagging, which means identifying parts of speech in text without pre-labeled examples. We show that you can get good results using a strong learning method called EM-HMM, especially when you start with good initial settings, even if your word lists (dictionaries) are not complete. We introduce a group of methods to make good starting guesses for which part of speech matches each word (p(t|w)). We test this approach on a complex task in Hebrew, reducing mistakes by 25% compared to a basic method. We also try it on a common English text task (WSJ) and get results similar to the best recent methods, while keeping the learning process simple and efficient. We use language knowledge to pick a good starting point for the EM method. We point out that fixing incorrect word lists manually is quite doable, and suggest focusing more on using human knowledge than just improving algorithms.	humans communicate
612	Exploiting Syntactic and Shallow Semantic Kernels for Question Answer Classification We study the impact of syntactic and shallow semantic information in automatic classification of questions and answers and answer re-ranking. We define (a) new tree structures based on shallow semantics encoded in Predicate Argument Structures (PASs) and (b) new kernel functions to exploit the representational power of such structures with Support Vector Machines. Our experiments suggest that syntactic information helps tasks such as question/answer classification and that shallow semantics gives remarkable contribution when a reliable set of PASs can be extracted, e.g. from answers. Our shallow semantic representations, bearing a more compact information, can prevent the sparseness of deep structural approaches and the weakness of BOW models.	Exploiting Syntactic and Shallow Semantic Kernels for Question Answer Classification We examine how using sentence structure (syntactic) and basic meaning (shallow semantic) information can help in automatically sorting questions and answers and improving answer organization. We introduce (a) new ways to organize information based on simple meanings found in Predicate Argument Structures (PASs), and (b) new mathematical tools (kernel functions) to take full advantage of these structures using a machine learning method called Support Vector Machines. Our tests show that understanding sentence structure helps with sorting questions and answers and that basic meaning information is very useful when we can reliably pull out PASs, like from answers. Our basic meaning representations, which are more straightforward, can avoid the problems of being too specific or too broad in other methods.	strategy
613	Chinese Segmentation with a Word-Based Perceptron Algorithm Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary. Discriminatively trained models based on local character features are used to make the tagging decisions, with Viterbi decoding finding the highest scoring segmentation. In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences. The generalized perceptron algorithm is used for discriminative training, and we use a beam-search decoder. Closed tests on the first and second SIGHAN bakeoffs show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora. We also provide a feature template for Chinese word segmentation.	Chinese Segmentation with a Word-Based Perceptron Algorithm Standard methods for Chinese word segmentation treat it like a labeling task, where each character is given a label to show if it is the end of a word. Models trained to spot differences use nearby character traits to decide on the labels, and Viterbi decoding is a method that picks the most accurate word breaks. In this paper, we suggest a different method that focuses on whole words and sequences of words, not just characters. We use the generalized perceptron algorithm to train our model to spot differences, and a beam-search decoder helps find the best word breaks. Tests from the SIGHAN bakeoffs, which are competitions, show that our system is as good as the top methods in research, getting the highest F-scores, which measure accuracy, for several text groups. We also offer a set of guidelines for identifying words in Chinese.	Support
614	Unsupervised Coreference Resolution in a Nonparametric Bayesian Model We present an unsupervised, nonparametric Bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document. While most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state. Despite being unsupervised, our system achieves a 70.3 MUC F1 measure on the MUC-6 test set, broadly in the range of some recent supervised results. In our model, we use the distinction bewtween pronouns, nominals, and proper nouns. We evaluate the clustering properties of DPMMs by performing anaphora resolution with good results.	Unsupervised Coreference Resolution in a Nonparametric Bayesian Model We introduce a method that doesn't rely on pre-labeled data (unsupervised) and uses a flexible statistical approach (nonparametric Bayesian) to solve the problem of identifying when different words refer to the same thing (coreference resolution). This method looks at the overall identity of entities (like people or things) across a whole set of documents (corpus) and how words refer back to things mentioned earlier within each document (sequential anaphoric structure). Unlike most existing methods that make decisions by comparing pairs of words, our approach generates each reference based on a mix of overall entity features and local focus of attention. Even though our system works without labeled data, it scores 70.3 in a measure (MUC F1) that evaluates how well it matches with some recent methods that use labeled data (supervised results) on a specific test set (MUC-6). In our approach, we differentiate between types of words like pronouns (he, she), nominals (common nouns), and proper nouns (names). We test how well our model groups similar references together (clustering properties of DPMMs) by resolving references to earlier words (anaphora resolution) with good outcomes.	differences
615	Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms. Using statistical machine translation techniques, a semantic parser based on a synchronous context-free grammar augmented with lambda-operators is learned given a set of training sentences and their correct logical forms. The resulting parser is shown to be the best-performing system so far in a database query domain. We demonstrate that our meaning representation language outperforms FUNQL, another meaning representation language.	Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus This paper shows the first practical results we know of for learning synchronous grammars that create logical forms. By using techniques from statistical machine translation (a method for converting one language to another using statistics), we create a semantic parser (a tool for understanding sentences) based on a specific type of grammar that uses lambda-operators (special symbols used in logic) and is learned from a set of training sentences with their correct logical forms. The parser we created is shown to be the best at understanding questions in a database. We show that our way of representing meaning is better than FUNQL, which is another way to represent meaning.	clustering
811	Japanese Dependency Structure Analysis Based On Support Vector Machines This paper presents a method of Japanese dependency structure analysis based on Support Vector Machines (SVMs). Conventional parsing techniques based on Machine Learning framework, such as Decision Trees and Maximum Entropy Models, have difficulty in selecting useful features as well as finding appropriate combination of selected features. On the other hand, it is well-known that SVMs achieve high generalization performance even with input data of very high dimensional feature space. Furthermore, by introducing the Kernel principle, SVMs can carry out the training in high-dimensional spaces with a smaller computational cost independent of their dimensionality. We apply SVMs to Japanese dependency structure identification problem. Experimental results on Kyoto University corpus show that our system achieves the accuracy of 89.09% even with small training data (7958 sentences). We introduce a new type of feature called dynamic features which are created dynamically during the parsing process.	Japanese Dependency Structure Analysis Based On Support Vector Machines This paper introduces a method for understanding Japanese sentence structure using Support Vector Machines (SVMs). Traditional methods for analyzing language using computers, like Decision Trees and Maximum Entropy Models, struggle to pick out useful information and combine it effectively. However, SVMs are known for their ability to work well even with very complex data. Additionally, SVMs use a technique called the Kernel principle, which allows them to handle complex calculations more easily and efficiently. We use SVMs to solve the problem of identifying how parts of Japanese sentences depend on each other. Tests conducted with data from Kyoto University show that our system is 89.09% accurate, even with a small amount of training data (7958 sentences). We also introduce a new kind of information called dynamic features, which are created during the analysis process.	already divided
616	Learning Multilingual Subjective Language via Cross-Lingual Projections This paper explores methods for generating subjectivity analysis resources in a new language by leveraging on the tools and resources available in English. Given a bridge between English and the selected target language (e.g., a bilingual dictionary or a parallel corpus), the methods can be used to rapidly create tools for subjectivity analysis in the new language. We discuss different shortcomings of lexicon-based translation scheme for the more semantic-oriented task subjective analysis. Instead, we proposed to use a parallel-corpus, apply the classifier in the source language and use the corresponding sentences in the target language to train a new classifier. We use a bilingual lexicon and a manually translated parallel corpus to generate a sentence classifier according to their level of subjectivity for Romanian.	Learning Multilingual Subjective Language via Cross-Lingual Projections This paper looks at ways to create tools for analyzing opinions or feelings in a new language by using the resources available in English. By using a connection between English and the chosen new language (like a bilingual dictionary or matching texts in both languages), these methods can quickly make tools for analyzing opinions in the new language. We talk about the problems with using a word-based translation for the more complex task of analyzing opinions. Instead, we suggest using matching texts in both languages, applying the tool in English, and then using the matching sentences in the new language to teach a new tool. We use a list of words in both languages and a manually translated set of matching texts to create a tool that sorts sentences by how much they express opinions for Romanian.	statistical
617	Weakly Supervised Learning for Hedge Classification in Scientific Literature We investigate automatic classification of speculative language ('hedging'), in biomedical text using weakly supervised machine learning. Our contributions include a precise description of the task with annotation guidelines, analysis and discussion, a probabilistic weakly supervised learning model, and experimental evaluation of the methods presented. We show that hedge classification is feasible using weakly supervised ML, and point toward avenues for future research. We use single words as input features in order to classify sentences from biological articles as speculative or non speculative. We extend the work of Light et al (2004) by refining their annotation guidelines and creating a publicly available data set (FlyBase data set) for speculative sentence classification. We find that our model is unsuccessful in identifying assertive statements of knowledge paucity which are generally marked rather syntactically than lexically.	Weakly Supervised Learning for Hedge Classification in Scientific Literature We explore how to automatically sort speculative language, or "hedging", in biomedical text using a type of machine learning that requires minimal supervision. Our work includes a clear explanation of the task with guidelines for marking text, analysis and discussion, a probabilistic model for weakly supervised learning, and tests of the methods we describe. We demonstrate that sorting speculative language is possible with this kind of machine learning and suggest ideas for future research. We use individual words as input to decide if sentences in biological articles are speculative or not. We build on the work of Light et al. (2004) by improving their guidelines and creating a public data set (called the FlyBase data set) for classifying speculative sentences. We find that our model struggles to identify strong statements about the lack of knowledge, which are usually marked by sentence structure rather than specific words.	analyzing
618	Moses: Open Source Toolkit for Statistical Machine Translation We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks. Our Moses decoder implements the factored phrase-based translation model.	Moses: Open Source Toolkit for Statistical Machine Translation We describe a free software toolkit for statistical machine translation, which is a way for computers to translate languages using statistics, with new features including (a) support for language-related elements, (b) a method for dealing with uncertain input called confusion network decoding, and (c) efficient ways to organize data for translation and language models. Besides the SMT decoder, which is the main translation program, the toolkit also has many tools for setting up, adjusting, and using the system for different translation jobs. Our Moses decoder uses the factored phrase-based translation model, which is a specific method for translating phrases using factors or elements related to language.	learning
619	The Tradeoffs Between Open and Traditional Relation Extraction Traditional Information Extraction (IE) takes a relation name and hand-tagged examples of that relation as input. Open IE is a relation-independent extraction paradigm that is tailored to massive and heterogeneous corpora such as the Web. An Open IE system extracts a diverse set of relational tuples from text without any relation-specific input. How is Open IE possible? We analyze a sample of English sentences to demonstrate that numerous relationships are expressed using a compact set of relation-independent lexico-syntactic patterns, which can be learned by an Open IE system. What are the tradeoffs between Open IE and traditional IE? We consider this question in the context of two tasks. First, when the number of relations is massive, and the relations themselves are not pre-specified, we argue that Open IE is necessary. We then present a new model for Open IE called O-CRF and show that it achieves increased precision and nearly double the recall than the model employed by TEXTRUNNER, the previous state-of-the-art Open IE system. Second, when the number of target relations is small, and their names are known in advance, we show that O-CRF is able to match the precision of a traditional extraction system, though at substantially lower recall. Finally, we show how to combine the two types of systems into a hybrid that achieves higher precision than a traditional extractor, with comparable recall. We use a Conditional Random Field (CRF) classifier to perform Open Relation Extraction, improving by more than 60% the F-score achieved by the Naive Bayes model in the TextRunner system. Our system is trained using a CRF classifier on S-V-O tuples from a parsed corpus as positive examples, and tuples that violate phrasal structure as negative ones.	The Tradeoffs Between Open and Traditional Relation Extraction Traditional Information Extraction (IE) uses a specific relation name and examples that are manually tagged as input. Open IE is a method that doesn't depend on specific relations and is designed for large and varied collections of text like the internet. An Open IE system can find many different types of relationships from text without needing specific information about each one. How does Open IE work? We look at English sentences and show that many relationships are shown using a small set of language patterns that don't rely on specific relations, which an Open IE system can learn. What are the differences between Open IE and traditional IE? We look at this question with two tasks. First, when there are many relations and they are not defined beforehand, Open IE becomes important. We introduce a new Open IE model called O-CRF and show that it is more accurate and finds almost twice as many relations as TEXTRUNNER, the previous best Open IE system. Second, when there are few target relations and their names are known ahead of time, O-CRF can match the accuracy of a traditional system, though it finds fewer relations. Finally, we show how to combine both systems into a hybrid that is more accurate than a traditional system, with a similar ability to find relations. We use a Conditional Random Field (CRF) classifier to improve Open Relation Extraction, achieving over 60% better performance than the Naive Bayes model in the TextRunner system. Our system is trained with a CRF classifier on subject-verb-object (S-V-O) patterns from a processed text collection as positive examples, and patterns that break language rules as negative ones.	machine
620	Bayesian Learning of Non-Compositional Phrases with Synchronous Parsing We combine the strengths of Bayesian modeling and synchronous grammar in unsupervised learning of basic translation phrase pairs. The structured space of a synchronous grammar is a natural fit for phrase pair probability estimation, though the search space can be prohibitively large. Therefore we explore efficient algorithms for pruning this space that lead to empirically effective results. Incorporating a sparse prior using Variational Bayes, biases the models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment. This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches. We suggest tic-tac-toe pruning, which uses Model 1 posteriors to exclude ranges of cells from being computed.	Bayesian Learning of Non-Compositional Phrases with Synchronous Parsing We combine the strengths of Bayesian modeling, which is a way of making predictions based on past data, and synchronous grammar, which is a set of rules for matching phrases, to learn basic translation phrase pairs without needing labeled examples. The structured space of a synchronous grammar naturally helps estimate how likely phrase pairs are, but the number of possibilities can be extremely large. Therefore, we look for smart methods to narrow down these possibilities, which leads to practically useful results. By using a method called Variational Bayes, which is a way to simplify models, we guide the models to choose simple and widely applicable parameter sets, which significantly improves the matching of words between languages. This choice for simpler solutions, along with smart narrowing methods, creates a way to match phrases that results in better overall translations than traditional word matching methods. We suggest "tic-tac-toe pruning," which uses a basic model to decide which parts are unnecessary to compute.	collections
667	Efficient Third-Order Dependency Parsers We present algorithms for higher-order dependency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they require only O(n4) time. Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively. The set of potential edges is pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent.	Efficient Third-Order Dependency Parsers We present methods for more advanced dependency parsing, meaning they can look at parts of sentences with three connections at once, and they are "efficient" because they only take O(n4) time, which is a technical way to say they work relatively fast. Importantly, our new parsers can use interactions between siblings (words at the same level) and grandchildren (words connected through another word). We test our parsers on two well-known language databases, the Penn Treebank and Prague Dependency Treebank, and get scores of 93.04% and 87.38%, showing how accurately they connect words without looking at the word labels. We make the set of possible word connections smaller using results from a simpler first-order parser, which was improved using a method called exponentiated gradient descent, a fancy way to say it learns and gets better over time.	grammars
621	Forest-Based Translation Among syntax-based translation models, the tree-based approach, which takes as input a parse tree of the source sentence, is a promising direction being faster and simpler than its string-based counterpart. However, current tree-based systems suffer from a major drawback: they only use the 2-best parse to direct the translation, which potentially introduces translation mistakes due to parsing errors. We propose a forest-based approach that translates a packed forest of exponentially many parses, which encodes many more alternatives than standard n-best lists. Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. At decoding time, we parse the input sentences into trees, and convert them into translation forest by rule pattern matching. We propose the first direct use of packed forest.	Forest-Based Translation Among syntax-based translation models, the tree-based approach, which uses a parse tree (a diagram that shows the structure) of the source sentence, is promising because it is faster and simpler than the string-based (line by line) method. However, current tree-based systems have a big problem: they only use the top 2-best parse (best interpretations) to guide translation, which can lead to mistakes due to errors in interpreting the sentence structure. We suggest a forest-based method that translates a packed forest (a collection) of exponentially many parses, offering many more options than the usual n-best lists (top interpretations). Large-scale tests show an improvement of 1.7 BLEU points (a score for translation accuracy) over the 1-best baseline (basic standard). This result is also 0.8 points better than using 30-best parses, and it is even faster. When translating, we turn input sentences into trees and convert them into a translation forest by matching rule patterns. We suggest the first direct use of a packed forest.	structured space
622	A Discriminative Latent Variable Model for Statistical Machine Translation Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems. We argue that a principle reason for this failure is not dealing with multiple, equivalent translations. We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised. Results show that accounting for multiple derivations does indeed improve performance. Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions. We show that marginalizing out the different segmentations during decoding leads to improved performance. We present a latent variable model that describes the relationship between translation and derivation clearly. For the hierarchical phrase-based approach, we present a discriminative rule model and show the difference between using only the viterbi alignment in training and using the full sum over all possible derivations.	A Discriminative Latent Variable Model for Statistical Machine Translation Large-scale discriminative machine translation, which uses advanced methods to improve translation quality, has not yet outperformed current systems that rely on simple counting techniques. We believe this is because these systems don't handle multiple ways of translating the same text. We introduce a translation model that considers hidden factors during both learning and translating, and it is designed to make the best overall decisions. The results indicate that considering multiple translation methods does indeed boost performance. Moreover, we show that using regularisation, a technique to prevent errors in statistical models, is crucial to avoid poor solutions. We demonstrate that considering all possible ways to divide text during translation improves results. We offer a model that explains how translation and the way it's built are connected. For the method that uses hierarchical phrases, we introduce a model that uses specific rules and explain the difference between relying on just one method of alignment during training and considering all possible methods.	translation
623	Vector-based Models of Semantic Composition This paper proposes a framework for representing the meaning of phrases and sentences in vector space. Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments. We propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression.	Vector-based Models of Semantic Composition This paper proposes a way to show the meaning of phrases and sentences using vectors, which are like arrows in math. Our main method uses vector composition, which means putting vectors together, using simple math actions like adding and multiplying. In this method, we create many composition models and test them to see how well they understand sentence similarity. The tests show that using multiplication works better than just adding when compared to what people think. We suggest a general method where the meaning of complex phrases is found by mixing the vectors of each word in that phrase.	specific
624	Refining Event Extraction through Cross-Document Inference We apply the hypothesis of "One Sense Per Discourse" (Yarowsky, 1995) to information extraction (IE), and extend the scope of "discourse" from one single document to a cluster of topically-related documents. We employ a similar approach to propagate consistent event arguments across sentences and documents. Combining global evidence from related documents with local decisions, we design a simple scheme to conduct cross-document inference for improving the ACE event extraction task. Without using any additional labeled data this new approach obtained 7.6% higher F-Measure in trigger labeling and 6% higher F-Measure in argument labeling over a state-of-the-art IE system which extracts events independently for each sentence. We employ a rule-based approach to propagate consistent triggers and arguments across topic-related documents.	Refining Event Extraction through Cross-Document Inference We use the idea called "One Sense Per Discourse" (from Yarowsky, 1995), which means a word or phrase usually has the same meaning throughout a discussion, to improve information extraction, and we expand "discourse" to mean a group of related documents, not just one. We use a similar method to keep event details consistent across sentences and documents. By combining overall information from related documents with specific decisions, we create a simple way to make cross-document inferences to enhance the ACE event extraction task. Without using any extra labeled data, this new method achieved a 7.6% higher F-Measure in identifying event triggers and a 6% higher F-Measure in identifying event arguments compared to a top IE system that analyzes each sentence separately. We use a rule-based method to maintain consistent event triggers and details across documents that are about similar topics.	sentences using
625	A Joint Model of Text and Aspect Ratings for Sentiment Summarization Online reviews are often accompanied with numerical ratings provided by users for a set of service or product aspects. We propose a statistical model which is able to discover corresponding topics in text and extract textual evidence from reviews supporting each of these aspect ratings – a fundamental problem in aspect-based sentiment summarization (Hu and Liu, 2004a). Our model achieves high accuracy, without any explicitly labeled data except the user provided opinion ratings. The proposed approach is general and can be used for segmentation in other applications where sequential data is accompanied with correlated signals. In contrast, MLSLDA draws on techniques that view sentiment as a regression problem based on the topics used in a document, as in supervised latent Dirichlet allocation (SLDA) (Blei and McAuliffe, 2007) or in finer-grained parts of a document (Titov and McDonald, 2008). We propose a joint model of text and aspect ratings which utilizes a modified LDA topic model to build topics that are representative of ratable aspects, and builds a set of sentiment predictors.	A Joint Model of Text and Aspect Ratings for Sentiment Summarization Online reviews often come with number ratings given by users for different parts or features of a service or product. We suggest a statistical method that can find and match topics in text and pull out supporting text from reviews for each of these ratings, which is a key issue in summarizing feelings or opinions about specific aspects. Our method works very accurately without needing any specially labeled data, except for the ratings given by users. This approach is versatile and can be applied to dividing up information in other areas where data that follows a sequence comes with related signals. Unlike other methods, MLSLDA uses techniques that treat feelings as something that can be measured based on the topics in a document, similar to a method called supervised latent Dirichlet allocation or in more detailed parts of a document. We suggest a combined model of text and aspect ratings that uses a changed LDA topic model to create topics that represent aspects you can rate, and it also creates a set of tools to predict sentiment.	document inferences
628	A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model In this paper, we propose a novel string-to-dependency algorithm for statistical machine translation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set. We presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically.	A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model In this paper, we propose a new method for translating languages using a string-to-dependency algorithm in statistical machine translation. With this new approach, we use a target dependency language model while translating to understand long-distance word connections, which a traditional n-gram model cannot do. Our experiments show that this string-to-dependency method improves translation quality by 1.48 points in BLEU (a measure of translation accuracy) and 2.53 points in TER (a measure of translation errors) compared to a standard system on the NIST 04 Chinese-English test. We introduce a model that ensures the target part of each translation rule forms a proper dependency tree fragment, and uses a dependency language model to make the translation more grammatically correct.	different
629	Forest Reranking: Discriminative Parsing with Non-Local Features Conventional n-best reranking techniques often suffer from the limited scope of the nbest list, which rules out many potentially good alternatives. We instead propose forest reranking, a method that reranks a packed forest of exponentially many parses. Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank. Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank. We show that the use of non-local features does in fact contribute substantially to parser performance. To prune the packed forests, we use inside and outside probabilities to compute the distance of the best derivation that traverses a hyper edge away from the globally best derivation.	Forest Reranking: Discriminative Parsing with Non-Local Features Conventional methods that pick the best option from a list of top choices often miss out on many other good options. We suggest forest reranking, which looks at a large group of possible solutions all at once. Because figuring out the exact best choice is too hard with complex factors, we offer a simpler method inspired by forest rescoring that makes it possible to improve training over the entire dataset. Our best result, a score of 91.7, beats both the top 50 and top 100 choices and is better than any system trained on the dataset before. We demonstrate that using complex factors really helps improve the performance of the parser. To narrow down the large group of possible solutions, we use calculations to find how far the best solution that follows a certain path is from the overall best solution.	translating languages
630	Simple Semi-supervised Dependency Parsing We present a simple and effective semi-supervised method for training dependency parsers. We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus. We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions. For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuracy of 92.02% to 93.16%, and in the case of Czech unlabeled second-order parsing, we improve from a baseline accuracy of 86.13% to 87.13%. In addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance. We show that for syntactic dependency parsing, combining Brown cluster features with word forms or POS tags yields high accuracy even with little training data. We propose to use word clusters as features to improve graph-based statistical dependency parsing for English and Czech.	Simple Semi-supervised Dependency Parsing We introduce an easy and effective way to train dependency parsers (tools that analyze sentence structure) using both labeled and unlabeled data. We focus on understanding words better by using word groups created from a large collection of texts without labels. We prove this method works well through tests on different datasets, like the Penn Treebank and Prague Dependency Treebank, showing that using word groups significantly improves the accuracy of parsing in many situations. For example, in English parsing without labels, accuracy improved from 92.02% to 93.16%, and for Czech, from 86.13% to 87.13%. Additionally, our method works well even with small amounts of training data, reducing the need for labeled data by about half to achieve the desired accuracy. We demonstrate that for analyzing sentence structure, combining word groups with word forms or part-of-speech tags (labels that indicate word types) results in high accuracy, even with limited training data. We suggest using word groups to enhance the statistical analysis of sentence structure for English and Czech.	improve training
631	Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data This paper provides evidence that the use of more unlabeled data in semi-supervised learning can improve the performance of Natural Language Processing (NLP) tasks, such as part-of-speech tagging, syntactic chunking, and named entity recognition. We first propose a simple yet powerful semi-supervised discriminative model appropriate for handling large scale unlabeled data. Then, we describe experiments performed on widely used test collections, namely, PTB III data, CoNLL’00 and ’03 shared task data for the above three NLP tasks, respectively. We incorporate up to 1G-words (one billion tokens) of unlabeled data, which is the largest amount of unlabeled data ever used for these tasks, to investigate the performance improvement. In addition, our results are superior to the best reported results for all of the above test collections. We run a baseline discriminative classifier on unlabeled data to generate pseudo examples, which are then used to train a different type of classifier for the same problem. We use the automatically labeled corpus to train HMMs.	Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data This paper shows that using a lot of unlabeled data (data without labels or tags) in semi-supervised learning—a way of teaching computers with some labeled data and lots of unlabeled data—can make tasks in Natural Language Processing (NLP) better, like identifying parts of speech, breaking sentences into chunks, and recognizing names of people or places. We first suggest a simple but strong semi-supervised model that works well with large amounts of unlabeled data. Next, we talk about tests we did using well-known datasets, like PTB III data, CoNLL’00 and ’03 data for the above three NLP tasks. We use up to 1 billion words of unlabeled data, the most ever used for these tasks, to see how much better it can perform. Our results are better than the best results previously reported for all of these datasets. We use an initial classifier on unlabeled data to create fake examples that help train another classifier for the same task. We use the automatically labeled data to train Hidden Markov Models (HMMs), which are tools to predict sequences, like words in sentences.	accuracy
632	Unsupervised Multilingual Learning for Morphological Segmentation For centuries, the deep connection between languages has brought about major discoveries about human communication. In this paper we investigate how this powerful source of information can be exploited for unsupervised language learning. In particular, we study the task of morphological segmentation of multiple languages. We present a non-parametric Bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identifies cross-lingual morpheme patterns, or abstract morphemes. We apply our model to three Semitic languages: Arabic, Hebrew, Aramaic, as well as to English. Our results demonstrate that learning morphological models in tandem reduces error by up to 24% relative to monolingual models. Furthermore, we provide evidence that our joint model achieves better performance when applied to languages from the same family. We use bilingual information but the segmentation is learned independently from translation modeling.	Unsupervised Multilingual Learning for Morphological Segmentation For centuries, the strong link between languages has led to important discoveries about how humans communicate. In this paper, we explore how this valuable information can be used for learning languages without direct guidance. Specifically, we focus on breaking down words into their smallest parts, called morphemes, in multiple languages. We introduce a flexible statistical model that simultaneously figures out these word parts for each language and identifies patterns that are common across different languages. We test our model on three related languages: Arabic, Hebrew, and Aramaic, as well as on English. Our findings show that learning these word structures together reduces mistakes by up to 24% compared to learning each language separately. Additionally, we show that our combined model works better when used on languages from the same family. We use information from two languages, but the word breakdown is learned separately from translation.	billion
634	Distributed Word Clustering for Large Scale Class-Based Language Modeling in Machine Translation In statistical language modeling, one technique to reduce the problematic effects of data sparsity is to partition the vocabulary into equivalence classes. In this paper we investigate the effects of applying such a technique to higher-order n-gram models trained on large corpora. We introduce a modification of the exchange clustering algorithm with improved efficiency for certain partially class-based models and a distributed version of this algorithm to efficiently obtain automatic word classifications for large vocabularies (>1 million words) using such large training corpora (>30 billion tokens). The resulting clusterings are then used in training partially class-based language models. We show that combining them with word-based n-gram models in the log-linear model of a state-of-the-art statistical machine translation system leads to improvements in translation quality as indicated by the BLEU score. We introduce the predictive class bigram model.	Distributed Word Clustering for Large Scale Class-Based Language Modeling in Machine Translation In statistical language modeling, one way to handle the problem of not having enough data is to group words into similar categories. In this paper, we look into how this method works when used with advanced n-gram models that are trained on large text collections. We present a change to the exchange clustering method that makes it run faster for some types of models that use word groups, and we also offer a version of this method that works across multiple computers to quickly sort words into categories when dealing with very large sets of words (more than 1 million words) and large text data (more than 30 billion words). These word groupings are then used to train language models that partly use these word groups. We demonstrate that when these models are combined with word-based n-gram models in a top-performing machine translation system, it results in better translation quality, as shown by the BLEU score (a measurement of translation accuracy). We introduce a new model called the predictive class bigram model.	knowledge
635	Learning Bilingual Lexicons from Monolingual Corpora We present a method for learning bilingual translation lexicons from monolingual corpora. Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings. Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings. We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types. We only use a small-sized bilingual lexicon containing 100 word pairs as seed lexicon. The availability of parsers is a more stringent constraint, but our results suggest that more basic NLP methods may be sufficient for bilingual lexicon extraction. In this work, we have used a set of seed translations (unlike e.g., Haghighi et al (2008)). We present a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account.	Learning Bilingual Lexicons from Monolingual Corpora We present a method for creating bilingual translation dictionaries from text written in only one language. Words in each language are described using features that only look at one language, like how words are used in sentences or parts of words. Translations are found using a model that creates results based on a method called canonical correlation analysis, which matches words from different languages by finding hidden connections. We show that accurate dictionaries can be made for different language pairs and types of text. We start with a small dictionary of 100 word pairs. Having language parsers is more challenging, but our findings suggest that simpler language processing tools might work well enough to find bilingual dictionaries. In this study, we used a set of starting translations, unlike some other studies. We present a model based on canonical correlation analysis, considering features like word context and parts of words.	billion
636	Unsupervised Learning of Narrative Event Chains Hand-coded scripts were used in the 1970-80s as knowledge backbones that enabled inference and other NLP tasks requiring deep semantic knowledge. We propose unsupervised induction of similar schemata called narrative event chains from raw newswire text. A narrative event chain is a partially ordered set of events related by a common protagonist. We describe a three step process to learning narrative event chains. The first uses unsupervised distributional methods to learn narrative relations between events sharing coreferring arguments. The second applies a temporal classifier to partially order the connected events. Finally, the third prunes and clusters self-contained chains from the space of events. We introduce two evaluations: the narrative cloze to evaluate event relatedness, and an order coherence task to evaluate narrative order. We show a 36% improvement over baseline for narrative prediction and 25% for temporal coherence. We investigate unsupervised learning of narrative event sequences using point wise mutual information (PMI) between syntactic positions.	Unsupervised Learning of Narrative Event Chains Hand-coded scripts were used in the 1970-80s as knowledge frameworks that helped with understanding and other Natural Language Processing (NLP) tasks requiring deep comprehension of meaning. We suggest learning similar frameworks called narrative event chains from raw news stories without supervision. A narrative event chain is a somewhat ordered list of events connected by a shared main character. We outline a three-step process to learn narrative event chains. The first step uses methods that don't need supervision to find relationships between events that share common references. The second step applies a time-based classifier to sort these connected events in a partial order. Lastly, the third step trims and groups independent chains from the pool of events. We present two evaluations: the narrative cloze to test how events relate to each other, and an order coherence task to check the sequence of events. We demonstrate a 36% improvement over the starting point for predicting narratives and a 25% improvement for maintaining the sequence order. We explore learning narrative event sequences without supervision using pointwise mutual information (PMI), which measures how often certain syntactic (sentence structure) positions appear together.	analysis
637	Joint Word Segmentation and POS Tagging Using a Single Perceptron For Chinese POS tagging, word segmentation is a preliminary step. To avoid error propagation and improve segmentation by utilizing POS information, segmentation and tagging can be performed simultaneously. A challenge for this joint approach is the large combined search space, which makes efficient decoding very hard. Recent research has explored the integration of segmentation and POS tagging, by decoding under restricted versions of the full combined search space. In this paper, we propose a joint segmentation and POS tagging model that does not impose any hard constraints on the interaction between word and POS information. Fast decoding is achieved by using a novel multiple-beam search algorithm. The system uses a discriminative statistical model, trained using the generalized perceptron algorithm. The joint model gives an error reduction in segmentation accuracy of 14.6% and an error reduction in tagging accuracy of 12.2%, compared to the traditional pipeline approach. We use an approximate decoding algorithm that keeps track of a set of partially built structures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning.	Joint Word Segmentation and POS Tagging Using a Single Perceptron For tagging parts of speech (POS) in Chinese, splitting words is a basic first step. To reduce mistakes and improve splitting by using POS details, both can be done at the same time. A problem with this combined method is the large number of possibilities to consider, making it hard to do efficiently. Recent studies have looked at combining word splitting and POS tagging by simplifying the process to make it easier. In this paper, we suggest a model that combines splitting and tagging without strict rules on how words and POS details interact. We achieve fast results by using a new method called a multiple-beam search algorithm. This system uses a decision-making model based on statistics, trained with a method called the generalized perceptron algorithm. The combined model reduces errors in word splitting by 14.6% and in POS tagging by 12.2%, compared to the usual step-by-step method. We use a method for finding solutions that keeps track of partially completed tasks for each character, similar to a simplified plan that is made smaller by removing unnecessary parts.	suggest learning
668	Word Representations: A Simple and General Method for Semi-Supervised Learning If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/ We show that monolingual word clusters are broadly applicable as features in monolingual models for linguistic structure prediction.	Word Representations: A Simple and General Method for Semi-Supervised Learning If we take an existing supervised NLP (Natural Language Processing) system, a simple and general way to improve accuracy is to use unsupervised word representations (ways to understand word meanings without labeled data) as extra word features. We evaluate Brown clusters (a way to group similar words), Collobert and Weston (2008) embeddings (a method to represent words as numbers), and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER (Named Entity Recognition) and chunking (grouping text into parts). We use nearly the best supervised baselines (standard methods), and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for easy use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/ We show that monolingual word clusters (word groups in one language) are broadly useful as features in monolingual models for predicting language structure.	parsers
638	A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging We propose a cascaded linear model for joint Chinese word segmentation and part-of-speech tagging. With a character-based perceptron as the core, combined with real-valued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly. Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging. On the Penn Chinese Treebank 5.0, we obtain an error reduction of 18.5% on segmentation and 12% on joint segmentation and part-of-speech tagging over the perceptron-only baseline. For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j.	A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging We suggest a step-by-step method for Chinese word splitting and labeling words with their types (like noun or verb) together. It uses a system that learns based on characters as its main part, along with additional tools like language models that use numbers. This setup can make use of information that is hard to put directly into the main system. Tests show this method is better at splitting words and doing both tasks together. On the Penn Chinese Treebank 5.0, we made 18.5% fewer mistakes in splitting words and 12% fewer mistakes in doing both tasks together compared to using only the basic system. For CTB-5, we use the division done by Duan et al (2007) called CTB-5d, and the one by Jiang et al (2008) called CTB-5j.	combined
639	Integrating Graph-Based and Transition-Based Dependency Parsers Previous studies of data-driven dependency parsing have shown that the distribution of parsing errors are correlated with theoretical properties of the models used for learning and inference. In this paper, we show how these results can be exploited to improve parsing accuracy by integrating a graph-based and a transition-based model. By letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL-X shared task. We first show how the MST Parser (McDonald et al, 2005) and the Malt Parser (Nivre et al, 2007) could be improved by stacking each parser on the predictions of the other. In the previous work, Nivre and McDonald (2008) have integrated MST Parser and Malt Parser by feeding one parser's output as features into the other.	Integrating Graph-Based and Transition-Based Dependency Parsers Previous studies about computer programs that help understand sentence structure have shown that the mistakes these programs make are linked to the ways the programs are designed to learn and make decisions. In this paper, we show how these findings can be used to make these programs better at understanding sentences by combining two different types of models. By allowing one program to help the other with hints or suggestions, we consistently make both programs more accurate, leading to a big improvement when tested on specific data collections. We first demonstrate how two specific programs, the MST Parser and the Malt Parser, can be made better by using each program's predictions to help the other. In earlier work, researchers Nivre and McDonald combined the MST Parser and Malt Parser by using the output of one as input hints for the other.	fewer mistakes
640	Efficient Feature-based Conditional Random Field Parsing Discriminative feature-based methods are widely used in natural language processing, but sentence parsing is still dominated by generative methods. While prior feature-based dynamic programming parsers have restricted training and evaluation to artificially short sentences, we present the first general, feature rich discriminative parser, based on a conditional random field model, which has been successfully scaled to the full WSJ parsing data. Our efficiency is primarily due to the use of stochastic optimization techniques, as well as parallelization and chart prefiltering. On WSJ15, we attain a state-of-the-art F-score of 90.9%, a 14% relative reduction in error over previous models, while being two orders of magnitude faster. On sentences of length 40, our system achieves an F-score of 89.0%, a 36% relative reduction in error over a generative baseline. In our model, distributed online learning has been done in a synchronous setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al, 2008).	Efficient Feature-based Conditional Random Field Parsing Discriminative feature-based methods, which focus on identifying specific characteristics in data, are commonly used in processing language naturally, but sentence parsing (breaking down sentences into parts) is still mainly done by generative methods (which create data based on rules). Previous feature-based parsers using dynamic programming have been limited to short sentences for training and testing, but we introduce the first general parser rich in features and based on a conditional random field model (a statistical tool), which works with the full WSJ parsing data (a large set of text data). We achieve efficiency mainly through stochastic optimization techniques (randomness-based methods for improvement), parallelization (using multiple processors at once), and chart prefiltering (sorting data before processing). On the WSJ15 dataset, we achieve a top-level F-score of 90.9% (a measure of accuracy), which is a 14% reduction in errors compared to previous models and is 100 times faster. For sentences with 40 words, our system achieves an F-score of 89.0%, a 36% reduction in errors compared to a basic generative model. In our approach, distributed online learning (learning while processing data) is done synchronously, meaning that a small set of data is split across several CPUs (computer processors), and the model is updated after all have finished processing (Finkel et al, 2008).	programs
641	Soft Syntactic Constraints for Hierarchical Phrased-Based Translation In adding syntax to statistical MT, there is a tradeoff between taking advantage of linguistic analysis, versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data. A number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then finding appropriate ways to soften that commitment. We present an approach that explores the tradeoff from the other direction, starting with a context-free translation model learned directly from aligned parallel text, and then adding soft constituent-level constraints based on parses of the source language. We obtain substantial improvements in performance for translation from Chinese and Arabic to English. We revise this method by distinguishing different constituent syntactic types, and defined features for each type to count whether a phrase matches or crosses the syntactic boundary. We find that their constituent constraints are sensitive to language pairs.	Soft Syntactic Constraints for Hierarchical Phrased-Based Translation In adding syntax (rules for sentence structure) to statistical Machine Translation (MT), there is a balance between using linguistic analysis (studying language structure) and allowing the model to use patterns not based on language rules from training data. Some past efforts have addressed this balance by focusing on language-based analysis first and then finding ways to relax that focus. We introduce a method that approaches the balance differently, by starting with a simple translation model directly from aligned text in two languages, then adding flexible rules about sentence parts based on language structure of the source text. This results in significant improvements in translating from Chinese and Arabic to English. We update this method by identifying different types of sentence parts and defining rules for each to check if a phrase matches or goes beyond these boundaries. We discover that these rules are affected by the specific languages being translated.	processing language
669	A Latent Dirichlet Allocation Method for Selectional Preferences The computation of selectional preferences, the admissible argument values for a relation, is a well-known NLP task with broad applicability. We present LDA-SP, which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences. By simultaneously inferring latent topics and topic distributions over relations, LDA-SP combines the benefits of previous approaches: like traditional class-based approaches, it produces human interpretable classes describing each relation’s preferences, but it is competitive with non-class-based methods in predictive power. We compare LDA-SP to several state-ofthe-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007). We also evaluate LDA-SP’s effectiveness at filtering improper applications of inference rules, where we show substantial improvement over Pantel et al.’s system (Pantel et al., 2007). We focus on inferring latent topics and their distributions over multiple arguments and relations (e.g., the subject and direct object of a verb).	A Latent Dirichlet Allocation Method for Selectional Preferences The calculation of selectional preferences, which are the acceptable argument values for a relationship, is an important task in natural language processing (NLP) with many uses. We introduce LDA-SP, a method that uses LinkLDA (a model from Erosheva et al., 2004) to understand selectional preferences. By figuring out hidden topics and how they relate to different relationships at the same time, LDA-SP combines the advantages of older methods: like traditional class-based methods, it creates easy-to-understand groups that explain each relationship's preferences, but it is also as strong as newer methods that don't use classes in predicting results. We compare LDA-SP to several advanced methods and achieve an 85% improvement in recall, which is the ability to find relevant information, at 0.9 precision, better than using mutual information (a technique from Erk, 2007). We also test LDA-SP's ability to filter out incorrect uses of inference rules and show significant improvements over Pantel et al.'s system (from Pantel et al., 2007). We concentrate on discovering hidden topics and their distributions across various arguments and relationships, like the subject and direct object of a verb.	structure
642	Generalizing Word Lattice Translation Word lattice decoding has proven useful in spoken language translation; we argue that it provides a compelling model for translation of text genres, as well. We show that prior work in translating lattices using finite state techniques can be naturally extended to more expressive synchronous context-free grammar-based models. Additionally, we resolve a significant complication that non-linear word lattice inputs introduce in reordering models. Our experiments evaluating the approach demonstrate substantial gains for Chinese-English and Arabic-English translation. In our model, several different segmenters for Chinese are combined to create the lattice. All of the systems we present use the lattice input format to Moses (Dyer et al, 2008), including the baselines which do not need them.	Generalizing Word Lattice Translation Word lattice decoding has been helpful in translating spoken language; we believe it also offers a strong method for translating different types of written texts. We explain how earlier methods of translating using simple state-based systems can be easily improved to use more advanced grammar-based systems. Furthermore, we address a major challenge that comes with non-linear (not straightforward) word lattice inputs in rearranging words. Our tests on this method show significant improvements in translating from Chinese to English and Arabic to English. In our model, several different methods for breaking down Chinese are combined to create the lattice (a network of possible translations). All the systems we discuss use the lattice input format compatible with Moses (Dyer et al, 2008), including the baseline systems that do not require it.	sentence
643	Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs We present a novel approach to weakly supervised semantic class learning from the web, using a single powerful hyponym pattern combined with graph structures, which capture two properties associated with pattern-based extractions: popularity and productivity. Intuitively, a candidate is popular if it was discovered many times by other instances in the hyponym pattern. A candidate is productive if it frequently leads to the discovery of other instances. Together, these two measures capture not only frequency of occurrence, but also cross-checking that the candidate occurs both near the class name and near other class members. We developed two algorithms that begin with just a class name and one seed instance and then automatically generate a ranked list of new class instances. We conducted experiments on four semantic classes and consistently achieved high accuracies. We introduce a bootstrapping scheme using the doubly-anchored pattern (DAP) that is guided through graph ranking.	Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs We present a new method for learning groups of related words from the web with little supervision, using a strong pattern and graph structures, which capture two important features: popularity (how often something is found) and productivity (how often it leads to finding more things). Simply put, a candidate is popular if it is found many times in the pattern. A candidate is productive if it often helps find other related items. These two features help verify that a candidate appears both near the group name and near other group members. We created two methods that start with a group name and one example, then automatically make a ranked list of new examples. We tested this on four groups and consistently got high accuracy. We introduce a method that improves itself using a special pattern (called DAP) and is guided by graph ranking.	rearranging
644	The Complexity of Phrase Alignment Problems Many phrase alignment models operate over the combinatorial space of bijective phrase alignments. We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard. On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient.	The Complexity of Phrase Alignment Problems Many phrase alignment models work within the complex area of matching phrases one-to-one. We prove that finding the best way to match these phrases is extremely difficult, while calculating the expected outcomes of these matches is also very hard. On the other hand, we show that finding the best match can be turned into a type of math problem called an integer linear program, which offers a straightforward method to make predictions about phrase matches that works quite well in practice.	graph structures
645	Enforcing Transitivity in Coreference Resolution A desirable quality of a coreference resolution system is the ability to handle transitivity constraints, such that even if it places high likelihood on a particular mention being coreferent with each of two other mentions, it will also consider the likelihood of those two mentions being coreferent when making a final assignment. This is exactly the kind of constraint that integer linear programming (ILP) is ideal for, but, surprisingly, previous work applying ILP to coreference resolution has not encoded this type of constraint. We train a coreference classifier over pairs of mentions, and show how to encode this type of constraint on top of the probabilities output from our pairwise classifier to extract the most probable legal entity assignments. We present results on two commonly used datasets which show that enforcement of transitive closure consistently improves performance, including improvements of up to 3.6% using the b3 scorer, and up to 16.5% using cluster f-measure. We present a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier.	Enforcing Transitivity in Coreference Resolution A good feature of a coreference resolution system, which links different words that refer to the same thing, is its ability to handle transitivity constraints. This means that if it thinks one mention (a word or phrase) is likely the same as two other mentions, it should also check if those two mentions are likely the same when deciding. This type of rule is what integer linear programming (ILP), a mathematical method, is great for, but surprisingly, past research using ILP for coreference resolution hasn't included this rule. We train a system that compares pairs of mentions and show how to add this rule to the probabilities from our system to find the most likely correct matches. We provide results from two common datasets showing that applying this rule consistently improves accuracy, with improvements up to 3.6% using a b3 scorer, a method for evaluating, and up to 16.5% using a cluster f-measure, another evaluation method. We introduce a supervised system, which is a system that learns from labeled examples, using ILP to improve the predictions from our mention comparison system.	matches
646	Self-Training for Biomedical Parsing Parser self-training is the technique of taking an existing parser, parsing extra data and then creating a second parser by treating the extra data as further training data. Here we apply this technique to parser adaptation. In particular, we self-train the standard Charniak/Johnson Penn-Treebank parser using unlabeled biomedical abstracts. This achieves an f-score of 84.3% on a standard test set of biomedical abstracts from the Genia corpus. This is a 20% error reduction over the best previous result on biomedical data (80.2% on the same test set).	Self-Training for Biomedical Parsing Parser self-training is a method where we use an existing tool (parser) to analyze more data, and then use that additional data to train a new version of the tool. Here, we use this method to improve parser performance. Specifically, we retrain the common Charniak/Johnson Penn-Treebank parser with biomedical abstracts that haven't been labeled. This method reached an accuracy score of 84.3% on a common test set of biomedical summaries from the Genia corpus. This is a 20% improvement in reducing mistakes compared to the best previous result on similar biomedical data, which was 80.2% on the same test set.	compares pairs
675	cdec: A Decoder Alignment and Learning Framework for Finite-State and Context-Free Translation Models We present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms. From this unified representation, the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. We present cdec, a fast implementation of hierarchical phrase-based translation models (Chiang, 2005), which represents a state-of-the-art translation system. Our cdec decoder learns word segmentation lattices from raw text in an unsupervised manner.	:cdec: A Decoder Alignment and Learning Framework for Finite-State and Context-Free Translation Models We present cdec, a free-to-use framework for decoding, aligning, and training different types of machine translation models, like word-based, phrase-based, and those using structured grammar rules. It uses a single way of representing translation options, keeping the specific translation rules separate from general processes like scoring, reducing unnecessary options, and making decisions. From this representation, the decoder can find not just the best translations, but also match them to a reference, or get the information needed for training using methods that adjust based on feedback or without feedback. Its efficient C++ coding means it uses less memory and runs faster than similar tools. We present cdec, a fast version of a top-level translation system that uses complex phrase-based models. Our cdec decoder learns how to break down words from plain text without needing prior examples.	picking
647	Reinforcement Learning for Mapping Instructions to Actions In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions. We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains -- Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples. We show that performing policy-gradient with this function is equivalent to training a fully supervised, stochastic gradient algorithm that optimizes conditional likelihood.	Reinforcement Learning for Mapping Instructions to Actions In this paper, we present a method using reinforcement learning to turn natural language instructions into a series of actions that can be performed. We assume there is a reward system that tells us how good the actions are. While learning, the system repeatedly creates action sequences for some documents, performs those actions, and sees the rewards it gets. We use a method called policy gradient to figure out the details of a model that helps choose actions. We test our method on two types of instructions: guides for fixing Windows problems and game tutorials. Our results show that this method can compete with traditional learning methods that use lots of labeled examples, but it needs very few or no labeled examples. We demonstrate that using policy-gradient with this approach is similar to training a fully supervised method that improves the chance of predicting the right actions.	mistakes
648	Learning Semantic Correspondences with Less Supervision A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state. To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty--Robocup sportscasting, weather forecasts (a new domain), and NFL recaps. We propose a probabilistic generative approach to produce a Viterbi alignment between NL and MRs. We use a hierarchical semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts.	Learning Semantic Correspondences with Less Supervision A main challenge in learning language connected to real-world situations is figuring out how words and phrases relate to the actual things they talk about. To handle the high level of confusion in this scenario, we introduce a model that breaks down sentences into parts and matches each part to its meaning based on the real-world context. We demonstrate that our model works well in three areas of growing complexity: Robocup sportscasting, weather forecasts (a new area), and NFL game summaries. We suggest a method that uses probability to align natural language (NL) with meaning representations (MRs). Our model follows a structured approach, first deciding what information to talk about and then creating sentences using the main actions and details of those facts.	system repeatedly
649	Efficient Minimum Error Rate Training and Minimum Bayes-Risk Decoding for Translation Hypergraphs and Lattices Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used in most current state-of-the-art Statistical Machine Translation (SMT) systems. The algorithms were originally developed to work with N-best lists of translations, and recently extended to lattices that encode many more hypotheses than typical N-best lists. We here extend lattice-based MERT and MBR algorithms to work with hypergraphs that encode a vast number of translations produced by MT systems based on Synchronous Context Free Grammars. These algorithms are more efficient than the lattice-based versions presented earlier. We show how MERT can be employed to optimize parameters for MBR decoding. Our experiments show speedups from MERT and MBR as well as performance improvements from MBR decoding on several language pairs. We describe an efficient approximate algorithm for computing n-gram posterior probabilities.	Efficient Minimum Error Rate Training and Minimum Bayes-Risk Decoding for Translation Hypergraphs and Lattices Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used in most top-level Statistical Machine Translation (SMT) systems today. These methods were first made to work with N-best lists, which are lists of the best translation options, and were later expanded to use lattices, which contain many more possible translations than N-best lists. In this paper, we further develop these methods to work with hypergraphs, which are structures that hold a huge number of translations created by machine translation systems using Synchronous Context Free Grammars. These new methods are more effective than the earlier lattice-based versions. We explain how MERT can be used to fine-tune settings for MBR decoding. Our tests show that using MERT and MBR speeds up the process and improves the performance of MBR decoding across different language pairs. We also explain a quick method for estimating n-gram posterior probabilities, which are chances of certain word sequences appearing in translations.	forecasts
650	Recognizing Stances in Online Debates This paper presents an unsupervised opinion analysis method for debate-side classification, i.e., recognizing which stance a person is taking in an online debate. In order to handle the complexities of this genre, we mine the web to learn associations that are indicative of opinion stances in debates. We combine this knowledge with discourse information, and formulate the debate side classification task as an Integer Linear Programming problem. Our results show that our method is substantially better than challenging baseline methods. We define stance as an overall position held by a person toward an object, idea or proposition.	Recognizing Stances in Online Debates This paper introduces a method for analyzing opinions without needing pre-labeled examples to figure out which side of an argument someone is taking in an online debate. To deal with the challenges of this type of discussion, we search the internet to find connections that suggest what opinion someone has in debates. We use this information along with how people express themselves and set up the task of deciding debate sides as a math problem that finds the best solution from a set of possibilities. Our results demonstrate that our method is much better than difficult standard methods. We describe stance as the overall viewpoint or opinion a person has toward a subject, idea, or proposal.	translation
689	Parsing with Compositional Vector Grammars Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%. It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments. Recursive neural networks, which have the ability to generate a tree structured output, have already been applied to natural language parsing, we extended them to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al, 2013).	Parsing with Compositional Vector Grammars Natural language parsing, which is about breaking down sentences to understand their structure, usually uses a few simple categories like NP (noun phrase) and VP (verb phrase). But this doesn't fully capture the complex meanings and structures of language. Trying to improve this by adding more details or splitting categories only partly solves the problem and makes it more complicated. Instead, we suggest using Compositional Vector Grammar (CVG) that combines a method called PCFGs with a type of artificial intelligence (AI) called a recursive neural network, which helps understand both the structure and meaning of sentences. The CVG improves the performance of the Stanford Parser by 3.8%, achieving a score of 90.4% accuracy. It is quick to train and works about 20% faster than the existing Stanford parser. The CVG also learns which words are most important and helps with understanding complex parts of sentences that need extra meaning, like prepositional phrase (PP) attachments. Recursive neural networks, which can create a tree-like structure for sentences, have been used in understanding language. We improved them to recursive neural tensor networks to better explore how meanings are built up (Socher et al, 2013).	digital
651	Co-Training for Cross-Lingual Sentiment Classification The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classification. However, there are many freely available English sentiment corpora on the Web. This paper focuses on the problem of cross-lingual sentiment classification, which  leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data. Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are  considered as two independent views of the classification problem. We propose a co-training approach to making use of unlabeled  Chinese data. Experimental results show the effectiveness of the proposed approach, which can outperform the standard inductive classifiers and the transductive classifiers. The proposed co-regression algorithm can make full use of both the features in the source language and the features in the target language in a unified framework. We propose to use ensemble method to train better Chinese sentiment classification model on English labeled data and their Chinese translation. We leveraged an available English corpus for Chinese sentiment classification by using the co-training approach to make full use of both English and Chinese features in a unified framework.	Co-Training for Cross-Lingual Sentiment Classification The lack of Chinese sentiment corpora (collections of text) limits research progress on understanding Chinese emotions in text. However, there are many freely available English collections on the Web. This paper focuses on the problem of cross-lingual sentiment classification, which means using an available English text collection to help with Chinese sentiment classification by treating the English text as training data. Machine translation services help bridge the language gap between the training set and test set, and English features and Chinese features are seen as two separate parts of the classification problem. We propose a co-training approach to use unlabeled (not marked with sentiment) Chinese data. Experiments show that our method works well, better than standard methods that only learn from labeled data and those that learn from the test data directly. The proposed co-regression (a method that predicts values) algorithm can fully use both the features in the source language (English) and the features in the target language (Chinese) in a combined system. We suggest using an ensemble method (a technique that combines several models) to train a better Chinese sentiment classification model using English labeled data and their Chinese translations. We used the available English text for Chinese sentiment classification by applying the co-training approach to fully use both English and Chinese features in a combined system.	Stances
652	Concise Integer Linear Programming Formulations for Dependency Parsing We formulate the problem of non-projective dependency parsing as a polynomial-sized integer linear program. Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data. In particular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearly-projective parses. The model parameters are learned in a max-margin framework by employing a linear programming relaxation. We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods. We introduce the multicommodity flow formulation.	Concise Integer Linear Programming Formulations for Dependency Parsing We describe the problem of non-projective dependency parsing (a type of language analysis) as a mathematical model that uses whole numbers and linear equations, but is not too large. Our method can efficiently consider complex relationships in the data; it works with strict rules already known and can also learn flexible rules from data. Specifically, our model can understand relationships between connected parts (like siblings and grandparents), how often words connect to others (word valency), and preferences for almost-linear structures. The model learns parameters using an advanced method called max-margin with a simplified approach. We test how well our parser works in different languages and find it performs better than the best current methods. We introduce a method called multicommodity flow formulation.	predicts values
653	Non-Projective Dependency Parsing in Expected Linear Time We present a novel transition system for dependency parsing, which constructs arcs only between adjacent words but can parse arbitrary non-projective trees by swapping the order of words in the input. Adding the swapping operation changes the time complexity for deterministic parsing from linear to quadratic in the worst case, but empirical estimates based on treebank data show that the expected running time is in fact linear for the range of data attested in the corpora. Evaluation on data from five languages shows state-of-the-art accuracy, with especially good results for the labeled exact match score. We present the projective Stack algorithm.	Non-Projective Dependency Parsing in Expected Linear Time We introduce a new method for dependency parsing, which connects words next to each other but can handle complex structures by changing the order of words. Adding this change makes the worst-case processing time longer, from fast to slower, but data suggests that on average, it remains fast for the types of data we have. Testing on five languages shows top-level accuracy, particularly for correctly matching labeled connections. We introduce the projective Stack algorithm.	language analysis
654	Dependency Grammar Induction via Bitext Projection Constraints Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poor languages. The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext. We consider generative and discriminative models for dependency grammar induction that use word-level alignments and a source language parser (English) to constrain the space of possible target trees. Unlike previous approaches, our framework does not require full projected parses, allowing partial, approximate transfer through linear expectation constraints on the space of distributions over trees. We consider several types of constraints that range from generic dependency conservation to language-specific annotation rules for auxiliary verb analysis. We evaluate our approach on Bulgarian and Spanish CoNLL shared task data and show that we consistently outperform unsupervised methods and can outperform supervised learning for limited training data. We use the posterior regularization (PR) approach in which a supervised English parser is used to generate constraints that are projected using a parallel corpus and used to regularize a target language parser.	Dependency Grammar Induction via Bitext Projection Constraints Broad-coverage annotated treebanks, which are detailed language maps needed to teach language analyzers, are not available for many less-studied languages. The large amount of translated text and good English language analyzers now make it possible to learn grammar by partially transferring knowledge across translated text (bitext). We explore models that help computers learn sentence structure by using word matches between languages and an English language analyzer to narrow down the possible sentence structures in the new language. Unlike past methods, our system doesn't need complete sentence structures to be copied over; it allows partial and estimated transfers using simple rules to guide the process. We test different rules, from general sentence structure preservation to specific rules for analyzing helping verbs in a language. We tested our method on Bulgarian and Spanish sentence data and found it works better than methods without guidance and can even beat methods that use some pre-existing knowledge when there's limited data to learn from. We use a method called posterior regularization (PR), where an English language analyzer creates rules based on translated text, which are then used to guide the learning process in the new language.	matching
655	Minimized Models for Unsupervised Part-of-Speech Tagging We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values. We evaluate our method on a standard test corpus using different standard tagsets (a 45-tagset as well as a smaller 17-tagset), and show that our approach performs better than existing state-of-the-art systems in both settings. We achieve the best results (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data. We propose a rigid mechanism for modeling sparsity that minimizes the size of tagging grammar as measured by the number of transition types. To avoid the need for manually pruning the tag dictionary, we propose that low-probability tags might be automatically filtered from the tag dictionary through a model minimization procedure applied to the raw text and constrained by the full tag dictionary.	Minimized Models for Unsupervised Part-of-Speech Tagging We explain a new way to do unsupervised POS (Part-of-Speech) tagging, which is labeling words in a sentence with their parts of speech like nouns or verbs, without using pre-labeled examples. This method uses a mathematical approach called integer programming to find the smallest model that can explain the data, and then uses EM (Expectation Maximization, a way to find the best guesses for certain numbers) to set the values needed for the model. We test our method on a standard set of texts with different sets of tags (one with 45 tags and another smaller one with 17 tags) and show that our method works better than the best current systems for both sets. We get the best score (92.3% accuracy in labeling words) by using a method called Minimum Description Length (a way to find the simplest explanation), with an integer program that finds a small bigram grammar (rules for word pairs) that follows the tag dictionary's rules and fits the observed data. We suggest a strict way to keep the tagging rules small by reducing the number of transition types, which are the ways one tag can change into another. Instead of removing tags manually from the tag dictionary, we suggest that tags with low chances of being correct can be automatically removed using a model that minimizes size and is applied to the text, keeping the full set of tags in mind.	language
656	An Error-Driven Word-Character Hybrid Model for Joint Chinese Word Segmentation and POS Tagging In this paper, we present a discriminative word-character hybrid model for joint Chinese word segmentation and POS tagging. Our word-character hybrid model offers high performance since it can handle both known and unknown words. We describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an error-driven policy that delivers such balance by acquiring examples of unknown words from particular errors in a training corpus. We describe an efficient framework for training our model based on the Margin Infused Relaxed Algorithm (MIRA), evaluate our approach on the Penn Chinese Treebank, and show that it achieves superior performance compared to the state-of-the-art approaches reported in the literature. We separate the processing of known words and unknown words, and uses a set of segmentation tags to represent the segmentation of characters.	An Error-Driven Word-Character Hybrid Model for Joint Chinese Word Segmentation and POS Tagging In this paper, we introduce a smart model that combines words and characters for both splitting Chinese words and identifying their parts of speech (POS). Our model works very well because it can deal with both familiar words and new words. We explain our methods that find a good balance for learning about familiar and new words. We suggest a policy that focuses on fixing mistakes, which helps us learn about new words by looking at specific errors in a training set. We describe an efficient system for training our model using a method called the Margin Infused Relaxed Algorithm (MIRA), test our method on a well-known Chinese language dataset, and show that it performs better than the best methods currently available. We handle familiar and new words separately and use a set of tags to show how characters are split into words.	approach
657	Unsupervised Learning of Narrative Schemas and their Participants We describe an unsupervised system for learning narrative schemas, coherent sequences or sets of events (arrested(POLICE,SUSPECT), convicted(JUDGE, SUSPECT)) whose arguments are filled with participant semantic roles defined over words (JUDGE = {judge, jury, court}, POLICE = {police, agent, authorities}). Unlike most previous work in event structure or semantic role learning, our system does not use supervised techniques, hand-built knowledge, or predefined classes of events or roles. Our unsupervised learning algorithm uses coreferring arguments in chains of verbs to learn both rich narrative event structure and argument roles. By jointly addressing both tasks, we improve on previous results in narrative/frame learning and induce rich frame-specific semantic roles. We describe a process to induce a partially ordered set of events related by a common protagonist by using an unsupervised distributional method to learn relations between events sharing co referring arguments, followed by temporal classification to induce partial order.	Unsupervised Learning of Narrative Schemas and their Participants. We describe a system that automatically learns narrative schemas, which are logical sequences or groups of events (like arrested by POLICE, SUSPECT and convicted by JUDGE, SUSPECT) where the roles are filled with words with specific meanings (JUDGE includes words like judge, jury, court; POLICE includes police, agent, authorities). Unlike most previous work on understanding event structure or the meaning of roles, our system does not rely on pre-labeled data, manually created information, or predefined sets of events or roles. Our automatic learning method uses repeated references in chains of action words to learn both detailed event structures and the roles involved. By handling both tasks together, we improve on past efforts in learning about narratives and frames, and we identify detailed roles specific to each frame. We describe a way to create a partly ordered list of events that relate to a main character by using an automatic method based on patterns in data to learn how events are related when they share common references, followed by classifying them in time to create a partial sequence.	specific errors
658	Conundrums in Noun Phrase Coreference Resolution: Making Sense of the State-of-the-Art We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the differences in the MUC and ACE task definitions, the assumptions made in evaluation methodologies, and inherent differences in text corpora. First, we examine three subproblems that play a role in coreference resolution: named entity recognition, anaphoricity determination, and coreference element detection. We measure the impact of each subproblem on coreference resolution and confirm that certain assumptions regarding these subproblems in the evaluation methodology can dramatically simplify the overall task. Second, we measure the performance of a state-of-the-art coreference resolver on several classes of anaphora and use these results to develop a quantitative measure for estimating coreference resolution performance on new data sets. We show that the co-reference resolution problem can be separated into different parts ac cording to the type of the mention.	Conundrums in Noun Phrase Coreference Resolution: Making Sense of the State-of-the-Art We aim to clarify the current best methods in NP (noun phrase) coreference resolution by breaking down the differences in the MUC and ACE task definitions, looking at the assumptions made during evaluation methods, and the natural differences in text collections. First, we look at three smaller problems that are important in coreference resolution: recognizing named entities (identifying names of people, places, etc.), determining anaphoricity (deciding if a word refers back to another word), and detecting coreference elements (finding words that refer to the same thing). We study how each of these smaller problems affects coreference resolution and confirm that certain assumptions about these problems in evaluation methods can make the overall task much easier. Second, we evaluate how well a top-performing coreference resolver (a tool that finds related words in text) works with different types of anaphora (words that refer back to something) and use these findings to create a way to estimate how well coreference resolution will perform on new sets of data. We demonstrate that the coreference resolution problem can be divided into different parts based on the type of mention (the way something is referred to).	POLICE includes
659	Automatic sense prediction for implicit discourse relations in text We present a series of experiments on automatically identifying the sense of implicit discourse relations, i.e. relations that are not marked with a discourse connective such as “but” or “because”. We work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses. We use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features. In addition, we revisit past approaches using lexical pairs from unannotated text as features, explain some of their shortcomings and propose modifications. Our best combination of features outperforms the baseline from data intensive approaches by 4% for comparison and 16% for contingency. Although on Comparison relation there is only as light improvement (+1.07%), our two best systems both got around 10% improvements of f score over a state-of-the-art system in (Pitler et al, 2009a). Our analysis shows that the top word pairs (ranked by information gain) all contain common functional words, and are not at all the semantically-related content words that were imagined.	Automatic sense prediction for implicit discourse relations in text We present a set of tests to automatically figure out the meaning of hidden (implicit) connections in text, which don't use obvious linking words like "but" or "because". We use a collection of these hidden connections found in newspaper articles and share results from a test set that represents how these connections naturally appear. We apply several language-based features, such as positive or negative word labels (polarity tags), types of verbs (Levin verb classes), how long verb phrases are, possibility markers (modality), surrounding text (context), and word features. Additionally, we look back at older methods that used word pairs from text without annotations as features, point out some issues with them, and suggest changes. Our best mix of features does better than the standard method that relies on a lot of data by 4% for comparison types and 16% for cause-and-effect (contingency). Even though the improvement for comparison is small (+1.07%), our top two systems both showed about 10% better performance in measuring accuracy (f score) compared to a leading system from previous research (Pitler et al, 2009a). Our study shows that the most important word pairs (ranked by how much information they add) all include common, everyday words, instead of the idea-related words we expected.	determining anaphoricity
660	A Gibbs Sampler for Phrasal Synchronous Grammar Induction We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches. We use Gibbs sampler for learning the SCFG by reasoning over the space of derivations (Blunsom et al, 2009). We present a method for maintaining table counts without needing to record the table assignments for each translation decision. We apply the technique of using multiple processors to perform approximate Gibbs sampling which we show achieve equivalent performance to the exact Gibbs sampler.	A Gibbs Sampler for Phrasal Synchronous Grammar Induction We introduce a model for understanding how phrases in different languages are equivalent in meaning. Unlike earlier methods, we don't use guesswork or rules based on aligning individual words. Instead, we create a grammar directly from texts that have been translated sentence by sentence. We use a method that prefers simpler grammars with smaller translation parts, guided by a Bayesian prior which is a statistical way to incorporate prior knowledge. We solve this using a new technique called a Gibbs sampler, which is a way to efficiently explore possible translations. This method avoids the complexity of older models, making it faster and able to handle larger sets of translations. We use Gibbs sampler for learning the SCFG (Synchronous Context-Free Grammar) by exploring different ways to translate (Blunsom et al, 2009). We explain how to keep track of translation choices without needing to record every decision. We also use several processors to make the Gibbs sampling faster, showing that this approach works just as well as the traditional, exact method.	these hidden
661	Application-driven Statistical Paraphrase Generation Paraphrase generation (PG) is important in plenty of NLP applications. However, the research of PG is far from enough. In this paper, we propose a novel method for statistical paraphrase generation (SPG), which can (1) achieve various applications based on a uniform statistical model, and (2) naturally combine multiple resources to enhance the PG performance. In our experiments, we use the proposed method to generate paraphrases for three different applications. The results show that the method can be easily transformed from one application to another and generate valuable and interesting paraphrases. We present a sentence paraphrasing method that can be configured for different tasks, including a form of sentence compression.	Application-driven Statistical Paraphrase Generation Paraphrase generation (PG) is important in many natural language processing (NLP) applications. However, research on PG is still lacking. In this paper, we introduce a new method for statistical paraphrase generation (SPG) that can (1) be used for different tasks using a single statistical model, and (2) easily combine various sources of information to improve how well PG works. In our tests, we use this method to create paraphrases for three different tasks. The results show that the method can be easily adapted from one task to another and can produce useful and interesting paraphrases. We demonstrate a sentence rewriting method that can be adjusted for different needs, including a way to shorten sentences.	translations
662	Better Word Alignments with Supervised ITG Models This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints. We consider maximum margin and conditional likelihood objectives, including the presentation of a new normal form grammar for canonicalizing derivations. Even for non-ITG sentence pairs, we show that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives. For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments. We describe a pruning heuristic that results in average case runtime of O (n 3).	Better Word Alignments with Supervised ITG Models This work looks into supervised word alignment methods that use inversion transduction grammar (ITG) rules. We explore goals like maximum margin (finding the best separation between data groups) and conditional likelihood (probability of data given some condition), and introduce a new way to standardize grammar for simplifying derivations (steps of breaking down sentences). Even for non-ITG sentence pairs, we demonstrate that it's possible to learn ITG alignment models by making simple adjustments to structured learning objectives (goals for learning patterns). For efficiency, we explain a set of pruning techniques (cutting down unnecessary parts) that together allow us to align sentences 100 times faster than basic bitext CKY parsing (a method for grammar analysis). Finally, we add many-to-one block alignment features, which greatly improve our ITG models. In total, our method achieves the best reported AER (alignment error rate) numbers for Chinese-English and improves performance by 1.1 BLEU (a score for translation quality) over GIZA++ alignments. We describe a pruning heuristic (rule of thumb) that results in an average case runtime of O(n^3) (a measure of how processing time increases with sentence length).	paraphrases
663	Distant supervision for relation extraction without labeled data Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACE-style algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large un-labeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression. Additionally, researchers have tried to automatically extract examples for supervised learning from resources such as Wikipedia (Weld et al,2008) and databases (Mintz et al, 2009), or attempted open information extraction (IE) (Banko et al, 2007) to extract all possible relations. Distant supervision (DS) can automatically gather labeled data by heuristically aligning entities in text with those in a knowledge base.	Distant supervision for relation extraction without labeled data Modern models of relation extraction (finding connections between things) for tasks like ACE (a specific task) use supervised learning, which means learning from small, manually labeled collections of text. We explore a different method that doesn't need labeled text collections, avoiding the specific focus of ACE-style methods, and allowing the use of text collections of any size. Our experiments use Freebase, a large database with many types of connections, to provide distant supervision (a way to learn without direct labels). For each pair of things that show a connection in Freebase, we find all sentences with those things in a large unlabeled text collection and collect text features to train a relation classifier (a tool that identifies connections). Our approach mixes the benefits of supervised learning (using 400,000 noisy pattern features in a probabilistic classifier) and unsupervised learning (finding many connections from large text collections of any type). Our model can find 10,000 examples of 102 connections with an accuracy of 67.6%. We also look at how features perform, showing that features based on sentence structure are especially useful for connections that are unclear or expressed in different words. Additionally, researchers have tried to automatically gather examples for supervised learning from resources like Wikipedia and databases, or tried open information extraction (IE) to find all possible connections. Distant supervision (DS) can automatically collect labeled data by smartly matching things in text with those in a knowledge base (a large collection of known information).	separation
664	Phrase Clustering for Discriminative Learning We present a simple and scalable algorithm for clustering tens of millions of phrases and use  the resulting clusters as features in  discriminative classifiers. To demonstrate the power and generality of this approach, we apply the method in two very different applications: named entity recognition and query classification. Our results show that phrase clusters offer significant improvements over word clusters. Our NER system achieves the best current result on the widely used CoNLL benchmark. Our query classifier is on par with the best system in KDDCUP 2005 without resorting to labor intensive knowledge engineering efforts. We explore a two-stage cluster-based approach: first clustering phrases and then relying on a supervised learner to identify useful clusters and assign proper weights to cluster features. We present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce.	Phrase Clustering for Discriminative Learning We introduce an easy and expandable method for grouping tens of millions of phrases and use these groups as tools in detailed learning systems. To show how powerful and flexible this method is, we use it in two very different cases: recognizing named entities (like names of people or places) and classifying search queries. Our findings reveal that phrase groups are much better than just word groups. Our system for recognizing named entities performs the best on the well-known CoNLL standard test. Our search query classifier matches the top system in the KDDCUP 2005 competition without needing a lot of manual knowledge work. We look into a two-step approach: first, grouping phrases, then using a guided learning system to find useful groups and give correct importance to these group features. We introduce a K-means-like (a common method for grouping) non-hierarchical (not ranked or layered) grouping method for phrases, which uses MapReduce (a way of processing large amounts of data).	automatically
665	Using Syntax to Disambiguate Explicit Discourse Connectives in Text Discourse connectives are words or phrases such as once, since, and on the contrary that explicitly signal the presence of a discourse relation. There are two types of ambiguity that need to be resolved during discourse processing. First, a word can be ambiguous between discourse or non-discourse usage. For example, once can be either a temporal discourse connective or a simply a word meaning “formerly”. Secondly, some connectives are ambiguous in terms of the relation they mark. For example since can serve as either a temporal or causal connective. We demonstrate that syntactic features improve performance in both disambiguation tasks. We report state-ofthe-art results for identifying discourse vs. non-discourse usage and human-level performance on sense disambiguation. We show that where explicit markers exist, the class of the relation can be disambiguated with f-scores higher than 90%. Predicting the class of implicit discourse relations, however, is much more difficult.	Using Syntax to Disambiguate Explicit Discourse Connectives in Text Discourse connectives are words or phrases like "once," "since," and "on the contrary" that clearly show a connection or relationship between parts of a text. There are two types of confusion that need to be cleared up when understanding these connections. First, a word can be unclear about whether it's showing a connection or not. For example, "once" can mean either a time-related connection or just mean "formerly." Secondly, some connectives are unclear about what kind of connection they show. For example, "since" can mean either a time-related or cause-and-effect connection. We show that using sentence structure helps improve figuring out these confusions. We report top-level results for telling apart when a word shows a connection or not, and achieving human-like accuracy in understanding the type of connection. We show that when there are clear markers, the type of connection can be figured out with scores over 90%. However, predicting the type of connection when it's not clearly marked is much harder.	competition
666	Bayesian Learning of a Tree Substitution Grammar Tree substitution grammars (TSGs) offer many advantages over context-free grammars (CFGs), but are hard to learn. Past approaches have resorted to heuristics. In this paper, we learn a TSG using Gibbs sampling with a nonparametric prior to control subtree size. The learned grammars perform significantly better than heuristically extracted ones on parsing accuracy.	Bayesian Learning of a Tree Substitution Grammar Tree substitution grammars (TSGs), which are a way to structure sentences, offer many advantages over context-free grammars (CFGs), but are hard to learn. Past approaches have resorted to heuristics, which are simple rules or shortcuts. In this paper, we learn a TSG using Gibbs sampling, a method for statistical analysis, with a nonparametric prior, a flexible model, to control subtree size. The learned grammars perform significantly better than those extracted using simple rules on parsing accuracy, which means they understand sentence structures more accurately.	Connectives
670	Practical Very Large Scale CRFs Conditional Random Fields (CRFs) are a widely-used approach for supervised sequence labelling, notably due to their ability to handle large description spaces and to integrate structural dependency between labels. Even for the simple linear-chain model, taking structure into account implies a number of parameters and a computational effort that grows quadratically with the cardinality of the label set. In this paper, we address the issue of training very large CRFs, containing up to hundreds output labels and several billion features. Efficiency stems here from the sparsity induced by the use of a lscript1 penalty term. Based on our own implementation, we compare three recent proposals for implementing this regularization strategy. Our experiments demonstrate that very large CRFs can be trained efficiently and that very large models are able to improve the accuracy, while delivering compact parameter sets.	Practical Very Large Scale CRFs Conditional Random Fields (CRFs) are a popular method for supervised sequence labeling, mainly because they can manage large amounts of data and connect the relationships between labels. Even for a simple linear model, considering the structure requires many settings and calculations that increase significantly with more labels. In this paper, we tackle the challenge of training very large CRFs, which may include hundreds of output labels and billions of features. The efficiency comes from using a special technique called a lscript1 penalty term that keeps things simple. Using our own setup, we compare three new ways to apply this method. Our tests show that very large CRFs can be trained effectively and these large models can improve accuracy while keeping the settings compact.	understand
671	Dynamic Programming for Linear-Time Incremental Parsing Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a major problem: the search is greedy and only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming. We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging “equivalent” stacks based on feature values. Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy. Better search also leads to better learning, and our final parser outperforms all previously reported dependency parsers for English and Chinese, yet is much faster. In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). The essential idea in our calculation is to delegate (in the computation of the Viterbi score) the scoring of sh transitions to the inference rules for la/ra.	Dynamic Programming for Linear-Time Incremental Parsing Incremental parsing techniques like shift-reduce have become popular because they are fast, but there is a big issue: the process is greedy, meaning it only looks at a small part of all the possibilities (even with beam search, which tries to consider more options) unlike dynamic programming, which examines more options. We demonstrate that, surprisingly, dynamic programming can actually work for many shift-reduce parsers by combining "equivalent" stacks based on shared features or characteristics. Based on our tests, our method works up to five times faster compared to a top shift-reduce dependency parser without losing any accuracy. This improved search method also improves learning, and our final parser performs better than all previously known dependency parsers for English and Chinese, while still being much faster. In this paper, we build on the research started by Huang and Sagae (2010) and use dynamic programming for (projective) transition-based dependency parsing (Nivre, 2008). The main concept in our method is to assign the task of scoring certain transitions to specific rules in the calculation process (Viterbi score).	increase
672	Supervised Noun Phrase Coreference Research: The First Fifteen Years The research focus of computationalcoreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago. We argue that the so called mention-pair model suffers from several design flaws which originate from the locally confined perspective of the model.	Supervised Noun Phrase Coreference Research: The First Fifteen Years The research focus of computational coreference resolution, which is a computer method for finding out if different phrases refer to the same thing, has changed from using simple rules to using machine learning methods in the past ten years. This paper reviews the important achievements in supervised coreference research, which is a method involving training computers with examples, since it started fifteen years ago. We believe that the mention-pair model, which is a specific way to identify coreferences, has several design problems because it only looks at things in a narrow way.	Dynamic Programming
673	Learning to Translate with Source and Target Syntax Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years. These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language. But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language. We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a simple approach that uses both source and target syntax for significant improvements in translation accuracy. We obtain significant improvement over his hierarchical baseline by using syntactic parse trees on both source and target sides to induce fuzzy (not exact) tree-to-tree rules and by also allowing syntactically mismatched substitutions. We show that the integration of syntactic information on both sides tends to decrease translation quality because the systems be come too restrictive.	Learning to Translate with Source and Target Syntax Statistical translation models that try to understand the repeated patterns in language have become popular in recent years. These models use different amounts of information from language theory: some use none, some focus on the grammar rules of the language being translated into (target language), and some focus on the grammar of the original language (source language). However, it's been harder to create translation models that can understand how the grammar of both languages relate to each other. We talk about why this is difficult, look at past attempts, and show how combining some old and new ideas can create a simple method that uses the grammar rules from both languages to make translation more accurate. We improve translation by using grammar structures (parse trees) from both the original and target languages to create flexible (not exact) rules that allow for mismatched substitutions. We find that using grammar information from both languages can actually lower translation quality because the systems become too strict.	mention
674	Intelligent Selection of Language Model Training Data We address the problem of selecting non-domain-specific language model training data to build auxiliary language models for use in tasks such as machine translation. Our approach is based on comparing the cross-entropy, according to domain-specific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model. We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods. In cross-entropy difference selection, a sentence's score is the in-domain cross-entropy minus the background cross-entropy. This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words).	Intelligent Selection of Language Model Training Data We tackle the issue of choosing general language data, not specific to any field, to create extra language models for tasks like translating languages. Our method involves comparing how well sentences fit into models specific to a field and those not specific, by measuring how unexpected or surprising a sentence is, called cross-entropy, in the text used to make the general model. We demonstrate that this results in better language models, using less data, compared to picking data randomly or using two other older methods. In choosing based on cross-entropy difference, a sentence is scored by subtracting the general surprise measure from the specific field surprise measure. This method has been applied to enhance European parliamentary texts (48 million words) with news articles (3.4 billion words).	information
676	Target-dependent Twitter Sentiment Classification Sentiment analysis on Twitter data has attracted much attention recently. In this paper, we focus on target-dependent Twitter sentiment classification; namely, given a query, we classify the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query. Here the query serves as the target of the sentiments. The state-of-the-art approaches for solving this problem always adopt the target-independent strategy, which may assign irrelevant sentiments to the given target. Moreover, the state-of-the-art approaches only take the tweet to be classified into consideration when classifying the sentiment; they ignore its context (i.e., related tweets). However, because tweets are usually short and more ambiguous, sometimes it is not enough to consider only the current tweet for sentiment classification. In this paper, we propose to improve target-dependent Twitter sentiment classification by 1) incorporating target-dependent features; and 2) taking related tweets into consideration. According to the experimental results, our approach greatly improves the performance of target-dependent sentiment classification. We incorporate target-dependent features and considers related tweets by utilizing a graph-based optimization. We combine the target-independent features (content and lexicon) and target-dependent features (rules based on the dependency parsing results) together in subjectivity classification and polarity classification for tweets.	Target-dependent Twitter Sentiment Classification Sentiment analysis on Twitter data has gained a lot of interest recently. In this paper, we focus on classifying sentiments in tweets based on a specific topic or "target." This means we look at whether tweets express positive, negative, or neutral feelings about a given topic or query. The query is the main focus of the sentiments. The best current methods usually don't consider the specific target and might incorrectly assign feelings not related to it. They only look at the tweet itself and ignore other related tweets, which can be important since tweets are often short and unclear. In this paper, we suggest improving this by 1) including features that depend on the target and 2) considering related tweets. Our experiments show that this approach significantly enhances the accuracy of classifying sentiments based on targets. We use target-specific details and related tweets through a method that optimizes connections like a network or "graph." We combine general features (like tweet content and words used) with specific features (rules from analyzing how words in a sentence relate) to better determine if tweets are subjective (opinion-based) or what their sentiment is (positive, negative, neutral).	reference
677	A New Dataset and Method for Automatically Grading ESOL Texts We demonstrate how supervised discriminative machine learning techniques can be used to automate the assessment of ‘English as a Second or Other Language’ (ESOL) examination scripts. In particular, we use rank preference learning to explicitly model the grade relationships between scripts. A number of different features are extracted and ablation tests are used to investigate their contribution to overall performance. A comparison between regression and rank preference models further supports our method. Experimental results on the first publicly available dataset show that our system can achieve levels of performance close to the upper bound for the task, as defined by the agreement between human examiners on the same corpus. Finally, using a set of ‘outlier’ texts, we test the validity of our model and identify cases where the model’s scores diverge from that of a human examiner. We publicly release a set of 1,244 FCE ESOL texts.	A New Dataset and Method for Automatically Grading ESOL Texts We show how supervised machine learning (a method where the computer learns from examples) can help grade 'English as a Second or Other Language' (ESOL) exam papers automatically. Specifically, we use a technique called rank preference learning to clearly understand how different grades relate to each other. We look at various features or characteristics of the texts and use ablation tests (which remove features one by one) to see which ones are most important for good performance. We compare two types of models, regression and rank preference, and find that our method is strong. Tests on the first dataset available to the public show our system works almost as well as human examiners, who sometimes don't fully agree with each other. Finally, we use some "outlier" texts, which are unusual cases, to check if our model is accurate and to find where it disagrees with a human examiner. We are also releasing 1,244 ESOL texts from the FCE exam to the public.	important since
678	Collecting Highly Parallel Data for Paraphrase Evaluation A lack of standard datasets and evaluation metrics has prevented the field of paraphrasing from making the kind of rapid progress enjoyed by the machine translation community over the last 15 years. We address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale. The highly parallel nature of this data allows us to use simple n-gram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates. In addition to being simple and efficient to compute, experiments show that these metrics correlate highly with human judgments. Our dataset consists of 1,500 pairs of short video descriptions collected using crowd sourcing.	Collecting Highly Parallel Data for Paraphrase Evaluation A lack of standard datasets and evaluation metrics has prevented the field of paraphrasing from making the kind of rapid progress enjoyed by the machine translation community over the last 15 years. We address both problems by presenting a new way to collect data that produces very similar text data cheaply and in large amounts. The very similar nature of this data allows us to use simple word pattern comparisons to measure both the meaning and word differences of paraphrase options. In addition to being simple and efficient to compute, experiments show that these measurements match well with human opinions. Our dataset consists of 1,500 pairs of short video descriptions collected using crowd sourcing, which means getting a large group of people to help online.	fully agree
679	Lexical Normalisation of Short Text Messages: Makn Sens a #twitter Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP. In this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words. Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity. Both word similarity and context are then exploited to select the most probable correction candidate for the word. The proposed method doesn't require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter. We use a classifier to detect ill formed words, and then generate correction candidates based on morphophonemic similarity.	Lexical Normalisation of Short Text Messages: Makn Sens a #twitter Twitter provides access to large amounts of data quickly, but it is often messy, making it hard to use for natural language processing (NLP). In this paper, we focus on words not typically found in dictionaries in short text messages and suggest a way to find and fix these misspelled words. Our method uses a computer program to spot misspelled words and suggests possible corrections based on how the words look and sound. We then use both word similarity and the surrounding text to choose the most likely correct word. The suggested method doesn't need any special labels or notes and works very well on a collection of text messages and a new set of data from Twitter. We use a computer program to find misspelled words and then suggest corrections based on how the words look and sound.	consists
690	Providing A Unified Account Of Definite Noun Phrases In Discourse Linguistic theories typically assign various linguistic phenomena to one of the categories, syntactic, semantic, or pragmatic, as if the phenomena in each category were relatively independent of those in the others. However, various phenomena in discourse do not seem to yield comfortably to any account that is strictly a syntactic or semantic or pragmatic one. This paper focuses on particular phenomena of this sort - the use of various referring expressions such as definite noun phrases and pronouns - and examines their interaction with mechanisms used to maintain discourse coherence. Even a casual survey of the literature on definite descriptions and referring expressions reveals not only defects in the individual accounts provided by theorists (from several different disciplines), but also deep confusions about the roles that syntactic, semantic, and pragmatic factors play in accounting for these phenomena. The research we have undertaken is an attempt to sort out some of these confusions and to create the basis for a theoretical framework that can account for a variety of discourse phenomena in which all three factors of language use interact. The major premise on which our research depends is that the concepts necessary for an adequate understanding of the phenomena in question are not exclusively either syntactic or semantic or pragmatic. The next section of this paper defines two levels of discourse coherence and describes their roles in accounting for the use of singular definite noun phrases. To illustrate the integration of factors in explaining the uses of referring expressions, their use on one of these levels, i.e., the local one, is discussed in Sections 3 and 4. This account requires introducing the notion of the centers of a sentence in a discourse, a notion that cannot be defined in terms of factors that are exclusively syntactic or semantic or pragmatic. In Section 5, the interactions of the two levels with these factors and their effects on the uses of referring expressions in discourse are discussed. To resolve referring expression, we develop centering theory.	Providing A Unified Account Of Definite Noun Phrases In Discourse Linguistic theories usually place language features into groups like syntax (sentence structure), semantics (meaning), or pragmatics (context and use), as if each group is separate from the others. But, many language features in conversation don't fit neatly into just one of these groups. This paper looks at specific features like how definite noun phrases (specific words to refer to things) and pronouns (he, she, it) are used, and studies how they help make conversations clear and connected. Even a quick look at studies on definite descriptions shows not only mistakes in individual theories by experts from different fields, but also big misunderstandings about how syntax, semantics, and pragmatics work together to explain these features. Our research tries to clear up some of these misunderstandings and build a theory that explains various conversation features by showing how all three language aspects work together. Our main idea is that to truly understand these features, we can't just focus on syntax, semantics, or pragmatics alone. The next part of this paper explains two levels of conversation clarity and how they help explain the use of specific noun phrases. To show how these language aspects work together to explain referring expressions, their use on one level, called the local level, is discussed in Sections 3 and 4. This explanation involves introducing the idea of the "centers" of a sentence in a conversation, an idea that can't be explained just by looking at syntax, semantics, or pragmatics alone. In Section 5, how these two levels interact with these language aspects and their impact on using referring expressions in conversation are explored. To solve how referring expressions are used, we develop "centering theory."	partly solves
680	Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web’s natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multiinstance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint — for example they cannot extract the pair Founded(Jobs, Apple) and CEO-of(Jobs, Apple). This paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extraction model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Freebase. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level. We apply the greedy inference algorithm. We use  multiple deterministic-OR constraint to train a sentential relation extractor.	Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations Information extraction (IE) aims to create a large set of knowledge from text on the internet. Knowledge-based weak supervision, which uses existing data to loosely label a training set, helps achieve this by automatically learning many ways to extract relationships from text. Recently, scientists have created learning methods to handle messy training data that can result from these loose labels, but these methods assume that relationships do not overlap — for example, they cannot extract both Founded(Jobs, Apple) and CEO-of(Jobs, Apple). This paper introduces a new method for learning with overlapping relationships that combines a model for extracting information from individual sentences with a straightforward method for combining these facts from the whole text. We use our model to learn how to extract information from NY Times articles using guidance from Freebase, a large database. Tests show that this method works fast and improves accuracy more than expected, both overall and in individual sentences. We use a straightforward decision-making process called a greedy inference algorithm. We apply multiple logical rules, called deterministic-OR constraints, to train a system that finds relationships within sentences.	likely correct
681	Learning Dependency-Based Compositional Semantics Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms. We DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins. Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees.	Learning Dependency-Based Compositional Semantics Compositional question answering starts by turning questions into logical forms, but training a semantic parser for this usually needs expensive labeling of the target logical forms. In this paper, we learn to connect questions to answers through hidden logical forms, which are automatically created from question-answer pairs. To solve this difficult learning problem, we introduce a new way to represent meaning that shows a similarity between dependency syntax (how words are connected) and the efficient assessment of logical forms. On two common tests for semantic parsing (GEO and JOBS), our system achieves the highest reported accuracy, even though it doesn't need any labeled logical forms. We use DCS for dependency-based compositional semantics, which shows a semantic parse as a tree with points representing database parts and actions, and connections representing relational joins. Dependency-based Compositional Semantics (DCS) offers an easy way to understand the meaning of questions by using simple tree-like structures.	apply multiple
682	Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages. We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg-Kirkpatrick et al., 2010). Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm. We build a dictionary for a particular language by transferring annotated data from a resource-rich language through the use of word alignments in parallel text.	Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections We describe a new way to develop part-of-speech taggers, which identify parts of speech like nouns and verbs, for languages that don't have labeled training data but have texts translated from a language with lots of resources. Our method doesn't require any prior knowledge about the language being studied, meaning it can be used for many languages that lack resources. We use a technique called graph-based label propagation to share knowledge across languages and use these shared labels as features in an unsupervised model (Berg-Kirkpatrick et al., 2010). For eight European languages, our method improves accuracy by 10.4% compared to a leading existing method, and by 16.7% compared to a basic model using the Expectation Maximization algorithm. We create a dictionary for a specific language by transferring labeled data from a language with many resources, using word alignments in texts that are available in both languages.	represent
683	Template-Based Information Extraction without the Templates Standard algorithms for template-based information extraction (IE) require predefined template schemas, and often labeled data, to learn to extract their slot fillers (e.g., an embassy is the Target of a Bombing template). This paper describes an approach to template-based IE that removes this requirement and performs extraction without knowing the template structure in advance. Our algorithm instead learns the template structure automatically from raw text, inducing template schemas as sets of linked events (e.g., bombings include detonate, set off, and destroy events) associated with semantic roles. We also solve the standard IE task, using the induced syntactic patterns to extract role fillers from specific documents. We evaluate on the MUC-4 terrorism dataset and show that we induce template structure very similar to hand-created gold structure, and we extract role fillers with an F1 score of .40, approaching the performance of algorithms that require full knowledge of the templates. We acquire event words from an external resource, group the event words to form event scenarios, and group extraction patterns for different event roles.	Template-Based Information Extraction without the Templates Standard algorithms for template-based information extraction (IE) usually need predefined templates, which are like outlines, and often need labeled data to learn how to fill in the blanks (e.g., an embassy is the Target of a Bombing template). This paper talks about a method that can do this without needing to know the outline beforehand. Our method instead learns the outline automatically from plain text by figuring out template outlines as sets of connected events (e.g., bombings include actions like detonate, set off, and destroy) linked to roles in the event. We also complete the usual IE task by using the learned patterns to find role fillers, or details, from specific documents. We test on the MUC-4 terrorism dataset and show that we create templates very similar to manually made ones, and we fill in the roles with an F1 score of .40, which is nearly as good as methods that need full template knowledge. We find event words from an outside source, group these words to form event scenarios, and organize patterns for different roles in events.	meaning
743	Verb Semantics And Lexical Selection This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT). Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentence as well as selection restrictions placed on the verb arguments. A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT. We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems. Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection. We design our mesure such that shallow nodes are less similar than nodes that are deeper in the WordNet hierarchy. Our mesure is purely taxonomic; it does not require any corpus statistics. Our similarity metric measures the depth of the two concepts in the WordNet taxonomy and the depth of the least common subsumer and then combines these figures into a similarity score.	Verb Semantics And Lexical Selection This paper will focus on how verbs are understood in computer systems and how this affects word choice problems in machine translation (MT). Two groups of verbs, one in English and one in Chinese, are studied to show that choosing the right words must consider both the meaning of the sentence and any limits placed on the verb's other words. A new way to represent this is suggested and compared with older methods used in translation that rely on these limits. Our method is similar to knowledge-based machine translation (KBMT) and can be added to current systems. Examples and test results will demonstrate that, using this method, even imperfect matches can lead to correct word choices. We design our measure so that nodes that are higher up in the WordNet hierarchy are considered less similar than those deeper down. Our measure is only based on the arrangement of concepts, and doesn't need any data from language examples. Our similarity measure evaluates how deep two concepts are in the WordNet system and how deep their closest shared concept is, then combines these numbers into a similarity score.	meaning
684	Local and Global Algorithms for Disambiguation to Wikipedia Disambiguating concepts and entities in a context sensitive way is a fundamental problem in natural language processing. The comprehensiveness of Wikipedia has made the online encyclopedia an increasingly popular target for disambiguation. Disambiguation to Wikipedia is similar to a traditional Word Sense Disambiguation task, but distinct in that the Wikipedia link structure provides additional information about which disambiguations are compatible. In this work we analyze approaches that utilize this information to arrive at coherent sets of disambiguations for a given document (which we call “global” approaches), and compare them to more traditional (local) approaches. We show that previous approaches for global disambiguation can be improved, but even then the local disambiguation provides a baseline which is very hard to beat.	Local and Global Algorithms for Disambiguation to Wikipedia Disambiguating concepts and entities, which means figuring out what words or phrases mean in their specific context, is an important problem in understanding language. The thoroughness of Wikipedia has made it a popular choice for solving these meaning-related issues. Solving these issues using Wikipedia is similar to traditional tasks where we try to determine the right meaning of a word, but it's different because Wikipedia's link structure offers extra clues about which meanings fit together. In this work, we look at methods that use this extra information to find sets of meanings that make sense for a document (we call these “global” methods) and compare them to older methods that focus on individual words or phrases (local approaches). We show that while we can make global methods better, the local methods are still a very strong standard that is tough to surpass.	learned patterns
685	Part-of-Speech Tagging for Twitter: Annotation Features and Experiments We address the problem of part-of-speech tagging for English data from the popular micro-blogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets. Recognizing the limitations of existing systems,we develop a POS tagger specifically for Twitter, by creating a training corpus as well as devising a tag set that includes parts of speech that are uniquely found in on line language, such as emoticons (smilies). We release the Twitter POS dataset consisting of approximately 26,000 words across 1,827 tweets. The Twitter dataset uses a domain-dependent tag set of 25 tags.	Part-of-Speech Tagging for Twitter: Annotation Features and Experiments We tackle the challenge of identifying parts of speech, like nouns and verbs, for English text from Twitter, a popular social media platform. We create a set of tags, label data, develop characteristics for analysis, and achieve tagging results close to 90% accuracy. The data and tools are shared with researchers to help better analyze text from Twitter and similar social media. Understanding current systems' limits, we build a special part-of-speech tagger for Twitter by making a training set and a tag set that includes online-specific language, like emoticons (smiley faces). We provide a Twitter dataset with about 26,000 words from 1,827 tweets. This dataset uses a specific set of 25 tags for Twitter.	different because
686	Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system. To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. In this paper, we consider how to make such experiments more statistically reliable. We provide a systematic analysis of the effects of optimizer instability — an extraneous variable that is seldom controlled for — on experimental outcomes, and make recommendations for reporting results more accurately. We implement a stratified approximate randomization test to account for multiple tuning replications.	Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability In statistical machine translation, a researcher wants to find out if a new idea (like a new feature, model, or method) makes translation better compared to a basic system. To figure this out, the researcher tests both systems using separate data. In this paper, we look at ways to make these tests more reliable. We analyze how changes in the optimizer (a tool that adjusts settings to get the best results) can affect the test results and suggest ways to report results more accurately. We use a special test method that considers multiple tuning attempts to make the results fairer.	Twitter
687	Transition-based Dependency Parsing with Rich Non-local Features Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations. In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems. In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transition-based parsing and rivaling the best results overall. For the Chinese Treebank, they give a signficant improvement of the state of the art. An open source release of our parser is freely available. We develop the feature template for the arc-eager model.	Transition-based Dependency Parsing with Rich Non-local Features Transition-based dependency parsers usually use rule-based methods for decision-making, but they can handle very detailed information. In this paper, we show that we can make these parsers more accurate by using even more detailed information than what was used before. In the usual Penn Treebank setup, our new features improve the accuracy of linking words from 91.4% to 92.9%, giving the best results so far for this method of parsing and competing with the best results overall. For the Chinese Treebank, they provide a big improvement on the best current methods. A free version of our parser is available for anyone to use. We create the feature pattern for the arc-eager model, which is a specific way of organizing the parsing process.	Testing
688	Improving Word Representations via Global Context and Multiple Word Prototypes Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models. Our representation is designed to capture word sense disambiguation.	Improving Word Representations via Global Context and Multiple Word Prototypes Unsupervised word representations are very useful in NLP (Natural Language Processing) tasks both as inputs to learning algorithms (methods for teaching computers) and as extra word features in NLP systems. However, most of these models are built with only local context (immediate surroundings of a word) and one representation per word. This is problematic because words often have multiple meanings (polysemous), and global context (overall meaning in a larger text) can also provide useful information for understanding word meanings. We present a new neural network architecture which 1) learns word embeddings (word meanings in a digital form) that better capture the semantics (meanings) of words by incorporating both local and global document context, and 2) accounts for homonymy (same spelling but different meanings) and polysemy (multiple meanings) by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context (within sentences), and evaluate our model on it, showing that our model outperforms competitive baselines (previous top methods) and other neural language models. Our representation is designed to capture word sense disambiguation (figuring out which meaning of a word is correct in a given context).	current methods
691	Deterministic Parsing Of Syntactic Non-Fluencies It is often remarked that natural language, used naturally, is unnaturally ungrammatical. Spontaneous speech contains all manner of false starts, hesitations, and self-corrections that disrupt the well-formedness of strings. It is a mystery then, that despite this apparent wide deviation from grammatical norms, people have little difficulty understanding the non-fluent speech that is the essential medium of everyday life. And it is a still greater mystery that children can succeed in acquiring the grammar of a language on the basis of evidence provided by a mixed set of apparently grammatical and ungrammatical strings. We address the problem of correcting self repairs by adding rules to a deterministic parser that would remove the necessary text. We define a typology of repairs and associated correction strategies in terms of extensions to a deterministic parser.	Deterministic Parsing Of Syntactic Non-Fluencies It is often noted that when people use language naturally, it tends to break grammar rules. Everyday talking includes false starts, pauses, and self-corrections that interrupt the proper structure of sentences. It's surprising that even with these grammar mistakes, people usually have no trouble understanding these imperfect conversations, which are common in daily life. It's even more surprising that children can learn the grammar of a language from a mix of correct and incorrect sentences. We tackle the issue of fixing self-corrections by adding rules to a deterministic parser (a tool that analyzes sentence structure) to remove unnecessary words. We categorize different types of repairs and how to fix them by updating a deterministic parser.	different fields
692	D-Theory: Talking About Talking About Trees Linguists, including computational linguists, have always been fond of talking about trees. In this paper, we outline a theory of linguistic structure which talks about talking about trees; we call this theory Description theory (D-theory). While important issues must be resolved before a complete picture of D-theory emerges (and also before we can build programs which utilize it), we believe that this theory will ultimately provide a framework for explaining the syntax and semantics of natural language in a manner which is intrinsically computational. This paper will focus primarily on one set of motivations for this theory, those engendered by attempts to handle certain syntactic phenomena within the framework of deterministic parsing. Our D-theory model is powerful in that it allows the right-most daughter of a node to be lowered under a sibling node.	D-Theory: Talking About Talking About Trees Linguists, including those who study language using computers, like to discuss the idea of trees in language. In this paper, we describe a theory about how we talk about these language trees, which we call Description theory (D-theory). Although there are important details to work out before we fully understand D-theory (and before we can create computer programs that use it), we think this theory will eventually help explain how natural language works in a way that computers can understand. This paper will mainly focus on one reason for this theory, which is dealing with certain language rules using a method called deterministic parsing. Our D-theory model is strong because it allows the last part of a tree (right-most daughter) to be moved under a nearby part (sibling node).	includes
693	Parsing As Deduction By exploring the relationship between parsing and deduction, a new and more general view of chart parsing is obtained, which encompasses parsing for grammar formalisms based on unification, and is the basis of the Earley Deduction proof procedure for definite clauses. The efficiency of this approach for an interesting class of grammars is discussed. We extend Earley deduction work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters. We present versions of Earley's algorithm for unification grammars, in which unification is the sole operation responsible for attribute valuation.	Parsing As Deduction By looking at how parsing (breaking down sentences) and deduction (logical reasoning) are related, a new and broader way to understand chart parsing is found. This method includes parsing for grammar rules that use unification (making things agree), and it forms the basis of the Earley Deduction process for definite clauses (clear statements). The effectiveness of this method for a certain type of grammar is discussed. We expand on Earley deduction, so it can be used for both breaking down and creating sentences by changing a few settings. We show versions of Earley's method for unification grammars, where unification is the only action used to assign values to attributes (characteristics or details).	because
694	Features And Values The paper discusses the linguistic aspects of a new general purpose facility for computing with features. The program was developed in connection with the course I taught at the University of Texas in the fall of 1983. It is a generalized and expanded version of a system that Stuart Shieber originally designed for the PATR-II project at SRI in the spring of 1983 with later modifications by Fernando Pereira and me. Like its predecessors, the new Texas version of the "DG {directed graph}" package is primarily intended for representing morphological and syntactic information but it may turn out to be very useful for semantic representations too. We provide examples of feature structures in which a negation operator might be useful.	Features And Values The paper talks about the language-related parts of a new tool for working with features in computing. This program was created for a class I taught at the University of Texas in the fall of 1983. It is a bigger and improved version of a system that Stuart Shieber first made for the PATR-II project at SRI in the spring of 1983, with later updates by Fernando Pereira and me. Like the earlier versions, the new Texas version of the "DG {directed graph}" package is mainly for showing how words are formed and how sentences are structured, but it might also be very helpful for showing meanings too. We give examples of feature structures where a negation operator, which is used to reverse meanings, might be useful.	deduction
695	Functional Unification Grammar: A Formalism For Machine Translation Functional Unification Grammar provides an opportunity to encompass within one formalism and computational system the parts of machine translation systems that have usually been treated separately, natably analysis, transfer, and synthesis. Many of the advantages of this formalism come from the fact that it is monotonic allowing data structures to grow differently as different nondeterministic alternatives in a computation are pursued, but never to be modified in any way. A striking feature of this system is that it is fundamental reversible, allowing a to translate as b only if b could translate as a.	Functional Unification Grammar: A Formalism For Machine Translation Functional Unification Grammar allows us to include different parts of machine translation systems, like understanding the input, changing it to another language, and creating the output, all in one system instead of handling them separately. One of the benefits of this approach is that it is consistent, meaning data structures can expand in various ways as different possible solutions are explored, but they are never changed once created. An important characteristic of this system is its reversibility, meaning if it can translate from language a to language b, it can also translate back from b to a.	package
696	The Design Of A Computer Language For Linguistic Information A considerable body of accumulated knowledge about the design of languages for communicating information to computers has been derived from the subfields of programming language design and semantics. It has been the goal of the PART group at SRI to utilize a relevant portion of this knowledge in implementing tools to facilitate communication of linguistic information to computers. The PATR-II formalism is our current computer language for encoding linguistic information. This paper, a brief overview of that formalism, attempts to explicate our design decisions in terms of a set of properties that effective computer languages should incorporate. PATR-II is a minimal constraint-based formalism that extends context-free grammar.	The Design Of A Computer Language For Linguistic Information A large amount of knowledge about creating languages to talk to computers comes from areas like programming language design and meaning. The goal of the PART group at SRI is to use some of this knowledge to create tools that help communicate language information to computers. The PATR-II formalism is our current computer language for encoding linguistic information. This paper gives a short overview of that formalism and explains our design choices based on a set of features that good computer languages should have. PATR-II is a simple rule-based system that builds on basic grammar rules.	language
697	A Syntactic Approach To Discourse Semantics A correct structural analysis of a discourse is a prerequisite for understanding it. This paper sketches the outline of a discourse grammar which acknowledges several different levels of structure. This grammar, the "Dynamic Discourse Model", uses an Augmented Transition Network parsing mechanism to build a representation of the semantics of a discourse in a stepwise fashion, from left to right, on the basis of the semantic representations of the individual clauses which constitute the discourse. The intermediate states of the parser model the intermediate states of the social situation which generates the discourse. The paper attempts to demonstrate that a discourse may indeed be viewed as constructed by means of sequencing and recursive nesting of discourse constituents. It gives rather detailed examples of discourse structures at various levels, and shows how these structures are described in the framework proposed here.	A Syntactic Approach To Discourse Semantics A correct structural analysis of a discourse (a structured conversation or text) is necessary for understanding it. This paper outlines a discourse grammar that recognizes different levels of structure. This grammar, called the "Dynamic Discourse Model", uses a special method (Augmented Transition Network parsing) to gradually build a representation of the meaning (semantics) of a discourse, moving from left to right, based on the meanings of the individual parts (clauses) that make up the discourse. The intermediate stages of this method represent the intermediate stages of the social situation that creates the discourse. The paper tries to show that a discourse can be seen as built through sequences and repeated patterns (recursive nesting) of its parts (constituents). It provides detailed examples of discourse structures at different levels and explains how these structures are described in the proposed framework.	linguistic
698	Ontological Promiscuity To facilitate work in discourse interpretation, the logical form of English sentences should be both close to English and syntactically simple. In this paper I propose a logical notation which is first-order and nonintensional, and for which semantic translation can be naively compositional. The key move is to expand what kinds of entities one allows in one's ontology, rather than complicating the logical notation, the logical form of sentences, or the semantic translation process. Three classical problems - opaque adverbials, the distinction between de re and de ditto belief reports, and the problem of identity in intensional contexts - are examined for the difficulties they pose for this logical notation, and it is shown that the difficulties can be overcome. The paper closes with a statement about the view of semantics that is presupposed by this approach. We advocate for an ontologically promiscuous representation that includes a wide variety of types of entities. We argue that semantic representations for natural language need not be higher-order in that ontological promiscuity can solve the problem.	Ontological Promiscuity To make it easier to understand how sentences work, the way we logically represent English sentences should be simple and similar to English. In this paper, I suggest a simple way to write logic that uses basic rules and isn't overly complicated, and where understanding meanings can be straightforward. The main idea is to allow more types of things in our way of thinking, instead of making the logic or sentence structure more complex. Three classic issues - tricky adverbs, types of belief statements, and identity problems in complex situations - are looked at to see how they challenge this logic method, and it's shown that these challenges can be solved. The paper ends by explaining the view of meaning that is assumed with this method. We support using a flexible approach that includes many different types of things. We suggest that understanding natural language doesn't need to be overly complex because this flexible approach can solve the problem.	represent
699	Some Computational Properties Of Tree Adjoining Grammers Tree Adjoining Grammar (TAG) is a formalism for natural language grammars. Some of the basic notions of TAG's were introduced in [Joshi, Levy, and Takakashi 1975] and by [Joshi, 1983]. A detailed investigation of the linguistic relevance of TAG's has been carried out in [Kroch and Joshi, 1985]. In this paper, we will describe some new results for TAG's, especially in the following areas: (1) parsing complexity of TAG's, (2) some closure results for TAG's, and (3) the relationship to Head grammars. We provide parsing algorithms for TAGs that could serve to parse the base formalism of a synchronous TAG. We present the first TAG parser in a CYK-like algorithm.	Some Computational Properties Of Tree Adjoining Grammers Tree Adjoining Grammar (TAG) is a system used to understand the structure of natural languages. Some of the basic ideas of TAGs were introduced in [Joshi, Levy, and Takakashi 1975] and by [Joshi, 1983]. A detailed study of how TAGs relate to language has been done in [Kroch and Joshi, 1985]. In this paper, we will explain some new findings for TAGs, especially in these areas: (1) how complex it is to analyze TAGs, (2) some results about how TAGs can be combined or changed, and (3) how TAGs relate to Head grammars. We provide methods to analyze TAGs that could be used for a basic form of a synchronized TAG. We present the first TAG analyzer using a method similar to CYK, which is a known algorithm for parsing.	flexible
700	Using Restriction To Extend Parsing Algorithms For Complex-Feature-Based Formalisms Grammar formalisms based on the encoding of grammatical information in complex-valued feature systems enjoy some currency both in linguistics and natural-language-processing research. Such formalisms can be thought of by analogy to context-free grammars as generalizing the notion of nonterminal symbol from a finite domain of atomic elements to a possibly infinite domain of directed graph structures of a certain sort. Unfortunately, in moving to an infinite nonterminal domain, standard methods of parsing may no longer be applicable to the formalism. Typically, the problem manifests itself as gross inefficiency or even nontermination of the algorithms. In this paper, we discuss a solution to the problem of extending parsing algorithms to formalisms with possibly infinite nonterminal domains, a solution based on a general technique we call restriction. As a particular example of such an extension, we present a complete, correct, terminating extension of Earley's algorithm that uses restriction to perform top-down filtering. Our implementation of this algorithm demonstrates the drastic elimination of chart edges that can be achieved by this technique. Finally, we describe further uses for the technique - including parsing other grammar formalisms, including definite-clause grammars; extending other parsing algorithms, including LR methods and syntactic preference modeling algorithms; and efficient indexing. We propose a modified version of the Earley-parser, using restricted top down prediction.	Using Restriction To Extend Parsing Algorithms For Complex-Feature-Based Formalisms Grammar systems that use complex details to describe grammar rules are popular in language studies and computer programs that deal with natural language. These systems can be compared to simple grammar systems, but instead of using a limited set of basic symbols, they use more complex and potentially endless structures like detailed graphs. Unfortunately, this complexity means standard methods for analyzing sentences (parsing) might not work because they become very slow or even get stuck. In this paper, we talk about how to improve parsing methods for these complex systems, using a method we call restriction to solve the problem. As an example, we show how to improve a well-known parsing method, Earley's algorithm, to work correctly and efficiently by using restriction to narrow down possibilities from the start. Our version of the algorithm proves to cut down unnecessary processes significantly. Lastly, we explain how this method can also be used for other grammar systems and parsing methods, like LR methods (another parsing approach) and models that predict sentence structure. We suggest a new version of the Earley-parser that uses this focused prediction from the start.	synchronized
701	Recovering Implicit Information This paper describes the SDC PUNDIT, (Prolog UNDerstands Integrated Text), system for processing natural language messages. PUNDIT, written in Prolog, is a highly modular system consisting of distinct syntactic, semantic and pragmatics components. Each component draws on one or more sets of data, including a lexicon, a broad-coverage grammar of English, semantic verb decompositions, rules mapping between syntactic and semantic constituents, and a domain model. This paper discusses the communication between the syntactic, semantic and pragmatic modules that is necessary for making implicit linguistic information explicit. The key is letting syntax and semantics recognize missing linguistic entities as implicit entities, so that they can be labelled as such, and referenee resolution can be directed to find specific referents for the entities. In this way the task of making implicit linguistic information explicit becomes a subset of the tasks performed by reference resolution. The success of this approach is dependent on marking missing syntactic constituents as elided and missing semantic roles as ESSENTIAL so that reference resolution can know when to look for referents. We propose the first attempt for the automatic annotation of implicit semantic roles. We make one of the earliest attempts to automatically recover extra-sentential arguments.	Recovering Implicit Information This paper describes the SDC PUNDIT, (Prolog UNDerstands Integrated Text), system for dealing with natural language messages. PUNDIT, written in Prolog, is a flexible system with separate parts for syntax (sentence structure), semantics (meaning), and pragmatics (context). Each part uses different data sets, such as a dictionary, a comprehensive English grammar, ways to break down verbs, rules for connecting sentence structure to meaning, and a model of the subject area. This paper talks about how these parts communicate to make hidden language information clear. The main idea is to let the system recognize missing parts of language as hidden, so they can be identified, and a process called reference resolution can find what these parts refer to. This makes the job of revealing hidden language information part of the reference resolution tasks. This method works well if missing parts of a sentence are marked as omitted and missing roles are marked as ESSENTIAL, helping reference resolution know when to find what they point to. We suggest a first try at automatically marking hidden roles in meaning. We also make an early attempt to automatically find arguments that are not in the sentence.	symbols
702	A Property-Sharing Constraint In Centering A constraint is proposed in the Centering approach to pronoun resolution in discourse. This "property-sharing" constraint requires that two pronominal expressions that retain the same Cb across adjacent utterances share a certain common grammatical property. This property is expressed along the dimension of the grammatical function SUBJECT for both Japanese and English discourses, where different pronominal forms are primarily used to realize the Cb. It is the zero pronominal in Japanese, and the (unstressed) overt pronoun in English. The resulting constraint complements the original Centering, accounting for its apparent violations and providing a solution to the interpretation of multi-pronominal utterances. It also provides an alternative account of anaphora interpretation that appears to be due to structural parallelism. This reconciliation of centering/focusing and parallelism is a major advantage. I will then add another dimension called the "speaker identification" to the constraint to handle a group of special eases in Japanese discourse. It indicates a close association between centering and the speaker's viewpoint, and sheds light on what underlies the effect of perception reports on pronoun resolution in general. These results, by drawing on facts in two very different languages, demonstrate the cross-linguistic applicability of the centering framework.	A Property-Sharing Constraint In Centering A rule is suggested in the Centering method to figure out which pronoun refers to what in a conversation. This "property-sharing" rule says that two pronouns that keep the same topic (called Cb) in nearby sentences must share a specific grammatical feature. This feature is about being the SUBJECT in both Japanese and English conversations, where different pronouns are used to represent the Cb. In Japanese, it's an invisible pronoun, and in English, it's a clear but not emphasized pronoun. This new rule helps fix issues in the original Centering by explaining where it seems wrong and helps understand sentences with many pronouns. It also offers another way to explain how pronouns are understood when sentences look similar in structure. This shows a big benefit of combining centering (focusing on a topic) and parallelism (similar sentence structure). I will then add a new feature called "speaker identification" to this rule to deal with special cases in Japanese conversations. It shows a strong link between centering and the speaker's point of view, and it helps explain how people's reports of what they see affect understanding pronouns. These findings, by using examples from two very different languages, show that the centering idea can work across different languages.	process
703	Characterizing Structural Descriptions Produced By Various Grammatical Formalisms We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate. In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties of their derivation trees. We find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammar. On the basis of this observation, we describe a class of formalisms which we call Linear Context-Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages. We introduce Linear context-free rewriting system (LCFRS), which is a mildly context-sensitive formalism that allows the derivation of tuples of strings, i.e., discontinuous phrases.	Characterizing Structural Descriptions Produced By Various Grammatical Formalisms We look at the structures created by different grammar rules, focusing on how complicated the paths are and how these paths relate to each other in the structures each system makes. When comparing these grammar rules, we find it helpful to ignore the specific details and instead look at the general way they create structures, shown by their tree-like diagrams. We notice that many of these grammar rules are quite similar because they create tree structures just like those made by simple grammar rules known as Context-Free Grammar. Based on this, we talk about a group of grammar rules called Linear Context-Free Rewriting Systems, which can be identified quickly and only produce simple languages. We introduce Linear Context-Free Rewriting System (LCFRS), a type of grammar rule that allows the creation of groups of separate strings, meaning it can handle phrases that are not continuous.	combining centering
704	A Centering Approach To Pronouns In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns. As described in [GJW86], the process of centering attention on entities in the discourse gives rise to the intersentential transitional states of continuing, retaining and shifting. We propose an extension to these states which handles some additional cases of multiple ambiguous pronouns. The algorithm has been implemented in an HPSG natural language system which serves as the interface to a database query application. Our centering algorithm extends the notion of centering transition relations, which hold across adjacent utterances, to differentiate types of shift. The most common classification of transitional states are predicted to be less and less coherent in the order of CONTINUE, RETAIN, SMOOTH-SHIFT, and ROUGH-SHIFT. The measure M.BFP uses a lexicographic ordering on 4-tuples to determine the transition state. Hard-core centering approaches only deal with the last sentence.	A Centering Approach To Pronouns In this paper, we explain a way to use the centering approach to understand and follow topics in conversations. We use this to create a step-by-step method to track conversation flow and link pronouns to the right nouns. According to [GJW86], focusing attention on things in a conversation creates states of continuing, retaining, and shifting between sentences. We suggest adding to these states to handle more cases where pronouns are unclear. The method is used in an HPSG (a type of language system) that helps interact with a database query tool. Our method expands on the idea of centering transition relations, which are connections between nearby sentences, to tell different types of shifts apart. The most common changes between states are expected to be less and less clear in the order of CONTINUE, RETAIN, SMOOTH-SHIFT, and ROUGH-SHIFT. The method M.BFP uses a specific way of ordering to figure out the change in state. Strict centering approaches only focus on the last sentence.	grammar
753	Efficient Normal-Form Parsing For Combinatory Categorial Grammar Under categorial grammars that have powerful rules like composition, a simple n-word sentence can have exponentially many parses. Generating all parses is inefficient and obscures whatever true semantic ambiguities are in the input. This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar, by means of an efficient, correct, and easy to implement normal-form parsing technique. The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses; that is, spurious ambiguity (as carefully defined) is shown to be both safely and completely eliminated. We provide a safe and complete parsing algorithm which can return non-NF derivations when necessary to preserve an interpretation if composition is bounded or the grammar is restricted in other ways.	Efficient Normal-Form Parsing For Combinatory Categorial Grammar Under categorial grammars that have powerful rules like composition, a simple n-word sentence can have exponentially many parses (many ways to break down and understand the sentence). Generating all parses is inefficient (takes too much time and effort) and obscures (hides) whatever true semantic ambiguities (real meaning differences) are in the input. This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar, by means of an efficient (quick and effective), correct, and easy to implement normal-form parsing technique (a method to break down sentences into a standard form). The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses; that is, spurious ambiguity (unnecessary confusion) is shown to be both safely and completely eliminated. We provide a safe and complete parsing algorithm (a step-by-step method) which can return non-NF derivations (non-standard breakdowns) when necessary to preserve an interpretation if composition is bounded (limited) or the grammar is restricted in other ways.	written words
705	A Unification Method For Disjunctive Feature Descriptions Although disjunction has been used in several unification-based grammar formalisms, existing methods of unification have been unsatisfactory for descriptions containing large quantities of disjunction, because they require exponential time. This paper describes a method of unification by successive approximation, resulting in better average performance. The general problem of unifying two disjunctive feature structures is non-polynomial in the number of disjunctions. We present a technique which, for every set of n conjoined disjunctions, checks the consistency first of single disjuncts against the definite part of the description, then that of pairs and so on up ton-tuples for full consistency.	A Unification Method For Disjunctive Feature Descriptions Although using "or" choices, known as disjunction, has been applied in several grammar systems that rely on combining parts, current methods have not worked well for descriptions with many "or" choices because they take an extremely long time to process. This paper explains a method of combining by making gradual improvements, leading to better average results. The overall problem of combining two structures with many "or" choices is very complex and grows rapidly with the number of choices. We introduce a method that, for every set of n connected "or" choices, first checks if single options fit with the definite part of the description, then checks pairs of options, and continues up to grouped sets of options to ensure everything fits together.	follow topics
706	Interpretation As Abduction An approach to abductive inference developed in the TACITUS project has resulted in a dramatic simplification of how the problem of interpreting texts is conceptualized. Its use in solving the local pragmatics problems of reference, compound nominals, syntactic ambiguity, and metonymy is described and illustrated. It also suggests an elegant and thorough integration of syntax, semantics, and pragmatics.	Interpretation As Abduction An approach to making educated guesses (abductive inference) developed in the TACITUS project has greatly simplified how we think about understanding texts. This method helps solve local language understanding issues like figuring out what words refer to, understanding complex noun phrases, resolving confusion in sentence structure, and interpreting indirect expressions (metonymy). It also offers a smooth and complete combination of sentence structure (syntax), word meanings (semantics), and language use (pragmatics).	Descriptions
707	Cues And Control In Expert-Client Dialogues We conducted an empirical analysis into the relation between control and discourse structure. We applied control criteria to four dialognes and identified 3 levels of discourse structure. We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control. Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not. We define initiative as being held by the speaker who is driving the conversation at any point in the conversation. We propose rules for tracking initiative based on utterance types: for example, statements, proposals and questions show initiative while answers and acknowledgements do not.	Cues And Control In Expert-Client Dialogues We did a study to look at how control is related to the way conversations are structured. We used criteria for control on four conversations and found 3 levels of structure in these talks. We looked at how control changes between these structures and discovered that the type of sentence used, not just specific words, indicated shifts in control. People used certain signals when conversation goals were going well but interrupted when they weren't. We say that initiative is with the person who is leading the conversation at any given moment. We suggest rules for keeping track of who has initiative based on the types of sentences: for example, making statements, offering proposals, and asking questions show initiative, while giving answers and acknowledging do not.	confusion
708	Planning Coherent Multisentential Text Though most text generators are capable of simply stringing together more than one sentence, they cannot determine which order will ensure a coherent paragraph. A paragraph is coherent when the information in successive sentences follows some pattern of inference or of knowledge with which the hearer is familiar. To signal such inferences, speakers usually use relations that link successive sentences in fixed ways. A set of 20 relations that span most of what people usually say in English is proposed in the Rhetorical Structure Theory of Mann and Thompson. This paper describes the formalization of these relations and their use in a prototype text planner that structures input elements into coherent paragraphs. We use plan-operators in order to create coherent stretches of text.	Planning Coherent Multisentential Text Though most text generators can put together more than one sentence, they can't figure out the best order to make a clear and logical paragraph. A paragraph is clear when the information in each sentence makes sense and is easy to follow for the reader. To make these connections, speakers often use certain ways to link sentences together. Mann and Thompson's Rhetorical Structure Theory suggests 20 common ways people usually connect ideas in English. This paper explains how to formalize these connections and use them in a test version of a text planner that organizes information into clear paragraphs. We use planning steps to create clear and logical text.	structure
709	A Semantic-Head-Driven Generation Algorithm For Unification-Based Formalisms We present an algorithm for generating strings from logical form encodings that improves upon previous algorithms in that it places fewer restrictions on the class of grammars to which it is applicable. In particular, unlike an Earley deduction generator (Shieber, 1988), it allows use of semantically nonmonotonic grammars, yet unlike topdown methods, it also permits left-recursion. The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion. We point out a termination problem in the left-recursive rules.	A Semantic-Head-Driven Generation Algorithm For Unification-Based Formalisms We present a method for creating strings (sequences of characters) from logical form encodings that is better than previous methods because it works with more types of grammar rules. Unlike an earlier method called Earley deduction generator, it can use grammars where meanings can change, and unlike top-down methods, it can also handle rules that refer back to themselves (left-recursion). The key design feature of this method is that it goes through the analysis tree (a structure showing how a sentence is understood) in a way that focuses on the main idea or "head" of the meaning. We mention a problem with ending the process when rules refer back to themselves.	paper explains
710	Cooking Up Referring Expressions This paper describes the referring expression generation mechanisms used in EPICURE, a computer program which produces natural language descriptions of cookery recipes. Major features of the system include: an underlying ontology which permits the representation of non-singular entities; a notion of discriminatory power, to determine what properties should be used in a description; and a PATR-like unification grammar to produce surface linguistic strings. We produce a description entailing the minimal number of attributes possible at the price of suffering NP-hard complexity. Our algorithm attempts to build a minimal distinguishing description by always selecting the most discriminatory property available. We define minimality as the proportion of descriptions produced by a system that are maximally brief. We propose a solution to the problem of generating definite descriptions that evoke a discourse entity already introduced in the context.	Cooking Up Referring Expressions This paper explains how the EPICURE computer program creates descriptions in natural language for cooking recipes. Key features of the system include: a basic framework (ontology) that allows for representing groups of things; a method to decide which details to use in a description based on their ability to distinguish; and a grammar system similar to PATR for creating sentences. We aim to create descriptions using the fewest details necessary, even though this is a complex task. Our method tries to make a simple unique description by picking the most distinguishing feature each time. We define "minimal" as how often the system makes very short descriptions. We offer a way to make sure descriptions bring up something already mentioned in a conversation.	methods
711	Word Association Norms Mutual Information And Lexicography The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word "nurse" if it follows a highly associated word such as "doctor"). We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable). The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words.	Word Association Norms Mutual Information And Lexicography The term word association is used in a very particular sense in the psycholinguistic literature. (Basically, people react faster to the word "nurse" if it comes after a word like "doctor" that is closely related). We will expand the term to form a statistical way of describing various interesting language features, ranging from meaning connections like doctor/nurse (both are main words) to rules about which verbs go with which prepositions (main word/helper word). This paper will suggest a new way to measure word connections using the idea of mutual information, to estimate word association patterns from large collections of text that computers can read. (The usual method of finding word association patterns by testing many people with a few words is expensive and not very reliable). The suggested measure, called the association ratio, calculates word association patterns directly from large text collections, allowing us to estimate patterns for tens of thousands of words.	creating
712	Evaluating Discourse Processing Algorithms In order to take steps towards establishing a methodology for evaluating Natural Language systems, we conducted a case study. We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues. We present the quantitative results of hand-simulating these algorithms, but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general. We illustrate the general difficulties encoun- tered with quantitative evaluation. These are problems with: (a) allowing for underlying assumptions, (b) determining how to handle underspecifications, and (c) evaluating the contribution of false positives and error chaining. We refer to error chaining as the case in which a pronoun x correctly says that it is coreferent with another pronoun y while the program misidentifies the antecedent of y.	Evaluating Discourse Processing Algorithms In order to develop a method for evaluating Natural Language systems, we did a detailed study. We tried to assess two different methods for understanding anaphors, which are words like pronouns that refer back to something mentioned earlier, by comparing how accurate and thorough two known algorithms are at identifying what pronouns refer to in real texts and conversations. We show the numerical results of manually testing these algorithms, but this analysis also leads to a quality assessment and suggestions for doing such evaluations in general. We show common challenges faced with numerical evaluation. These issues include: (a) accounting for basic assumptions, (b) deciding how to deal with incomplete specifications, and (c) assessing the effect of incorrect matches and mistakes that compound. We describe error chaining as a situation where a pronoun x correctly links to another pronoun y, but the system incorrectly identifies what y refers to.	patterns
713	Structural Disambiguation With Constraint Propagation We present a new grammatical formalism called Constraint Dependency Grammar (CDG) in which every grammatical rule is given as a constraint on word-to-word modifications. CDG parsing is formalized as a constraint satisfaction problem over a finite domain so that efficient constraint-propagation algorithms can be employed to reduce structural ambiguity without generating individual parse trees. The weak generative capacity and the computational complexity of CDG parsing are also discussed. Our constraint dependency grammar maps to the notation of constratin satisfaction techniques.	Structural Disambiguation With Constraint Propagation We introduce a new way of understanding grammar called Constraint Dependency Grammar (CDG), where each grammar rule is a restriction on how words change each other. CDG parsing is set up as a problem of meeting constraints within a limited set, allowing for the use of effective methods to clarify sentence structure without creating separate sentence diagrams. We also talk about how much language CDG parsing can handle and how complex it is to compute. Our constraint dependency grammar connects to the methods used in solving constraint problems.	correctly links
714	Mixed Initiative In Dialogue: An Investigation Into Discourse Segmentation Conversation between two people is usually of MIXED-INITIATIVE, with CONTROL over the conversation being transferred from one person to another. We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns. The application of the control rules lets us derive domain-independent discourse structures. The derived structures indicate that initiative plays a role in the structuring of discourse. In order to explore the relationship of control and initiative to discourse processes like centering, we analyze the distribution of four different classes of anaphora for two data sets. This distribution indicates that some control segments are hierarchically related to others. The analysis suggests that discourse participants often mutually agree to a change of topic. We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types. These differences can be explained in terms of collaborative planning principles. We find that as initiative passes back and fourth between discourse participants, control over the conversation similarly transfers from one speaker to another. We develope utterance-based rules for allocation of control.	Mixed Initiative In Dialogue: An Investigation Into Discourse Segmentation Conversation between two people often involves both taking turns to lead, with the lead changing from one person to another. We use a set of guidelines for how to change the lead in 4 sets of conversations with a total of 1862 exchanges. Using these guidelines, we can identify general conversation patterns. These patterns show that taking the lead helps shape the conversation. To understand how taking the lead affects conversation processes like focusing on a topic, we look at how four types of referring back to something in the conversation are spread out in two sets of data. This spread shows that some parts of the conversation are more important than others. The study shows that people in a conversation often agree together on when to change the subject. We also looked at conversations aimed at completing tasks and those giving advice, finding that the way people take and give up the lead is very different in these types. These differences can be explained by how people work together in planning. We find that as people take turns leading, control over the conversation also shifts between speakers. We create rules based on what is said to decide who leads.	effective
715	Automatically Extracting And Representing Collocations For Language Generation Collocational knowledge is necessary for language generation. The problem is that collocations come in a large variety of forms. They can involve two, three or more words, these words can be of different syntactic categories and they can be involved in more or less rigid ways. This leads to two main difficulties: collocational knowledge has to be acquired and it must be represented flexibly so that it can be used for language generation. We address both problems in this paper, focusing on the acquisition problem. We describe a program, Xtract, that automatically acquires a range of collocations from large textual corpora and we describe how they can be represented in a flexible lexicon using a unification based formalism.	Automatically Extracting And Representing Collocations For Language Generation Collocational knowledge, which means knowing how words naturally go together, is important for creating language. The challenge is that these word combinations, called collocations, come in many different forms. They can have two, three, or more words, and these words can belong to different parts of speech, like nouns, verbs, or adjectives, and can be connected in different ways. This causes two main problems: first, we need to learn about these word combinations, and second, we need to keep this information in a way that is flexible and useful for creating language. In this paper, we tackle both issues, especially focusing on how to learn about these word combinations. We talk about a program called Xtract, which automatically finds different word combinations from large collections of text, and we explain how these can be stored in a flexible dictionary using a method that allows them to be easily combined.	between
716	Noun Classification From Predicate-Argument Structures A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described. The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification. We make use of a mutual-information based metric to determine noun similarity. We use thenotion of distributional similarity, i.e., two words with similar meanings will be used in similar contexts.	Noun Classification From Predicate-Argument Structures A method to find out how similar nouns are by measuring how they appear with subjects, verbs, and objects in a large collection of texts is explained. This way of grouping nouns by meaning shows that words used in similar ways tend to have similar meanings, which could be useful for tasks like automatic sorting, understanding compound words, and figuring out how words change in meaning. We use a method called mutual-information, which is a way to measure how related nouns are. We use the idea that words with similar meanings will appear in similar situations.	combined
717	Two Languages Are More Informative Than One This paper presents a new approach for resolving lexical ambiguities in one language using statistical data on lexical relations in another language. This approach exploits the differences between mappings of words to senses in different languages. We concentrate on the problem of target word selection in machine translation, for which the approach is directly applicable, and employ a statistical model for the selection mechanism. The model was evaluated using two sets of Hebrew and German examples and was found to be very useful for disambiguation.	Two Languages Are More Informative Than One This paper introduces a new method for solving word meaning confusion (lexical ambiguities) in one language by using statistical information about word relationships (lexical relations) in another language. This method takes advantage of how words are connected to meanings differently in various languages. We focus on choosing the right word in translation (machine translation), where this method can be used directly, and use a statistical model to help choose the correct word. The model was tested with examples from Hebrew and German and was found to be very helpful for clarifying meanings (disambiguation).	Predicate
718	Aligning Sentences In Parallel Corpora In this paper we describe a statistical technique for aligning sentences with their translations in two parallel corpora. In addition to certain anchor points that are available in our data, the only information about the sentences that we use for calculating alignments is the number of tokens that they contain. Because we make no use of the lexical details of the sentence, the alignment computation is fast and therefore practical for application to very large collections of text. We have used this technique to align several million sentences in the English-French Hansard corpora and have achieved an accuracy in excess of 99% in a random selected set of 1000 sentence pairs that we checked by hand. We show that even without the benefit of anchor points the correlation between the lengths of aligned sentences is strong enough that we should expect to achieve an accuracy of between 96% and 97%. Thus, the technique may be applicable to a wider variety of texts than we have yet tried. We are able to achieve these results while completely ignoring the lexical content of the tests.	Aligning Sentences In Parallel Corpora In this paper we describe a method that uses statistics to match sentences with their translations in two sets of related texts. Besides certain key points in our data, the only thing we use to match sentences is the number of words they have. Because we don't use the specific words in the sentence, the process is quick and practical for very large text collections. We used this method to match several million sentences in the English-French Hansard texts, achieving over 99% accuracy in a random set of 1000 sentence pairs we checked manually. We show that even without key points, the match between the lengths of sentences is strong enough to expect 96% to 97% accuracy. Thus, this method can potentially be used for more types of texts than we have tested. We achieve these results while completely ignoring the actual words in the sentences.	connected
719	A Program For Aligning Sentences In Bilingual Corpora Researchers in both machine Iranslation (e.g., Brown et al., 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann, 1990) have recently become interested in studying parallel texts, texts such as the Canadian Hansards (parliamentary proceedings) which are available in multiple languages (French and English). This paper describes a method for aligning sentences in these parallel texts, based on a simple statistical model of character lengths. The method was developed and tested on a small trilingual sample of Swiss economic reports. A much larger sample of 90 million words of Canadian Hansards has been aligned and donated to the ACL/DCI. We extract pairs of anchor words such as numbers, proper nouns (organiziation, person, title), dates and monetary information. We find that the byte length ratio of target sentence to source sentence is normally distributed. We demonstrate the effectiveness of a global alignment dynamic program algorithm where the basic similarity score is based on the difference in sentence lengths, measured in characters.	A Program For Aligning Sentences In Bilingual Corpora Researchers in both machine translation (the process of using computers to translate text between languages) and bilingual lexicography (the study of dictionaries in two languages) have recently become interested in studying parallel texts, which are documents like the Canadian Hansards (records of parliamentary meetings) available in multiple languages (French and English). This paper describes a method for matching sentences in these parallel texts, based on a simple statistical model of character lengths. The method was developed and tested on a small three-language sample of Swiss economic reports. A much larger sample of 90 million words from the Canadian Hansards has been matched and given to the ACL/DCI. We pull out pairs of key words such as numbers, proper nouns (like names of organizations, people, titles), dates, and money information. We find that the ratio of sentence lengths in bytes between the target and source sentences follows a normal distribution (a common pattern in statistics). We show how well a global alignment dynamic program algorithm works, where the basic similarity score is based on the difference in sentence lengths, measured in characters.	manually
754	A Polynomial-Time Algorithm For Statistical Machine Translation We introduce a polynomial-time algorithm for statistical machine translation. This algorithm can be used in place of the expensive, slow best-first search strategies in current statistical translation architectures. The approach employs the stochastic bracketing transduction grammar (SBTG) model we recently introduced to replace earlier word alignment channel models, while retaining a bigram language model. The new algorithm in our experience yields major speed improvement with no significant loss of accuracy. We test our algorithm on Chinese-English translation.	A Polynomial-Time Algorithm For Statistical Machine Translation We introduce a fast algorithm for statistical machine translation. This new method can replace the costly and slow search methods currently used in translation systems. It uses a model called stochastic bracketing transduction grammar (SBTG), which we recently introduced to improve upon older methods of matching words between languages, while still using a bigram language model that looks at pairs of words. In our experience, this new algorithm is much quicker and does not lose accuracy. We tested our algorithm on translating Chinese to English.	fairly general
720	Automatic Acquisition Of Subcategorization Frames From Untagged Text This paper describes an implemented program that takes a raw, untagged text corpus as its only input (no open-class dictionary) and generates a partial list of verbs occurring in the text and the subcategorization frames (SFs) in which they occur. Verbs are detected by a novel technique based on the Case Filter of Rouvret and Vergnaud (1980). The completeness of the output list increases monotonically with the total number of occurrences of each verb in the corpus. False positive rates are one to three percent of observations. Five SFs are currently detected and more are planned. Ultimately, I expect to provide a large SF dictionary to the NLP community and to train dictionaries for specific corpora.	Automatic Acquisition Of Subcategorization Frames From Untagged Text This paper talks about a computer program that takes plain text as input (without any special dictionary) and creates a partial list of verbs found in the text and the structures (subcategorization frames, or SFs) in which they appear. Verbs are found using a new method based on a linguistic rule called the Case Filter by Rouvret and Vergnaud (1980). The list gets more complete as the verbs appear more often in the text. Mistakes in identifying the verbs happen in one to three percent of the cases. Currently, five types of structures are identified, and more will be added. In the future, the goal is to make a large dictionary of these structures for the language processing community and to create dictionaries for specific groups of texts.	sentence
721	Structural Ambiguity And Lexical Relations We propose that ambiguous prepositional phrase attachment can be resolved on the basis of the relative strength of association of the preposition with noun and verb, estimated on the basis of word distribution in a large corpus. This work suggests that a distributional approach can be effective in resolving parsing problems that apparently call for complex reasoning. We find that human arbiters constitently reach a higher agreement in propositional phrase attachment when they are given the entire sentence rather than just the four words concerned.	Structural Ambiguity And Lexical Relations We suggest that unclear prepositional phrase placement can be figured out by how strongly the preposition connects with a noun or a verb, which we estimate by looking at how words are spread out in a large collection of texts. This study shows that using this distributional method can help solve sentence structure problems that seem to need complicated thinking. We discovered that people deciding on sentence structure agree more often when they see the whole sentence instead of just the four specific words involved.	Untagged
722	Word-Sense Disambiguation Using Statistical Methods We describe a statistical technique for assigning senses to words. An instance of a word is assigned a sense by asking a question about the context in which the word appears. The question is constructed to have high mutual information with the translation of that instance in another language. When we incorporated this method of assigning senses into our statistical machine translation system, the error rate of the system decreased by thirteen percent. We propose a word-sense disambiguation algorithm to disambiguate English translations of French target words based on the single most imformative context feature. We perform unsupervised word alignment and determine the most appropriate translation for a target word from a set of contextual features.	Word-Sense Disambiguation Using Statistical Methods We explain a statistical method for figuring out the meanings of words. A word is given a meaning by looking at the situation or context around it. We create a question that strongly relates to how that word translates into another language. When we used this method in our translation system, the number of mistakes went down by 13%. We suggest a way to clear up confusion about English translations of French words by focusing on the most helpful context clue. We match words without prior labeling and choose the best translation for a word by examining the situation around it.	complicated thinking
723	Monotonic Semantic Interpretation Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations. The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation. Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved. We make use of Quasi Logical Form, a monotonic representation for compositional semantics. A quasi logical form allows the under-specification of several types of information, such as anaphoric references, ellipsis and semantic relations.	Monotonic Semantic Interpretation Aspects of understanding meaning, like figuring out what parts of a sentence refer to and how they relate, often involve complex processes that can lose information and change the meaning in a non-linear way. The paper explains how these can be done in a straightforward way using an updated representation called Quasi Logical Form (QLF). The meanings for QLF are shown where the meanings of expressions grow consistently as QLF expressions are worked out. We use Quasi Logical Form, a straightforward way to represent how meanings are built up. A quasi logical form allows leaving some details open to interpretation, like references to earlier parts of the text, missing words, and connections in meaning.	context
724	Integrating Multiple Knowledge Sources For Detection And Correction Of Repairs In Human-Computer Dialog We have analyzed 607 sentences of spontaneous human-computer speech data containing repairs, drawn from a total corpus of 10,718 sentences. We present here criteria and techniques for automatically detecting the presence of a repair, its location, and making the appropriate correction. The criteria involve integration of knowledge from several sources: pattern matching, syntactic and semantic analysis, and acoustics. We are able to correctly identify 309 of 406 utterences containing nontrivial repairs, while 191 fluent utterances were incorrectly identified as containing repairs. We speculate that acoustic information might be used to filter out false positives for canditate matching. We find location of word fragments to be an invaluable cue to both the detection and correction of disfluencies.	Integrating Multiple Knowledge Sources For Detection And Correction Of Repairs In Human-Computer Dialog We have studied 607 sentences from real conversations between people and computers that had mistakes, from a larger set of 10,718 sentences. We show ways to automatically find where a mistake is and fix it. This involves using different kinds of knowledge: looking for patterns, checking grammar and meaning, and listening to sounds. We successfully found 309 out of 406 sentences with significant mistakes, but 191 smooth sentences were wrongly marked as having mistakes. We think sound information might help avoid wrongly marking sentences as having mistakes. We find that knowing where parts of words are cut off is very helpful in finding and fixing speech errors.	expressions
725	Inside-Outside Reestimation From Partially Bracketed Corpora The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information (constituent bracketing) in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modeling of hierarchical structure than the original one. In particular, over 90% test set bracketing accuracy was achieved for grammars inferred by our algorithm from a training set of hand-parsed part-of-speech strings for sentences in the Air Travel Information System spoken language corpus. Finally, the new algorithm has better time complexity than the original one when sufficient bracketing is provided. We adapt the inside-outside algorithm to apply over semi-supervised data extract from the Penn TreeBank.	Inside-Outside Reestimation From Partially Bracketed Corpora The inside-outside algorithm, which is a method for finding the rules of a random grammar system, is improved to use information about sentence parts (constituent bracketing) in a partially analyzed collection of text. Tests on both formal language and natural language collections show that the new method works faster and better at understanding sentence structure than the old one. Specifically, over 90% accuracy was achieved in identifying sentence parts for grammars learned by our method using a training set of sentences that were manually analyzed into parts of speech, from the Air Travel Information System spoken language collection. Finally, the improved method is quicker than the original when there is enough information about sentence parts. We modify the inside-outside algorithm to work with partly analyzed data taken from the Penn TreeBank, which is a large collection of text used for research.	knowing
726	Estimating Upper And Lower Bounds On The Performance Of Word-Sense Disambiguation Programs We have recently reported on two new word-sense disambiguation systems, one trained on bilingual material (the Canadian Hansards) and the other trained on monolingual material (Roget's Thesaurus and Grolier's Encyclopedia). After using both the monolingual and bilingual classifiers for a few months, we have convinced ourselves that the performance is remarkably good. Nevertheless, we would really like to be able to make a stronger statement, and therefore, we decided to try to develop some more objective evaluation measures. Although there has been a fair amount of literature on sense-disambiguation, the literature does not offer much guidance in how we might establish the success or failure of a proposed solution such as the two systems mentioned in the previous paragraph. Many papers avoid quantitative evaluations altogether, because it is so difficult to come up with credible estimates of performance. This paper will attempt to establish upper and lower bounds on the level of performance that can be expected in an evaluation. An estimate of the lower bound of 75% (averaged over ambiguous types) is obtained by measuring the performance produced by a baseline system that ignores context and simply assigns the most likely sense in all cases. An estimate of the upper bound is obtained by assuming that our ability to measure performance is largely limited by our ability obtain reliable judgments from human informants. Not surprisingly, the upper bound is very dependent on the instructions given to the judges. Jorgensen, for example, suspected that lexicographers tend to depend too much on judgments by a single informant and found considerable variation over judgments (only 68% agreement), as she had suspected. In our own experiments, we have set out to find word-sense disambiguation tasks where the judges can agree often enough so that we could show that they were outperforming the baseline system. Under quite different conditions, we have found 96.8% agreement over judges. We argue that any wide-coverage WSD program must be able to perform significantly better than the most-frequent-sense classifier to be worth of serious consideration.	Estimating Upper And Lower Bounds On The Performance Of Word-Sense Disambiguation Programs We have recently reported on two new word-sense disambiguation systems, one trained using materials in two languages (the Canadian Hansards) and the other trained using materials in one language (Roget's Thesaurus and Grolier's Encyclopedia). After using both the one-language and two-language classifiers for a few months, we are convinced that the performance is very good. However, we want to make a stronger claim, so we decided to develop some more objective ways to evaluate them. Although there is a fair amount of research on sense-disambiguation, it doesn't give much advice on how to determine if a proposed solution, like our two systems, is successful. Many papers avoid giving numerical evaluations because it is hard to come up with believable performance estimates. This paper will try to set upper and lower limits on the performance level expected in an evaluation. We estimate the lower limit of 75% (average for unclear types) by checking how well a basic system performs that ignores context and always picks the most common meaning. We estimate the upper limit by assuming our performance measurement is mostly limited by how well we can get consistent judgments from people. Not surprisingly, the upper limit depends a lot on the instructions given to the judges. Jorgensen, for example, thought that dictionary writers rely too much on one person's judgment and noticed a lot of differences in judgments (only 68% agreement), as she suspected. In our own experiments, we looked for word-sense disambiguation tasks where judges often agree so we could show they did better than the basic system. Under very different conditions, we found 96.8% agreement among judges. We argue that any WSD program that covers a wide range must perform much better than the most-common-meaning classifier to be seriously considered.	grammar
727	Char Align: A Program For Aligning Parallel Texts At The Character Level There have been a number of recent papers on aligning parallel texts at the sentence level, e.g., Brown et al (1991), Gale and Church (to appear), Isabelle (1992), Kay and R/Ssenschein (to appear), Simard et al (1992), Warwick-Armstrong and Russell (1990). On clean inputs, such as the Canadian Hansards, these methods have been very successful (at least 96% correct by sentence). Unfortunately, if the input is noisy (due to OCR and/or unknown markup conventions), then these methods tend to break down because the noise can make it difficult to find paragraph boundaries, let alone sentences. This paper describes a new program, charalign, that aligns texts at the character level rather than at the sentence/paragraph level, based on the cognate approach proposed by Simard et al. We show that cheap alignment of text segments is possible by exploiting orthographic cognates. Char_align is designed for language pairs that share a common alphabet.	Char Align: A Program For Aligning Parallel Texts At The Character Level There have been several recent studies on matching up parallel texts at the sentence level, for example, by Brown et al (1991), Gale and Church (upcoming), Isabelle (1992), Kay and R/Ssenschein (upcoming), Simard et al (1992), Warwick-Armstrong and Russell (1990). These methods work well on clean inputs, like the Canadian Hansards, achieving at least 96% accuracy in matching sentences. However, if the input has errors (due to OCR, which is optical character recognition, and/or unfamiliar formatting), these methods often fail because the errors make it hard to identify where paragraphs or sentences start and end. This paper introduces a new program called charalign, which matches texts at the character level instead of the sentence or paragraph level, using a method suggested by Simard et al. We demonstrate that aligning parts of the text can be done cheaply by using similar-looking words from both languages. Char_align is intended for languages that use the same alphabet.	measurement
728	Aligning Sentences In Bilingual Corpora Using Lexical Information In this paper, we describe a fast algorithm for aligning sentences with their translations in a bilingual corpus. Existing efficient algorithms ignore word identities and only consider sentence length (Brown el al., 1991b; Gale and Church, 1991). Our algorithm constructs a simple statistical word-to-word translation model on the fly during alignment. We find the alignment that maximizes the probability of generating the corpus with this translation model. We have achieved an error rate of approximately 0.4% on Canadian Hansard data, which is a significant improvement over previous results. The algorithm is language independent. We find that sentence length based methods suffer when the texts to be aligned contain small passages, or the languages involved share few cognates. We find that dynamic programming is particularly susceptible to deletions occurring in one of the two languages. We use manually aligned pairs of sentences to train our alignment models.	Aligning Sentences In Bilingual Corpora Using Lexical Information In this paper, we explain a quick method for matching sentences with their translations in a set of texts in two languages. Existing fast methods don't look at specific words and just consider sentence length. Our method builds a simple model that matches words between languages as it aligns sentences. We aim to find the best match that increases the chance of correctly creating the text with this model. We have reached a low error rate of about 0.4% on Canadian Hansard data, which is much better than before. The method works for any language. We notice that methods based only on sentence length struggle when the texts are short or when the languages don't have many similar words. We see that using a flexible method is likely to miss parts that are only in one language. We train our matching models using sentences that have been manually matched.	character
764	The Rhetorical Parsing Of Unrestricted Natural Language Texts We derive the rhetorical structures of texts by means of two new, surface-form-based algorithms: one that identifies discourse usages of cue phrases and breaks sentences into clauses, and one that produces valid rhetorical structure trees for unrestricted natural language texts. The algorithms use information that was derived from a corpus analysis of cue phrases. We describe a method for text summarization based on a nuclearity and selective retention of hierarchical fragments.	The Rhetorical Parsing Of Unrestricted Natural Language Texts We figure out the structure of texts using two new methods that look at the surface form of the text: one method finds how cue phrases (words or phrases signaling the structure of the text) are used in speech and breaks sentences into parts, and the other method creates valid structure maps for any natural language text. The methods use information gathered from studying a large collection of text to understand cue phrases. We explain a way to summarize text by focusing on the most important parts and keeping key pieces of the structure.	Sense Ambiguity
729	An Algorithm For Finding Noun Phrase Correspondences In Bilingual Corpora The paper describes an algorithm that employs English and French text taggers to associate noun phrases in an aligned bilingual corpus. The taggets provide part-of-speech categories which are used by finite-state recognizers to extract simple noun phrases for both languages. Noun phrases are then mapped to each other using an iterative re-estimation algorithm that bears similarities to the Baum-Welch algorithm which is used for training the taggers. The algorithm provides an alternative to other approaches for finding word correspondences, with the advantage that linguistic structure is incorporated. Improvements to the basic algorithm are described, which enable context to be accounted for when constructing the noun phrase mappings. We attempt to find noun phrase correspondence in parallel corpora using part-of-speech tagging and noun phrase recognition methods.	An Algorithm For Finding Noun Phrase Correspondences In Bilingual Corpora The paper describes a method that uses English and French text tools to link noun phrases in a bilingual text collection that is lined up. These tools give word type categories, which are used by simple pattern matchers to pick out basic noun phrases for both languages. Noun phrases are then connected to each other using a repeated updating method similar to the Baum-Welch method, which helps train the text tools. This method offers another way to find word connections, with the benefit of including language structure. Improvements to the basic method are explained, allowing for context to be considered when creating the noun phrase links. We try to find noun phrase connections in matched text collections using word type tagging and noun phrase finding techniques.	reached
730	Towards History-Based Grammars: Using Richer Models For Probabilistic Parsing We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.	Towards History-Based Grammars: Using Richer Models For Probabilistic Parsing We explain a model for understanding languages called HBG, which uses detailed language information to clear up confusion. HBG uses information about words, sentence structure, meaning, and how sentences are built, in a new way to make sentences clearer. We use a collection of sentences with marked structures, called a Treebank, along with decision trees to identify important parts of a sentence's structure that help figure out the correct way to understand it. This is different from the usual method of trying to adjust grammar rules based on language analysis to get the right sentence structure. In direct comparisons with one of the best existing models for understanding language called P-CFG, the HBG model performs much better, improving the accuracy from 60% to 75%, reducing mistakes by 37%.	including
731	GEMINI: A Natural Language System For Spoken-Language Understanding We report a syntactic and semantic coverage of 86% for the DARPA Airline reservation corpus. Gemini is an expressive formalism in which to write formal grammars. We present Gemini natural language parser/generator, which attempts to parse the speech recognition output.	GEMINI: A Natural Language System For Spoken-Language Understanding We report a syntactic and semantic coverage of 86% for the DARPA Airline reservation corpus. Gemini is a system that uses a special way of writing rules for language. We present Gemini natural language parser/generator, which tries to understand and create language from the speech recognition output.	reducing
732	Principle-Based Parsing Without Overgeneration Overgeneration is the main source of computational complexity in previous principle-based parsers. This paper presents a message passing algorithm for principle-based parsing that avoids the overgeneration problem. This algorithm has been implemented in C++ and successfully tested with example sentences from (van Riemsdijk and Williams, 1986). Our parser produces functional relations for the components in a sentence, including subject and object relations with respect to a verb. In our dependency trees nodes represent text expressions and edges represent the syntactic relations between them.	Principle-Based Parsing Without Overgeneration Overgeneration, or creating too many possibilities, is the main reason why earlier principle-based parsers, which are programs that analyze sentences based on rules, were complex to use. This paper introduces a message passing method, which is a way for parts of the program to communicate, to analyze sentences based on principles without creating too many options. This method was programmed in C++ and was tested successfully using sample sentences from a study by van Riemsdijk and Williams in 1986. Our parser, or sentence analyzer, identifies how parts of a sentence function together, like how the subject or object relates to the verb. In our structure diagrams, called dependency trees, points represent parts of the text and lines show the grammatical connections between them.	Language
733	Intention-Based Segmentation: Human Reliability And Correlation With Linguistic Cues Certain spans of utterances in a discourse, referred to here as segments, are widely assumed to form coherent units. Further, the segmental structure of discourse has been claimed to constrain and be constrained by many phenomena. However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them. We present quantitative results of a two part study using a corpus of spontaneous, narrative monologues. The first part evaluates the statistical reliability of human segmentation of our corpus, where speaker intention is the segmentation criterion. We then use the subjects' segmentations to evaluate the correlation of discourse segmentation with three linguistic cues (referential noun phrases, cue words, and pauses), using information retrieval metrics. We conflat multiple manual segmentations into one that contains only those boundaries which the majority of coders agree upon. We adopt a flat model of topic segmentation for our gold standard based on discourse segment purpose.	Intention-Based Segmentation: Human Reliability And Correlation With Linguistic Cues Certain parts of speech, called segments, in a conversation are believed to form meaningful groups. The way these segments are structured is thought to affect and be affected by many factors. However, there isn't a strong agreement on what exactly segments are or how to identify or create them. We show detailed results from a study using a collection of spontaneous, storytelling speeches. The first part checks how consistently people can identify segments in our collection, using the speaker's purpose as the guide. We then use the participants' segmentations to see how well dividing the conversation aligns with three language clues (naming words, signal words, and pauses), using methods to measure information. We combine different manual divisions into one, only keeping the breaks that most people agree on. We use a simple model of dividing topics for our best example, based on the purpose of the conversation segments.	earlier principle
863	A Statistical Approach To The Semantics Of Verb-Particles This paper describes a distributional approach to the semantics of verb-particle constructions (e.g. put up, make off). We report first on a framework for implementing and evaluating such models. We then go on to report on the implementation of some techniques for using statistical models acquired from corpus data to infer the meaning of verb-particle constructions.	A Statistical Approach To The Semantics Of Verb-Particles This paper explains a method that uses patterns in language to understand the meaning of verb-particle combinations (like "put up" or "make off"). We first explain a system for creating and testing these models. Next, we discuss how we use certain methods that apply statistics from large collections of texts to figure out what these verb-particle combinations mean.	ICTCLAS
734	Contextual Word Similarity And Estimation From Sparse Data In recent years there is much interest in word cooccurrence relations, such as n-grams, verb-object combinations, or cooccurrence within a limited context. This paper discusses how to estimate the probability of cooccurrences that do not occur in the training data. We present a method that makes local analogies between each specific unobserved cooccurrence and other cooccurrences that contain similar words, as determined by an appropriate word similarity metric. Our evaluation suggests that this method performs better than existing smoothing methods, and may provide an alternative to class based models. We argue that using a relatively small number of classes to model the similarity between words may lead to substantial loss of information. Clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time.	Contextual Word Similarity And Estimation From Sparse Data In recent years, there is a lot of interest in how often words appear together, like in phrases (n-grams), pairs like verb-object, or when they appear close to each other in a small text. This paper talks about how to guess the chance of word pairings that don't show up in the training data. We introduce a method that uses local comparisons between unseen word pairings and other pairings with similar words, using a suitable way to measure word similarity. Our evaluation shows that this method works better than current smoothing methods and might be a good alternative to class-based models. We claim that using only a few groups to model word similarity can lead to losing a lot of information. Groups of similar words are checked by how well they can identify data pieces that are temporarily removed from the input text collection one at a time.	consistently
735	Towards The Automatic Identification Of Adjectival Scales: Clustering Adjectives According To Meaning In this paper we present a method to group adjectives according to their meaning, as a first step towards the automatic identification of adjectival scales. We discuss the properties of adjectival scales and of groups of semantically related adjectives and how they imply sources of linguistic knowledge in text corpora. We describe how our system exploits this linguistic knowledge to compute a measure of similarity between two adjectives, using statistical techniques and without having access to any semantic information about the adjectives. We also show how a clustering algorithm can use these similarities to produce the groups of adjectives, and we present results produced by our system for a sample set of adjectives. We conclude by presenting evaluation methods for the task at hand, and analyzing the significance of the results obtained. We learn attributes by clustering adjectives that denote values of the same attribute.	Towards The Automatic Identification Of Adjectival Scales: Clustering Adjectives According To Meaning In this paper, we present a way to group adjectives based on their meaning, as a first step towards automatically recognizing scales of adjectives. We talk about the features of adjective scales and groups of adjectives that are related in meaning, and how they suggest sources of language knowledge in text collections. We explain how our system uses this language knowledge to figure out how similar two adjectives are, using statistical methods without needing any detailed meaning information about the adjectives. We also demonstrate how a clustering method can use these similarities to create groups of adjectives, and we show results from our system for a sample set of adjectives. We finish by showing methods to evaluate this task and examining the importance of the results we got. We learn features by grouping adjectives that show values of the same feature.	between
736	Distributional Clustering Of English Words We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical "soft" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data. We make use of deterministic annealing to cluster verb-argument pairs into classes of verbs and nouns.	Distributional Clustering Of English Words We describe and test a method for grouping words based on how they are used in certain sentence structures. Words are shown by how often they appear in different situations, and we use a method called relative entropy to measure how similar these word situations are for grouping them. Groups are shown by the average situations of the words in them, based on how likely the words are to be in the group. Often, these groups can show broad meanings of the words. We use a method called deterministic annealing, where initially stable groups can split into more detailed groups as the process goes on, creating a layered and flexible grouping of the words. These groups help create models to understand how words appear together, and we check how well these models work using separate test data. We also use deterministic annealing to group pairs of verbs and their related nouns into categories.	these similarities
737	Automatic Acquisition Of A Large Sub Categorization Dictionary From Corpora This paper presents a new method for producing a dictionary of subcategorization frames from unlabelled text corpora. It is shown that statistical filtering of the results of a finite state parser running on the output of a stochastic tagger produces high quality results, despite the error rates of the tagger and the parser. Further, it is argued that this method can be used to learn all subcategorization frames, whereas previous methods are not extensible to a general solution to the problem. We used the 4 million word corpus of the New York Times and selected only clauses with auxiliary verbs followed by automatically analyzing them with a finite-state parser.	Automatic Acquisition Of A Large Sub Categorization Dictionary From Corpora This paper introduces a new way to create a dictionary that shows how different verbs and nouns work together in sentences, using large collections of text that haven't been manually labeled. It demonstrates that by using a statistical method to clean and improve the results from a computer program that identifies sentence structures, we can get very good results, even if the program makes some mistakes. Additionally, it claims that this technique can help identify all types of verb and noun patterns, unlike older methods which can't solve the problem completely. We used a collection of 4 million words from the New York Times and focused on parts of sentences that have helping verbs, then analyzed them automatically with a program that identifies sentence structures.	broad meanings
738	Automatic Grammar Induction And Parsing Free Text: A Transformation-Based Approach In this paper we describe a new technique for parsing free text: a transformational grammar I is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled. The algorithm works by beginning in a very naive state of knowledge about phrase structure. By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce error. After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction. Our transformation-based tagging requires a handtagged text for training.	Automatic Grammar Induction And Parsing Free Text: A Transformation-Based Approach In this paper, we describe a new method for analyzing free text: a transformational grammar, which is a set of rules, is automatically learned to accurately break down text into binary-branching syntactic trees (a structure that shows how words group together), with the internal nodes (nonterminals) not labeled. The algorithm starts with very basic knowledge about sentence structure. By repeatedly comparing the current way of grouping words with the correct way given in the training data, the system learns a set of simple changes to improve accuracy. After explaining the algorithm, we show the results and compare them to other recent methods in automatic grammar learning. Our method requires a manually tagged text for training.	results
739	Text Segmentation Based On Similarity Between Words This paper proposes a new indicator of text structure, called the lexical cohesion profile (LCP), which locates segment boundaries in a text. A text segment is a coherent scene; the words in a segment are linked together via lexical cohesion relations. LCP records mutual similarity of words in a sequence of text. The similarity of words, which represents their cohesiveness, is computed using a semantic network. Comparison with the text segments marked by a number of subjects shows that LCP closely correlates with the human judgments. LCP may provide valuable information for resolving anaphora and ellipsis. We find that using a domain independent source of knowledge for text segmentation doesn't necessarily lead to better results than work that is based only on word distribution in texts.	Text Segmentation Based On Similarity Between Words This paper introduces a new way to understand text structure, called the lexical cohesion profile (LCP), which helps find where different parts of a text begin and end. A text segment is like a scene where words are connected to each other through relationships of meaning. LCP keeps track of how similar words are to each other in a stretch of text. The similarity of words, showing how well they go together, is calculated using a network that understands word meanings. When compared to parts of the text marked by different people, LCP matches well with what people think. LCP can be useful for dealing with language shortcuts like anaphora (referring back to something already mentioned) and ellipsis (leaving out parts of a sentence). We discovered that using general knowledge to divide text into parts doesn't always work better than only looking at how words are spread out in the text.	branching syntactic
740	Multi-Paragraph Segmentation Of Expository Text This paper describes TextTiling, an algorithm for partitioning expository texts into coherent multi-paragraph discourse units which reflect the subtopic structure of the texts. The algorithm uses domain-independent lexical frequency and distribution information to recognize the interactions of multiple simultaneous themes. Two fully-implemented versions of the algorithm are described and shown to produce segmentation that corresponds well to human judgments of the major subtopic boundaries of thirteen lengthy texts. We compute similarities between textual units based on the similarities of word space vectors. TextTiling is able to partition messages into multi-paragraph segments with an overall precision of 83% and recall of 78%.	Multi-Paragraph Segmentation Of Expository Text This paper describes TextTiling, a method for dividing informative texts into clear sections that show the smaller topics within the texts. The method uses general word frequency and spread to identify how different themes interact at the same time. Two complete versions of the method are explained and shown to divide texts in a way that matches well with human opinions on where the main topic changes are in thirteen long texts. We calculate likenesses between text parts based on how similar their word arrangements are. TextTiling can split messages into sections with more than one paragraph with an overall accuracy of 83% and an ability to find relevant sections of 78%.	shortcuts
741	Aligning A Parallel English-Chinese Corpus Statistically With Lexical Criteria We describe our experience with automatic alignment of sentences in parallel English-Chinese texts. Our report concerns three related topics: (1) progress on the HKUST English-Chinese Parallel Bilingual Corpus; (2) experiments addressing the applicability of Gale and Church's (1991) length-based statistical method to the task of alignment involving a non-Indo-European language; and (3) an improved statistical method that also incorporates domain-specific lexical cues. We find that the lengths of English and Chinese texts are not as highly correlated as in the French-English task, leading to lower success rates for length-based aligners.	Aligning A Parallel English-Chinese Corpus Statistically With Lexical Criteria We describe our experience with automatically matching sentences in English-Chinese texts. Our report covers three related topics: (1) progress on creating the HKUST English-Chinese Parallel Bilingual Corpus; (2) experiments testing if Gale and Church's (1991) method, which uses sentence length to align, works with a language that's not from the Indo-European family; and (3) a better method that uses specific words related to the topic to help with alignment. We find that English and Chinese texts don't match up in length as well as French and English do, leading to lower success rates for methods that rely on sentence length.	divide texts
742	Decision Lists For Lexical Ambiguity Resolution: Application To Accent Restoration In Spanish And French This paper presents a statistical decision procedure for lexical ambiguity resolution. The algorithm exploits both local syntactic patterns and more distant collocational evidence, generating an efficient, effective, and highly perspicuous recipe for resolving a given ambiguity. By identifying and utilizing only the single best disambiguating evidence in a target context, the algorithm avoids the problematic complex modeling of statistical dependencies. Although directly applicable to a wide class of ambiguities, the algorithm is described and evaluated in a realistic case study, the problem of restoring missing accents in Spanish and French text. Current accuracy exceeds 99% on the full task, and typically is over 90% for even the most difficult ambiguities. We note that lemmatization allows for more compact and generalizable data by clustering all inflected forms of an ambiguous word together. The strategy we adopt to model syntagmatic relations is to provide bigrams and trigrams of collocated words as features to descrive local contexts.	Decision Lists For Lexical Ambiguity Resolution: Application To Accent Restoration In Spanish And French This paper introduces a method using statistics for solving problems when words have more than one meaning. The method looks at nearby word patterns and also word combinations that are farther apart, creating a clear and effective way to decide which meaning is correct. It focuses on finding the best clue in the surrounding words to avoid complicated math models. While it can be used for many types of word meaning problems, it is specifically tested on the task of putting missing accent marks back in Spanish and French text. It works with over 99% accuracy for the whole task and over 90% even for the hardest problems. We note that grouping different forms of a word helps make the data easier to manage and apply. The approach we use to understand word relationships is to look at pairs and triplets of words that appear together to describe nearby settings.	matching
744	Word-Sense Disambiguation Using Decomposable Models Most probabilistic classifiers used for word-sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features. In this paper, a different approach to formulating a probabilistic model is presented along with a case study of the performance of models produced in this manner for the disambiguation of the noun interest. We describe a method for formulating probabilistic models that use multiple contextual features for word-sense disambiguation, without requiring untested assumptions regarding the form of the model. Using this approach, the joint distribution of all variables is described by only the most systematic variable interactions, thereby limiting the number of parameters to be estimated, supporting computational efficiency, and providing an understanding of the data. We manually assign 2,476 usages of interest with sense tags from the Longman Dictionary of Contemporary English.	Word-Sense Disambiguation Using Decomposable Models Most probabilistic classifiers used for word-sense disambiguation (figuring out the meaning of a word based on context) have either relied on just one nearby word or used a model that guesses how multiple nearby words relate to each other. In this paper, we introduce a new way to create a probabilistic model, along with a study showing how well these models work to clarify the meaning of the noun "interest." We explain a method for creating probabilistic models that use several nearby words to determine word meanings, without needing unverified guesses about how the model should look. With this method, we describe all the variable interactions in a simple way, which reduces the number of estimates needed, making it faster to compute and easier to understand the data. We manually tag 2,476 instances of the word "interest" with meanings from the Longman Dictionary of Contemporary English.	translation
745	Corpus Statistics Meet The Noun Compound: Some Empirical Results A variety of statistical methods for noun compound analysis are implemented and compared. The results support two main conclusions. First, the use of conceptual association not only enables a broad coverage, but also improves the accuracy. Second, an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents, even though the latter is more prevalent in the literature. We propose an unsupervised method for estimating the frequencies of the competing bracketings based on a taxonomy or a thesaurus. We test both adjacency and dependency models on 244 compounds extracted from Grolier's encyclopedia, a corpus of 8 million words.	Corpus Statistics Meet The Noun Compound: Some Empirical Results A variety of statistical methods for analyzing noun compounds (combinations of nouns) are used and compared. The results lead to two main conclusions. First, using conceptual association (connecting ideas) not only covers a wide range but also makes the analysis more accurate. Second, a model based on dependency grammar (how words depend on each other) is much more accurate than one based on deepest constituents (the main parts of a sentence), even though the latter is more common in studies. We suggest a method that doesn't need prior learning to estimate the frequencies of different groupings based on a classification system or a thesaurus (a book of words and their synonyms). We test both models that use closeness and dependency on 244 noun compounds taken from Grolier's encyclopedia, which has 8 million words.	meanings
746	D-Tree Grammars DTG are designed to share some of the advantages of TAG while overcoming some of its limitations. DTG involve two composition operations called subsertion and sister-adjunction. The most distinctive feature of DTG is that, unlike TAG, there is complete uniformity in the way that the two DTG operations relate lexical items: subsertion always corresponds to complementation and sister-adjunction to modification. Furthermore, DTG, unlike TAG, can provide a uniform analysis for em wh-movement in English and Kashmiri, despite the fact that the wh element in Kashmiri appears in sentence-second position, and not sentence-initial position as in English. In the quest of modeling dependency correctly, we expand weak generative capacity and thus end up with much greater parsing complexity.	D-Tree Grammars DTG are made to share some of the good points of TAG but fix some of its problems. DTG use two ways to put sentences together called subsertion and sister-adjunction. The main difference of DTG is that, unlike TAG, it treats the two operations in the same way with words: subsertion always means adding a complement and sister-adjunction means adding extra information. Also, DTG, unlike TAG, can explain wh-movement (movement of question words) in both English and Kashmiri, even though in Kashmiri the wh word comes second in the sentence, not first like in English. In trying to model how words depend on each other correctly, we increase the ability to generate sentences but end up with more complex sentence processing.	frequencies
747	Unsupervised Word Sense Disambiguation Rivaling Supervised Methods This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints - that words tend to have one sense per discourse and one sense per collocation - exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%. We introduce the idea of sense consistency and extend it to operator across related documents. We propose the self training, a semi-supervised algorithm which we apply do word sense disambiguation.	Unsupervised Word Sense Disambiguation Rivaling Supervised Methods This paper presents an unsupervised learning method for word sense disambiguation, which means figuring out the meaning of words. It can work as well as supervised methods, which usually need a lot of time to prepare by manually labeling data. This method uses two strong ideas: words usually have one meaning in a conversation and one meaning when combined with certain other words. These ideas are used in a step-by-step process that helps the method improve itself. The tested accuracy is over 96%. We introduce the idea of keeping word meanings consistent and apply it to related documents. We suggest self-training, a method that partly learns on its own, which we use for figuring out word meanings.	explain
770	A Memory-Based Approach to Learning Shallow Natural Language Patterns Recognizing shallow linguistic patterns, such as basic syntactic relationships between words, is a common task in applied natural language and text processing. The common practice for approaching this task is by tedious manual definition of possible pattern structures, often in the form of regular expressions or finite automata. This paper presents a novel memory-based learning method that recognizes shallow patterns in new text based on a bracketed training corpus. The training data are stored as-is, in efficient suffix-tree data structures. Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus. This way, no information in the training is lost, as can happen in other learning systems that construct a single generalized model at the time of training. The paper presents experimental results for recognizing noun phrase, subject-verb and verb-object patterns in English. Since the learning approach enables easy porting to new domains, we plan to apply it to syntactic patterns in other languages and to sub-language patterns for information extraction. We segment the POS sequence of a multi-word into small POS titles, count tile frequency in the new word and non-new-word on the training set respectively and detect new words using these counts.	A Memory-Based Approach to Learning Shallow Natural Language Patterns Recognizing simple language structures, like basic relationships between words, is a common task in language and text processing. Usually, this task requires manually defining possible pattern structures, often using regular expressions (patterns to search text) or finite automata (simple computing models). This paper introduces a new memory-based learning method that finds simple patterns in new text using a marked-up training set. The training data is stored unchanged in an efficient structure called a suffix tree. When recognizing patterns, comparisons are made between parts of the new text and examples (evidence) from the training data. This method ensures no information is lost from the training data, unlike other systems that create a single simplified model during training. The paper shows test results for finding simple structures like noun phrases, subject-verb, and verb-object patterns in English. Because this learning method can easily be used in new areas, we plan to apply it to language patterns in other languages and to patterns for extracting information. We break down the sequence of part-of-speech (POS) tags (grammatical markers) of a multi-word phrase into small POS units, count how often these units appear in both new and known words in the training data, and detect new words using these counts.	Translational Equivalence
748	Two-Level Many-Paths Generation Large-scale natural language generation requires the integration of vast mounts of knowledge: lexical, grammatical, and conceptual. A robust generator must be able to operate well even when pieces of knowledge are missing. It must also be robust against incomplete or inaccurate inputs. To attack these problems, we have built a hybrid generator, in which gaps in symbolic knowledge are filled by statistical methods. We describe algorithms and show experimental results. We also discuss how the hybrid generation model can be used to simplify current generators and enhance their portability, even when perfect knowledge is in principle obtainable. We use a sampling technique in which a set of sentences is sampled from a data structure created from an underlying grammar and ranked according to how well they meet the communicative goal.	Two-Level Many-Paths Generation Large-scale natural language generation, which is creating text automatically, needs a lot of knowledge: words and their meanings (lexical), rules for sentence structure (grammatical), and understanding of ideas (conceptual). A strong text generator should work well even when some knowledge is missing. It should also handle incomplete or wrong information effectively. To solve these issues, we have created a mixed-type generator that uses statistics to fill in missing knowledge. We explain how this works with algorithms and show test results. We also talk about how this model can make current text generators simpler and easier to use in different situations, even when complete knowledge is technically possible. We use a method where we pick sentences from a structured set based on grammar and rank them by how well they achieve the communication goal.	certain
749	Statistical Decision-Tree Models For Parsing Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to text-processing in general. In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result. This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing n-gram modeling techniques are inadequate for parsing models. In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser. Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATTER achieves 86% precision, 86% recall, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91% pre- cision, 90% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length. We create FTB-UC-DEP, a depenency tree bank derived from FTB-UC using the technique of head propagation rules. We find that lexicalization substantially improves performance compared to an unlexicalized baseline model such as a probabilistic context-free grammar.	Statistical Decision-Tree Models For Parsing Syntactic natural language parsers, which help computers understand sentences, struggle with complicated texts like the Wall Street Journal. They don't work well because they can't handle a lot of vocabulary and confusing grammar. That's why people are moving away from using them for text processing. In this paper, I talk about SPATTER, a new type of parser that uses decision-tree learning, which is a method to make decisions based on questions. It can understand every part of a sentence much better than other methods. This work is based on three ideas: (1) creating grammar rules for computers is too complicated to do by hand for interesting topics; (2) to understand sentences correctly, parsers need to pay close attention to the words and the context around them; and (3) current methods using n-gram, which predict words, are not good enough for parsers. In tests, SPATTER did much better than IBM's parser, which uses grammar rules. When checking SPATTER's performance with the Penn Treebank Wall Street Journal collection, using a method called PARSEVAL, SPATTER got 86% precision (correctness), 86% recall (finding all the right parts), and 1.3 mistakes per sentence for sentences with 40 words or less. For shorter sentences of 10 to 20 words, it got 91% precision, 90% recall, and 0.5 mistakes. We create FTB-UC-DEP, which is a collection of sentence structures derived from FTB-UC using a method called head propagation rules. We find that adding specific word information, known as lexicalization, greatly improves performance compared to a basic model that doesn't use this information, like a probabilistic context-free grammar.	method where
750	Identifying Word Translations In Non-Parallel Texts Common algorithms for sentence and word-alignment allow the automatic identification of word translations from parallel texts. This study suggests that the identification of word translations should also be possible with non-parallel and even unrelated texts. The method proposed is based on the assumption that there is a correlation between the patterns of word co-occurrences in texts of different languages. We propose a computationally demanding matrix purmutation method which maximizes a similarity between co-occurence matrices in two languages. An underlying assumption in our work is that translations of words that are related in one language are also related in the other language.	Identifying Word Translations In Non-Parallel Texts Common algorithms for sentence and word-alignment, which are methods for matching sentences and words, allow the automatic identification of word translations from parallel texts, which are texts in two languages that line up perfectly. This study suggests that identifying word translations should also be possible with non-parallel, meaning not perfectly aligned, and even unrelated texts. The method proposed is based on the assumption that there is a correlation, or connection, between the patterns of word co-occurrences, which means how often words appear together, in texts of different languages. We propose a computationally demanding, which means it requires a lot of computer power, matrix permutation method, which is a way to rearrange a grid of numbers, that maximizes a similarity between co-occurrence matrices, or charts showing word pairings, in two languages. An underlying assumption in our work is that translations of words that are related, or connected, in one language are also related in the other language.	interesting
751	Integrating Multiple Knowledge Sources To Disambiguate Word Sense: An Exemplar-Based Approach In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm. This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation. We tested our WSD program, named LEXAS, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed. LEXAS achieves a higher accuracy on the common data set, and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of WoRDNET. We obtain an overall accuracy for the noun interest of 87% and find that when our feature sets consists only of co-occurrence features the accuracy only drops to 80%. We conclude taht collocational information is more important than syntactic information to WSD. Our DSO corpus focuses on 191 frequent and polysemous words (nouns and verbs) and contains around 1,000 sentences per words.	Integrating Multiple Knowledge Sources To Disambiguate Word Sense: An Exemplar-Based Approach In this paper, we introduce a new method to make word meanings clearer, called word sense disambiguation (WSD), using a learning method that focuses on examples. This method combines different types of information to clarify word meanings, such as the type of nearby words (like nouns or verbs), word structure, the group of words around it, common word pairings, and the relationship between verbs and objects. We tested our WSD program, called LEXAS, on both a standard set of data used in past research and on a large collection of text where every word's meaning was labeled. LEXAS shows better accuracy on the standard data set and outperforms the most common method for guessing meanings on very confusing words in the large collection labeled with detailed meanings from WoRDNET. We achieve an overall accuracy of 87% for the noun "interest" and find that if we only use features that look at word pairings, the accuracy only falls to 80%. We conclude that common word pairings are more important than sentence structure for understanding word meanings. Our DSO collection focuses on 191 common words with multiple meanings (both nouns and verbs) and includes about 1,000 sentences for each word.	occurrence matrices
752	A Fully Statistical Approach To Natural Language Interfaces We present a natural language interface system which is based entirely on trained statistical models. The system consists of three stages of processing: parsing, semantic interpretation, and discourse. Each of these stages is modeled as a statistical process. The models are fully integrated, resulting in an end-to-end system that maps input utterances into meaning representation frames. Our approach is fully supervised and produces a final meaning representation in SQL. We compute the probability that a constituent such as Atlanta filled a semantic slot such as Destination in a semantic frame for air travel.	A Fully Statistical Approach To Natural Language Interfaces We present a natural language interface system which is based entirely on trained statistical models. The system consists of three stages of processing: parsing (analyzing sentence structure), semantic interpretation (understanding meaning), and discourse (context in conversation). Each of these stages is modeled as a statistical process (using math to predict outcomes). The models are fully integrated, resulting in an end-to-end system that maps input utterances (spoken or written words) into meaning representation frames. Our approach is fully supervised (trained with examples) and produces a final meaning representation in SQL (a language for managing data). We compute the probability that a constituent such as Atlanta filled a semantic slot such as Destination in a semantic frame for air travel (we calculate how likely it is that a word like Atlanta fits into a category like Destination for flight-related information).	standard
755	Parsing Algorithms And Metrics Many different metrics exist for evaluating parsing results, including Viterbi, Crossing Brackets Rate, Zero Crossing Brackets Rate, and several others. However, most parsing algorithms, including the Viterbi algorithm, attempt to optimize the same metric, namely the probability of getting the correct labelled tree. By choosing a parsing algorithm appropriate for the evaluation metric, better performance can be achieved. We present two new algorithms: the "Labelled Recall Algorithm," which maximizes the expected Labelled Recall Rate, and the "Bracketed Recall Algorithm," which maximizes the Bracketed Recall Rate. Experimental results are given, showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria, especially the ones that they optimize. We observe that the Viterbi parse is in general not the optimal parse for evaluation metrics such as f-score that are based on the number of correct constituents in a parse.	Parsing Algorithms And Metrics Many different ways exist for evaluating parsing results, including methods like Viterbi, Crossing Brackets Rate, Zero Crossing Brackets Rate, and several others. However, most parsing methods, including the Viterbi method, try to improve the same measure, which is the chance of getting the right labeled tree. By selecting a parsing method that fits the evaluation measure, better results can be achieved. We introduce two new methods: the "Labelled Recall Algorithm," which increases the expected Labelled Recall Rate, and the "Bracketed Recall Algorithm," which increases the Bracketed Recall Rate. Test results show that the two new methods perform better than the Viterbi method on many criteria, especially those they are designed to improve. We notice that the Viterbi parse is generally not the best parse for evaluation measures like f-score that are based on the number of correct parts in a parse.	recently
756	A New Statistical Parser Based On Bigram Lexical Dependencies This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy. We make use of a backed-off smoothing technique to alleviate sparse data problems.	A New Statistical Parser Based On Bigram Lexical Dependencies This paper describes a new statistical parser, which is a tool that analyzes sentences based on the likelihood of connections between main words in a sentence structure. Usual methods for estimating bigram probabilities, which deal with pairs of words, are expanded to figure out the likelihood of connections between word pairs. Tests using Wall Street Journal data show that this method works as well as SPATTER (Magerman 95; Jelinek et al. 94), which is known to have the best results for this type of tool. The approach is simple, allowing the tool to learn from 40,000 sentences in less than 15 minutes. By using a beam search strategy, which is a method to quickly find the best option, the speed at which sentences are analyzed can increase to over 200 per minute without much loss in accuracy. We use a backed-off smoothing technique, a method to handle problems when there is limited data.	selecting
757	Chart Generation Charts constitute a natural uniform architecture for parsing and generation provided string position is replaced by a notion more appropriate to logical forms and that measures are taken to curtail generation paths containing semantically incomplete phrases. We propose to reduce the number of constituents build during realisation by only considering for combination constituents with non overlapping semantics and compatible indices. We propose a chart based generation process which takes packed representations as input and generates all paraphrases without expanding first into disjunctive normal form.	Chart Generation Charts make a simple and consistent system for understanding and creating language if we replace tracking string position with a more logical approach and take steps to avoid creating paths with phrases that don't fully make sense. We suggest lessening the number of parts created by only combining parts that have meanings that don't overlap and have matching indices. We suggest a chart-based process that uses grouped data as input to create all different ways to say something without first breaking it down into a complicated standard form.	Statistical
758	A Prosodic Analysis Of Discourse Segments In Direction-Giving Monologues This paper reports on corpus-based research into the relationship between intonational variation and discourse structure. We examine the effects of speaking style (read versus spontaneous) and of discourse segmentation method (text-alone versus text-and-speech) on the nature of this relationship. We also compare the acoustic-prosodic features of initial, medial, and final utterances in a discourse segment. We find that speech is able to improve inter-annotator agreement in discourse segmentation of monologues. We introduce the Boston Directions Corpus, a publicly available speech corpora with manual ToBI annotations intended for experiments in automatic prosody labeling.	A Prosodic Analysis Of Discourse Segments In Direction-Giving Monologues This paper examines how changes in voice pitch and rhythm relate to the structure of speech. We look at how different speaking styles (reading from text versus speaking naturally) and methods of dividing the speech into parts (using only text versus using both text and speech) affect this relationship. We also compare the sound features of the beginning, middle, and end parts of speech segments. We discover that using speech helps people agree more when dividing speech into parts. We introduce the Boston Directions Corpus, a publicly available collection of recorded speech with detailed notes, meant for testing automatic voice pattern labeling.	grouped
792	Development And Use Of A Gold-Standard Data Set For Subjectivity Classifications This paper presents a case study of analyzing and improving intercoder reliability in discourse tagging using statistical techniques. Biascorrected tags are formulated and successfully used to guide a revision of the coding manual and develop an automatic classifier. We use a sentence-level Naive Bayes classifier using as features the presence or absence of particular syntactic classes (pronouns, adjectives, cardinal numbers, modal verbs, adverbs), punctuation and sentence position. We define a subjective sentences as sentences expressing evaluations, opinions, emotions and speculations.	Development And Use Of A Gold-Standard Data Set For Subjectivity Classifications This paper shows a study about how to make sure different people agree when labeling parts of text by using statistical methods. Corrected labels are created and used to update the guidelines for tagging and to create a program that can automatically label text. We use a simple computer program, called Naive Bayes classifier, that looks at sentences based on certain features like the presence or absence of certain word types (like pronouns, adjectives, number words, helping verbs, and adverbs), punctuation, and where the sentence appears. We describe subjective sentences as those that express personal thoughts, feelings, or guesses.	manually
759	An Empirical Study Of Smoothing Techniques For Language Modeling We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods. Our smoothing technique can smooth together the predictions of unigram, bigram, trigram and potentially higher n-gram sequences to obtain accurate probability estimates in the face of data sparsity.	An Empirical Study Of Smoothing Techniques For Language Modeling We present a detailed practical comparison of several smoothing methods in language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We explore for the first time how factors such as the size of training data, type of text collection (e.g., Brown versus Wall Street Journal), and n-gram order (pairs of words versus groups of three words) impact the performance of these methods, which we measure by checking the cross-entropy (a way to evaluate how well the model predicts) of test data. Additionally, we introduce two new smoothing techniques, one is a modified version of Jelinek-Mercer smoothing and the other is a very simple method that combines predictions linearly, both of which perform better than existing methods. Our smoothing technique can combine the predictions of single words, word pairs, word triples, and possibly longer sequences to get accurate probability estimates even when data is limited.	collection
760	Minimizing Manual Annotation Cost In Supervised Training From Corpora Corpus-based methods for natural language processing often use supervised training, requiring expensive manual annotation of training corpora. This paper investigates methods for reducing annotation cost by sample selection. In this approach, during training the learning program examines many unlabeled examples and selects for labeling (annotation) only those that are most informative at each stage. This avoids redundantly annotating examples that contribute little new information. This paper extends our previous work on committee-based sample selection for probabilistic classifiers. We describe a family of methods for committee-based sample selection, and report experimental results for the task of stochastic part-of-speech tagging. We find that all variants achieve a significant reduction in annotation cost, though their computational efficiency differs. In particular, the simplest method, which has no parameters to tune, gives excellent results. We also show that sample selection yields a significant reduction in the size of the model used by the tagger. We use HMMs for POS tagging and find that selective samplying of sentences can significantly reduce the number of samples required to achieve desirable tag accuracies. We use the vote entropy metric, the entropy of the distribution of labels assigned to an example by the ensemble of classifiers, to estimate the disagreement within an ensemble.	Minimizing Manual Annotation Cost In Supervised Training From Corpora Corpus-based methods for natural language processing often use supervised training, which needs costly manual labeling of training materials. This paper looks into ways to lower labeling costs by choosing specific examples to label. In this method, while training, the learning program reviews many unlabeled examples and chooses only the most useful ones for labeling at each stage. This stops unnecessary labeling of examples that don't add much new information. This paper builds on our earlier work on selecting samples using a group of models for probabilistic classifiers. We explain a group of methods for this type of sample selection and share test results for the task of randomly tagging parts of speech. We find that all methods significantly cut down on labeling costs, although they vary in speed. Notably, the simplest method, which doesn't need any adjustments, works very well. We also show that choosing samples lessens the amount of information the tagging model needs. We use Hidden Markov Models (HMMs) for part-of-speech tagging and find that picking certain sentences can greatly reduce the number of examples needed to reach desired tagging accuracy. We use the "vote entropy" measure, which is the uncertainty in the labels given to an example by a group of classifiers, to gauge the disagreement within the group.	model predicts
761	Three Generative Lexicalized Models For Statistical Parsing In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96). We provide a 29-million word parsed corpus from the Wall Street Journal.	Three Generative Lexicalized Models For Statistical Parsing In this paper, we first suggest a new way to analyze sentences using statistics, which involves a method to create sentences based on a type of grammar that includes words (lexicalized context-free grammar). We then improve this method to better handle certain sentence structures and question words (subcategorisation and wh-movement). Tests on Wall Street Journal articles show that our sentence analyzer correctly identifies parts of sentences with 88.1% accuracy and 87.5% recall, which is on average 2.3% better than an earlier model by Collins from 1996. We also offer a large collection of 29 million words from Wall Street Journal articles that have been analyzed using our method.	Corpora
762	Automatic Detection Of Text Genre As the text databases available to users become larger and more heterogeneous, genre becomes increasingly important for computational linguistics as a complement to topical and structural principles of classification. We propose a theory of genres as bundles of facets, which correlate with various surface cues, and argue that genre detection based on surface cues is as successful as detection based on deeper structural properties. We believe that parsing and word-sense disambiguation can also benefit from genre classification. We avoid structured markers since they require tagged or parsed text and replace them with character-level markers (e.g., punctuation mark counts) and derivative markers, i.e., ratios and variation measures derived from measure of lexical and character-level markers.	Automatic Detection Of Text Genre As the text collections available to users get larger and more varied, the type or style of text (genre) becomes more important for computer language studies, in addition to sorting by topic or structure. We suggest a theory that sees genres as groups of features that are linked with obvious clues in the text, and we claim that identifying genre using these clues is as effective as using complex structural traits. We think that analyzing sentence structure (parsing) and clarifying word meanings (word-sense disambiguation) can also improve with genre classification. We avoid using structured markers that need text to be tagged or analyzed and instead use simple signs like the number of punctuation marks or calculated markers like ratios and variation measures based on word and character signs.	earlier model
763	Using Syntactic Dependency As Local Context To Resolve Word Sense Ambiguity Most previous corpus-based algorithms disambiguate a word with a classifier trained from previous usages of the same word. Separate classifiers have to be trained for different words. We present an algorithm that uses the same knowledge sources to disambiguate different words. The algorithm does not require a sense-tagged corpus and exploits the fact that two different words are likely to have similar meanings if they occur in identical local contexts. We define the similarity between two objects to be the amount of information contained in the commonolity between the objects divided by the amount of information in the descriptions of the objects.	Using Syntactic Dependency As Local Context To Resolve Word Sense Ambiguity Most past methods that use large collections of text figure out the meaning of a word by training a program to recognize how the word was used before. Different programs need to be trained for each word. We introduce a method that uses the same information to figure out the meaning of different words. This method doesn't need a special collection of text where the meanings are already labeled. It works by noticing that two different words probably mean similar things if they appear in the same nearby words or sentences. We say two things are similar if the shared information between them is high compared to the information in their descriptions.	meanings
765	Machine Transliteration It is challenging to translate names and technical terms across languages with different alphabets and sound inventories. These items are commonly transliterated, i.e., replaced with approximate phonetic equivalents. For example, computer in English comes out as (konpyuutaa) in Japanese. Translating such items from Japanese back to English is even more challenging, and of practical interest, as transliterated items make up the bulk of text phrases not found in bilingual dictionaries. We describe and evaluate a method for performing backwards transliterations by machine. This method uses a generative model, incorporating several distinct stages in the transliteration process. We propose to compose a set of weighted finite state transducers to solve the problem of back-transliteration from Japanese Katakana to English.	Machine Transliteration It is hard to translate names and technical terms between languages with different alphabets and sounds. These words are often transliterated, meaning they are changed to similar sounding words. For example, "computer" in English becomes "konpyuutaa" in Japanese. Translating these words from Japanese back to English is even harder and important, because many phrases in texts are not in bilingual dictionaries. We explain and assess a method for doing transliterations backwards using machines. This method uses a model that includes several different steps in the transliteration process. We suggest using a set of tools called weighted finite state transducers to solve the problem of changing words from Japanese Katakana back to English.	surface
766	Predicting The Semantic Orientation Of Adjectives We identify and validate from a large corpus constraints from conjunctions on the positive or negative semantic orientation of the conjoined adjectives. A log-linear regression model uses these constraints to predict whether conjoined adjectives are of same or different orientations, achieving 82% accuracy in this task when each conjunction is considered independently. Combining the constraints across many adjectives, a clustering algorithm separates the adjectives into groups of different orientations, and finally, adjectives are labeled positive or negative. Evaluations on real data and simulation experiments indicate high levels of performance: classification precision is more than 90% for adjectives that occur in a modest number of conjunctions in the corpus. We cluster adjectives into + and - sets based on conjunction constructions, weighted similarity graphs, minimum-cuts, supervised learning and clustering.	Predicting The Semantic Orientation Of Adjectives We find and check, using a large collection of text, rules from joining words that help show if the adjectives joined together are positive or negative. A statistical model uses these rules to guess if the joined adjectives share the same positive or negative meaning, reaching 82% accuracy when looking at each joining word on its own. By combining rules across many adjectives, a grouping method sorts the adjectives into positive or negative groups, and then they are labeled as positive or negative. Tests on real data and simulated tests show high performance: the accuracy is over 90% for adjectives that appear in a decent number of joining word examples in the text collection. We group adjectives into positive and negative sets based on how they are joined together, using methods like similarity graphs, cutting techniques, guided learning, and grouping.	computer
767	PARADISE: A Framework For Evaluating Spoken Dialogue Agents This paper presents PARADISE (PARAdigm for Dialogue System Evaluation), a general framework for evaluating spoken dialogue agents. The framework decouples task requirements from an agent's dialogue behaviors, supports comparisons among dialogue strategies, enables the calculation of performance over subdialogues and whole dialogues, specifies the relative contribution of various factors to performance, and makes it possible to compare agents performing different tasks by normalizing for task complexity. We identify three factors which carry an influence on the performance of SDSs: agent factors (mainly related to the dialogue and the system itself), task factors (related to how the SDS captures the task it has been developed for) and environmental factor (e.g. factors related to the acoustic environment and the transmission channel. We aim to evaluate diaglogue agent strategies by relating overall user satisfaction to other metrics such as task success, efficiency measure and qualitative measures.	PARADISE: A Framework For Evaluating Spoken Dialogue Agents This paper introduces PARADISE, a general system for assessing spoken dialogue agents. The system separates the task needs from how the agent talks, allows for comparison of different talking methods, lets us measure performance in parts and as a whole, explains how different things contribute to success, and lets us compare agents doing different jobs by adjusting for how hard the task is. We point out three things that affect how well these systems work: agent factors (mainly about the dialogue and the system itself), task factors (related to how well the system understands its job), and environmental factors (like noise and connection quality). We want to judge dialogue agent methods by linking overall user happiness to other measures such as task success, speed, and quality.	combining
768	A Trainable Rule-Based Algorithm For Word Segmentation This paper presents a trainable rule-based algorithm for performing word segmentation. The algorithm provides a simple, language-independent alternative to large-scale lexicai-based segmenters requiring large amounts of knowledge engineering. As a stand-alone segmenter, we show our algorithm to produce high performance Chinese segmentation. In addition, we show the transformation-based algorithm to be effective in improving the output of several existing word segmentation algorithms in three different languages. Our Chinese segmenter makes use of only a manually segmented corpus without referring to any lexicon.	A Trainable Rule-Based Algorithm For Word Segmentation This paper presents a trainable rule-based method for dividing text into words. The method offers an easy and language-agnostic (works for any language) alternative to complex systems needing a lot of detailed language data. By itself, our method shows high accuracy in splitting Chinese text into words. Additionally, it successfully enhances the results of other existing word-splitting methods in three languages. Our Chinese word separator uses only a hand-divided sample text without using a dictionary.	different things
769	A Word-To-Word Model Of Translational Equivalence Many multilingual NLP applications need to translate words between different languages, but cannot afford the computational expense of inducing or applying a full translation model. For these applications, we have designed a fast algorithm for estimating a partial translation model, which accounts for translational equivalence only at the word level. The model's precision/recall trade-off can be directly controlled via one threshold parameter. This feature makes the model more suitable for applications that are not fully statistical. The model's hidden parameters can be easily conditioned on information extrinsic to the model, providing an easy way to integrate pre-existing knowledge such as part-of-speech, dictionaries, word order, etc.. Our model can link word tokens in parallel texts as well as other translation models in the literature. Unlike other translation models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy. We propose the Competitive Linking Algorithm for linking word pairs and a method which calculates the optimized correspondence level between the word pairs by hill climbing. One problem that arises in word-to-word alignment is as follows: if e1 is the translation of f1 and f2 has a strong monolingual association with f1, e1 and f2 will also have a strong correlation.	A Word-To-Word Model Of Translational Equivalence Many applications that use multiple languages need to translate words between languages but can't afford to use a full translation system because it's too expensive in terms of computer power. For these situations, we created a quick method to estimate a simpler translation system that only focuses on matching words directly. You can control how accurate or complete the model is with a single setting. This makes the model better for tasks that aren't entirely based on statistics. You can easily adjust the model using extra information like grammar, dictionaries, or word order. Our model can connect words in texts that are translated side by side and can work with other translation systems. Unlike others, it can create large translation word lists automatically, with over 99% accuracy. We suggest using the Competitive Linking Algorithm to match word pairs and a technique that finds the best match between word pairs by gradually improving the connection. A challenge in matching word for word is if e1 is the translation of f1 and f2 is closely linked to f1 in the same language, then e1 and f2 will also seem to be strongly connected.	existing
771	Entity-Based Cross-Document Core f erencing Using the Vector Space Model Cross-document coreference occurs when the same person, place, event, or concept is discussed in more than one text source. Computer recognition of this phenomenon is important because it helps break "the document boundary" by allowing a user to examine information about a particular entity from multiple text sources at the same time. In this paper we describe a cross-document coreference resolution algorithm which uses the Vector Space Model to resolve ambiguities between people having the same name. In addition, we also describe a scoring algorithm for evaluating the cross-document coreference chains produced by our system and we compare our algorithm to the scoring algorithm used in the MUC-6 (within document) coreference task.We proposed entity-based cross-document co-referencing which uses co-reference chains of each document to generate its summary and then use the summary rather than the whole article to select informative words to be the features of the document.	Entity-Based Cross-Document Coreferencing Using the Vector Space Model Cross-document coreference happens when the same person, place, event, or idea is mentioned in more than one text source. Computer recognition of this is important because it helps connect information from different texts, letting a user see details about a particular topic from multiple places at once. In this paper, we explain a method for solving cross-document coreference that uses the Vector Space Model to clear up confusion when people have the same name. We also explain a scoring method to evaluate the links between documents made by our system, and we compare our method to the scoring method used in the MUC-6 task, which deals with coreference within a single document. We suggested using entity-based cross-document co-referencing that uses links between references in each document to create a summary. This summary, rather than the whole article, helps in choosing important words to represent the document.	object patterns
772	The Berkeley FrameNet Project FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year (NSF IRI-9618838, "Tools for Lexicon Building"). The project's key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics. The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each accompanied by (c) a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between "frame elements" and their syntactic realizations (e.g. grammatical function, phrase type, and other syntactic traits). This report will present the project's goals and workflow, and information about the computational tools that have been adapted or created in-house for this work. We present the FrameNet project in which we havee been developing a frame-semantic lexicon for the core vocabulary of English.	The Berkeley FrameNet Project FrameNet is a three-year project funded by NSF (National Science Foundation) focused on studying language using real-world text examples, now in its second year. The project's main features are (a) using real examples from text to understand meaning and grammar rules, and (b) showing how words (mainly nouns, adjectives, and verbs) are used in different contexts called "frames." The database will include (a) explanations of the frames that give words their meanings, and (b) how thousands of words and phrases are used in a sentence, each with (c) a collection of examples from texts, showing how "frame elements" (parts of the frame) connect to their use in sentences (like their role in a sentence, type of phrase, and other sentence features). This report will explain the project's goals and process, and provide details about the computer tools developed or adjusted for this work. We present the FrameNet project where we are creating a dictionary based on frame semantics for important English words.	suggested using
773	Classifier Combination for Improved Lexical Disambiguation One of the most exciting recent directions in machine learning is the discovery that the combination of multiple classifiers often results in significantly better performance than what can be achieved with a single classifier. In this paper, we first show that the errors made from three different state of the art part of speech taggers are strongly complementary. Next, we show how this complementary behavior can be used to our advantage. By using contextual cues to guide tagger combination, we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers. We define the complementarity between two learners in order to quantify the percentage of time when one system is wrong while another system is correct, therefore providing an upper bound on combination accuracy.	Classifier Combination for Improved Lexical Disambiguation One of the most exciting recent directions in machine learning is the discovery that using multiple classifiers together often gives much better results than using just one classifier. In this paper, we first show that the mistakes made by three top-level part of speech taggers (tools that label words as nouns, verbs, etc.) are very different from each other. Next, we show how we can use these differences to our benefit. By using clues from the surrounding text to help combine these taggers, we create a new tagger that works much better than any of the single ones alone. We explain how to measure the complementarity, or how well two learners work together by looking at how often one is right when the other is wrong, to estimate the best possible accuracy when they are combined.	important
774	Error-Driven Pruning of Treebank Grammars for Base Noun Phrase Identification Finding simple, non-recursive, base noun phrases is an important subtask for many natural language processing applications. While previous empirical methods for base NP identification have been rather complex, this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task. In particular, we present a corpus-based approach for finding base NPs by matching part-of-speech tag sequences. The training phase of the algorithm is based on two successful techniques: first the base NP grammar is read from a "treebank" corpus; then the grammar is improved by selecting rules with high "benefit" scores. Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on the Penn Treebank Wall Street Journal. We store POS tag sequences that make up complete chunks and use these sequences as rules for classifying unseen data.	Error-Driven Pruning of Treebank Grammars for Base Noun Phrase Identification Finding simple, short, basic noun phrases (groups of words that work together as a noun) is an important part of many natural language processing tasks. While earlier methods for finding these basic noun phrases were quite complicated, this paper suggests a much simpler method that suits the straightforward nature of the task. Specifically, we use a method that relies on a large collection of text to find basic noun phrases by matching sequences of part-of-speech (POS) tags, which are labels that describe the function of each word in a sentence, like noun or verb. The training part of the method uses two successful strategies: first, the rules for finding noun phrases are learned from a "treebank" corpus, which is a structured collection of text; then, these rules are improved by picking the ones that score high in "benefit," meaning they are effective in identifying noun phrases. By using this simple method with a basic rule-matching approach, we achieve surprisingly good results when tested on the Penn Treebank Wall Street Journal, a well-known collection of sentences. We keep sequences of POS tags that form complete noun phrases and use these as rules to identify new, unseen data.	benefit
775	Exploiting Syntactic Structure for Language Modeling The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies. The model assigns probability to every joint sequence of words-binary-parse-structure with headword annotation and operates in a left-to-right manner -- therefore usable for automatic speech recognition. The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved. We choose the lexical heads of the two previous constituents as determined by a shift-reduce parser and find that this works better than a trigram model. We condition on linguistically relevant words by assigning partial phrase structure to the history and percolating headwords.	Exploiting Syntactic Structure for Language Modeling The paper introduces a language model that builds sentence structures to get useful information from past words, allowing it to understand long-range word connections. The model gives a likelihood to each group of words with a simple tree structure and key word labels, working from left to right, making it useful for speech recognition. The model, its probability setup, and a series of tests to check its prediction ability are discussed; it performs better than the usual three-word sequence model. We select the main words from the two previous parts as found by a type of sentence analyzer and discover it is more effective than a three-word model. We focus on important words by assigning partial sentence structures to past words and moving key words up in the structure.	straightforward nature
776	Investigating Regular Sense Extensions based on Intersective Levin Classes In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases. We see verb classes as the key to making generalizations about regular extensions of meaning. Current approaches to English classification, Levin classes and WordNet, have limitations in their applicability that impede their utility as general classification schemes. We present a refinement of Levin classes, intersective sets, which are a more fine-grained classification and have more coherent sets of syntactic frames and associated semantic components. We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the original Levin classes. We also have begun to examine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties. We show that multiple listings could in some cases be interpreted as regular sense extensions and defined intersective Levin classes, which are a more fine-grained, syntactically and semantically coherent refinement of basic Levin classes. We argue that the use of syntactic frames and verb classes can simplify the definition of different verb senses.	Investigating Regular Sense Extensions based on Intersective Levin Classes In this paper we specifically explore questions about polysemy, which means a single word having multiple meanings, focusing on verbs and how these meanings can regularly change through adding specific sentence structures. We believe that verb groups are essential for understanding how meanings can regularly change. Current methods for classifying English verbs, like Levin classes and WordNet, have limitations that make them less useful as general classification systems. We introduce an improved version of Levin classes called intersective sets, which offer a more detailed classification with more consistent sentence patterns and related meanings. We have early signs that our intersective sets will match better with WordNet compared to the original Levin classes. We have also started looking at similar verb groups in Portuguese and found that these verbs show consistent sentence patterns and meanings too. We demonstrate that multiple meanings can sometimes be seen as regular changes in meaning and define intersective Levin classes as more detailed and consistent improvements of basic Levin classes. We suggest that using sentence patterns and verb groups can make it easier to define different verb meanings.	probability setup
777	An IR Approach for Translating New Words from Nonparallel Comparable Texts We demonstrate that the associations between a word and its context seed words are preserved in comparable texts of different languages. We propose to represent the contexts of a word or phrase with a real-valued vector, which one element corresponds to one word in the contexts.	An IR Approach for Translating New Words from Nonparallel Comparable Texts We show that the connections between a word and the words around it are kept in similar texts from different languages. We suggest using a method to represent the surroundings of a word or phrase with a set of numbers, where each number matches one word in those surroundings.	multiple
778	Improving Data Driven Wordclass Tagging by System Combination In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system. We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging. Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second stage classifiers. All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger. We suggest three voting strategies: equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair-wise voting.	Improving Data Driven Wordclass Tagging by System Combination In this paper we look at how using different methods to train systems for the same natural language processing (NLP) task can help achieve better accuracy than using just the best single system. We do this through an experiment focused on a task called morpho-syntactic wordclass tagging, which involves identifying parts of speech or word classes in text. We trained four popular tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules, and Maximum Entropy) using the same set of text data. After comparing their results, we combined their outputs using different voting methods and additional classification techniques. All combined systems performed better than the best individual system, with the top combination reducing errors by 19.1% compared to the best single system. We propose three voting strategies: equal vote, where each system's vote has the same weight; overall accuracy, where the vote's weight is based on how accurate a system generally is; and pair-wise voting.	connections
779	Pseudo-Projectivity A Polynomially Parsable Non-Projective Dependency Grammar The pseudo-projective grammar we propose can be parsed in polynomial time and captures non-local dependencies through a form of gap-threading, but the structures generated by the grammar are strictly projective.	Pseudo-Projectivity A Polynomially Parsable Non-Projective Dependency Grammar The pseudo-projective grammar we suggest can be analyzed quickly (in polynomial time) and can handle complex connections that are not next to each other (non-local dependencies) using a method called gap-threading. However, the structures that this grammar creates are strictly orderly (projective).	achieve
780	Role of Verbs in Document Analysis We present results of two methods for assessing the event profile of news articles as a function of verb type. The unique contribution of this research is the focus on the role of verbs, rather than nouns. Two algorithms are presented and evaluated, one of which is shown to accurately discriminate documents by type and semantic properties, i.e. the event profile. The initial method, using WordNet (Miller et al. 1990), produced multiple cross-classification of articles, primarily due to the bushy nature of the verb tree coupled with the sense disambiguation problem. Our second approach using English Verb Classes and Alternations (EVCA) Levin (1993) showed that monosemous categorization of the frequent verbs in WSJ made it possible to usefully discriminate documents. For example, our results show that articles in which communication verbs predominate tend to be opinion pieces, whereas articles with a high percentage of agreement verbs tend to be about mergers or legal cases. An evaluation is performed on the results using Kendall's tau. We present convincing evidence for using verb semantic classes as a discriminant in document classification. We demonstrate that document type is correlated with the presence of many verbs of a certain EVCA class.	Role of Verbs in Document Analysis We show results from two ways of analyzing news articles based on the type of verbs used. What makes this research special is that it looks at the role of verbs instead of nouns. Two methods are tested, and one can successfully sort documents by type and meaning, or the "event profile." The first method, using WordNet (a tool that helps understand word meanings), had trouble correctly classifying articles because verbs have many different meanings. Our second method, using English Verb Classes and Alternations (EVCA), which categorizes verbs clearly, helped us separate documents better. For instance, articles with many communication verbs are often opinion pieces, while those with many agreement verbs are often about business mergers or legal issues. We checked the results using a method called Kendall's tau. We provide strong proof that using verb types can help classify documents. We show that the type of document is linked to many verbs from a certain EVCA class.	projective
809	Use Of Support Vector Learning For Chunk Identification	Use Of Support Vector Learning For Chunk Identification Support Vector Learning is a method from computer science used to help computers recognize patterns. In this context, "chunk identification" refers to the process of finding and labeling parts of a sentence that belong together, like phrases. This technique is useful for understanding language better and can be applied in areas like speech recognition or text analysis. The support vector method involves creating a model that can make decisions about which words or groups of words form a "chunk" based on examples it has learned from. This approach can improve how well a computer understands and organizes information in a text.	information
781	Automatic Retrieval and Clustering of Similar Words Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurns is significantly closer to WordNet than Roget Thesaurus is. We use dependency relation as word features to compute word similarities from large corpora.	Automatic Retrieval and Clustering of Similar Words Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words (how words are used together). The similarity measure allows us to construct a thesaurus (a book of synonyms and related words) using a parsed corpus (a large collection of texts that have been analyzed for grammar and structure). We then present a new evaluation methodology (a way to test and measure) for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet (a large database of words) than Roget Thesaurus is. We use dependency relation (how words depend on each other in a sentence) as word features to compute word similarities from large corpora (collections of written or spoken language).	articles
782	Robust Pronoun Resolution with Limited Knowledge Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge-based system, however, is that it is a very labour-intensive and time-consuming task. This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the success rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be successfully adapted for other languages with minimum modifications. We first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors. We find that the current evaluation of anaphora resolution algorithms and systems is befeft of any common ground for comparison due to the difference in evaluation data as well as the diversity of pre-processing tools employed by each anaphora resolution system.	Robust Pronoun Resolution with Limited Knowledge Most traditional ways to solve anaphora (referring back to something mentioned earlier) depend a lot on language and subject knowledge. One downside of creating a system based on knowledge is that it's very hard work and takes a lot of time. This paper shows a strong, simple method for figuring out pronouns (like he, she, it) in technical manuals, using text that has been pre-processed by a tool that identifies parts of speech (like nouns, verbs). The input is checked to see if it matches and if it has certain clues pointing to what the pronoun could refer to. Each clue gives possible answers a score, and the one with the highest score is chosen. Tests show a success rate of 89.7%, which is better than the methods we compared it with, using the same data. Also, early tests indicate that this method can be easily adjusted for other languages without much change. We first apply some rules to remove grammatically unsuitable options and then rank the rest based on how noticeable they are. We find that current evaluations of anaphora resolution systems lack a common basis for comparison because they use different data and tools to process the text before analysis.	Bootstrapping
783	Multilingual Authoring using Feedback Texts There are obvious reasons for trying to automate the production of multilingual documentation, especially for routine subject-matter in restricted domains (e.g. technical instructions). Two approaches have been adopted: Machine Translation (MT) of a source text, and Multilingual Natural Language Generation (M-NLG) from a knowledge base. For MT, information extraction is a major difficulty, since the meaning must be derived by analysis of the source text; M-NLG avoids this difficulty but seems at first sight to require an expensive phase of knowledge engineering in order to encode the meaning. We introduce here a new technique which employs M-NLG during the phase of knowledge editing. A 'feedback text', generated from a possibly incomplete knowledge base, describes in natural language the knowledge encoded so far, and the options for extending it. This method allows anyone speaking one of the supported languages to produce texts in all of them, requiring from the author only expertise in the subject-matter, not expertise in knowledge engineering. We propose WYSIWYM (What You See Is What You Mean) as a method for the authoring of semantic information through direct manipulation of structures rendered in natural language text. In this system logical forms are entered interactively and the corresponding linguistic realization of the expressions is generated in several languages.	Multilingual Authoring using Feedback Texts There are clear reasons for trying to automate the creation of documents in multiple languages, especially for routine topics in specific areas (like technical instructions). Two methods have been used: Machine Translation (MT) which translates a text from one language to another, and Multilingual Natural Language Generation (M-NLG) which creates text in different languages using a database of information. For MT, extracting information is a big challenge, because the meaning has to be figured out from the original text; M-NLG skips this problem but seems to need a costly step of organizing the information. We introduce a new method that uses M-NLG during the step of editing information. A 'feedback text', created from possibly incomplete information, explains in everyday language what is known so far and how it can be expanded. This approach lets anyone who speaks one of the supported languages write texts in all of them, needing only to know about the subject, not how to organize the information. We suggest WYSIWYM (What You See Is What You Mean) as a way to write meaningful information by directly working with structures shown in everyday language text. In this system, logical forms are entered interactively and the related language expressions are created in several languages.	something mentioned
784	Statistical Models for Unsupervised Prepositional Phrase Attachment We present several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task. Our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms, as opposed to attachment information. It is therefore less resource-intensive and more portable than previous corpus-based algorithm proposed for this task. We present results for prepositional phrase attachment in both English and Spanish. We first assume noun attachment for all of-PPs and then apply our disambiguation methods to all remaining PPs.	Statistical Models for Unsupervised Prepositional Phrase Attachment We introduce several models that don't need prior examples (unsupervised) to decide where prepositional phrases (like "in the park") should be attached in a sentence. These models almost match the accuracy of the best models that do use prior examples (supervised). Our method uses a simple rule (heuristic) based on how close words are to each other and learns from text that only has basic word information like parts of speech (nouns, verbs, etc.) and root word forms, without needing specific attachment details. This makes it less demanding on resources and easier to use in different situations compared to older methods that relied heavily on large collections of text (corpus-based). We show results for both English and Spanish. We first assume that all "of" phrases are connected to nouns, and then we use our methods to clarify where all other phrases should be attached.	languages
785	MindNet: Acquiring and Structuring Semantic Information from Text As a lexical knowledge base constructed automatically from the definitions and example sentences in two machine-readable dictionaries (MRDs), MindNet embodies several features that distinguish it from prior work with MRDs. It is, however, more than this static resource alone. MindNet represents a general methodology for acquiring, structuring, accessing, and exploiting semantic information from natural language text. MindNet is both an extraction methodology and a lexical ontology different from a word net since it was created automatically from a dictionary and its structure is based on such resources.	MindNet: Acquiring and Structuring Semantic Information from Text As a word-based knowledge collection created automatically from the definitions and example sentences in two digital dictionaries (MRDs), MindNet has several features that make it different from earlier work with these dictionaries. It is, however, more than just a fixed resource. MindNet is a general method for gathering, organizing, accessing, and using meaning-based information from regular language text. MindNet is both a method for extracting information and a word-based structure, distinct from a word net because it was automatically created from a dictionary and its structure is based on such sources.	first assume
786	Noun-Phrase Co-occurrence Statistics for Semi-Automatic Semantic Lexicon Construction Generating semantic lexicons semi-automatically could be a great time saver, relative to creating them by hand. In this paper, we present an algorithm for extracting potential entries for a category from an online corpus, based upon a small set of exemplars. Our algorithm finds more correct terms and fewer incorrect ones than previous work in this area. Additionally, the entries that are generated potentially provide broader coverage of the category than would occur to an individual coding them by hand. Our algorithm finds many terms not included within Wordnet (many more than previous algorithms), and could be viewed as an "enhancer" of existing broad-coverage resources. We use co-occurrence statistics in local context to discover sibling relations. Our experiments were performed using the MUC-4 and Wall Street Journal corpuses (about 30 million words). To select seed words we rank all of the head nouns in the training corpus by frequency and manually select the first 10 nouns that unambiguously belong to each category. We find that 3 of every 5 words learned by our system are not present in WordNet.	Noun-Phrase Co-occurrence Statistics for Semi-Automatic Semantic Lexicon Construction Generating semantic lexicons semi-automatically could save a lot of time compared to making them by hand. In this paper, we introduce a method for finding possible entries for a category from online text collections, using a small set of examples. Our method finds more correct terms and fewer wrong ones than previous methods. Also, the entries created might cover more of the category than if one person did it by hand. Our method finds many terms not in WordNet (many more than previous methods) and can be seen as a way to improve existing large resources. We use statistics on how often words appear together in nearby contexts to find related terms. We tested our method using the MUC-4 and Wall Street Journal text collections (about 30 million words). To choose starting words, we list all the main nouns in the training text by how often they appear and pick the first 10 nouns that clearly fit each category. We find that 3 out of every 5 words our system learns are not in WordNet.	MindNet
787	Never Look Back: An Alternative to Centering I propose a model for determining the hearer's attentional state which depends solely on a list of salient discourse entities (S-list). The ordering among the elements of the S-list covers also the function of the backward-looking center in the centering model. The ranking criteria for the S-list are based on the distinction between hearer-old and hearer-new discourse entities and incorporate preferences for inter- and intra-sentential anaphora. The model is the basis for an algorithm which operates incrementally, word by word. We argue that the information status of an antecedent is more important than the grammatical role in which it occurs. We evaluate on hand-annotated data. We restrict our algorithm to the current and last sentence.	Never Look Back: An Alternative to Centering I suggest a model to understand what the listener is focusing on, which relies only on a list of important conversation topics (S-list). The order of items in the S-list also serves the purpose of the backward-looking center from the centering model. The way we rank the S-list depends on whether topics are already known or new to the listener, and it considers preferences for references within and between sentences. The model supports a step-by-step process, analyzing one word at a time. We believe the importance of earlier mentioned information matters more than its grammatical role. We test our model with data that has been manually checked. We limit our model to focus on the current and previous sentence.	fewer wrong
788	Measures Of Distributional Similarity We study distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences. We use verb-object relations in both active and passive voice constructions. We find that our asymmetric skew divergence, a generalisation of Kullback-Leibler divergence, persorms best for improving probability estimates for unseen word co-occurrences.	Measures Of Distributional Similarity We examine ways to measure how similar distributions are to help predict the chance of events we haven't seen yet. We look at the relationship between verbs and objects, whether the verb is doing the action or having the action done to it. We discover that our method, called asymmetric skew divergence, which is an advanced version of Kullback-Leibler divergence (a mathematical way to measure differences), works best for predicting chances of word pairings we haven't seen before.	considers
789	Finding Parts In Very Large Corpora We present a method for extracting parts of objects from wholes (e.g."speedometer" from "car"). Given a very large corpus our method finds part words with 55% accuracy for the top 50 words as ranked by the system. The part list could be scanned by an end-user and added to an existing ontology (such as WordNet), or used as a part of a rough semantic lexicon. To filter out attributes that are regarded as qualities (like driving ability) rather than parts (like steering wheels), we remove words ending with the suffixes -ness, -ing, and -ity.	Finding Parts In Very Large Corpora We present a way to identify parts of objects from the whole object (like finding "speedometer" from "car"). Using a very large collection of text, our method identifies part words with 55% accuracy for the top 50 words as ranked by the system. The list of parts can be reviewed by users and added to an existing knowledge database (like WordNet), or used as part of a basic word meaning dictionary. To exclude words that describe qualities (such as driving ability) instead of parts (like steering wheels), we remove words ending with -ness, -ing, and -ity.	measure differences
790	Inducing A Semantically Annotated Lexicon Via EM-Based Clustering We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evalutated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries. We test 3000 random verb-noun pairs, requiring the erbs and nouns to appear between 30 and 3000 times in training. We use soft clustering to form classes for generalization and do not take recourse to any hand-crafter resources in our approach to selectional preference induction.	Inducing A Semantically Annotated Lexicon Via EM-Based Clustering We present a method for automatically creating slot labels for subcategorization frames, using a system that finds hidden groups with a statistical method called EM (Expectation-Maximization). The models are tested using a general decision-making test. Creating slot labels for subcategorization frames is done by applying EM again, tested on data from analyzing large collections of text. We explain how the learned information can be seen as detailed language dictionary entries. We test 3000 random pairs of verbs and nouns, ensuring the verbs and nouns appear between 30 and 3000 times during training. We use a technique called soft clustering to create groups for general use and do not rely on any manually created resources in our approach to understanding how words prefer to combine.	meaning dictionary
791	Automatic Construction Of A Hypernym-Labeled Noun Hierarchy From Text Previous work has shown that automatic methods can be used in building semantic lexicons. This work goes a step further by automatically creating not just clusters of related words, but a hierarchy of nouns and their hypernyms, akin to the hand-built hierarchy in WordNet. We let three judges evaluate ten internal nodes in the hyponym hierarchy that had at least twenty descendants.	Automatic Construction Of A Hypernym-Labeled Noun Hierarchy From Text Previous work has shown that automatic methods can be used in building dictionaries that explain word meanings. This work goes a step further by automatically creating not just groups of related words, but a hierarchy (like a family tree) of nouns and their hypernyms (general terms), similar to the manually created hierarchy in WordNet (a large database of words). We had three judges evaluate ten main points in the more specific term hierarchy that had at least twenty related words.	Maximization
810	Two Statistical Parsing Models Applied To The Chinese Treebank This paper presents the first-ever results of applying statistical parsing models to the newly-available Chinese Treebank. We have employed two models, one extracted and adapted from BBN's SIFT System (Miller et al. , 1998) and a TAG-based parsing model, adapted from (Chiang, 2000). On sentences with < 40 words, the former model performs at 69% precision, 75% recall, and the latter at 77% precision and 78% recall. Our parser operates at word-level with the assumption that input sentences are pre-segmented.	Two Statistical Parsing Models Applied To The Chinese Treebank This paper shows the first results of using statistical parsing models, which are computer methods to understand sentence structure, on the newly-available Chinese Treebank, a collection of Chinese sentences. We used two models: one taken and changed from BBN's SIFT System and another based on TAG (a type of grammar). For sentences with less than 40 words, the first model correctly identifies 69% of the sentence parts (precision) and finds 75% of all parts it should find (recall). The second model correctly identifies 77% and finds 78%. Our parser works at the word-level, meaning it looks at individual words, assuming the sentences are already divided into words.	Support Vector
793	Automatic Identification Of Non-Compositional Phrases Non-compositional expressions present a special challenge to NLP applications. We present a method for automatic identification of non-compositional expressions using their statistical properties in a text corpus. Our method is based on the hypothesis that when a phrase is non-composition, its mutual information differs significantly from the mutual informations of phrases obtained by substituting one of the word in the phrase with a similar word. We use LSA to distinguish between compositional and non compositional verb-particle constructions and noun noun compounds. We define a decision criterion for non compositional phrases based on the change in the mutual information of a phrase when substituting one word for a similar one based on an automatically constructed thesaurus.	Automatic Identification Of Non-Compositional Phrases Non-compositional expressions, which are phrases that don't mean what their individual words suggest, are hard for computer language tools to handle. We introduce a way to automatically find these tricky phrases by looking at their statistical patterns in a large collection of text. Our approach is based on the idea that if a phrase is non-compositional, the mutual information (a measure of how words relate to each other) changes a lot when you swap one word in the phrase with another similar word. We use a technique called LSA (Latent Semantic Analysis, a method to understand word meanings) to tell apart normal phrases from non-compositional ones, like verb-particle combinations (e.g., "give up") and noun-noun pairs (e.g., "coffee cup"). We set up a rule to decide if a phrase is non-compositional by checking how much the mutual information changes when we replace one word with a similar word using a computer-generated thesaurus (a list of words with similar meanings).	helping
794	Deep Read: A Reading Comprehension System This paper describes initial work on Deep Read, an automated reading comprehension system that accepts arbitrary text input (a story) and answers questions about it. We have acquired a corpus of 60 development and 60 test stories of 3rd to 6th grade material; each story is followed by short-answer questions (an answer key was also provided). We used these to construct and evaluate a baseline system that uses pattern matching (bag-of-words) techniques augmented with additional automated linguistic processing (stemming, name identification, semantic class identification, and pronoun resolution). This simple system retrieves the sentence containing the answer 30-40% of the time. We use a statistical bag-of-words approach, matching the question with the lexically most similar sentence in the story.	Deep Read: A Reading Comprehension System This paper talks about the early stages of Deep Read, a computer program that helps understand reading by taking any story and answering questions about it. We gathered a collection of 60 practice stories and 60 test stories for kids in 3rd to 6th grade; each story has short-answer questions, along with an answer guide. We used these stories to build and test a basic system that finds patterns in words (bag-of-words) and improves it with extra language processing like breaking words down to their root form (stemming), recognizing names, figuring out word types (semantic class identification), and figuring out what pronouns refer to (pronoun resolution). This simple system finds the sentence with the answer 30-40% of the time. We use a word-count method (bag-of-words), comparing the question to the sentence in the story that has the most similar words.	generated thesaurus
795	Corpus-Based Identification Of Non-Anaphoric Noun Phrases Coreference resolution involves finding antecedents for anaphoric discourse entities, such as definite noun phrases. But many definite noun phrases are not anaphoric because their meaning can be understood from general world knowledge (e.g., "the White House" or "the news media"). We have developed a corpus-based algorithm for automatically identifying definite noun phrases that are non-anaphoric, which has the potential to improve the efficiency and accuracy of coreference resolution systems. Our algorithm generates lists of non-anaphoric noun phrases and noun phrase patterns from a training corpus and uses them to recognize non-anaphoric noun phrases in new texts. Using 1600 MUC-4 terrorism news articles as the training corpus, our approach achieved 78% recall and 87% precision at identifying such noun phrases in 50 test documents. We develop a system for identifying discourse-new DDs that incorporates, in addition to syntax-based heuristics aimed at recogznizing predicative and established DDs, additional techniques for mining from corpora unfamiliar DDS including proper names, larger situation and semantically functional. We develop an unsupervised learning algorithm that automatically recognizes definite NPs that are existential without syntactic modification because their meaning is universally understood.	Corpus-Based Identification Of Non-Anaphoric Noun Phrases Coreference resolution is about finding the first mention (antecedents) for words or phrases that refer back to something mentioned earlier (anaphoric discourse entities), like certain nouns. However, many nouns don't refer back to something and make sense on their own because we understand them through common knowledge (like "the White House" or "the news media"). We created a method using a collection of texts (corpus-based algorithm) to automatically find these non-referring nouns, which could make systems that track these references work better and faster. Our method makes lists of these non-referring nouns and patterns from a set of training texts and uses them to spot similar nouns in new texts. By using 1600 news articles about terrorism (MUC-4) as practice material, our method was able to correctly identify these nouns 78% of the time and was accurate 87% of the time in 50 test documents. We built a system to identify new noun phrases (DDs) in writing, using not only rules based on sentence structure (syntax-based heuristics) to spot known nouns but also new techniques to find unfamiliar names and phrases from text collections (corpora), including proper names and meaningful phrases. We also developed a method that learns on its own (unsupervised learning algorithm) to recognize universally understood nouns without changing their structure.	questions
796	Efficient Parsing For Bilexical Context-Free Grammars And Head Automaton Grammars Several recent stochastic parsers use bilexical grammars, where each word type idiosyncratically prefers particular complements with particular head words. We present O(n4) parsing algorithms for two bilexical formalisms, improving the prior upper bounds of O(n5). For a common special case that was known to allow O(n3) parsing (Eisner, 1997), we present an O(n3) algorithm with an improved grammar constant. We show that the dynamic programming algorithms for lexicalized PCFGs require O(m3) states.	Efficient Parsing For Bilexical Context-Free Grammars And Head Automaton Grammars Several recent statistical parsers (a tool for analyzing sentences) use bilexical grammars, where each word has specific preferences for certain complements (additional words) with specific main words. We provide O(n4) parsing methods for two bilexical systems, which is faster than the previous limit of O(n5). For a common special case known to allow O(n3) parsing (Eisner, 1997), we offer an O(n3) method with a better grammar constant (meaning it works more efficiently). We demonstrate that the dynamic programming techniques for lexicalized PCFGs (a type of grammar that includes word information) need O(m3) states (steps or conditions).	articles about
797	A Statistical Parser For Czech This paper considers statistical parsing of Czech, which differs radically from English in at least two respects: (1) it is a highly inflected language, and (2) it has relatively free word order. These differences are likely to pose new problems for techniques that have been developed on English. We describe our experience in building on the parsing model of (Collins 97). Our final results - 80% dependency accuracy - represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text. We use a transformed tree bank from the Prague Dependency Treebank for constituent parsing on Czech.	A Statistical Parser For Czech This paper looks at statistical parsing, which is a method of analyzing sentences, for the Czech language. Czech is very different from English in two main ways: (1) it uses many word endings to show the role of words in a sentence (highly inflected), and (2) the order of words in sentences can change a lot (relatively free word order). These differences may cause new challenges for methods that were originally designed for English. We talk about our experience in using the parsing model created by Collins in 1997. Our final results show 80% accuracy in understanding word relationships, which is good progress towards the 91% accuracy achieved with English text (specifically from the Wall Street Journal). We use a changed version of a collection of sentence structures (transformed tree bank) from the Prague Dependency Treebank to help analyze Czech sentences.	parsing methods
798	Automatic Identification Of Word Translations From Unrelated English And German Corpora Algorithms for the alignment of words in translated texts are well established. However, only recently new approaches have been proposed to identify word translations from non-parallel or even unrelated texts. This task is more difficult, because most statistical clues useful in the processing of parallel texts cannot be applied to non-parallel texts. Whereas for parallel texts in some studies up to 99% of the word alignments have been shown to be correct, the accuracy for non-parallel texts has been around 30% up to now. The current study, which is based on the assumption that there is a correlation between the patterns of word co-occurrences in corpora of different languages, makes a significant improvement to about 72% of word translations identified correctly. We create bag-of-words context vectors around both the source and target language words and then project the source into the English target space via the current small translation dictionary. We filter out bilingual term pairs with low monolingual frequencies (those below 100 times). We show that accurate translations can be learned for 100 German nouns that are not contained in the seed bilingual dictionary.	Automatic Identification Of Word Translations From Unrelated English And German Corpora Algorithms for matching words in translated texts are well known. However, recently, new methods have been suggested to find word translations from texts that are not directly related or translated. This is harder because most helpful hints used in directly translated texts don't work for unrelated ones. For directly translated texts, studies have shown up to 99% accuracy in matching words, but for unrelated texts, it has been about 30% until now. This study, which is based on the idea that there's a connection between how words appear together in different languages, improves accuracy to about 72% for correctly identifying word translations. We create groups of words called "context vectors" around words in both languages and then match the original language's words to English using a small existing translation dictionary. We remove word pairs that don't show up often enough (less than 100 times). We demonstrate that accurate translations can be found for 100 German nouns that aren't in the initial translation dictionary.	final results
799	Mining The Web For Bilingual Text STRAND (Resnik, 1998) is a language-independent system for automatic discovery of text in parallel translation on the World Wide Web. This paper extends the preliminary STRAND results by adding automatic language identification, scaling up by orders of magnitude, and formally evaluating performance. The most recent end-product is an automatically acquired parallel corpus comprising 2491 English-French document pairs, approximately 1.5 million words per language. We use structure markup information from pages without looking at their content to attempt to align them.	Mining The Web For Bilingual Text STRAND (Resnik, 1998) is a system that works with any language to automatically find texts that are translated in parallel on the internet. This paper improves the early STRAND results by adding features that automatically detect languages, making the system much bigger and testing how well it works in a structured way. The latest result is a collection of 2491 pairs of English and French documents, with about 1.5 million words in each language. We use information from the page layout, not the actual text, to try to match them.	English
800	Estimators For Stochastic Unification-Based Grammars Log-linear models provide a statistically sound framework for Stochastic "Unification-Based" Grammars (SUBGs) and stochastic versions of other kinds of grammars. We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical-Functional Grammar. We incorporate general linguistic principles into a log-linear model. We use parses generated by a LFG parser as input to a MRF approach.	Estimators For Stochastic Unification-Based Grammars Log-linear models offer a reliable statistical method for handling Stochastic "Unification-Based" Grammars (SUBGs) and other grammar types that include random elements. We explain two practical methods for figuring out the rules of these grammars using a set of example sentences with their structures, and then use these methods to create a random version of Lexical-Functional Grammar. We add broad language rules into a log-linear model. We use sentence structures made by a Lexical-Functional Grammar (LFG) parser as input for a Markov Random Field (MRF) method.	features
801	Information Fusion In The Context Of Multi-Document Summarization We present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents. Our approach is unique in its usage of language generation to reformulate the wording of the summary. We observe for that task of multi-document summarization of news articles that extraction may be inappropriate because it may produce summaries which are overly verbose or biased towards some sources.	Information Fusion In The Context Of Multi-Document Summarization We introduce a way to automatically create a short summary by finding and combining similar points from several related documents. Our method is special because it uses language generation, which means it changes the wording to create the summary. We notice that when summarizing news articles from multiple documents, just pulling out pieces of text (extraction) may not work well because it can make summaries too long or unfairly favor certain sources.	figuring
802	SemEval-2010 Task 13: TempEval-2 Tempeval-2 comprises evaluation tasks for time expressions, events and temporal relations, the latter of which was split up in four subtasks, motivated by the notion that smaller subtasks would make both data preparation and temporal relation extraction easier. Manually annotated data were provided for six languages: Chinese, English, French, Italian, Korean and Spanish. One of the tasks of this workshop is to determine the temporal relation between an event and a time expression in the same sentence.	SemEval-2010 Task 13: TempEval-2 TempEval-2 includes tasks for understanding time-related words, events, and how they relate over time. It was divided into four smaller tasks to make it easier to prepare data and figure out these time relationships. Data with human-made notes were given for six languages: Chinese, English, French, Italian, Korean, and Spanish. One task in this workshop is to find out how an event and a time-related word in the same sentence are connected.	special
803	SemEval-2010 Task 14: Word Sense Induction &#x26;Disambiguation This paper presents the description and evaluation framework of SemEval-2010 Word Sense Induction & Disambiguation task, as well as the evaluation results of 26 participating systems. In this task, participants were required to induce the senses of 100 target words using a training set, and then disambiguate unseen instances of the same words using the induced senses. System answers were evaluated in: (1) an unsupervised manner by using two clustering evaluation measures, and (2) a supervised manner in a WSD task. In constructing the dataset we use WordNet to first randomly select one sense of the word and then construct a set of words in relation to the first word's chosen synset.	SemEval-2010 Task 14: Word Sense Induction &#x26;Disambiguation This paper explains the setup and evaluation process of the SemEval-2010 task on Word Sense Induction & Disambiguation, and shows the results from 26 systems that took part. In this task, participants had to figure out the meanings (senses) of 100 target words using a training set, and then identify the correct meanings in new examples of those words. System answers were evaluated in two ways: (1) without supervision, by using two ways to measure how well items are grouped together (clustering evaluation), and (2) with supervision, in a Word Sense Disambiguation (WSD) task. To build the dataset, we used WordNet (a large database of words) to randomly select one meaning of a word, and then created a group of words related to that chosen meaning (synset).	Italian
804	Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content Synchronization This paper presents the first round of the task on Cross-lingual Textual Entailment for Content Synchronization, organized within SemEval-2012. The task was designed to promote research on semantic inference over texts written in different languages, targeting at the same time a real application scenario. Participants were presented with datasets for different language pairs, where multi-directional entailment relations (forward, backward, bidirectional, no entailment) had to be identified. We report on the training and test data used for evaluation, the process of their creation, the participating systems (10 teams, 92 runs), the approaches adopted and the results achieved.	Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content Synchronization This paper introduces the first part of a challenge called Cross-lingual Textual Entailment for Content Synchronization, which is part of SemEval-2012. The challenge was made to encourage study on understanding meaning (semantic inference) in texts written in different languages, focusing on a real-world use case. Participants were given data sets for different language combinations, where they had to identify how texts relate to each other (do they imply or infer the same meaning: in one direction, both directions, or not at all). We describe the training and test data used for judging, how they were created, the teams that took part (10 teams, 92 attempts), the methods they used, and the results they got.	Sense Induction
805	Centroid-Based Summarization Of Multiple Documents: Sentence Extraction Utility-Based Evaluation And User Studies We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization. Our centroid-based extractive summarizer scores sentences based on sentence-level and inter-sentence features which indicate the quality of the sentence as a summary sentence.	Centroid-Based Summarization Of Multiple Documents: Sentence Extraction Utility-Based Evaluation And User Studies We present a tool called MEAD that creates summaries of several documents by using cluster centers (centroids) identified by a system that detects and tracks topics. We also explain two new methods, based on how useful a sentence is and the idea of subsumption (the idea that one idea can include another), which we used to evaluate summaries of both single and multiple documents. Finally, we describe two studies with users that test how our models summarize multiple documents. Our summarizer, which looks at main points of a text, rates sentences using features within the sentence and between sentences to see how well they work as part of a summary.	challenge
806	Knowledge-Free Induction Of Morphology Using Latent Semantic Analysis Morphology induction is a subproblem of important tasks like automatic learning of machine-readable dictionaries and grammar induction. Previous morphology induction approaches have relied solely on statistics of hypothesized stems and affixes to choose which affixes to consider legitimate. Relying on stem-and-affix statistics rather than semantic knowledge leads to a number of problems, such as the inappropriate use of valid affixes ("ally" stemming to "all"). We introduce a semantic-based algorithm for learning morphology which only proposes affixes when the stem and stem-plus-affix are sufficiently similar semantically. We implement our approach using Latent Semantic Analysis and show that our semantics-only approach provides morphology induction results that rival a current state-of-the-art system. We generate a list of N candidate suffixes and use this list to identify word pairs which share the same stem. We attempt to cluster morphologically related words starting with an unrefined trie search, which contains a parameter of minimum possible stem length and an upper bound on potential affix candidates, that is constrained by semantic similarity in a word context vector space.	Knowledge-Free Induction Of Morphology Using Latent Semantic Analysis Morphology induction is about figuring out the structure of words and is important for tasks like creating computer-friendly dictionaries and understanding grammar rules automatically. Previous methods for figuring out word structure depended only on counting how often certain word parts like stems (main part of a word) and affixes (word endings or beginnings) appeared together to decide which affixes are valid. This focus on counting rather than understanding word meanings led to problems, such as using correct affixes incorrectly (like turning "ally" into "all"). We present a new method based on word meanings to learn word structures, suggesting word parts only when the main part and the whole word with the affix have similar meanings. We use a method called Latent Semantic Analysis to show that focusing only on word meanings can produce results as good as the best current system for figuring out word structures. We make a list of possible word endings and use it to find words that share the same main part. We try to group related words by starting with a simple search, which involves setting limits on the shortest possible main part and the maximum possible word parts, controlled by how similar the words are in meaning when looked at in a complex word meaning space.	tracks topics
807	Inducing Syntactic Categories By Context Distribution Clustering This paper addresses the issue of the automatic induction of syntactic categories from unannotated corpora. Previous techniques give good results, but fail to cope well with ambiguity or rare words. An algorithm, context distribution clustering (CDC), is presented which can be naturally extended to handle these problems. We apply syntactic clustering and dimensionality reduction in a knowledge-free setting to obtain meaningful clusters. In our bootstrapping approach we first cluster the most distributionally reliable words and then incrementallly augment each cluster with words that are distributionally similar to those already in the cluster.	Inducing Syntactic Categories By Context Distribution Clustering This paper deals with the automatic creation of syntactic categories (like nouns, verbs, etc.) from large collections of text that haven't been labeled or annotated. Past methods work well, but struggle with words that have multiple meanings or are not used often. A new method, called context distribution clustering (CDC), is introduced that can naturally handle these challenges. We use syntactic grouping and reduce complexity without needing prior knowledge to create meaningful groups. In our step-by-step method, we first group words that are most clearly related by their usage in sentences and then slowly add words that are similarly used to these groups.	appeared
808	Introduction To The CoNLL-2000 Shared Task: Chunking We give background information on the data sets, present a general overview of the systems that have taken part in the shared task and briefly discuss their performance. The dataset is extracted from the WSJ Penn Tree bank and contains 211,727 training examples and 47,377 test instances.	Introduction To The CoNLL-2000 Shared Task: Chunking We provide background information about the data sets, give a general summary of the systems that participated in the shared task, and briefly talk about how well they performed. The dataset is taken from the WSJ Penn Treebank, which is a large collection of language data, and includes 211,727 examples for training and 47,377 examples for testing.	syntactic categories
812	Enriching The Knowledge Sources Used In A Maximum Entropy Part-Of-Speech Tagger This paper presents results for a maximum-entropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging. In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs. The best resulting accuracy for the tagger on the Penn Treebank is 96.86% overall, and 86.91% on previously unseen words. We achieve 96.9% on seen words and 86.9% on unseen with a MEMM.	Enriching The Knowledge Sources Used In A Maximum Entropy Part-Of-Speech Tagger This paper shows results for a maximum-entropy-based part of speech tagger, which performs better mainly by adding more information for tagging. We improve results by adding these features: (i) better handling of capital letters in unknown words; (ii) features to help decide verb tense; (iii) features to tell apart particles from prepositions and adverbs. The best accuracy for the tagger on the Penn Treebank is 96.86% overall, and 86.91% on new words. We achieve 96.9% on known words and 86.9% on new words with a MEMM (Maximum Entropy Markov Model).	using computers
813	Evaluation Metrics For Generation Certain generation applications may profit from the use of stochastic methods. In developing stochastic methods, it is crucial to be able to quickly assess the relative merits of different approaches or models. In this paper, we present several types of intrinsic (system internal) metrics which we have used for baseline quantitative assessment. This quantitative assessment should then be augmented to a fuller evaluation that examines qualitative aspects. To this end, we describe an experiment that tests correlation between the quantitative metrics and human qualitative judgment. The experiment confirms that intrinsic metrics cannot replace human evaluation, but some correlate significantly with human judgments of quality and understandability and can be used for evaluation during development. We propose Simple String Accuracy as a baseline evaluation metric for natural language generation.	Evaluation Metrics For Generation Certain generation applications may benefit from using random (stochastic) methods. When creating these random methods, it's important to quickly compare the strengths of different approaches or models. In this paper, we show several types of internal system measures (intrinsic metrics) that we have used for basic numerical assessment. This numerical assessment should then be expanded to a more complete evaluation that looks at quality aspects. To do this, we describe an experiment that checks the connection between the numerical measures and human judgment of quality. The experiment confirms that internal measures cannot replace human evaluation, but some match well (correlate significantly) with human opinions on quality and understandability and can be useful for checking during development. We suggest using Simple String Accuracy as a basic measure for evaluating how well computer systems generate natural language.	information
814	Robust Applied Morphological Generation In practical natural language generation systems it is often advantageous to have a separate component that deals purely with morphological processing. We present such a component: a fast and robust morphological generator for English based on finite-state techniques that generates a word form given a specification of the lemma, part-of-speech, and the type of inflection required. We describe how this morphological generator is used in a prototype system for automatic simplification of English newspaper text, and discuss practical morphological and orthographic issues we have encountered in generation of unrestricted text within this application.	Robust Applied Morphological Generation In practical systems that create natural language, it's often helpful to have a separate part that focuses only on handling the forms of words (morphological processing). We introduce such a part: a quick and strong tool that creates different forms of English words. It uses simple, step-by-step methods (finite-state techniques) to make a word form when given the base form of a word (lemma), its role in a sentence (part-of-speech), and the type of word change needed (inflection). We explain how this tool is used in an early version of a system that automatically makes English newspaper text easier to read, and talk about practical problems with word forms and spelling (orthographic issues) we faced when creating text without limits in this system.	expanded
815	Limitations Of Co-Training For Natural Language Learning From Large Datasets Co-Training is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifiers using separate views of the same data. This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data. This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels. Using base noun phrase bracketing as a case study, we find that co-training reduces by 36% the difference in error between classifiers and supervised clastrained on a labeled version all available data. However, degradation in the quality of the bootstrapped data arises as an obstacle to further improvement. To address this, we propose a moderately supervised variant of cotraining in which a human corrects the mistakes made during automatic labeling. Our analysis suggests that corrected co-training and similar moderately supervised methods may help cotraining scale to large natural language learning tasks. We show that the quality of the automatically labeled training data is crucial for co-training to perform well because too many tagging errors prevent a high performing model from being learned.	Limitations Of Co-Training For Natural Language Learning From Large Datasets Co-Training is a type of learning where two computer programs, called classifiers, learn using different parts of the same data. This helps start learning from a small amount of labeled data (data with answers) by using a large amount of unlabeled data (data without answers). This study looks at how well co-training works for tasks that usually need a lot of examples to work well. When we use it for a specific task, like finding base noun phrases in text, we see that co-training can reduce errors by 36% compared to when using only labeled data. However, the quality of the new data created during co-training becomes a problem because it can be less accurate. To fix this, we suggest a version of co-training where a person checks and fixes mistakes made by the computer. Our study shows that this corrected method and others like it could help co-training work better for big language tasks. We demonstrate that it is very important for the data labeled by the computer to be accurate because too many mistakes stop the learning process from being successful.	techniques
816	Classifying The Semantic Relations In Noun Compounds Via A Domain-Specific Lexical Hierarchy We are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in traditional knowledge representation systems). In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds. We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves. We classify noun compounds from the domain of medicine using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. We use a discriminative classifier to assing 18 relations for noun compounds from biomedical text and achieve 60% accuracy.	Classifying The Semantic Relations In Noun Compounds Via A Domain-Specific Lexical Hierarchy We are developing techniques based on collections of text to identify how words relate to each other. These techniques are more detailed than simple grammar rules but not as complex as advanced knowledge systems. In this paper, we explain a method for finding relationships between two-word noun combinations. We discovered that a straightforward approach using a computer algorithm and a specialized word hierarchy works well. It can apply learned patterns to new words better than just using the words themselves in training. We categorize noun combinations in the medical field into 13 types based on how the main noun and the describing noun relate. We use a type of decision-making tool to assign 18 types of relationships to noun pairs from medical writings and reach 60% accuracy.	compared
817	Is Knowledge-Free Induction Of Multiword Unit Dictionary Headwords A Solved Problem? We seek a knowledge-free method for inducing multiword units from text corpora for use as machine-readable dictionary headwords. We provide two major evaluations of nine existing collocation-finders and illustrate the continuing need for improvement. We use Latent Semantic Analysis to make modest gains in performance, but we show the significant challenges encountered in trying this approach. We show that WordNet is as effective an evaluation resource as the web for MWE detection methods, despite its inherent size limitations and static nature. We compare the semantic vector of a phrase and the vectors of its component words in two ways: one includes the phrases's contexts in the construction of the semantic vectors of the parts and one does not.	Is Knowledge-Free Induction Of Multiword Unit Dictionary Headwords A Solved Problem? We are looking for a way to find groups of words that go together (multiword units) from large text collections without using prior knowledge, to use them as dictionary entries that computers can read. We review and test nine existing tools that find these word groups and show that they still need improvement. We use a method called Latent Semantic Analysis (a technique to understand relationships between words) to make some progress, but we also highlight the big challenges we face with this method. We demonstrate that WordNet (a large database of words and meanings) is as useful for testing methods to find multiword expressions as the internet, even though WordNet is smaller and doesn't change over time. We compare the meaning or context of a phrase and its individual words in two ways: one way uses the phrase's context to create the meaning for the parts, and the other way does not.	techniques
818	Latent Semantic Analysis For Text Segmentation This paper describes a method for linear text segmentation that is more accurate or at least as accurate as state-of-the-art methods (Utiyama and Isahara, 2001; Choi, 2000a). Inter-sentence similarity is estimated by latent semantic analysis (LSA). Boundary locations are discovered by divisive clustering. Test results show LSA is a more accurate similarity measure. We use all vocabulary words to compute low-dimensional document vectors.	Latent Semantic Analysis For Text Segmentation This paper explains a way to divide text into parts that is as good or better than the best current methods. We measure how similar sentences are using latent semantic analysis (LSA), a technique that finds hidden meanings and relationships. We find where to divide the text using a method called divisive clustering, which breaks text into smaller groups. Tests show that LSA is a better way to measure similarity. We use all the words in the text to create simple, smaller versions called low-dimensional document vectors.	meanings
819	Corpus Variation And Parser Performance Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank. While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might affect parser performance, and how portable parsing models are across corpora. We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained. This leads us to a technique for pruning parameters to reduce the size of the parsing model. We show that the accuracy of parsers trained on the Penn Treebank degrades when applied to different genres and domains. We report results on sentences of 40 or less words on all the Brown corpus sections combined, for which we obtain 80.3%/81.0% recall/precision when training only on data from the WSJ corpus, and 83.9%/84.8% when training on data from the WSJ corpus and all sections of the Brown corpus.	Corpus Variation And Parser Performance Most work in statistical parsing, which is about analyzing sentences using statistics, has focused on just one set of text: the Wall Street Journal part of the Penn Treebank. While this has made it easier to compare different parsing methods, it doesn't answer how other kinds of text might change how well parsers work, or how easily parsing models can be used with different text collections. We look into these questions by comparing results from the Brown and WSJ text collections, and also check which parts of the parser's probability model are specifically adjusted for the text collection it was trained on. This leads us to a method for cutting down unnecessary details to make the parsing model smaller. We show that the accuracy of parsers trained on the Penn Treebank becomes less precise when used on different types of writings and subject areas. We present results on sentences with 40 or fewer words across all sections of the Brown text collection, where we get 80.3%/81.0% recall/precision when training only on data from the WSJ collection, and 83.9%/84.8% when training on data from both the WSJ collection and all sections of the Brown collection.	relationships
820	Assigning Time-Stamps To Event-Clauses We describe a procedure for arranging into a time-line the contents of news stories describing the development of some situation. We describe the parts of the system that deal with 1. breaking sentences into event-clauses and 2. resolving both explicit and implicit temporal references. Evaluations show a performance of 52%, compared to humans. We infer time values based on the most recently assigned date of the date of the article.	Assigning Time-Stamps To Event-Clauses We explain a method for organizing the events in news stories into a timeline to show how a situation unfolds. We explain the parts of the system that focus on 1. splitting sentences into event-clauses (parts of sentences that describe events) and 2. figuring out both clear and hidden time references. Evaluations show a success rate of 52%, compared to humans. We guess the time based on the most recently assigned date or the date of the article.	collections
821	Building A Discourse-Tagged Corpus In The Framework Of Rhetorical Structure Theory We describe our experience in developing a discourse-annotated corpus for community-wide use. Working in the framework of Rhetorical Structure Theory, we were able to create a large annotated resource with very high consistency, using a well-defined methodology and protocol. This resource is made publicly available through the Linguistic Data Consortium to enable researchers to develop empirically grounded, discourse-specific applications. In our Discourse Tree Bank only 26% of Contrast relations are indicated by cue phrases while in NTC-7 about 70% of Contrast were indicated by cue phrases. Our corpus contains 385 Wall Street Journal articles annotated following the Rhetorical Structure Theory.	Building A Discourse-Tagged Corpus In The Framework Of Rhetorical Structure Theory We share our experience in creating a collection of texts that are marked for their structure and meaning for others to use. Using Rhetorical Structure Theory, a method to analyze how parts of a text connect, we made a large collection with very consistent results by following a clear method and plan. This collection is available to everyone through the Linguistic Data Consortium so researchers can build applications that are based on real examples and focused on how texts are structured. In our Discourse Tree Bank, only 26% of Contrast (differences) relations are shown by signal words or phrases, while in NTC-7, about 70% of Contrast relations are shown this way. Our collection includes 385 Wall Street Journal articles marked according to Rhetorical Structure Theory.	situation
822	NLTK: The Natural Language Toolkit NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware. NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora. Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset. NLTK, the Natural Language Toolkit, is a suite of Python modules providing many NLP data types, processing tasks, corpus samples and readers, together with animated algorithms, tutorials and problem sets.	NLTK: The Natural Language Toolkit NLTK, the Natural Language Toolkit, is a collection of free software tools, guides, and practice exercises, offering easy-to-use resources for studying how computers understand human language. NLTK includes methods for both rule-based and data-driven language processing and connects to databases of language examples. Students enhance and modify existing tools, learn organized coding by example, and work with complex models from the start. NLTK, the Natural Language Toolkit, is a set of Python tools providing various language data types, processing tasks, sample texts, and reading materials, along with step-by-step examples, guides, and practice exercises.	signal words
823	Tuning Support Vector Machines For Biomedical Named Entity Recognition We explore the use of Support Vector Machines (SVMs) for biomedical named entity recognition. To make the SVM training with the available largest corpus, the GENIA corpus, tractable, we propose to split the non-entity class into sub-classes, using part-of-speech information. In addition, we explore new features such as word cache and the states of an HMM trained by unsupervised learning. Experiments on the GENIA corpus show that our class splitting technique not only enables the training with the GENIA corpus but also improves the accuracy. The proposed new features also contribute to improve the accuracy. We compare our SVM-based recognition system with a system using Maximum Entropy tagging method. For protein name recognition we achieve scores of 0.492, 0.664 and 0.565 for precision, recall and f-score respectively. We use a feature set containing lexical information, POS tags, affixes and their combinations in order to recognise and classify terms into a set of general biological classes used within the GENIA project (GENIA, 2003).	Tuning Support Vector Machines For Biomedical Named Entity Recognition We explore using Support Vector Machines (SVMs) to identify names in biomedical texts. To handle the large amount of data in the GENIA corpus (a collection of biomedical texts), we suggest breaking down the non-entity class (words that are not names) into smaller groups using part-of-speech (POS) information, which tells us the role of a word in a sentence. We also try new tools like a word cache (a storage for frequently used words) and states from a Hidden Markov Model (HMM), which is a statistical model learned without direct supervision. Tests on the GENIA corpus show that our method of splitting classes not only makes it possible to train with the GENIA corpus but also makes the results more accurate. The new tools we use also help improve accuracy. We compare our SVM system to another system that uses a method called Maximum Entropy tagging. For identifying protein names, we achieve scores of 0.492 for precision (correctness), 0.664 for recall (completeness), and 0.565 for f-score (a balance between precision and recall). We use a set of features that include lexical information (word meanings), POS tags, affixes (word parts like prefixes or suffixes), and their combinations to recognize and classify terms into general biological categories used in the GENIA project (GENIA, 2003).	Toolkit
824	Machine Transliteration Of Names In Arabic Texts We present a transliteration algorithm based on sound and spelling mappings using finite state machines. The transliteration models can be trained on relatively small lists of names. We introduce a new spelling-based model that is much more accurate than state-of-the-art phonetic-based models and can be trained on easier-to-obtain training data. We apply our transliteration algorithm to the transliteration of names from Arabic into English. We report on the accuracy of our algorithm based on exact-matching criterion and based on human-subjective evaluation. We also compare the accuracy of our system to the accuracy of human translators. We transliterate named entities in Arabic text to English by combining phonetic-based and spelling-based models, and re-ranking candidates with full-name web counts, named entities co-reference, and contextual web counts. We show that the use of outside linguistic resources such as WWW counts of transliteration candidates can greatly boost transliteration accuracy. Out spelling-based model directly maps English letter sequences into Arabic letter sequences with associated probability that are trained on a small English/Arabic name list without the need for English pronunciations.	Machine Transliteration Of Names In Arabic Texts We present a method for changing names from one language to another using sound and spelling rules with a simple machine process. This method can be learned using relatively small lists of names. We introduce a new spelling-focused method that is more accurate than the latest models that focus on sounds and can be trained with data that is easier to obtain. We use our method to change names from Arabic to English. We measure how accurate our method is by checking if it matches exactly and by asking people for their opinions. We also compare how accurate our method is with how accurate human translators are. We change names in Arabic text to English by combining methods that focus on sounds and spellings, and by re-ranking options using web data, name references, and context. We show that using extra language resources, like web data counts, can greatly improve accuracy. Our spelling-focused method directly changes English letters to Arabic letters and uses probability, which is learned from a small list of English and Arabic names, without needing to know how words are pronounced in English.	project
825	Unsupervised Discovery Of Morphemes We present two methods for unsupervised segmentation of words into morpheme-like units. The model utilized is especially suited for languages with a rich morphology, such as Finnish. The first method is based on the Minimum Description Length (MDL) principle and works online. In the second method, Maximum Likelihood (ML) optimization is used. The quality of the segmentations is measured using an evaluation method that compares the segmentations produced to an existing morphological analysis. Experiments on both Finnish and English corpora show that the presented methods perform well compared to a current state-of-the-art system. Our method is based on jointly minimizing the size of the morph codebook and the encoded size of all the word forms using the minimum description length MDL cost function.	Unsupervised Discovery Of Morphemes We introduce two ways to automatically break down words into smaller parts called morpheme-like units without needing pre-labeled data. The approach is especially good for languages with complex word structures, like Finnish. The first way uses a principle called Minimum Description Length (MDL) and works as it goes. The second way uses a technique called Maximum Likelihood (ML) optimization. We check how good the word breakdowns are by comparing them to existing word structure analyses. Tests on Finnish and English text collections show that our methods work well compared to a leading current system. Our approach involves reducing both the list of word parts and the size of all word forms using the MDL cost function, which helps in finding the simplest explanation for the word structures.	matches
826	Building A Sense Tagged Corpus With Open Mind Word Expert Open Mind Word Expert is an implemented active learning system for collecting word sense tagging from the general public over the Web. It is available at http://teach-computers.org. We expect the system to yield a large volume of high-quality training data at a much lower cost than the traditional method of hiring lexicographers. We thus propose a Senseval-3 lexical sample activity where the training data is collected via Open Mind Word Expert. If successful, the collection process can be extended to create the definitive corpus of word sense information. Finally, in an effort related to the Wikipedia collection process, we implemente the Open Mind Word Expert system for collecting sense annotations from volunteer contributors over the Web. we presented another interesting proposal which turns to Web users to produce sense-tagged corpora.	Building A Sense Tagged Corpus With Open Mind Word Expert Open Mind Word Expert is a system that uses active learning, which means it learns by asking questions, to collect word meanings from people on the internet. It can be found at http://teach-computers.org. We hope this system will gather a large amount of good quality training data for understanding word meanings at a much cheaper price than the usual way of paying experts called lexicographers. Therefore, we suggest an activity called Senseval-3, where we gather training data using Open Mind Word Expert. If it works well, we can use this process to create a complete collection of word meanings. Lastly, similar to how Wikipedia gathers information, we use the Open Mind Word Expert system to collect word meaning details from volunteers online. We introduced another interesting idea that asks internet users to help create collections of words with their meanings.	analyses
827	Learning A Translation Lexicon From Monolingual Corpora This paper presents work on the task of constructing a word-level translation lexicon purely from unrelated mono-lingual corpora. We combine various clues such as cognates, similar context, preservation of word similarity, and word frequency. Experimental results for the construction of a German-English noun lexicon are reported. Noun translation accuracy of 39% scored against a parallel test corpus could be achieved. We automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts.	Learning A Translation Lexicon From Monolingual Corpora This paper discusses how to create a dictionary for translating words using only separate collections of texts in one language. We use different hints like words that look similar (cognates), words used in similar situations, keeping word similarities, and how often words appear. We tested this by creating a list of German-English nouns. We achieved a 39% accuracy in translating nouns when checked against a matching test set. We automatically create the first basic dictionary using features like words with the same spelling (cognates) and similar word usage.	another
828	Improvements In Automatic Thesaurus Extraction The use of semantic resources is common in modern NLP systems, but methods to extract lexical semantics have only recently begun to perform well enough for practical use. We evaluate existing and new similarity metrics for thesaurus extraction, and experiment with the trade-off between extraction performance and efficiency. We propose an approximation algorithm, based on canonical attributes and coarse and fine-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty. We show that synonymy extraction for lexical semantic resources using distributional similarity produces continuing gains in accuracy as the volume of input data increases. We demonstrate that dramatically increasing the quantity of text used to extract contexts significantly improves synonym quality. We find the JACCARD measure and the TTEST weight to have the best performance in our comparison of distance measures.	Improvements In Automatic Thesaurus Extraction The use of semantic resources (tools that understand word meanings) is common in modern NLP (Natural Language Processing) systems, but methods to extract lexical semantics (word meanings) have only recently begun to perform well enough for practical use. We evaluate existing and new similarity metrics (ways to measure how alike something is) for thesaurus extraction, and experiment with the trade-off between extraction performance and efficiency (how well it works versus how fast it works). We propose an approximation algorithm (a simplified method), based on canonical attributes (standard features) and coarse and fine-grained matching (broad and detailed matching), that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty (a slight decrease in how well it works). We show that synonymy extraction (finding words with similar meanings) for lexical semantic resources using distributional similarity (comparing how words are used in context) produces continuing gains in accuracy as the volume of input data increases. We demonstrate that dramatically increasing the quantity of text used to extract contexts significantly improves synonym quality (better matching words with similar meanings). We find the JACCARD measure and the TTEST weight to have the best performance in our comparison of distance measures (ways to measure similarity).	English nouns
829	Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random fields (CRFs). The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates. We describe theory justifying the algorithms through a modification of the proof of convergence of the perceptron algorithm for classification problems. We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger. We describe how the voted perceptron can be used to train maximum-entropy style taggers and also give a discussion of the theory behind the perceptron algorithm applied to ranking tasks. Voted perceptron training attempts to minimize the difference between the global feature vector for a training instance and the same feature vector for the best-scoring labeling of that instance according to the current model.	Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms We explain new methods for teaching tagging models, which are another option instead of maximum-entropy models or conditional random fields (CRFs). These methods use Viterbi decoding, a way to look at training examples, together with simple step-by-step updates. We explain the reasoning behind the methods by changing the proof that shows how the perceptron algorithm (a type of learning algorithm) works for sorting problems. We show test results on labeling parts of speech and identifying base noun phrases, and in both cases, our methods work better than those for a maximum-entropy tagger. We explain how the voted perceptron can be used to teach maximum-entropy style taggers and discuss the ideas behind using the perceptron algorithm for ranking tasks. Voted perceptron training tries to reduce the gap between the overall feature list for a training example and the same list for the best possible label for that example according to the current model.	distributional similarity
830	An Empirical Evaluation Of Knowledge Sources And Learning Algorithms For Word Sense Disambiguation In this paper, we evaluate a variety of knowledge sources and supervised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data. Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations. The learning algorithms evaluated include Support Vector Machines (SVM), Naive Bayes, AdaBoost, and decision tree algorithms. We present empirical results showing the relative contribution of the component knowledge sources and the different learning algorithms. In particular, using all of these knowledge sources and SVM (i.e., a single learning algorithm) achieves accuracy higher than the best official scores on both SENSEVAL-2 and SENSEVAL-1 test data. Our feature set consists of the following four types: local context n-grams of nearby words, global context from all the words in the given context, parts-of-speech n-grams of nearby words and syntactic information obtained from parser output.	An Empirical Evaluation Of Knowledge Sources And Learning Algorithms For Word Sense Disambiguation In this paper, we assess different information sources and teaching methods used by computers to understand the meaning of words in context, using data from SENSEVAL-2 and SENSEVAL-1 competitions. Our information sources include the type of words next to the target word (part-of-speech), individual nearby words, common word pairings, and the grammatical relationships between words. The computer learning methods we tested include Support Vector Machines (SVM), Naive Bayes, AdaBoost, and decision tree techniques. We share results showing how each information source and learning method contributes to understanding word meanings. Notably, using all information sources together with SVM (a type of computer learning method) leads to better accuracy than the best official results from both SENSEVAL-2 and SENSEVAL-1 data. Our features, or pieces of information we use, include: small groups of nearby words (local context), all words in the context (global context), groups of word types (parts-of-speech), and grammatical details from sentence analysis.	training
831	Thumbs Up? Sentiment Classification Using Machine Learning Techniques We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques deflnitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classiflcation, and support vector machines) do not perform as well on sentiment classiflcation as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classiflcation problem more challenging. We collect reviews form a movie database and rate them as positive, negative or neutral based on the training given by the reviewer. We suggest that term-based models perform better than the frequency-based alternatives.	Thumbs Up? Sentiment Classification Using Machine Learning Techniques We look at the challenge of sorting documents by their overall feeling or mood, like deciding if a review is good or bad, instead of by topic. When we use movie reviews as examples, we discover that typical computer methods for learning (machine learning) do a much better job than the basic methods made by people. However, the three computer methods we used (which are Naive Bayes, maximum entropy classification, and support vector machines) are not as good at identifying feelings as they are at sorting topics. We finish by looking into reasons why identifying feelings is harder. We gather movie reviews and label them as good, bad, or neutral based on the reviewer's guidance. We propose that models focusing on individual words do better than those that count word appearances.	showing
832	A Phrase-Based Joint Probability Model For Statistical Machine Translation We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora. Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4. We propose a joint probability model which searches the phrase alignment space, simultaneously learning translations lexicons for words and phrases without consideration of potentially suboptimal word alignments and heuristics for phrase extraction.	A Phrase-Based Joint Probability Model For Statistical Machine Translation We introduce a joint probability model for statistical machine translation that automatically figures out word and phrase matches from bilingual text collections. Translations made using parameters from this joint model are more precise than those using IBM Model 4. Our joint probability model explores the phrase alignment space, simultaneously learning translation dictionaries for words and phrases without worrying about possibly less-than-perfect word matches and shortcuts for phrase extraction.	entropy
833	Generation Of Word Graphs In Statistical Machine Translation The paper describes a natural language based expert system route advisor for the public bus transport in Trondheim, Norway. The system is available on the Internet, and has been intstalled at the bus company's web server since the beginning of 1999. The system is bilingual, relying on an internal language independent logic representation. In between the question and the answer is a process of lexical analysis, syntax analysis, semantic analysis, pragmatic reasoning and database query processing. One could argue that the information content could be solved by an interrogation, whereby the customer is asked to produce 4 items: station of departure, station of arrival, earliest departure time and/or latest arrival time. We generate word graphs for a bottom-top search with the IBM constraints. A word graph is a weighted directed acyclic graph in which each node represents a partial translation hypothesis and each edge is labelled with a word of the target sentence and is weighted according to the scores given by the model.	Generation Of Word Graphs In Statistical Machine Translation The paper talks about a smart system that helps people in Trondheim, Norway, find bus routes using natural language. You can use this system on the Internet, and it has been on the bus company's website since 1999. The system works in two languages and uses a logic system that doesn't depend on any specific language. Between the user asking a question and getting an answer, the system goes through steps like understanding words, sentence structure, meaning, practical reasoning, and searching a database. It can be said that the needed information can be obtained by asking the customer four things: where they are leaving from, where they are going, the earliest they can leave, and/or the latest they can arrive. We create word graphs to help search from the bottom up, using IBM's rules. A word graph is a special kind of diagram where each point represents a part of the possible translation, and each line has a word from the sentence we are translating into, and the lines are given weights or scores by the model.	Probability
834	A Bootstrapping Method For Learning Semantic Lexicons Using Extraction Pattern Contexts This paper describes a bootstrapping algorithm called Basilisk that learns high-quality semantic lexicons for multiple categories. Basilisk begins with an unannotated corpus and seed words for each semantic category, which are then bootstrapped to learn new words for each category. Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts. We evaluate Basilisk on six semantic categories. The semantic lexicons produced by Basilisk have higher precision than those produced by previous techniques, with several categories showing substantial improvement. We learn multiple semantic categories simultaneously, relying on the assumption that a word cannot belong to more than one semantic category.	A Bootstrapping Method For Learning Semantic Lexicons Using Extraction Pattern Contexts This paper explains a step-by-step method called Basilisk that helps create high-quality lists of words with similar meanings (semantic lexicons) for different groups. Basilisk starts with a large collection of texts that are not labeled and a few example words for each group, which it uses to find new words for each group. Basilisk guesses the meaning group of a word by looking at a lot of information from different ways the word is used in sentences. We test Basilisk on six groups of similar-meaning words. The word lists created by Basilisk are more accurate than those made by older methods, with some groups showing a big improvement. We learn many groups of similar-meaning words at once, based on the idea that a word can't belong to more than one group.	point represents
835	Phrasal Cohesion And Statistical Machine Translation There has been much interest in using phrasal movement to improve statistical machine translation. We explore how well phrases cohere across two languages, specifically English and French, and examine the particular conditions under which they do not. We demonstrate that while there are cases where coherence is poor, there are many regularities which can be exploited by a statistical machine translation system. We also compare three variant syntactic representations to determine which one has the best properties with respect to cohesion. We measure phrasal cohesion in gold standard alignments by counting crossings. We compare tree-bank parser style analyses, a variant with flattened VPs and dependency structures.	Phrasal Cohesion And Statistical Machine Translation There has been a lot of interest in using the way phrases move to make statistical machine translation better. We look into how well phrases stick together across two languages, specifically English and French, and check the specific situations where they don't. We show that even though sometimes phrases don't stick together well, there are many patterns that a statistical machine translation system can use. We also compare three different grammar structures to see which one keeps phrases together the best. We measure how well phrases stick together in the most accurate alignments by counting how often they cross over each other. We compare analyses from a type of grammar that uses tree-like structures, a version with simplified verb phrases (VPs), and structures that show how words depend on each other.	different
836	Efficient Deep Processing Of Japanese We present a broad coverage Japanese grammar written in the HPSG formalism with MRS semantics. The grammar is created for use in real world applications, such that robustness and performance issues play an important role. It is connected to a POS tagging and word segmentation tool. This grammar is being developed in a multilingual context, requiring MRS structures that are easily comparable across languages. Our hand-crafted Japanese HPSG grammar, JACY, provides semantic information as well as linguistically motivated analysis of complex constructions.	Efficient Deep Processing Of Japanese We present a wide-ranging Japanese grammar written in the HPSG formalism (a framework for understanding sentence structure) with MRS semantics (a way to represent meanings). The grammar is made for real-world use, so being strong and fast is important. It is linked to a tool that tags parts of speech (like nouns and verbs) and splits words. This grammar is being developed to work with multiple languages, needing MRS structures that can be easily compared between languages. Our carefully crafted Japanese HPSG grammar, JACY, gives meaning information and analyzes complex sentence structures in a way that makes sense linguistically.	patterns
837	The Grammar Matrix: An Open-Source Starter-Kit For The Rapid Development Of Cross-Linguistically Consistent Broad-Coverage Precision Grammars The grammar matrix is an open-source starter-kit for the development of broad-coverage HPSGs. By using a type hierarchy to represent cross-linguistic generalizations and providing compatibility with other open-source tools for grammar engineering, evaluation, parsing and generation, it facilitates not only quick start-up but also rapid growth towards the wide coverage necessary for robust natural language processing and the precision parses and semantic representations necessary for natural language understanding. Our LinGO Grammar Matrix project is both a repository of reusable linguistic knowledge and a method of delivering this knowledge to a user in the form of an extensible precision implemented grammar.	The Grammar Matrix: An Open-Source Starter-Kit For The Rapid Development Of Cross-Linguistically Consistent Broad-Coverage Precision Grammars The grammar matrix is a free-to-use kit that helps create detailed language rules using HPSGs (Head-driven Phrase Structure Grammars). It uses a type hierarchy (a system to organize language features) to show common patterns across different languages and works with other free tools for building, testing, understanding, and creating language rules. This helps users start quickly and expand to cover more language details needed for strong language processing and understanding. Our LinGO Grammar Matrix project is a collection of reusable language knowledge and a way to give users this knowledge in a form that can be expanded and used to create precise language rules.	sentence
838	The Parallel Grammar Project We report on the Parallel Grammar (ParGram) project which uses the XLE parser and grammar development platform for six languages: English, French, German, Japanese, Norwegian, and Urdu. The ParGram English LFG is a hand-crafter broad-coverage grammar develope d with the XLE platform.	The Parallel Grammar Project We discuss the Parallel Grammar (ParGram) project that uses a tool called the XLE parser and grammar development platform to work with six languages: English, French, German, Japanese, Norwegian, and Urdu. The ParGram English LFG is a detailed set of language rules created manually using the XLE platform.	languages
839	Japanese Dependency Analysis Using Cascaded Chunking In this paper, we propose a new statistical Japanese dependency parser using a cascaded chunking model. Conventional Japanese statistical dependency parsers are mainly based on a probabilistic model, which is not always efficient or scalable. We propose a new method that is simple and efficient, since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side. Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency. Our cascaded chunking model does not require the probabilities of dependencies and parses a sentence deterministically.	Japanese Dependency Analysis Using Cascaded Chunking In this paper, we introduce a new way to analyze Japanese sentence structure using a step-by-step grouping technique. Traditional methods for analyzing Japanese sentences rely on guessing models, which are not always fast or able to handle large amounts of data. We suggest a new approach that is simple and effective because it processes a sentence by only deciding if the current part connects to the part directly next to it on the right. Tests using a set of sentences from Kyoto University show that our method works better than older systems and makes understanding and learning sentences faster. Our step-by-step grouping model doesn't need guessing the connections between words and processes a sentence in a straightforward manner.	Japanese
840	A Comparison Of Algorithms For Maximum Entropy Parameter Estimation Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing. However, the flexibility of ME models is not without cost. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters. In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Surprisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limited-memory variable metric algorithm outperformed the other choices. We introduce the open-source Toolkit for Advanced Discriminative Model which uses a limited-memory variable metric.	A Comparison Of Algorithms For Maximum Entropy Parameter Estimation Conditional maximum entropy (ME) models are a general machine learning tool that has been used successfully in different areas like computer vision and econometrics, and for many types of sorting tasks in language processing. However, ME models' flexibility comes with a cost. Although estimating parameters (key numbers) for ME models is simple in theory, in real-world language tasks, ME models are usually very large and may contain thousands of adjustable parameters. In this paper, we look at several methods for estimating these parameters, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Surprisingly, the commonly used iterative scaling methods work poorly compared to others, and in all tests, a limited-memory variable metric method did better than the rest. We present the open-source Toolkit for Advanced Discriminative Model, which uses this efficient limited-memory variable metric method.	guessing models
841	Introduction To The CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition We describe the CoNLL-2002 shared task: language-independent named entity recognition. We give background information on the data sets and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance. We focus on named entity recognition for Spanish and Dutch.	Introduction To The CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition We explain the CoNLL-2002 shared task, which is about recognizing named entities (like names of people, places, or organizations) in a way that works for any language. We provide some background on the data used and how we measure success, give a general look at the different systems that participated, and talk about how well they did. We specifically focus on recognizing named entities in Spanish and Dutch.	econometrics
842	Inducing Translation Lexicons Via Diverse Similarity Measures And Bridge Languages This paper presents a method for inducing translation lexicons between two distant languages without the need for either parallel bilingual corpora or a direct bilingual seed dictionary. The algorithm successfully combines temporal occurrence similarity across dates in news corpora, wide and local cross-language context similarity, weighted Levenshtein distance, relative frequency and burstiness similarity measures. These similarity measures are integrated with the bridge language concept under a robust method of classifier combination for both the Slavic and Northern Indian language families. We induce translation lexicons for languages without common parallel corpora using a bridge language that is related to the target languages. We create bag-of-words context vectors around both the source and target language words and then project the source vectors into the target space via the current small translation dictionary.	Inducing Translation Lexicons Via Diverse Similarity Measures And Bridge Languages This paper introduces a way to create translation dictionaries for two very different languages without needing paired language texts or an initial bilingual dictionary. The method effectively uses patterns of word appearance over time in news articles, compares word usage in different languages both broadly and in specific contexts, measures differences in spelling using a technique called weighted Levenshtein distance, and looks at how often and suddenly words appear. These techniques are combined using a bridge language, which is a language related to the ones being translated, and a strong method of combining different classification approaches for both Slavic and Northern Indian languages. We build translation dictionaries for languages that don't share common texts by using a bridge language related to the target languages. We create collections of words' contexts around both the starting and target language words and then convert the starting language's word context into the target language's context using a small existing translation dictionary.	measure
843	An Evaluation Exercise For Word Alignment This paper presents the task definition, resources, participating systems, and comparative results for the shared task on word alignment, which was organized as part of the HLT/NAACL 2003 Workshop on Building and Using Parallel Texts. The shared task included Romanian-English and English-French sub-tasks, and drew the participation of seven teams from around the world. We present a small dataset of 447 pairs of non-overlapping sentences which can be used to evaluate the performance of word-alignment systems.	An Evaluation Exercise For Word Alignment This paper explains the task, materials, systems that took part, and the comparison results for the shared task on word alignment, which was organized as part of the HLT/NAACL 2003 Workshop on Building and Using Parallel Texts. The shared task included sub-tasks for Romanian-English and English-French and had seven teams from around the world join in. We provide a small set of 447 pairs of sentences that do not overlap, which can be used to check how well word-alignment systems work.	Inducing
844	Learning Subjective Nouns Using Extraction Pattern Bootstrapping We explore the idea of creating a subjectivity classifier that uses lists of subjective nouns learned by bootstrapping algorithms. The goal of our research is to develop a system that can distinguish subjective sentences from objective sentences. First, we use two bootstrapping algorithms that exploit extraction patterns to learn sets of subjective nouns. Then we train a Naive Bayes classifier using the subjective nouns, discourse features, and subjectivity clues identified in prior research. The bootstrapping algorithms learned over 1000 subjective nouns, and the subjectivity classifier performed well, achieving 77% recall with 81% precision. We use manually derived pattern templates to extract subjective nouns by bootstrapping. We mine subjective nouns from unannotated texts with two bootstrapping algorithms that exploit lexico-syntactic extraction patterns and manually-selected subjective seeds.	Learning Subjective Nouns Using Extraction Pattern Bootstrapping We explore the idea of creating a tool that can tell if a sentence expresses personal opinions (subjective) or facts (objective) by using lists of opinion-based words (subjective nouns) learned through a process called bootstrapping. Our research goal is to build a system that can tell the difference between sentences based on opinions and sentences based on facts. First, we use two special methods (bootstrapping algorithms) that use patterns to find and learn lists of opinion-based words. Then, we train a simple computer program (Naive Bayes classifier) using these lists of words, conversation features, and hints about opinions found in earlier studies. The special methods found over 1000 opinion-based words, and the tool worked well, correctly identifying 77% of opinion sentences and doing so accurately 81% of the time. We use hand-made pattern guides to find opinion-based words with bootstrapping. We search for opinion-based words in texts that haven't been labeled with two bootstrapping methods that use patterns involving word choice and order and starting points that we choose by hand.	paper explains
845	Unsupervised Personal Name Disambiguation This paper presents a set of algorithms for distinguishing personal names with multiple real referents in text, based on little or no supervision. The approach utilizes an unsupervised clustering technique over a rich feature space of biographic facts, which are automatically extracted via a language-independent bootstrapping process. The induced clustering of named entities are then partitioned and linked to their real referents via the automatically extracted biographic data. Performance is evaluated based on both a test set of hand-labeled multi-referent personal names and via automatically generated pseudonames. We extract biographic facts such as date or place of birth, occupation, relatives among others to help resolve ambiguous names of people.	Unsupervised Personal Name Disambiguation This paper presents a set of methods to tell apart people's names that refer to more than one person in text, with little or no guidance. The method uses a grouping technique that doesn't need prior labeling, based on detailed information about people, which is automatically gathered using a process that doesn't depend on the language. The grouped names are then divided and connected to the right people using the gathered information. We check how well this works using both a set of names that have been manually labeled and fake names created automatically. We gather details like birth date or place, job, and family members to help clear up confusion about people's names.	accurately
846	Bootstrapping POS-Taggers Using Unlabelled Data This paper investigates booststrapping part-of-speech taggers using co-training, in which two taggers are iteratively re-trained on each other's output. Since the output of the taggers is noisy, there is a question of which newly labelled examples to add to the training set. We investigate selecting examples by directly maximising tagger agreement on unlabelled data, a method which has been theoretically and empirically motivated in the co-training literature. Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably out-performs self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. We report positive results with little labeled training data but negative results when the amount of labeled training data increases. We define self-training as a procedure in which a tagger is retrained on its own labeled cache at each round.	Bootstrapping POS-Taggers Using Unlabelled Data This paper looks into improving part-of-speech taggers (tools that label words in a sentence as nouns, verbs, etc.) by using co-training. In co-training, two taggers (labeling tools) are retrained using each other's results over several rounds. Since the taggers' results can be messy, we need to decide which new examples to include for learning. We explore choosing examples by increasing how much the taggers agree with each other on data that hasn't been labeled yet. This approach has support from both theory and previous studies. Our findings show that when taggers agree more with each other, it can greatly enhance their performance, especially when starting with a small amount of labeled data. More findings reveal that this method is much better than self-training, where a tagger improves using only its own previous results. However, we also find that just retraining with all the new labeled data can sometimes give similar results as the more complex agreement method, but with much less computer work. We see good results with little labeled data but not so good when we have more labeled data. Self-training is defined here as retraining a tagger with its own labeled examples at each step.	names created
847	Introduction To The CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.	Introduction To The CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition We explain the CoNLL-2003 shared task, which is about recognizing names, places, or organizations in text without being limited to a specific language. We provide background information on the data sets used (English and German) and describe how the systems were tested, give a general overview of the systems that participated in the task and talk about how well they performed.	include
848	Language Independent NER Using A Maximum Entropy Tagger Named Entity Recognition (NER) systems need to integrate a wide variety of information for optimal performance. This paper demonstrates that a maximum entropy tagger can effectively encode such information and identify named entities with very high accuracy. The tagger uses features which can be obtained for a variety of languages and works effectively not only for English, but also for other languages such as German and Dutch. We condition the label of a token at a particular position on the label of the most recent previous instance of that same token in a prior sentence of the same document. Our named entity recogniser is run on pos-tagged and chunked documents in the corpus to identify and extract named entities as potential topics.	Language Independent NER Using A Maximum Entropy Tagger Named Entity Recognition (NER) systems need to combine a lot of different information to work well. This paper shows that a maximum entropy tagger, which is a type of software, can effectively use this information to find named entities (like names of people or places) with high accuracy. The tagger uses features that can be found in many languages and works well not just for English, but also for languages like German and Dutch. We decide the label (or name) of a word based on the label of the last time we saw that word in a previous sentence in the same document. Our named entity recognizer is used on documents that are already analyzed for parts of speech and sentence structure to find and pick out named entities as possible topics.	systems
849	Named Entity Recognition Through Classifier Combination This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions. When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data. We test different methods for combining the results of four systems and found that robust risk minimization works best.	Named Entity Recognition Through Classifier Combination This paper presents a method to recognize named entities (like names of people, places, or organizations) by combining the results of four different types of computer programs (classifiers). These classifiers are a strong linear classifier, maximum entropy, transformation-based learning, and a hidden Markov model. When we don't use extra resources like gazetteers (which are lists of names, places, etc.) or other training tools, the combined system achieves a performance score of 91.6F on the English test data. However, when we include these lists and use more general data to train the system, we can reduce the error rate by 15 to 21% on the English data. We tried different ways to combine the results from the four systems and found that a method called robust risk minimization gives the best results.	software
850	Named Entity Recognition With Character-Level Models We discuss two named-entity recognition models which use characters and character n-grams either exclusively or as an important part of their data representation. The first model is a character-level HMM with minimal context information, and the second model is a maximum-entropy conditional markov model with substantially richer context features. Our best model achieves an overall F1 of 86.07% on the English test data (92.31% on the development data). This number represents a 25% error reduction over the same model without word-internal (substring) features. We find that the introduction of character n-gram features improved the overall F1 score by over 20%.	Named Entity Recognition With Character-Level Models We discuss two named-entity recognition models which use characters and character n-grams (small groups of characters) either exclusively or as an important part of their data representation. The first model is a character-level Hidden Markov Model (HMM) with minimal context information, and the second model is a maximum-entropy conditional Markov model with much richer context features. Our best model achieves an overall F1 score of 86.07% on the English test data (92.31% on the development data). This number represents a 25% error reduction over the same model without word-internal (substring) features. We find that the introduction of character n-gram features improved the overall F1 score by over 20%.	minimization gives
851	Early Results For Named Entity Recognition With Conditional Random Fields Feature Induction And Web-Enhanced Lexicons	Early Results For Named Entity Recognition With Conditional Random Fields Feature Induction And Web-Enhanced Lexicons Named Entity Recognition (NER) is a way to identify important information like names of people, places, or organizations in text. The study uses a method called Conditional Random Fields (CRF), which is a type of machine learning that helps in finding patterns in data. Feature induction in CRF is about automatically finding the best characteristics or features in the data to improve accuracy. The study also uses web-enhanced lexicons, which are lists of words or phrases collected from the internet, to help the system recognize more entities accurately.	achieves
852	Hedge Trimmer: A Parse-And-Trim Approach To Headline Generation This paper presents Hedge Trimmer, a HEaDline GEneration system that creates a headline for a newspaper story using linguistically-motivated heuristics to guide the choice of a potential headline. We present feasibility tests used to establish the validity of an approach that constructs a headline by selecting words in order from a story. In addition, we describe experimental results that demonstrate the effectiveness of our linguistically-motivated approach over a HMM-based model, using both human evaluation and automatic metrics for comparing the two approaches. Our approach focuses on extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length.	Hedge Trimmer: A Parse-And-Trim Approach To Headline Generation This paper introduces Hedge Trimmer, a system for making newspaper headlines using smart language-based rules to pick a possible headline. We show tests that prove our method works by choosing words straight from a story to make a headline. We also share test results showing our smart language method works better than an HMM-based model (a different method), using both opinions from people and automatic tools to compare them. Our method focuses on picking one or two important sentences from the document and changing them using language rules to make the summary shorter.	improve accuracy
853	Use Of Deep Linguistic Features For The Recognition And Labeling Of Semantic Arguments We use deep linguistic features to predict semantic roles on syntactic arguments, and show that these perform considerably better than surface-oriented features. We also show that predicting labels from a light-weight parser that generates deep syntactic features performs comparably to using a full parser that generates only surface syntactic features. We argue that deep linguistic features harvested from FrameNet are beneficial for the successful assignment of PropBank roles to constituents. We use LTAG-based decomposition of parse trees for SRL. Instead of using the typical parse tree features used in SRL models, we use the path within the elementary tree from the predicate to the constituent argument.	Use Of Deep Linguistic Features For The Recognition And Labeling Of Semantic Arguments We use deep linguistic features, which are detailed language characteristics, to predict roles in sentences, and these work much better than simpler features. We also show that using a simple parser that finds these detailed features works almost as well as using a full parser that only finds basic features. We believe that detailed language features from FrameNet, a database of word meanings, help to successfully assign PropBank roles, which are specific roles in sentences, to parts of sentences. We use a specific method of breaking down sentence structures for Semantic Role Labeling (SRL), which is identifying what parts of a sentence do. Instead of using the usual sentence structure features in SRL, we use the path in a basic tree from the main action to the part of the sentence it affects.	choosing words
854	Identifying Semantic Roles Using Combinatory Categorial Grammar We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar. This system performs at least as well as a system based on a traditional Treebank parser, and outperforms it on core argument roles. We find that using features extracted from a Combinatory Categorical Grammar representation improves semantic labeling performance on core arguments.	Identifying Semantic Roles Using Combinatory Categorial Grammar We introduce a system that automatically identifies PropBank-style semantic roles, which are different parts played by words in a sentence, using results from a statistical parser (a tool that breaks down sentences into parts) for Combinatory Categorial Grammar (a type of grammar). This system works as well as one based on a traditional Treebank parser (another type of sentence analyzing tool) and does even better with main parts of a sentence. We discovered that using details from a Combinatory Categorial Grammar setup makes recognizing key parts of sentences more accurate.	linguistic
855	A General Framework For Distributional Similarity We present a general framework for distributional similarity based on the concepts of precision and recall. Different parameter settings within this framework approximate different existing similarity measures as well as many more which have, until now, been unexplored. We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns. We propose a general framework for distributional similarity that consists of notions of precision and recall.	A General Framework For Distributional Similarity We present a general framework for distributional similarity based on the concepts of precision (how accurate the results are) and recall (how many relevant results are found). Different settings within this framework can mimic existing ways to measure similarity, as well as create new ways that haven't been tried yet. We demonstrate that the best settings perform better than two current top methods for measuring similarity in tests using common and rare nouns. We propose a general framework for distributional similarity that consists of notions of precision and recall.	Combinatory
856	Learning Extraction Patterns For Subjective Expressions This paper presents a bootstrapping process that learns linguistically rich extraction patterns for subjective (opinionated) expressions. High-precision classifiers label unannotated data to automatically create a large training set, which is then given to an extraction pattern learning algorithm. The learned patterns are then used to identify more subjective sentences. The bootstrapping process learns many subjective patterns and increases recall while maintaining high precision. We construct a high precision classifier for contiguous sentences using the number of strong and weak subjective words in current and nearby sentences. We introduce a bootstrapping method to learn subjective extraction patterns that match specific syntactic templates using a high-precision sentence-level subjectivity classifier and a large unannotated corpus.	Learning Extraction Patterns For Subjective Expressions This paper introduces a step-by-step process that teaches how to find detailed patterns for opinion-based expressions. Accurate tools label unmarked data to automatically create a large set of examples, which is then used by a learning algorithm to find these patterns. The patterns found are then used to spot more opinion-based sentences. This step-by-step process learns many opinion-based patterns and finds more of them while staying very accurate. We create a very accurate tool for continuous sentences by counting strong and weak opinion words in current and nearby sentences. We present a method to learn opinion-based patterns that fit certain sentence structures using a very accurate tool for sentence-level opinion detection and a large set of unmarked text.	settings
857	Towards Answering Opinion Questions: Separating Facts From Opinions And Identifying The Polarity Of Opinion Sentences Opinion question answering is a challenging task for natural language processing. In this paper, we discuss a necessary component for an opinion question answering system: separating opinions from fact, at both the document and sentence level. We present a Bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the significantly harder task of detecting opinions at the sentence level. We also present a first model for classifying opinion sentences as positive or negative in terms of the main perspective being expressed in the opinion. Results from a large collection of news stories and a human evaluation of 400 sentences are reported, indicating that we achieve very high performance in document classification (upwards of 97% precision and recall), and respectable performance in detecting opinions and classifying them at the sentence level as positive, negative, or neutral (up to 91% accuracy). At sentence level, we propose to classify opinion sentences as positive or negative in terms of the main perspective being expressed in opinionated sentences.	Towards Answering Opinion Questions: Separating Facts From Opinions And Identifying The Polarity Of Opinion Sentences Opinion question answering is a difficult task for computers that understand human language. In this paper, we talk about an important part of a system that answers opinion questions: telling the difference between opinions (what people think or feel) and facts (what is true), both for entire documents and individual sentences. We introduce a method called a Bayesian classifier to separate documents that mostly contain opinions, like opinion pieces, from regular news articles. We also explain three methods that don't need labeled data to solve the tougher task of finding opinions in sentences. Additionally, we introduce our first model to decide if opinion sentences are positive (good) or negative (bad) based on the main viewpoint being shared in the opinion. We share results from many news articles and a review by people of 400 sentences, showing that we are very accurate in classifying documents (over 97% accuracy) and quite good at finding and classifying opinions in sentences as positive, negative, or neutral (up to 91% accuracy). At the sentence level, we aim to sort opinion sentences as positive or negative based on the main viewpoint in those sentences.	Accurate tools
858	Improved Automatic Keyword Extraction Given More Linguistic Knowledge In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed. The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on statistics (such as term frequency and n-grams), a better result is obtained as measured by keywords previously assigned by professional indexers. In more detail, extracting NP-chunks gives a better precision than n-grams, and by adding the POS tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied. We propose a system for keyword extraction from abstracts that uses supervised learning with lexical and syntactic features, which proved to improve significantly over previously published results.	Improved Automatic Keyword Extraction Given More Linguistic Knowledge In this paper, experiments on automatic keyword extraction from summaries using a machine learning method that learns from examples are explained. The main idea is that by adding language understanding (like grammar details), instead of just using numbers (like how often a word appears and word groups), we get better results compared to keywords chosen by experts. Specifically, using noun phrases (NP-chunks) is more accurate than using simple word groups (n-grams), and by including the part of speech (POS) tags for words, we see a big improvement in results, no matter how the words are chosen. We suggest a system for pulling out keywords from summaries that uses learning from examples with word and grammar details, which has shown to be much better than past methods.	Opinion Sentences
859	Transliteration Of Proper Names In Cross-Lingual Information Retrieval We address the problem of transliterating English names using Chinese orthography in support of cross-lingual speech and text processing applications. We demonstrate the application of statistical machine translation techniques to "translate" the phonemic representation of an English name, obtained by using an automatic text-to-speech system, to a sequence of initials and finals, commonly used subword units of pronunciation for Chinese. We then use another statistical translation model to map the initial/final sequence to Chinese characters. We also present an evaluation of this module in retrieval of Mandarin spoken documents from the TDT corpus using English text queries. We adopt the noisy channel modeling framework.	Transliteration Of Proper Names In Cross-Lingual Information Retrieval We deal with the challenge of converting English names into Chinese writing to help with understanding speech and text in different languages. We show how using statistical machine translation methods can "translate" how an English name sounds into a series of sounds (called initials and finals) used in Chinese pronunciation. After that, we use another statistical translation model to turn these sounds into Chinese characters. We also test how well this method works by retrieving Mandarin spoken documents using English text searches. We use a method called the noisy channel model, which is a way to handle uncertain information.	machine learning
860	The First International Chinese Word Segmentation Bakeoff This paper presents the results from the ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff held in 2003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan. We give the motivation for having an international segmentation contest (given that there have been two within-China contests to date) and we report on the results of this first international contest, analyze these results, and make some recommendations for the future.	The First International Chinese Word Segmentation Bakeoff This paper shares the outcomes from the First International Chinese Word Segmentation Bakeoff, a competition sponsored by ACL-SIGHAN, held in 2003 and announced at the Second SIGHAN Workshop on Chinese Language Processing in Sapporo, Japan. We explain why there was a need for an international segmentation contest (since there had been two contests only within China before), and we provide the results of this first international contest, examine these results, and offer some suggestions for the future.	translation
861	Chinese Word Segmentation As LMR Tagging In this paper we present Chinese word segmentation algorithms based on the so-called LMR tagging. Our LMR taggers are implemented with the Maximum Entropy Markov Model and we then use Transformation-Based Learning to combine the results of the two LMR taggers that scan the input in opposite directions. Our system achieves F-scores of 95.9% and 91.6% on the Academia Sinica corpus and the Hong Kong City University corpus respectively. we describe for the first time the character classification approach for Chinese word segmentation, where each character is given a boundary tag denoting its relative position in a word.	Chinese Word Segmentation As LMR Tagging In this paper, we present methods for dividing Chinese text into words using something called LMR tagging. Our LMR taggers use a model called the Maximum Entropy Markov Model, and we combine the results with a method called Transformation-Based Learning to improve accuracy. Our system correctly segments words 95.9% of the time on a test from Academia Sinica and 91.6% on a test from Hong Kong City University. We also introduce a new method for Chinese word segmentation that classifies each character by giving it a tag to show its position in a word.	Workshop
862	HHMM-Based Chinese Lexical Analyzer ICTCLAS This document presents the results from Inst. of Computing Tech., CAS in the ACL SIGHAN-sponsored First International Chinese Word Segmentation Bake-off. The authors introduce the unified HHMM-based frame of our Chinese lexical analyzer ICTCLAS and explain the operation of the six tracks. Then provide the evaluation results and give more analysis. Evaluation on ICTCLAS shows that its performance is competitive. Compared with other system, ICTCLAS has ranked top both in CTB and PK closed track. In PK open track, it ranks second position. ICTCLAS BIG5 version was transformed from GB version only in two days; however, it achieved well in two BIG5 closed tracks. Through the first bakeoff, we could learn more about the development in Chinese word segmentation and become more confident on our HHMM-based approach. At the same time, we really find our problems during the evaluation. The bakeoff is interesting and helpful.	HHMM-Based Chinese Lexical Analyzer ICTCLAS This document shares the results from the Institute of Computing Technology, Chinese Academy of Sciences (CAS) in the First International Chinese Word Segmentation competition organized by ACL SIGHAN. The authors introduce the HHMM-based (a type of statistical model) framework of our Chinese word analyzer ICTCLAS and explain how it works across six different tests. Then they show the evaluation results and provide more analysis. Testing on ICTCLAS shows that it works well compared to others. Compared with other systems, ICTCLAS was ranked at the top in the CTB and PK closed track tests. In the PK open track test, it got second place. The ICTCLAS BIG5 version, which is a different text format, was created from the GB version in just two days and performed well in two BIG5 closed track tests. Through the first competition, we learned more about the progress in Chinese word separation and are more confident in our HHMM-based method. At the same time, we found some issues during the evaluation. The competition was fun and useful.	method called
864	Detecting A Continuum Of Compositionality In Phrasal Verbs We investigate the use of an automatically acquired thesaurus for measures designed to indicate the compositionality of candidate multiword verbs, specifically English phrasal verbs identified automatically using a robust parser. We examine various measures using the nearest neighbours of the phrasal verb, and in some cases the neighbours of the simplex counterpart and show that some of these correlate significantly with human rankings of compositionality on the test set. We also show that whilst the compositionality judgements correlate with some statistics commonly used for extracting multiwords, the relationship is not as strong as that using the automatically constructed thesaurus.	Detecting A Continuum Of Compositionality In Phrasal Verbs We explore using a computer-generated thesaurus to measure how much different multiword verbs (like phrasal verbs in English) are made up of their parts. We find these verbs using a reliable computer program. We look at different ways to measure this by checking words that are similar to the phrasal verb. Sometimes we also look at similar words for the simple version of the verb. We find that some of these methods match well with how humans rate these verbs in terms of being made up of their parts. We also find that while these human ratings do match some common statistics used to find multiword phrases, they match even better with the computer-generated thesaurus.	meaning
865	An Empirical Model Of Multiword Expression Decomposability This paper presents a construction-inspecific model of multiword expression decomposability based on latent semantic analysis. We use latent semantic analysis to determine the similarity between a multiword expression and its constituent words, and claim that higher similarities indicate greater decomposability. We test the model over English noun-noun compounds and verb-particles, and evaluate its correlation with similarities and hyponymy values in WordNet. Based on mean hyponymy over partitions of data ranked on similarity, we furnish evidence for the calculated similarities being correlated with the semantic relational content of WordNet. we studied vector extraction for phrases because they were interested in the decomposability of multi word expressions. we propose a LSA-based model for measuring the decomposability of MWEs by examining the similarity between them and their constituent words, with higher similarity indicating the greater decomposability.	An Empirical Model Of Multiword Expression Decomposability This paper introduces a specific way of understanding how multiword expressions (like phrases made of several words) can be broken down into their parts, using a method called latent semantic analysis (LSA), which helps understand the meaning of words. We use LSA to measure how similar a phrase is to the individual words it contains, suggesting that if they are more similar, the phrase can be broken down more easily. We test this idea on English phrases made of two nouns or a verb and a particle (like "pick up"), and see how well it matches with a tool called WordNet, which groups words by meaning. By looking at average meanings of word groups based on similarity, we show that these similarities match up with the meanings in WordNet. We looked into how to pull out meaning from phrases because we wanted to understand how phrases can be split into their parts. We suggest an LSA-based model to measure how phrases can be broken down by checking how similar they are to their individual words, with more similarity showing they can be broken down more.	multiword
866	Incrementality In Deterministic Dependency Parsing Deterministic dependency parsing is a robust and efficient approach to syntactic parsing of unrestricted natural language text. In this paper, we analyze its potential for incremental processing and conclude that strict incrementality is not achievable within this framework. However, we also show that it is possible to minimize the number of structures that require non-incremental processing by choosing an optimal parsing algorithm. This claim is substantiated with experimental evidence showing that the algorithm achieves incremental parsing for 68.9% of the input when tested on a random sample of Swedish text.	Incrementality In Deterministic Dependency Parsing Deterministic dependency parsing is a strong and fast method for analyzing the structure of sentences in any natural language text. In this paper, we look into its ability to process sentences bit by bit as they are read and find that it cannot fully do this in its current form. However, we also demonstrate that we can reduce the times it needs to stop and look at the whole sentence by picking the best parsing method. We back this up with tests showing that the method can handle sentences in parts for 68.9% of the time when we tried it on some Swedish text.	their parts
867	Senseval-3 Task: Automatic Labeling Of Semantic Roles The SENSEVAL-3 task to perform automatic labeling of semantic roles was designed to encourage research into and use of the FrameNet dataset. The task was based on the considerable expansion of the FrameNet data since the baseline study of automatic labeling of semantic roles by Gildea and Jurafsky. The FrameNet data provide an extensive body of "gold standard" data that can be used in lexical semantics research, as the basis for its further exploitation in NLP applications. Eight teams participated in the task, with a total of 20 runs. Discussions among participants during development of the task and the scoring of their runs contributed to a successful task. Participants used a wide variety of techniques, investigating many aspects of the FrameNet data. They achieved results showing considerable improvements from Gildea and Jurafsky’s baseline study. Importantly, their efforts have contributed considerably to making the complex FrameNet dataset more accessible. They have amply demonstrated that FrameNet is a substantial lexical resource that will permit extensive further research and exploitation in NLP applications in the future. we conduct an evaluation exercise in the Senseval-3 workshop.	Senseval-3 Task: Automatic Labeling Of Semantic Roles The SENSEVAL-3 task to perform automatic labeling of semantic roles was created to promote research and use of the FrameNet dataset. This task was built on the large increase in FrameNet data since the first study of automatic role labeling by Gildea and Jurafsky. The FrameNet data offer a large, reliable set of information ("gold standard") used in word meaning research and as a base for further use in language processing technology (NLP). Eight teams took part in the task, with a total of 20 attempts. Talks among participants during the task's development and evaluation of their attempts led to a successful task. Participants used many different methods, exploring various parts of the FrameNet data. They achieved results that greatly improved from Gildea and Jurafsky’s original study. Importantly, their work has helped make the complex FrameNet dataset easier to use. They have clearly shown that FrameNet is a significant resource for language study and will allow for much more research and use in NLP applications in the future. We conduct an evaluation exercise in the Senseval-3 workshop.	demonstrate
868	The Senseval-3 English Lexical Sample Task This paper presents the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was organized as part of the SENSEVAL-3 evaluation exercise. The task drew the participation of 27 teams from around the world, with a total of 47 systems.	The Senseval-3 English Lexical Sample Task This paper explains the task details, materials used, teams involved, and comparison of results for the English word meaning task, which was part of the SENSEVAL-3 testing event. The task involved 27 teams from different countries, with 47 systems in total.	applications
869	The English All-Words Task We describe our experience in preparing the sense-tagged corpus used in the English all-words task and we tabulate the scores.	The English All-Words Task We talk about our experience in getting the sense-tagged corpus (a collection of texts where words are labeled with their meanings) ready for the English all-words task, and we present the scores in a table.	teams involved
894	TextRank: Bringing Order Into Texts In this paper, we introduce TextRank - a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications. In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks. we propose TextRank, which is one of the most well-known graph based approaches to key phrase extraction. we propose the TextRank model to rank key words based on the co-occurrence links between words.	TextRank: Bringing Order Into Texts In this paper, we introduce TextRank - a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications. In particular, we propose two new methods that don't require pre-labeled data for finding important words and sentences, and show that the results are as good or better than results from other well-known tests. We propose TextRank, which is one of the most well-known graph-based methods for extracting key phrases. We propose the TextRank model to rank important words based on how often they appear together with other words.	bootstrapping
870	ROUGE: A Package For Automatic Evaluation Of Summaries ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluations. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.	ROUGE: A Package For Automatic Evaluation Of Summaries ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes ways to automatically check how good a summary is by comparing it to other perfect summaries made by people. These methods count the number of similar parts like short word sequences (n-gram), word order, and word pairs between the summary created by a computer and the perfect summaries made by people. This paper introduces four different ROUGE methods: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their tests. Three of them have been used in the Document Understanding Conference (DUC) 2004, a big event for testing summaries, sponsored by NIST.	tagged corpus
871	Biomedical Named Entity Recognition Using Conditional Random Fields And Rich Feature Sets	Biomedical Named Entity Recognition Using Conditional Random Fields And Rich Feature Sets Biomedical Named Entity Recognition is a process of identifying specific names or terms in the field of medicine and biology. It uses a method called Conditional Random Fields, which is a mathematical model to predict sequences, along with detailed sets of features or characteristics to improve accuracy.	Evaluation
872	The ICSI Meeting Recorder Dialog Act (MRDA) Corpus We describe a new corpus of over 180,000 hand-annotated dialog act tags and accompanying adjacency pair annotations for roughly 72 hours of speech from 75 naturally-occurring meetings. We provide a brief summary of the annotation system and labeling procedure, inter-annotator reliability statistics, overall distributional statistics, a description of auxiliary files distributed with the corpus, and information on how to obtain the data.	The ICSI Meeting Recorder Dialog Act (MRDA) Corpus We describe a new collection of more than 180,000 hand-labeled dialogue act tags (labels that explain parts of conversation) and related adjacency pair annotations (connections between parts of a conversation) for about 72 hours of talks from 75 real meetings. We provide a short overview of the labeling system and process, statistics on how much different people agree on labels, overall distribution numbers, a description of extra files that come with the collection, and details on how to get the data.	identifying specific
873	A Linear Programming Formulation For Global Inference In Natural Language Tasks Given a collection of discrete random variables representing outcomes of learned local predictors in natural language, e.g., named entities and relations, we seek an optimal global assignment to the variables in the presence of general (non-sequential) constraints. Examples of these constraints include the type of arguments a relation can take, and the mutual activity of different relations, etc. We develop a linear programming formulation for this problem and evaluate it in the context of simultaneously learning named entities and relations. Our approach allows us to efficiently incorporate domain and task specific constraints at decision time, resulting in significant improvements in the accuracy and the human-like quality of the inferences. we use ILP to deal with the joint inference problem of named entity and relation identification. we applied an ILP model to the task of the simultaneous assignment of semantic roles to the entities mentioned in a sentence and recognition of the relations holding between them. we described a classification-based framework in which they jointly learn to identify named entities and relations.	A Linear Programming Formulation For Global Inference In Natural Language Tasks Given a collection of separate random variables that show the outcomes of local predictors in natural language, like named entities (names of people, places, etc.) and relationships (how these entities are connected), we want to find the best overall assignment to these variables while considering general rules or limitations that are not in order. These rules include things like what type of arguments a relationship can have and how different relationships can be active at the same time. We create a method using linear programming (a mathematical way to find the best outcome) to solve this problem and test it while learning about named entities and relationships at the same time. Our method helps us quickly include specific rules for the task or field when making decisions, leading to big improvements in how accurate and human-like the conclusions are. We use ILP (Integer Linear Programming, a specific type of linear programming) to handle the problem of identifying named entities and relationships together. We used an ILP model to assign roles to the entities in a sentence and to recognize the relationships between them at the same time. We explained a framework based on classification (sorting things into categories) that learns to identify named entities and relationships together.	labeled
874	Word Sense Discrimination By Clustering Contexts In Vector And Similarity Spaces This paper systematically compares unsupervised word sense discrimination techniques that cluster instances of a target word that occur in raw text using both vector and similarity spaces. The context of each instance is represented as a vector in a high dimensional feature space. Discrimination is achieved by clustering these context vectors directly in vector space and also by finding pairwise similarities among the vectors and then clustering in similarity space. We employ two different representations of the context in which a target word occurs. First order context vectors represent the context of each instance of a target word as a vector of features that occur in that context. Second order context vectors are an indirect representation of the context based on the average of vectors that represent the words that occur in the context. We evaluate the discriminated clusters by carrying out experiments using sense–tagged instances of 24 SENSEVAL2 words and the well known Line, Hard and Serve sense–tagged corpora.	Word Sense Discrimination By Clustering Contexts In Vector And Similarity Spaces This paper carefully compares methods that automatically figure out different meanings of a word by grouping examples of the word from plain text using both vector and similarity spaces. Each example's context is shown as a vector in a space with many features. The grouping is done by directly clustering these context vectors in the vector space and also by finding how similar the vectors are to each other and then clustering them in the similarity space. We use two different ways to show the context where a word appears. First-order context vectors show the context as a group of features that appear together. Second-order context vectors give an indirect view by averaging vectors of words that appear in the context. We test the grouped results by doing experiments with examples of 24 words from SENSEVAL2 and the well-known collections of Line, Hard, and Serve with meanings already tagged.	outcome
895	Sentiment Analysis Using Support Vector Machines With Diverse Information Sources This paper introduces an approach to sentiment analysis which uses support vector machines (SVMs) to bring together diverse sources of potentially pertinent information, including several favorability measures for phrases and adjectives and, where available, knowledge of the topic of the text. Models using the features introduced are further combined with unigram models which have been shown to be effective in the past (Pang et al., 2002) and lemmatized versions of the unigram models. Experiments on movie review data from the Internet Movie Database demonstrate that hybrid SVMs which combine unigram-style feature-based SVMs with those based on real-valued favorability measures obtain superior performance, producing the best results yet published using this data. Further experiments using a feature set enriched with topic information on a smaller dataset of music reviews hand-annotated for topic are also reported, the results of which suggest that incorporating topic information into such models may also yield improvement.	Sentiment Analysis Using Support Vector Machines With Diverse Information Sources This paper presents a method for understanding opinions or emotions in text using support vector machines (SVMs), which are a type of computer program. It combines different types of useful information, like ratings for phrases and adjectives and, when possible, knowledge about the subject of the text. The new method is combined with a simple word-counting model known as a unigram model, which has worked well before (Pang et al., 2002), and versions of this model that group similar words together. Tests on movie reviews from the Internet Movie Database show that mixing these different approaches using SVMs provides better results than any previous methods. Additional tests on a smaller set of music reviews, which were labeled by hand to show their topics, also show that adding topic information to these models might make them even better.	labeled
875	Memory-Based Dependency Parsing This paper reports the results of experiments using memory-based learning to guide a deterministic dependency parser for unrestricted natural language text. Using data from a small treebank of Swedish, memory-based classifiers for predicting the next action of the parser are constructed. The accuracy of a classifier as such is evaluated on held-out data derived from the treebank, and its performance as a parser guide is evaluated by parsing the held-out portion of the treebank. The evaluation shows that memory-based learning gives a signficant improvement over a previous probabilistic model based on maximum conditional likelihood estimation and that the inclusion of lexical features improves the accuracy even further.	Memory-Based Dependency Parsing This paper shares the results of tests using memory-based learning to help a simple rule-following parser for any kind of natural language text. Using data from a small collection of Swedish sentences organized in a tree structure, classifiers (tools) that predict the parser's next move are created. The correctness of such a classifier is checked using separate test data from the tree collection, and its effectiveness as a parser helper is tested by analyzing the separate part of the tree collection. The results show that memory-based learning greatly improves over an earlier probability-based model that used complex math, and adding word-related features makes it even more accurate.	different meanings
876	Models For The Semantic Classification Of Noun Phrases This paper presents an approach for detecting semantic relations in noun phrases. A learning algorithm, called semantic scattering, is used to automatically label complex nominals, genitives and adjectival noun phrases with the corresponding semantic relation. We propose a 35 class scheme to classify relations in various phrases. We propose a method called semantic scattering for interpreting NCs.	Models For The Semantic Classification Of Noun Phrases This paper presents a method for finding meaning connections in noun phrases. A learning method, called semantic scattering, is used to automatically label complex noun phrases, possessive phrases, and adjective noun phrases with the correct meaning connection. We suggest a 35-category system to classify these connections in different phrases. We suggest a method called semantic scattering for understanding noun compounds (NCs).	collection
877	The NomBank Project: An Interim Report This paper describes NomBank, a project that will provide argument structure for instances of common nouns in the Penn Treebank II corpus. NomBank is part of a larger effort to add additional layers of annotation to the Penn Treebank II corpus. The University of Pennsylvania’s PropBank, NomBank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text. This paper describes the NomBank project in detail including its specifications and the process involved in creating the resource. we provide coarse annotations for some of the possessive con st ructions in the Penn Treebank, but only those that meet their criteria.	The NomBank Project: An Interim Report This paper talks about NomBank, a project that will show how common nouns (naming words) are used in sentences in the Penn Treebank II collection of texts. NomBank is part of a bigger plan to add more detailed labels to the Penn Treebank II text collection. The University of Pennsylvania's PropBank, NomBank, and other labeling projects together aim to create better tools for automatically understanding text. This paper explains the NomBank project in detail, including its features and how it was made. We provide simple labels for some of the possessive forms (showing ownership) in the Penn Treebank, but only those that fit their rules.	phrases
878	The Language Of Bioscience: Facts Speculations And Statements In Between We explore the use of speculative language in MEDLINE abstracts. Results from a manual annotation experiment suggest that the notion of speculative sentence can be reliably annotated by humans. In addition, an experiment with automated methods also suggest that reliable automated methods might also be developed. Distributional observations are also presented as well as a discussion of possible uses for a system that can recognize speculative language. we focus on introducing the problem, exploring annotation issues and outlining potential applications rather than on the specificities of the ML approach, and present some results using a manually crafted substring matching classifier and a supervised SVM on a collection of Medline abstracts. we explore issues with annotating speculative language in biomedicine and outline potential applications. we present a study on annotating hedges in biomedical documents.	The Language Of Bioscience: Facts Speculations And Statements In Between We look into how uncertain or speculative language is used in summaries from MEDLINE, a medical research database. Results from a manual labeling test show that identifying speculative sentences can be done accurately by people. Additionally, a test using computer methods also indicates that reliable computer methods might be created. We also present observations on how this language is spread and discuss how a system that can identify speculative language could be used. We focus on explaining the problem, exploring labeling challenges, and describing possible uses rather than the details of the machine learning (ML) approach, and show some results using a manually made substring matching tool and a supervised Support Vector Machine (SVM), which is a type of computer algorithm, on a collection of Medline summaries. We look into challenges with labeling speculative language in biomedical fields and describe possible uses. We present a study on labeling uncertain claims in biomedical documents.	Project
879	Integrated Annotation For Biomedical Information Extraction We describe an approach to two areas of biomedical information extraction, drug development and cancer genomics. We have developed a framework which includes corpus annotation integrated at multiple levels: a Treebank containing syntactic structure, a Propbank containing predicate-argument structure, and annotation of entities and relations among the entities. Crucial to this approach is the proper characterization of entities as relation components, which allows the integration of the entity annotation with the syntactic structure while retaining the capacity to annotate and extract more complex events. We are training statistical taggers using this annotation for such extraction as well as using them for improving the annotation process.	Integrated Annotation For Biomedical Information Extraction We explain a method for getting important information in two areas of medical research: creating new medicines and studying cancer genes. We've made a system that includes different levels of text labeling: a Treebank that shows sentence structure, a Propbank that shows how actions connect with things, and labeling of items and how they relate to each other. A key part of this method is correctly identifying items as parts of relationships, which helps combine item labeling with sentence structure and still allows labeling and finding more complicated events. We are teaching computer programs to recognize patterns using this labeling to help find information and make the labeling better.	speculative
880	Max-Margin Parsing We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines. Our formulation uses a factorization analogous to the standard dynamic programs for parsing. In particular, it allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates. Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness. We provide an efficient algorithm for learning such models and show experimental evidence of the model’s improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar. we suggested a method for maximal margin parsing which employs the dynamic programming approach to decoding and parameter estimation problems.	Max-Margin Parsing We introduce a new method for sentence structure analysis that is inspired by the idea of creating a clear distinction between categories, similar to how support vector machines work. Our method breaks down the problem in a way similar to traditional methods used for sentence analysis. This means it can quickly learn to identify the entire range of possible sentence structures, not just the best few options. Our models can use any details from input sentences, allowing them to include important word information without making the process more complicated by focusing on specific word roles. We offer a fast method for learning these models and provide evidence that they perform better than a simple model and a more complex word-based model. We proposed a way for analyzing sentence structures that uses a step-by-step approach to solve problems of decoding and estimating values.	includes
881	VerbOcean: Mining The Web For Fine-Grained Semantic Verb Relations Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks. We present a semi-automatic method for extracting fine-grained semantic relations between verbs. We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web. On a set of 29,165 strongly associated verb pairs, our extraction algorithm yielded 65.5 % accuracy. Analysis of error types shows that on the relation strength we achieved 75 % accuracy. We provide the resource, called VERBOCEAN, for download at http://semantics.isi.edu/ocean/. we introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. we use patterns to extract a set of relations between verbs, such as similarity, strength and antonymy.	:VerbOcean: Mining The Web For Fine-Grained Semantic Verb Relations Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks. We present a semi-automatic method for extracting detailed meaning connections between verbs. We find similarity, strength, opposite meanings, enablement (how one action allows another), and time-order relations between pairs of closely related verbs using word and sentence patterns found on the Web. On a set of 29,165 closely related verb pairs, our extraction method was correct 65.5% of the time. Analysis of mistakes shows that for the relation strength we were correct 75% of the time. We provide the resource, called VERBOCEAN, for download at http://semantics.isi.edu/ocean/. We introduce a 5-class set, designed specifically for describing how verbs relate in meaning. We use patterns to find a set of relations between verbs, such as similarity, strength, and opposite meanings.	solve problems
882	Scaling Web-Based Acquisition Of Entailment Relations Paraphrase recognition is a critical step for natural language interpretation. Accordingly, many NLP applications would benefit from high coverage knowledge bases of paraphrases. However, the scalability of state-of-the-art paraphrase acquisition approaches is still limited. We present a fully unsupervised learning algorithm for Web-based extraction of entailment relations, an extended model of paraphrases. We focus on increased scalability and generality with respect to prior work, eventually aiming at a full scale knowledge base. Our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the Web for related syntactic entailment templates. Experiments show promising results with respect to the ultimate goal, achieving much better scalability than prior Web-based methods. we describe the TEASE method for extracting entailing relation templates from the Web.	Scaling Web-Based Acquisition Of Entailment Relations Paraphrase recognition, which means identifying when different sentences mean the same thing, is an important part of understanding language. Many computer programs that deal with language would benefit from having a large database of different ways to say the same thing. However, the current methods for collecting these paraphrases don't work well when trying to gather a lot of information. We introduce a new automatic method for finding entailment relations, which are a broader version of paraphrases, from the internet. Our focus is on making this method work on a larger scale and in more general situations compared to previous attempts, with the goal of creating a complete database of these relationships. Our system starts with a list of verbs and looks online for patterns that show how these verbs are used in related ways. Tests show that our method works well towards our main goal, handling larger amounts of data better than previous internet-based approaches. We explain the TEASE method, which is our way of finding these related patterns from the web.	meanings
883	Bilingual Parsing With Factored Estimation: Using English To Parse Korean We describe how simple, commonly understood statistical models, such as statistical dependency parsers, probabilistic context-free grammars, and word-to-word translation models, can be effectively combined into a unified bilingual parser that jointly searches for the best English parse, Korean parse, and word alignment, where these hidden structures all constrain each other. The model used for parsing is completely factored into the two parsers and the TM, allowing separate parameter estimation. We evaluate our bilingual parser on the Penn Korean Treebank and against several baseline systems and show improvements parsing Korean with very limited labeled data. we proposed to merge an English parser, a word alignment model, and a Korean PCFG parser trained from a small number of Korean parse trees under a unified log linear model.	Bilingual Parsing With Factored Estimation: Using English To Parse Korean We explain how basic and well-known statistical models, like tools that identify sentence structure (statistical dependency parsers), rules for building sentences (probabilistic context-free grammars), and methods for matching words between languages (word-to-word translation models), can be effectively combined into one bilingual tool that simultaneously finds the best English sentence structure, Korean sentence structure, and word matching between the two languages, where these hidden parts all influence each other. The model used for finding sentence structures is completely divided into the two parsers and the translation model, allowing each part to be adjusted separately. We test our bilingual parser on the Penn Korean Treebank and compare it to several basic systems, showing improvements in understanding Korean with very little labeled data. We suggested combining an English parser, a word matching model, and a Korean sentence rules parser trained from a small number of Korean sentence structures into one unified model that uses a specific method called a log linear model.	important
893	Statistical Significance Tests For Machine Translation Evaluation Automatic evaluation metrics for Machine Translation (MT) systems, such as BLEU, METEOR and the related NIST metric, are becoming increasingly important in MT research and development. This paper presents a significance test-driven comparison of n-gram-based automatic MT evaluation metrics. Statistical significance tests use bootstrapping methods to estimate the reliability of automatic machine translation evaluations. Based on this reliability estimation, we study the characteristics of different MT evaluation metrics and how to construct reliable and efficient evaluation suites.	Statistical Significance Tests For Machine Translation Evaluation Automatic evaluation methods for Machine Translation (MT) systems, like BLEU, METEOR, and the similar NIST method, are becoming more crucial in MT research and development. This paper shows a comparison of n-gram-based automatic MT evaluation methods using significance tests, which help determine how meaningful the results are. Statistical significance tests use bootstrapping methods (a way to estimate accuracy by repeatedly sampling data) to judge how reliable automatic machine translation evaluations are. Based on this reliability check, we explore the features of different MT evaluation methods and how to build reliable and efficient evaluation groups.	similar sentences
884	Mining Very-Non-Parallel Corpora: Parallel Sentence And Lexicon Extraction Via Bootstrapping And EM We present a method capable of extracting parallel sentences from far more disparate “very-non-parallel corpora” than previous “comparable corpora” methods, by exploiting bootstrapping on top of IBM Model 4 EM. Step 1 of our method, like previous methods, uses similarity measures to find matching documents in a corpus first, and then extracts parallel sentences as well as new word translations from these documents. But unlike previous methods, we extend this with an iterative bootstrapping framework based on the principle of “find-one-get-more”, which claims that documents found to contain one pair of parallel sentences must contain others even if the documents are judged to be of low similarity. We re-match documents based on extracted sentence pairs, and refine the mining process iteratively until convergence. This novel “find-one-get-more” principle allows us to add more parallel sentences from dissimilar documents, to the baseline set. Experimental results show that our proposed method is nearly 50% more effective than the baseline method without iteration. We also show that our method is effective in boosting the performance of the IBM Model 4 EM lexical learner as the latter, though stronger than Model 1 used in previous work, does not perform well on data from very-non-parallel corpus.	Mining Very-Non-Parallel Corpora: Parallel Sentence And Lexicon Extraction Via Bootstrapping And EM We present a method that can find matching sentences from very different collections of texts, called "very-non-parallel corpora," better than older methods called "comparable corpora." We do this by using a process called bootstrapping on top of a system known as IBM Model 4 EM. In the first step, like older methods, we look for similar documents and then find matching sentences and new word translations. But unlike older methods, we add a repeating process based on the idea of "find-one-get-more," which means if one matching sentence is found, more can be in the same document even if it's not very similar. We keep matching documents based on the sentences we find and keep improving the process until it works well. This new "find-one-get-more" idea helps us find more matching sentences from different documents. Tests show our method is almost 50% better than the basic method without repeating. We also show that our method improves the results of the IBM Model 4 EM, which is better than the older Model 1 but struggles with very-different text collections.	Korean sentence
885	Calibrating Features For Semantic Role Labeling This paper takes a critical look at the features used in the semantic role tagging literature and show that the information in the input, generally a syntactic parse tree, has yet to be fully exploited. We propose an additional set of features and our experiments show that these features lead to fairly significant improvements in the tasks we performed. We further show that different features are needed for different subtasks. Finally, we show that by using a Maximum Entropy classifier and fewer features, we achieved results comparable with the best previously reported results obtained with SVM models. We believe this is a clear indication that developing features that capture the right kind of information is crucial to advancing the stateof-the-art in semantic analysis.	Calibrating Features For Semantic Role Labeling This paper examines the elements used in semantic role tagging (a method for understanding sentence roles) and shows that the information from the input, usually a structured sentence diagram called a syntactic parse tree, hasn't been fully used. We suggest more elements, and our tests show that these new elements lead to noticeable improvements in the tasks we did. We also show that different tasks need different elements. Lastly, we demonstrate that by using a Maximum Entropy classifier (a type of machine learning model) with fewer elements, we achieved results similar to the best results previously reported with SVM models (another type of machine learning model). We think this clearly shows that creating elements that capture the right information is important for improving semantic analysis (understanding language meaning).	Lexicon Extraction
886	Unsupervised Semantic Role Labeling We present an unsupervised method for labelling the arguments of verbs with their semantic roles. Our bootstrapping algorithm makes initial unambiguous role assignments, and then iteratively updates the probability model on which future assignments are based. A novel aspect of our approach is the use of verb, slot, and noun class information as the basis for backing off in our probability model. We achieve 50–65% reduction in the error rate over an informed baseline, indicating the potential of our approach for a task that has heretofore relied on large amounts of manually generated training data. we present an unsupervised method for labeling the arguments of verbs with their semantic roles. we perform unsupervised semantic role labeling by using hand-crafted verb lexicons to replace supervised semantic role training data.	Unsupervised Semantic Role Labeling We introduce a method that doesn't need pre-labeled data to identify the roles of words connected to verbs in sentences. Our process starts with clear role assignments and keeps improving by updating the probability model, which helps make future decisions. A unique part of our method is using information about verbs, their positions in sentences, and groups of similar nouns to improve the probability model. We successfully reduce errors by 50-65% compared to a baseline that uses some prior knowledge, showing promise for a task that typically needs a lot of manually created training data. We perform this task without supervision by using specially created verb dictionaries instead of relying on pre-labeled data for training.	suggest
887	Monolingual Machine Translation For Paraphrase Generation We apply statistical machine translation (SMT) tools to generate novel paraphrases of input sentences in the same language. The system is trained on large volumes of sentence pairs automatically extracted from clustered news articles available on the World Wide Web. Alignment Error Rate (AER) is measured to gauge the quality of the resulting corpus. A monotone phrasal decoder generates contextual replacements. Human evaluation shows that this system outperforms baseline paraphrase generation techniques and, in a departure from previous work, offers better coverage and scalability than the current best-of-breed paraphrasing approaches. we built a paraphrase generation model from a monolingual comparable corpus based on a statistical machine translation framework, where the language model assesses the grammaticality of the translations ,i.e., generated expressions. we present an end-to-end paraphrasing system inspired by phrase-based machine translation that can both ac quire paraphrases and use them to generate new strings.	Monolingual Machine Translation For Paraphrase Generation We use tools from statistical machine translation (SMT) to create new ways to say the same sentences in one language. The system learns from lots of sentence pairs that are automatically taken from grouped news articles found online. We measure Alignment Error Rate (AER) to check how good the collection of sentences is. A simple tool called a monotone phrasal decoder makes changes based on the context. Human evaluation shows that this system works better than basic methods for generating paraphrases and, unlike past efforts, it covers more and can handle larger tasks than the best current methods. We developed a model to generate paraphrases from a set of similar sentences in one language using a statistical machine translation method, where the language model checks if the new sentences are grammatically correct. We introduce a complete paraphrasing system inspired by a method called phrase-based machine translation that can both find new paraphrases and use them to create new sentences.	process
905	SPMT: Statistical Machine Translation With Syntactified Target Language Phrases We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a human-based quality metric that ranks translations on a scale from 1 to 5.	SPMT: Statistical Machine Translation With Syntactified Target Language Phrases We introduce SPMT, a new type of statistical Translation Models that use structured (syntactified) phrases in the language being translated to. The SPMT models perform better than the latest standard phrase-based model by 2.64 Bleu points on the NIST 2003 Chinese-English test set and by 0.28 points on a human-based quality measure that rates translations from 1 to 5.	Analysis
888	Applying Conditional Random Fields To Japanese Morphological Analysis This paper presents Japanese morphological analysis based on conditional random fields (CRFs). Previous work in CRFs assumed that observation sequence (word) boundaries were fixed. However, word boundaries are not clear in Japanese, and hence a straightforward application of CRFs is not possible. We show how CRFs can be applied to situations where word boundary ambiguity exists. CRFs offer a solution to the long-standing problems in corpus-based or statistical Japanese morphological analysis. First, flexible feature designs for hierarchical tagsets become possible. Second, influences of label and length bias are minimized. We experiment CRFs on the standard testbed corpus used for Japanese morphological analysis, and evaluate our results using the same experimental dataset as the HMMs and MEMMs previously reported in this task. Our results confirm that CRFs not only solve the long-standing problems but also improve the performance over HMMs and MEMMs. we studied Japanese word segmentation and POS tagging using conditional random fields (CRFs) and rule based unknown word processing.	Applying Conditional Random Fields To Japanese Morphological Analysis This paper discusses how to analyze Japanese words using a method called conditional random fields (CRFs). Previous studies using CRFs assumed that the start and end of words were already known. But in Japanese, it's not always clear where one word ends, and another begins, so we can't use CRFs directly without adjustments. We explain how CRFs can be used even when it's hard to tell where words start and end. CRFs help solve long-standing issues in analyzing Japanese language using large sets of text data or statistics. First, they allow for flexible ways to categorize words in layers. Second, they reduce errors related to labeling and word length. We tested CRFs with a standard set of Japanese text data and compared our results to older methods called HMMs and MEMMs. Our findings show that CRFs not only address old problems but also work better than HMMs and MEMMs. We looked into dividing Japanese text into words and identifying parts of speech using CRFs and rules for dealing with unknown words.	complete
889	Chinese Part-Of-Speech Tagging: One-At-A-Time Or All-At-Once? Word-Based Or Character-Based? Chinese part-of-speech (POS) tagging assigns one POS tag to each word in a Chinese sentence. However, since words are not demarcated in a Chinese sentence, Chinese POS tagging requires word segmentation as a prerequisite. We could perform Chinese POS tagging strictly after word segmentation (one-at-a-time approach), or perform both word segmentation and POS tagging in a combined, single step simultaneously (all-at- once approach). Also, we could choose to assign POS tags on a word-by-word basis, making use of word features in the surrounding context (word-based), or on a character-by-character basis with character features (character-based). This paper presents an in-depth study on such issues of processing architecture and feature representation for Chinese POS tagging, within a maximum entropy framework. We found that while the all-at-once, character-based approach is the best, the one-at-a-time, character-based approach is a worthwhile compromise, performing only slightly worse in terms of accuracy, but taking shorter time to train and run. As part of our investigation, we also built a state-of-the-art Chinese word segmenter, which outperforms the best SIGHAN 2003 word segmenters in the closed track on 3 out of 4 test corpora.	Chinese Part-Of-Speech Tagging: One-At-A-Time Or All-At-Once? Word-Based Or Character-Based? Chinese part-of-speech (POS) tagging is about giving each word in a Chinese sentence a label that tells what part of speech it is, like noun or verb. But since Chinese sentences don't have clear spaces between words, we first need to split the sentence into words, which is called word segmentation. We can do the tagging after this splitting step (one-at-a-time approach), or we can do both splitting and tagging together in one step (all-at-once approach). We can also choose to tag based on whole words by looking at nearby words (word-based), or tag based on individual characters by looking at nearby characters (character-based). This paper explores these different methods of processing and feature use for Chinese POS tagging, using a maximum entropy framework, which is a way to predict outcomes based on known data. We found that doing everything at once with character-based tagging works best, but doing it one step at a time with character-based tagging is almost as good, and it's faster to train and use. In our study, we also created an advanced Chinese word splitter that works better than the top performers from the SIGHAN 2003 competition in most tests.	unknown
890	Adaptation Of Maximum Entropy Capitalizer: Little Data Can Help A Lot A novel technique for maximum “a posteriori” (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented. The technique is applied to the problem of automatically capitalizing uniformly cased text. Automatic capitalization is a practically relevant problem: speech recognition output needs to be capitalized; also, modern word processors perform capitalization among other text proofing algorithms such as spelling correction and grammar checking. Capitalization can be also used as a preprocessing step in named entity extraction or machine translation. A “background” capitalizer trained on 20 M words of Wall Street Journal (WSJ) text from 1987 is adapted to two Broadcast News (BN) test sets – one containing ABC Primetime Live text and the other NPR Morning News/CNN Morning Edition text – from 1996. The “in-domain” performance of the WSJ capitalizer is 45% better relative to the 1-gram baseline, when evaluated on a test set drawn from WSJ 1994. When evaluating on the mismatched “out-of-domain” test data, the 1-gram baseline is outperformed by 60% relative; the improvement brought by the adaptation technique using a very small amount of matched BN data – 25–70k words – is about 20–25% relative. Overall, automatic capitalization error rate of 1.4% is achieved on BN data. The performance gain obtained by employing our adaptation technique using a tiny amount of out-of-domain training data on top of the background data is striking: as little as 0.14 M words of in-domain data brings more improvement than using 10 times more background training data (from 2 M words to 20 M words). we proposed method for transfer learning in Maximum Entropy models involves modifying the mu's of this Gaussian prior. we use the parameters of the source domain maximum entropy classifier as the means of a Gaussian prior when training a new model on the target data.	Adaptation Of Maximum Entropy Capitalizer: Little Data Can Help A Lot A new method for adjusting maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is introduced. This method helps computers automatically capitalize text that is all in lowercase. Automatic capitalization is important because speech recognition systems need to make text look correct with capital letters, and word processors also fix capitalization along with spelling and grammar. Capitalization can also help in finding names and translating languages. A basic capitalizer trained on 20 million words from the Wall Street Journal (WSJ) of 1987 is adjusted for two sets of news text from 1996: one from ABC Primetime Live and the other from NPR/CNN. The WSJ capitalizer works 45% better than a simple method when tested on a 1994 WSJ set. On different news data, it performs 60% better than the simple method, and using just a little bit of matching news text (25-70k words) improves it by 20-25%. An error rate of 1.4% is achieved for news data. Using our method with just 140,000 words of specific training data gives more improvement than using up to 20 million words of general training data. Our method for transferring learning in Maximum Entropy models involves changing certain values in a mathematical formula. We use the settings from an old model as a starting point when training a new model on different data.	individual
891	A Boosting Algorithm For Classification Of Semi-Structured Text The focus of research in text classification has expanded from simple topic identification to more challenging tasks such as opinion/modality identification. Unfortunately, the latter goals exceed the ability of the traditional bag-of-word representation approach, and a richer, more structural representation is required. Accordingly, learning algorithms must be created that can handle the structures observed in texts. In this paper, we propose a Boosting algorithm that captures sub-structures embedded in texts. The proposal consists of i) decision stumps that use subtrees as features and ii) the Boosting algorithm which employs the subtree-based decision stumps as weak learners. We also discuss the relation between our algorithm and SVMs with tree kernel. Two experiments on opinion/modality classification confirm that subtree features are important. We adopt the BACT learning algorithm to effectively learn subtrees useful for both antecedent identification and zero pronoun detection.	A Boosting Algorithm For Classification Of Semi-Structured Text The research in sorting text by category has grown from just figuring out the topic to more difficult tasks like figuring out opinions or the way something is said. Sadly, these harder tasks can't be done well with the old method of just counting words, so we need a better way to represent text. So, we need to create learning programs that can understand the structure of texts. In this paper, we suggest a Boosting algorithm, which is a method to improve prediction, that can find small structures hidden in texts. Our idea includes i) simple decision rules that use parts of sentence trees as features and ii) the Boosting method which uses these tree-based simple rules as basic tools to learn. We also talk about how our method relates to SVMs (Support Vector Machines), a type of machine learning model, with tree kernel, which is a way to measure similarity using trees. Two tests on finding opinions or modality show that using tree parts is important. We use the BACT learning method to effectively learn tree parts that are useful for identifying what comes before something (antecedent) and finding hidden subjects in sentences (zero pronoun detection).	models involves
892	LexPageRank: Prestige In Multi-Document Text Summarization Multidocument extractive summarization relies on the concept of sentence centrality to identify the most important sentences in a document. Centrality is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank. In this model, a sentence connectivity matrix is constructed based on cosine similarity. If the cosine similarity between two sentences exceeds a particular predefined threshold, a corresponding edge is added to the connectivity matrix. We provide an evaluation of our method on DUC 2004 data. The results show that our approach outperforms centroid-based summarization and is quite successful compared to other summarization systems. we propose Lex PageRank, which is an approach for computing sentence importance based on the concept of eigenvector centrality.	LexPageRank: Prestige In Multi-Document Text Summarization Multidocument extractive summarization uses the idea of sentence centrality, which helps find the most important sentences in a document. Centrality usually means how often important words appear or how similar a sentence is to a main idea sentence. We are now looking at a method to figure out sentence importance using eigenvector centrality (prestige), which we call LexPageRank. In this model, we create a sentence connection chart using cosine similarity, a way to measure how similar sentences are. If the similarity between two sentences is higher than a set level, we connect them in the chart. We tested our method with data from DUC 2004. The results show that our method does better than the centroid-based summarization and is very successful compared to other summarization systems. We propose LexPageRank, a way to measure sentence importance using eigenvector centrality.	learning
896	A Statistical Semantic Parser That Integrates Syntax And Semantics We introduce a learning semantic parser, SCISSOR, that maps natural-language sentences to a detailed, formal, meaning- representation language. It first uses an integrated statistical parser to produce a semantically augmented parse tree, in which each non-terminal node has both a syntactic and a semantic label. A compositional-semantics procedure is then used to map the augmented parse tree into a final meaning representation. We evaluate the system in two domains, a natural-language database interface and an interpreter for coaching instructions in robotic soccer. We present experimental results demonstrating that SCISSOR produces more accurate semantic representations than several previous approaches. we introduced an approach, SCISSOR, where the composition of meaning representations is guided by syntax.	A Statistical Semantic Parser That Integrates Syntax And Semantics We introduce a learning tool called SCISSOR that converts natural-language sentences into a detailed, formal language that explains their meaning. It first uses a combined statistical parser to create a tree structure that includes both syntax (sentence structure) and semantics (meaning) for each part of the sentence. Then, a step-by-step process is used to turn this tree into a final representation of meaning. We test the system in two areas: a database interface that uses natural language and a program that interprets coaching instructions for robotic soccer. Our experiments show that SCISSOR creates more accurate representations of meaning than several older methods. We introduced a method, SCISSOR, where creating meaning representations is guided by sentence structure.	group similar
897	Generalized Inference With Multiple Semantic Role Labeling Systems We present an approach to semantic role labeling (SRL) that takes the output of multiple argument classifiers and combines them into a coherent predicate-argument output by solving an optimization problem. The optimization stage, which is solved via integer linear programming, takes into account both the recommendation of the classifiers and a set of problem specific constraints, and is thus used both to clean the classification results and to ensure structural integrity of the final role labeling. We illustrate a significant improvement in overall SRL performance through this inference. we adopted the outputs of multiple SRL systems (each on a single parse tree) and combined them into a coherent predicate argument output by solving an optimization problem.	Generalized Inference With Multiple Semantic Role Labeling Systems We present a way to assign roles to words in a sentence (semantic role labeling or SRL) by using the results from different systems that identify roles, then combining these results into one clear outcome. This is done by solving a best-fit problem (optimization problem). The optimization step uses a method called integer linear programming, which considers both the suggestions from the systems and specific rules that need to be followed. This step helps tidy up the results and make sure the final role assignments make sense. We show that this method greatly improves how well SRL works overall. We used the results from different SRL systems (each working on a single structure of a sentence) and combined them into a clear outcome by solving this best-fit problem.	interprets coaching
898	Syntactic Features For Evaluation Of Machine Translation Automatic evaluation of machine translation, based on computing n-gram similarity between system output and human reference translations, has revolutionized the development of MT systems. We explore the use of syntactic information, including constituent labels and head-modier dependencies, in computing similarity between output and reference. Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments. we measure the syntactic similarity between MT output and reference translation. we used syntactic structure and dependency information to go beyond the surface level matching.	Syntactic Features For Evaluation Of Machine Translation Automatic evaluation of machine translation, based on computing n-gram similarity (comparing short sequences of words) between system output and human reference translations, has revolutionized the development of MT (Machine Translation) systems. We explore the use of syntactic information (grammar-related details), including constituent labels (parts of sentence like subject or object) and head-modifier dependencies (how words relate to each other), in computing similarity between output and reference. Our results show that adding syntactic information to the evaluation metric (method of measuring) improves both sentence-level and corpus-level (overall text) correlation with human judgments. We measure the syntactic similarity (grammar-based comparison) between MT output and reference translation. We used syntactic structure (sentence grammar layout) and dependency information to go beyond the surface level matching (basic word comparison).	considers
899	METEOR: An Automatic Metric For MT Evaluation With Improved Correlation With Human Judgments We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machine-produced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets. We perform segment-by-segment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigram-precision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules.	METEOR: An Automatic Metric For MT Evaluation With Improved Correlation With Human Judgments We describe METEOR, a tool that automatically checks how good machine translations are by comparing them to reference translations done by humans. It works by matching single words or parts of words (unigrams) between the machine and human translations, based on how they look, their root forms, and their meanings. METEOR can also be adapted to use more advanced matching techniques. After finding all the matches, METEOR calculates a score using three things: how many unigrams match (unigram-precision), how many of the important ones match (unigram-recall), and how well the order of matched words follows the human translation. We test METEOR by seeing how its scores match up with human opinions of translation quality. We measure the Pearson R correlation, which is a statistical measure, between METEOR's scores and human quality ratings for Arabic-to-English and Chinese-to-English translations from a 2003 dataset. By checking each part of the translation, we find METEOR has a 0.347 correlation for Arabic and 0.331 for Chinese, which is better than just using simple match counts. We also do tests to see how different parts of METEOR contribute to its success.	judgments
900	Measuring The Semantic Similarity Of Texts This paper presents a knowledge-based method for measuring the semantic-similarity of texts. While there is a large body of previous work focused on finding the semantic similarity of concepts and words, the application of these word-oriented methods to text similarity has not been yet explored. In this paper, we introduce a method that combines word-to-word similarity metrics into a text-to-text metric, and we show that this method outperforms the traditional text similarity metrics based on lexical matching. we proposed a hybrid method by combining six existing knowledge-based methods.	Measuring The Semantic Similarity Of Texts This paper presents a knowledge-based method for measuring how similar the meanings of texts are. While there is a lot of previous research on finding how similar the meanings of concepts and words are, using these word-focused methods to find text similarity hasn't been fully explored yet. In this paper, we introduce a method that combines measurements of word-to-word similarity into a way to measure text-to-text similarity, and we show that this method works better than the traditional methods that rely on matching exact words. We proposed a mixed method by combining six existing knowledge-based methods.	calculates
901	Better K-Best Parsing We discuss the relevance of k-best parsing to recent applications in natural language processing, and develop efficient algorithms for k-best trees in the framework of hypergraph parsing. To demonstrate the efficiency, scal- ability and accuracy of these algorithms, we present experiments on Bikel’s implementation of Collins’ lexicalized PCFG model, and on Chiang’s CFG-based decoder for hierarchical phrase-based translation. We show in particular how the improved output of our algorithms has the potential to improve results from parse reranking systems and other applications.	Better K-Best Parsing We talk about how important k-best parsing is for recent uses in natural language processing (how computers understand human language) and create fast methods for finding the best k results (k-best trees) using hypergraph parsing (a complex structure to analyze language). To prove how fast, scalable (can handle large data), and accurate these methods are, we perform tests using Bikel’s version of Collins’ PCFG model (a type of grammar model), and on Chiang’s CFG-based decoder (a tool for translating languages in a structured way). We specifically show how the better results from our methods can improve the outcomes of parse reranking systems (tools that reorder results) and other uses.	measuring
902	A Classifier-Based Parser With Linear Run-Time Complexity We present a classifier-based parser that produces constituent trees in linear time. The parser uses a basic bottom-up shift-reduce algorithm, but employs a classifier to determine parser actions instead of a grammar. This can be seen as an extension of the deterministic dependency parser of Nivre and Scholz (2004) to full constituent parsing. We show that, with an appropriate feature set used in classification, a very simple one-path greedy parser can perform at the same level of accuracy as more complex parsers. We evaluate our parser on section 23 of the WSJ section of the Penn Treebank, and obtain precision and recall of 87.54% and 87.61%, respectively. we propose a constituency based parsing method to determine sentence dependency structures.	A Classifier-Based Parser With Linear Run-Time Complexity We introduce a tool that breaks down sentences into parts quickly. It uses a simple method to decide how to break down sentences, not by following strict rules, but by using a decision-making tool. This method is similar to an older technique by Nivre and Scholz (2004) but is more advanced. We demonstrate that by choosing the right details to focus on, this straightforward method can be just as accurate as more complicated ones. We tested our tool on a specific section of a well-known collection of language data and found it to be quite accurate, with scores of 87.54% and 87.61%. We suggest this method to figure out the structure of sentences.	grammar model
903	Extracting Opinions Opinion Holders And Topics Expressed In Online News Media Text This paper presents a method for identifying an opinion with its holder and topic, given a sentence from online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using data from FrameNet. We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles. For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet. Our experimental results show that our system performs significantly better than the baseline. we identify opinion holders and targets with semantic role labeling.	Extracting Opinions Opinion Holders And Topics Expressed In Online News Media Text This paper introduces a way to find an opinion, who holds it, and what it's about, using a sentence from online news articles. We explain a method that uses the meaning structure of a sentence, focusing on a verb or adjective that shows opinion. This approach involves identifying roles in the sentence to determine who holds the opinion and what it is about, using a resource called FrameNet. We break down the task into three steps: finding the word that shows opinion, labeling related parts of the sentence, and then identifying who has the opinion and what it's about from these labeled parts. To cover more cases, we also use a grouping method to guess the most likely situation for a word not defined in FrameNet. Our tests show that our system works much better than a simple starting point. We identify who holds opinions and what they target using a method of labeling sentence parts.	demonstrate
904	Automatic Identification Of Non-Compositional Multi-Word Expressions Using Latent Semantic Analysis Making use of latent semantic analysis, we explore the hypothesis that local linguistic context can serve to identify multi-word expressions that have non-compositional meanings. We propose that vector-similarity between distribution vectors associated with an MWE as a whole and those associated with its constitutent parts can serve as a good measure of the degree to which the MWE is compositional. We present experiments that show that low (cosine) similarity does, in fact, correlate with non-compositionality. we devise a supervised method in which they compute the meaning vectors for the literal and non literal usages of a given expression in the trainningdata. we used a supervised learning method to distinguish between compositional and non-compositional uses of an expression (in German text) by using contextual information in the form of Latent Semantic Analy sis (LSA) vectors.	Automatic Identification Of Non-Compositional Multi-Word Expressions Using Latent Semantic Analysis Making use of latent semantic analysis (a method to understand the meaning of words using math), we explore the idea that the nearby words in a sentence can help identify groups of words that don't mean what you'd expect from the individual words. We suggest that comparing how often these word groups appear together compared to their separate parts can tell us how much they make sense together. We present experiments that show when this similarity is low, it often means the group of words has a unique meaning. We create a supervised method (a guided approach) where we figure out the meaning of a word group when it's used normally and when it has a special meaning, using example data. We used a supervised learning method to tell apart when a group of words is used in the usual way or in a special way (in German text) by using information about nearby words, understood through Latent Semantic Analysis (LSA) vectors.	labeling
907	Domain Adaptation With Structural Correspondence Learning Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resource-rich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger. Following (Blitzer et al, 2006), we present an application of structural correspondence learning to non-projective dependency parsing (McDonald et al, 2005). Our approach is to train a separate out-of-domain parser, and use this to generate additional features on the supervised and unsupervised in-domain data. We introduce SCL that is one feature representation approach that has been effective on certain high-dimensional NLP problems, including part-of-speech tagging and sentiment classification. We apply the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. We append the source domain labeled data with predicted pivots (i.e. words that appear in both the source and target domains) to adapt a POS tagger to a target domain.	Domain Adaptation With Structural Correspondence Learning Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are from the same type of data set. For many NLP tasks, however, we face new areas where labeled data (data that is already categorized or tagged) is limited or not available. In such cases, we try to adapt existing models from a data-rich area to a data-poor area. We introduce structural correspondence learning to automatically create connections between features (important parts) from different areas. We test our method on part of speech tagging (identifying the role of words, like noun or verb) and show it performs better with different amounts of training data from both areas, as well as improves the accuracy of understanding the target area using our improved tagger. Following (Blitzer et al, 2006), we show how structural correspondence learning can be applied to non-projective dependency parsing (a method to understand sentence structure) (McDonald et al, 2005). Our method is to train a separate parser for data outside the target area and use this to generate extra features for both labeled and unlabeled data within the target area. We introduce SCL, a way to represent features effectively in complex NLP problems, like part-of-speech tagging and sentiment analysis (understanding opinions). We use the multitask algorithm of (Ando and Zhang, 2005) for adapting NLP tasks to new areas. We add predicted pivots (words appearing in both the original and new areas) to the source domain (original area) labeled data to adapt a POS tagger to a new area.	commonly
908	Incremental Integer Linear Programming For Non-Projective Dependency Parsing Integer Linear Programming has recently been used for decoding in a number of probabilistic models in order to enforce global constraints. However, in certain applications, such as non-projective dependency parsing and machine translation, the complete formulation of the decoding problem as an integer linear program renders solving intractable. We present an approach which solves the problem incrementally, thus we avoid creating intractable integer linear programs. This approach is applied to Dutch dependency parsing and we show how the addition of linguistically motivated constraints can yield a significant improvement over state-of-the-art. For dependency parsing, we study a method using integer linear programming which can incorporate global linguistic constraints. Our work in dependency parsing demonstrate that it is possible to use ILP to perform efficient inference for very large programs when used in an incremental manner. We show that even exponentially large decoding problems may be solved efficiently using ILP solvers if a Cutting-Plane Algorithm (Dantzig et al, 1954) is used. We tackle the MAP problem for dependency parsing by an incremental approach that starts with a relaxation of the problem, solves it, and adds additional constraints only if they are violated.	Incremental Integer Linear Programming For Non-Projective Dependency Parsing Integer Linear Programming (ILP), a mathematical approach for decision-making, has recently been used for decoding in many models that predict outcomes, to make sure all rules are followed. However, in specific tasks like non-projective dependency parsing (a type of language analysis) and translating languages, fully setting up the problem using ILP makes it too complicated to solve. We introduce a method that solves the problem step-by-step, avoiding these overly complex ILP issues. This method is used for analyzing Dutch sentence structures, showing that adding rules based on language understanding significantly improves results compared to the best current methods. In analyzing sentence structures (dependency parsing), we explore using ILP to include overall language rules. Our work shows that ILP can efficiently handle very large problems if tackled in steps. We demonstrate that even extremely large problems can be solved efficiently with ILP tools if a Cutting-Plane Algorithm, a step-by-step method from 1954, is used. We approach the Maximum A Posteriori (MAP) problem, which finds the most likely sentence structure, by starting with a simplified version, solving it, and only adding more rules if needed.	Domain Adaptation
909	Get Out The Vote: Determining Support Or Opposition From Congressional Floor-Debate Transcripts We investigate whether one can determine from the transcripts of U.S. Congressional floor debates whether the speeches represent support of or opposition to proposed legislation. To address this problem, we exploit the fact that these speeches occur as part of a discussion; this allows us to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another. We find that the incorporation of such information yields substantial improvements over classifying speeches in isolation. We present a method based on support vector machines to determine whether the speeches made by participants represent support or opposition to proposed legislation, using transcripts of U.S. congressional floor debates.	Get Out The Vote: Determining Support Or Opposition From Congressional Floor-Debate Transcripts We look into whether we can tell from the records of U.S. Congressional floor debates if the speeches are for or against proposed laws. To tackle this, we use the fact that these speeches are part of a larger conversation. This lets us gather information from how the parts of the discussion relate to each other, like if one person's speech agrees with another's opinion. We find that using this extra information helps a lot more than just looking at each speech alone. We introduce a method using support vector machines (a type of computer program) to decide if the speeches show support or opposition to proposed laws, using the records from these debates.	Integer
910	Fully Automatic Lexicon Expansion For Domain-Oriented Sentiment Analysis This paper proposes an unsupervised lexicon building method for the detection of polar clauses, which convey positive or negative aspects in a specific domain. The lexical entries to be acquired are called polar atoms, the minimum human-understandable syntactic structures that specify the polarity of clauses. As a clue to obtain candidate polar atoms, we use context coherency, the tendency for same polarities to appear successively in contexts. Using the overall density and precision of coherency in the corpus, the statistical estimation picks up appropriate polar atoms among candidates, without any manual tuning of the threshold values. The experimental results show that the precision of polarity assignment with the automatically acquired lexicon was 94% on average, and our method is robust for corpora in diverse domains and for the size of the initial lexicon. We validate that polar text units with the same polarity tend to appear together to make contexts coherent. We propose an algorithm to automatically expand an initial opinion lexicon based on context coherency, the tendency for same polarities to appear successively in contexts. We use conjunction rules to solve this problem from large domain corpora. We adopt domain knowledge by extracting sentiment words from the domain-specific corpus.	Fully Automatic Lexicon Expansion For Domain-Oriented Sentiment Analysis This paper suggests a way to automatically create a list of words or phrases that show positive or negative feelings in a specific area, without needing human guidance. These words or phrases are called polar atoms, which are the smallest understandable parts that show if a sentence is positive or negative. To find these polar atoms, we look at context coherency, which is the pattern of similar positive or negative words appearing together in a text. By looking at how often and how accurately these patterns show up in a large collection of text, we can identify the right polar atoms without having to adjust any settings manually. Experiments show that using this automatically created list, we can correctly identify if sentences are positive or negative 94% of the time, and this method works well across different topics and starting points. We confirm that sentences with the same positive or negative feelings often appear together, making the text flow smoothly. We suggest a method to automatically expand an initial list of opinion words based on context coherency, using the pattern of similar feelings appearing together. We use linking rules to handle this issue with large collections of text from specific areas. We incorporate knowledge about the area by picking out emotion-related words from texts specific to that area.	computer program
911	Joint Extraction Of Entities And Relations For Opinion Recognition We present an approach for the joint extraction of entities and relations in the context of opinion recognition and analysis. We identify two types of opinion-related entities — expressions of opinions and sources of opinions — along with the linking relation that exists between them. Inspired by Roth and Yih (2004), we employ an integer linear programming approach to solve the joint opinion recognition task, and show that global, constraint-based inference can significantly boost the performance of both relation extraction and the extraction of opinion-related entities. Performance further improves when a semantic role labeling system is incorporated. The resulting system achieves F-measures of 79 and 69 for entity and relation extraction, respectively, improving substantially over prior results in the area. We propose an ILP approach to jointly identify opinion holders, opinion expressions and their IS-FROM linking relations, and demonstrated the effectiveness of joint inference. Others extend the token-level approach to jointly identify opinion holders (Choi et al 2006), and to determine the polarity and intensity of the opinion expressions (Choi and Cardie, 2010).	Joint Extraction Of Entities And Relations For Opinion Recognition We present a method to find both entities (like things or people) and their relationships in opinion analysis. We look for two types of opinion-related entities: the opinions themselves and who is expressing them, and how they are connected. Inspired by Roth and Yih (2004), we use a method called integer linear programming (a math-based technique) to handle this task, showing that thinking about the whole picture with rules can greatly improve how well we identify relationships and opinion-related entities. It gets even better when we add a system that labels semantic roles (or the meaning of words in a sentence). Our system reaches scores of 79 and 69 out of 100 for identifying entities and relationships, much better than previous work. We suggest using the ILP method to jointly find who holds opinions, the opinions themselves, and how they are connected, showing this combined approach works well. Others have expanded this approach to also find out who holds opinions at the word level (Choi et al 2006) and to figure out if the opinions are positive or negative and how strong they are (Choi and Cardie, 2010).	specific areas
912	Broad-Coverage Sense Disambiguation And Information Extraction With A Supersense Sequence Tagger In this paper we approach word sense disambiguation and information extraction as a unified tagging problem. The task consists of annotating text with the tagset defined by the 41 Wordnet supersense classes for nouns and verbs. Since the tagset is directly related to Wordnet synsets, the tagger returns partial word sense disambiguation. Furthermore, since the noun tags include the standard named entity detection classes – person, location, organization, time, etc. – the tagger, as a by-product, returns extended named entity information. We cast the problem of supersense tagging as a sequential labeling task and investigate it empirically with a discriminatively-trained Hidden Markov Model. Experimental evaluation on the main sense-annotated datasets available, i.e., Semcor and Senseval, shows considerable improvements over the best known "first-sense" baseline. Our supersense tagger annotates text with a 46-label tag set of WNSS categories.	Broad-Coverage Sense Disambiguation And Information Extraction With A Supersense Sequence Tagger In this paper we tackle the challenge of understanding word meanings and extracting information from text as a single problem using tagging. The task involves labeling text with a set of 41 categories from Wordnet, which is like a dictionary, for nouns and verbs. Since this set is linked to Wordnet's word meanings, the tagger partly helps figure out word meanings. Also, because the noun labels include standard categories for identifying names of people, places, organizations, time, etc., the tagger also provides extra information about these names. We treat the task of assigning supersense labels as a process of labeling text in order, and we test this approach using a method called a Hidden Markov Model, which learns from examples to make predictions. Tests on the main datasets that have word meanings labeled, like Semcor and Senseval, show significant improvements over the commonly used basic method that guesses the most common meaning first. Our supersense tagger labels text with a set of 46 categories from Wordnet supersense categories (WNSS).	opinion analysis
913	Using WordNet-Based Context Vectors To Estimate The Semantic Relatedness Of Concepts In this paper, we introduce a WordNet-based measure of semantic relatedness by combining the structure and content of WordNet with co–occurrence information derived from raw text. We use the co–occurrence information along with the WordNet definitions to build gloss vectors corresponding to each concept in WordNet. Numeric scores of relatedness are assigned to a pair of concepts by measuring the cosine of the angle between their respective gloss vectors. We show that this measure compares favorably to other measures with respect to human judgments of semantic relatedness, and that it performs well when used in a word sense disambiguation algorithm that relies on semantic relatedness. This measure is flexible in that it can make comparisons between any two concepts without regard to their part of speech. In addition, it can be adapted to different domains, since any plain text corpus can be used to derive the co–occurrence information. We create aggregate co-occurrence vectors for a WordNet sense by adding the co-occurrence vectors of the words in its WordNet gloss. We introduce a vector measure to determine the relatedness between pairs of concepts.	Using WordNet-Based Context Vectors To Estimate The Semantic Relatedness Of Concepts In this paper, we introduce a way to measure how closely related concepts are in meaning using WordNet, a database of English words, by combining its structure and content with information from how words appear together in text. We use this information along with WordNet definitions to create "gloss vectors," which are like maps for each concept in WordNet. We give scores to show how related a pair of concepts are by looking at the angle between their gloss vectors. We show that this method is good at matching human judgments of how related concepts are in meaning, and it works well in algorithms that figure out which meaning of a word is being used based on relatedness. This method is flexible because it can compare any two concepts no matter what part of speech they are (like nouns or verbs). Plus, it can be adapted for different fields because any collection of text can be used to get the word appearance information. We make combined co-occurrence vectors for a WordNet sense by adding up the vectors of the words in its WordNet explanation. We introduce a way to use vectors to determine how closely concepts are related.	identifying names
914	Which Side Are You On? Identifying Perspectives At The Document And Sentence Levels In this paper we investigate a new problem of identifying the perspective from which a document is written. By perspective we mean a point of view, for example, from the perspective of Democrats or Republicans. Can computers learn to identify the perspective of a document? Not every sentence is written strongly from a perspective. Can computers learn to identify which sentences strongly convey a particular perspective? We develop statistical models to capture how perspectives are expressed at the document and sentence levels, and evaluate the proposed models on articles about the Israeli-Palestinian conflict. The results show that the proposed models successfully learn how perspectives are reflected in word usage and can identify the perspective of a document with high accuracy. We use hierarchical Bayesian modelling for opinion modelling (Lin et al, 2006). Our experiments were conducted in political debate corpus (Lin et al 2006). We explore relationships between sentence-level and document-level classification for a stance-like prediction task. We introduce implicit sentiment a topic of study in computational linguistics under the rubric of identifying perspective, though similar work had begun earlier in the realm of political science.	Which Side Are You On? Identifying Perspectives At The Document And Sentence Levels In this paper we explore a new problem of figuring out the point of view from which a document is written. By perspective, we mean a point of view, like from Democrats or Republicans. Can computers learn to find out the perspective of a document? Not every sentence clearly shows a perspective. Can computers learn to find which sentences clearly show a particular perspective? We create statistical models to understand how perspectives are shown in both the whole document and individual sentences, and test these models on articles about the Israeli-Palestinian conflict. The results show that the models can successfully learn how perspectives are shown in the words used and can identify the perspective of a document very accurately. We use hierarchical Bayesian modeling, a type of advanced statistical method, for understanding opinions (Lin et al, 2006). Our tests were done on a collection of political debates (Lin et al 2006). We look into connections between sentence-level and document-level classification for a task like predicting stance or viewpoint. We introduce implicit sentiment, which is a topic in computational linguistics (the study of language using computers) under the area of identifying perspective, although similar work started earlier in political science.	Based Context
915	CoNLL-X Shared Task On Multilingual Dependency Parsing Each year the Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems. The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing. In this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured. We also give an overview of the parsing approaches that participants took and the results that they achieved. Finally, we try to draw general conclusions about multi-lingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser? The CoNLL-X shared tasks focused on multilingual dependency parsing.	CoNLL-X Shared Task On Multilingual Dependency Parsing Each year the Conference on Computational Natural Language Learning (CoNLL) has a shared task where participants train and test their systems using the same data sets to easily compare how well they work. The tenth CoNLL (CoNLL-X) had a shared task on Multilingual Dependency Parsing, which is about understanding sentence structure in different languages. In this paper, we explain how language data for 13 languages was changed to the same format and how we checked the performance. We also provide a summary of the methods participants used and the results they got. Finally, we try to understand what makes a language or its data easier or harder to analyze and which parts are difficult for any system to understand. The CoNLL-X shared tasks focused on multilingual dependency parsing.	Perspectives
916	Experiments With A Multilanguage Non-Projective Dependency Parser We present a deterministic classifier-based Shift/Reduce parser. We develop DeSR, an incremental deterministic classifier-based parser. We propose a transition system whose individual transitions can deal with non-projective dependencies only to a limited extent, depending on the distance in the stack of the nodes involved in the newly constructed dependency.	Experiments With A Multilanguage Non-Projective Dependency Parser We introduce a Shift/Reduce parser that uses a fixed method to classify words. We create DeSR, a parser that builds sentence structure step-by-step using a fixed method. We suggest a system for changing sentence structure where each step can only handle non-projective dependencies (complex sentence structures) to a certain degree, based on how far apart the related words are in the sentence.	tenth CoNLL
917	Multilingual Dependency Analysis With A Two-Stage Discriminative Parser We present a two-stage multilingual dependency parser and evaluate it on 13 diverse languages. The first stage is based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages. The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph. We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis. We use post-processing for non-projective dependencies and for labeling. We treat the labeling of dependencies as a sequence labeling problem. The specific graph-based model studied in this work factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc.	Multilingual Dependency Analysis With A Two-Stage Discriminative Parser We introduce a two-step system to analyze sentence structures in multiple languages and test it on 13 different languages. The first step uses a model for finding the basic structure of sentences, as explained by researchers McDonald and Pereira in 2006, and adds details about word forms for some languages. The second step takes the result from the first step and adds labels to all parts of the sentence structure using a classifier trained to look at the whole structure. We share our findings based on specific data sets and discuss the mistakes found. We adjust the results afterward for certain complex structures and for adding labels. We consider adding labels to sentence parts as a problem of labeling things in a sequence. The model we explore looks at scores over pairs of connections between words, instead of just one at a time, and uses a thorough search for finding basic structures, along with a separate tool to add labels to each connection.	classify
918	Labeled Pseudo-Projective Dependency Parsing With Support Vector Machines We use SVM classifiers to predict the next action of a deterministic parser that builds labeled projective dependency graphs in an incremental fashion. Non-projective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser. We present evaluation results and an error analysis focusing on Swedish and Turkish. Our pseudo-projective approach transforms non-projective training trees to projective ones but encode the information necessary to make the inverse transformation in the DEPREL, so that this inverse transformation can also be carried out on the test trees (Nivre et al, 2006).	Labeled Pseudo-Projective Dependency Parsing With Support Vector Machines We use SVM (Support Vector Machine) classifiers to predict the next step of a parser (a tool that analyzes sentence structure) that builds labeled projective dependency graphs step-by-step. Non-projective dependencies (complex sentence connections) are handled indirectly by changing the training data for the classifiers to fit a simpler model and then changing it back for the parser's output. We show evaluation results and analyze errors, focusing on Swedish and Turkish languages. Our pseudo-projective method changes complex training trees into simpler ones but keeps the information needed to change them back in the DEPREL (dependency relation), allowing this reverse change on test trees as well (Nivre et al, 2006).	explained
919	Why Generative Phrase Models Underperform Surface Heuristics We investigate why weights from generative models underperform heuristic estimates in phrase-based machine translation. We first propose a simple generative, phrase-based model and verify that its estimates are inferior to those given by surface statistics. The performance gap stems primarily from the addition of a hidden segmentation variable, which increases the capacity for overfitting during maximum likelihood training with EM. In particular, while word level models benefit greatly from re-estimation, phrase-level models do not: the crucial difference is that distinct word alignments cannot all be correct, while distinct segmentations can. Alternate segmentations rather than alternate alignments compete, resulting in increased determinization of the phrase table, decreased generalization, and decreased final BLEU score. We also show that interpolation of the two methods can result in a modest increase in BLEU score. We try a different generative phrase translation model analogous to IBM word-translation Model 3 (Brown et al., 1993), and again find that the standard model outperforms their generative model. We explore estimation using EM of phrase pair probabilities under a conditional translation model based on the original source-channel formulation. We conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator.	Why Generative Phrase Models Underperform Surface Heuristics We look into why generative models, which create translations based on learned patterns, perform worse than simple rule-based estimates in translating phrases. We start by suggesting a basic model that creates phrases and find that it gives worse results than using straightforward statistics. The main reason for this is adding a hidden segmentation variable, which makes it easier to fit the model too closely to the training data during a method called maximum likelihood training with EM (a way to find best-fit parameters). Specifically, while models that translate individual words greatly improve with repeated estimates, models that translate phrases don't. This is because different ways to connect words can't all be right, but different ways to divide phrases can. These different divisions, rather than connections, compete with each other, leading to less flexible phrase lists, less ability to generalize, and lower BLEU scores (a measure of translation quality). We also show that mixing the two methods can slightly improve BLEU scores. We test another model similar to IBM's word-translation Model 3 and again find the simple model does better. We look at estimating the chance of phrase pairs using a conditional translation model, which is based on an older method of source-channel models. We conclude that using segmentation variables in the generative model leads to fitting the training data too closely, even though it makes the data look more likely than when using rule-based estimates.	changes complex
920	Discriminative Reordering Models For Statistical Machine Translation We present discriminative reordering models for phrase-based statistical machine translation. The models are trained using the maximum entropy principle. We use several types of features: based on words, based on word classes, based on the local context. We evaluate the overall performance of the reordering models as well as the contribution of the individual feature types on a word-aligned corpus. Additionally, we show improved translation performance using these reordering models compared to a state-of-the-art baseline system. Despite their high perplexities, reordered LMs yield some improvements when integrated to a PSMT baseline that already includes a discriminative phrase orientation model. To lexicalize reordering, a discriminative reordering model (Zens and Ney, 2006a) is used. We use clustered word classes in a discriminate reordering model, and show that they reduce the classification error rate.	Discriminative Reordering Models For Statistical Machine Translation We present discriminative reordering models for phrase-based statistical machine translation. These models help decide the best order of words in translations. The models are trained using the maximum entropy principle, a method that uses probabilities to make decisions. We use several types of features: based on words, based on word classes (groups of similar words), and based on the local context (nearby words). We evaluate how well these reordering models work overall and how each type of feature helps by testing on a word-aligned corpus (a set of matched words in different languages). Additionally, we show improved translation performance using these reordering models compared to a state-of-the-art baseline system, meaning our method is better than the most advanced existing system. Despite their high perplexities (how uncertain or confused the models are), reordered LMs (language models) yield some improvements when integrated into a PSMT (phrase-based statistical machine translation) baseline that already includes a discriminative phrase orientation model (a system that decides the order of phrases). To lexicalize reordering (making the word order more natural), a discriminative reordering model (Zens and Ney, 2006a) is used. We use clustered word classes in a discriminative reordering model and show that they reduce the classification error rate (the number of mistakes in putting things in the correct order).	connections
921	Manual And Automatic Evaluation Of Machine Translation Between European Languages We evaluated machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back. Evaluation was done automatically using the BLEU score and manually on fluency and adequacy. The results of the workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems. We report and analyze several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric.	Manual And Automatic Evaluation Of Machine Translation Between European Languages We checked how well machine translation works for six European language pairs that were part of a shared project: translating texts between French, German, Spanish, and English. We used the BLEU score, an automatic measurement, and also checked by hand for how natural and correct the translations were. The workshop results showed that the BLEU score often underestimated the quality of translations made by systems based on set rules. We discuss and look into several cases where human judges and the BLEU score strongly disagreed on which systems were the best.	maximum entropy
922	Syntax Augmented Machine Translation Via Chart Parsing We present translation results on the shared task ”Exploiting Parallel Texts for Statistical Machine Translation” generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories. We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence. Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. We present results on the French-to-English task for this workshop, representing significant improvements over the workshop’s baseline system. Our translation system is available open-source under the GNU General Public License. In our work, syntax is successfully integrated into hierarchical SMT. We start with a complete set of phrases as extracted by traditional PBMT heuristics, and then annotated the target side of each phrasal entry with the label of the constituent node in the target-side parse tree that subsumes the span. We use broken syntactic fragments to augment their grammars to increase the rule coverage; while we learn optimal tree fragments transformed from the original ones via a generative framework, they enumerate the fragments available from the original trees without learning process.	Syntax Augmented Machine Translation Via Chart Parsing We show translation results for the task called "Using Parallel Texts for Statistical Machine Translation." This is created by using a special chart parsing decoder that works with phrase tables, which are lists of word groups, that are improved and expanded with grammar rules from the target language. We use a tool called a parser to create tree diagrams for each sentence in the target language part of the training data. These diagrams are matched with word group maps created for the source language sentence. By looking at word groups that match the grammar rules in these tree diagrams, we develop methods to improve (assign a grammar-based category to a pair of words) and expand (create mixed phrases using both basic and complex phrases) the word list into a bilingual grammar system. We show outcomes for translating French to English, which are much better than the basic system used in the workshop. Our translation system is available for free under the GNU General Public License. In our research, we successfully add grammar rules to a complex type of machine translation. We start with a complete list of word groups made by standard methods, then add grammar labels to each word group in the target language based on the grammar tree. We use broken grammar pieces to improve their rules and increase the number of rules; while we learn the best tree pieces changed from original ones using a special method, they just list the pieces from the original trees without a learning process.	Automatic
923	A Syntax-Directed Translator With Extended Domain Of Locality A syntax-directed translator first parses the source-language input into a parse tree, and then recursively converts the tree into a string in the target-language. We model this conversion by an extended tree-to-string transducer that have multi-level trees on the source-side, which gives our system more expressive power and flexibility. We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation. The model is then extended to the general log-linear frame work in order to rescore with other features like n-gram language models. We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring. Initial experimental results on English-to-Chinese translation are presented. We study a TSG-based tree-to-string alignment model. We define the Extended Tree-to-String Transducer.	A Syntax-Directed Translator With Extended Domain Of Locality A syntax-directed translator first breaks down the input language into a structured format, and then step-by-step changes this structure into a string in the desired language. We demonstrate this change using an advanced method that works with multi-layered structures, giving our system more ability to express and adapt. We also create a straightforward probability model and use a fast algorithm to find the best translation. The model is then expanded to a broader approach to improve results with additional tools like language models that predict word sequences. We create an easy yet effective method to produce multiple top translations without repeats for further evaluation. Initial test results on translating English to Chinese are shared. We explore a model based on tree structures for aligning language elements. We explain the Extended Tree-to-String Transducer.	target language
924	Seeing Stars When There Aren’t Many Stars: Graph-Based Semi-Supervised Learning For Sentiment Categorization We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference. Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., “4 stars”), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text. In particular, we are interested in the situation where labeled data is scarce. We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve rating inference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training. We adapt semi-supervised graph-based methods for sentiment analysis but do not incorporate lexical prior knowledge in the form of labeled features. We propose a semisupervised learning approach to the rating inference problem in scenarios where labeled training data is scarce.	Seeing Stars When There Aren’t Many Stars: Graph-Based Semi-Supervised Learning For Sentiment Categorization We present a method that uses graphs and a mix of labeled and unlabeled data to help figure out the sentiment or feeling behind a text, like movie reviews, even when we don't have many examples with known ratings, like "4 stars." The goal is to guess the ratings of texts we don't know yet, based on the mood or emotion they show. We focus on situations where we don't have a lot of labeled data, meaning examples where we already know the rating. We show that using both known and unknown ratings together helps make better guesses about the unknown ones. We do this by making a graph, a kind of map, that includes both types of data to help us make educated guesses. Then, we solve a math problem to smooth out the ratings across this map. This approach does a better job at predicting ratings than other methods that don't use the extra data. We use this graph method for figuring out sentiment but we don't use any pre-existing word knowledge or features. We suggest using this method to solve the problem of guessing ratings when we have very few examples to learn from.	Directed Translator
925	Chinese Whispers - An Efficient Graph Clustering Algorithm And Its Application To Natural Language Processing Problems We introduce Chinese Whispers, a randomized graph-clustering algorithm, which is time-linear in the number of edges. After a detailed definition of the algorithm and a discussion of its strengths and weaknesses, the performance of Chinese Whispers is measured on Natural Language Processing (NLP) problems as diverse as language separation, acquisition of syntactic word classes and word sense disambiguation. At this, the fact is employed that the small-world property holds for many graphs in NLP. We introduce the co-occurrence based graph clustering framework.	Chinese Whispers - An Efficient Graph Clustering Algorithm And Its Application To Natural Language Processing Problems We introduce Chinese Whispers, a graph-clustering algorithm that uses randomness and works quickly as it only needs to consider the connections (edges) once. After we clearly explain how the algorithm works and discuss its good and bad points, we test how well Chinese Whispers performs on different Natural Language Processing (NLP) tasks like separating languages, finding word types based on grammar, and figuring out the meaning of words in context. In doing so, we use the fact that many graphs in NLP have a feature called the small-world property, meaning they are highly connected. We also introduce a method for grouping based on how often things occur together in a graph.	educated
926	Inversion Transduction Grammar for Joint Phrasal Translation Modeling We present a phrasal inversion transduction grammar as an alternative to joint phrasal translation models. This syntactic model is similar to its flat-string phrasal predecessors, but admits polynomial-time algorithms for Viterbi alignment and EM training. We demonstrate that the consistency constraints that allow flat phrasal models to scale also help ITG algorithms, producing an 80-times faster inside-outside algorithm. We also show that the phrasal translation tables produced by the ITG are superior to those of the flat joint phrasal model, producing up to a 2.5 point improvement in BLEU score. Finally, we explore, for the first time, the utility of a joint phrasal translation model as a word alignment method. We use synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences.	Inversion Transduction Grammar for Joint Phrasal Translation Modeling We introduce a new type of grammar called "phrasal inversion transduction grammar" to use instead of the usual joint phrasal translation models. This type of grammar works like previous models that use a straightforward string of phrases, but it allows us to use faster methods to align words and train the model. We show that the rules that help older models work better also make these new methods 80 times faster. We also prove that the translation tables made by this new grammar are better, improving a quality score called BLEU by up to 2.5 points. Lastly, we look at how this joint phrasal translation model can help us match words correctly for the first time. We use a method called synchronous ITG and some rules to find phrases that don't directly translate word-for-word.	different Natural
927	CCG Supertags in Factored Statistical Machine Translation Combinatorial Categorial Grammar (CCG) supertags present phrase-based machine translation with an opportunity to access rich syntactic information at a word level. The challenge is incorporating this information into the translation process. Factored translation models allow the inclusion of supertags as a factor in the source or target language. We show that this results in an improvement in the quality of translation and that the value of syntactic supertags in flat structured phrase-based models is largely due to better local reorderings. we exploit factored phrase-based translation models to associate each word with a supertag, which contains most of the information needed to build a full parse.	CCG Supertags in Factored Statistical Machine Translation Combinatorial Categorial Grammar (CCG) supertags give phrase-based machine translation a chance to use detailed grammatical information for each word. The challenge is how to use this information in the translation process. Factored translation models let us add supertags as a part of either the original or the translated language. We demonstrate that this improves translation quality and that the main benefit of using these grammatical supertags in basic phrase-based models is mainly due to better local rearrangements of words. We use factored phrase-based translation models to link each word with a supertag, which holds most of the information needed to understand the entire sentence structure.	align words
928	Mixture-Model Adaptation for SMT We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components. We investigate a number of variants on this approach, including cross-domainversus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; different methods of assigning weights; and granularity of the source unit being adapted to. The best methods achieve gains of approximately one BLEU percentage point over a state-of-the art non-adapted baseline system. We conclude that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log linearly. We interpolate the in and general-domain phrase tables together, assigning either linear or log-linear weights to the entries in the tables before combining overlapping entries; this is now standard practice.	Mixture-Model Adaptation for SMT We explain a method called mixture-model to adjust a Statistical Machine Translation System for new topics, using weights that depend on how close the text is to different parts of the model. We look into different versions of this method, including changing between topics versus adapting as you go; using straight-line versus log-line combinations; adjusting language and translation models; different ways to assign weights; and how detailed the part of the text being adapted is. The best methods improve by about one BLEU percentage point over a top-level system that hasn’t been adjusted. We find that the best way is to linearly combine submodels of the same kind (like several different Translation Models or several different Language Models), while combining models of different kinds (like a mix of a Translation Model with a Language Model) using log-line methods. We mix the specific topic and general topic phrase tables together, giving either straight-line or log-line weights to the parts in the tables before combining parts that overlap; this is now common practice.	grammatical
929	(Meta-) Evaluation of Machine Translation This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back. We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process. We measured timing and intra- and inter-annotator agreement for three types of subjective evaluation. We measured the correlation of automatic evaluation metrics with human judgments. This meta-evaluation reveals surprising facts about the most commonly used methodologies. We show that ranking sentences gives higher inter-annotator agreement than scoring adequacy and fluency.	(Meta-) Evaluation of Machine Translation This paper looks at how well machine translation systems can translate between 8 language pairs, specifically French, German, Spanish, and Czech to English and the other way around. We did a detailed evaluation by people, which helped us not only list the different machine translation (MT) systems in order of quality but also allowed us to look deeper into how we evaluate them. We checked the time taken and how much the evaluators agreed with each other, both within the same group (intra-annotator) and between different groups (inter-annotator), for three types of personal judgment. We also compared automatic evaluation tools with human opinions to see how well they match. This deeper examination (meta-evaluation) shows surprising things about the most commonly used methods. We found that organizing sentences in order works better for getting evaluators to agree than just scoring how accurate and smooth the sentences are.	tables before
930	Experiments in Domain Adaptation for Statistical Machine Translation The special challenge of the WMT 2007 shared task was domain adaptation. We took this opportunity to experiment with various ways of adapting a statistical machine translation systems to a special domain (here: news commentary), when most of the training data is from a different domain (here: European Parliament speeches). This paper also gives a description of the submission of the University of Edinburgh to the shared task. Factored translation models is used for the integration of domain adaptation. We use two language models and two translation models: one in-domain and other out-of-domain to adapt the system. We learn mixture weights for language models trained with in-domain and out of-domain data respectively by minimizing the perplexity of a tuning (development) set and interpolating the models.	Experiments in Domain Adaptation for Statistical Machine Translation The special challenge of the WMT 2007 shared task was domain adaptation. We used this chance to try different ways of adjusting a statistical machine translation system to a specific topic area (here: news commentary), even though most training information comes from a different area (here: European Parliament speeches). This paper also explains the submission from the University of Edinburgh for the shared task. We used factored translation models, which are a type of model that considers different aspects of language to help with domain adaptation. We use two language models and two translation models: one for the specific topic and one for the other topic, to adjust the system. We find the best combination of these models by using weights for language models trained with both specific topic and other topic data, aiming to make a test set easier to understand and blending the models together.	translation
931	METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments Meteor is an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more commonly used Bleu metric. It is one of several automatic metrics used in this year’s shared task within the ACL WMT-07 workshop. This paper recaps the technical details underlying the metric and describes recent improvements in the metric. The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and German, in addition to English. In an MT evaluation setting, sense clusters have been integrated into an Mt evaluation metric (METEOR) and brought about an increase of the metric's correlation with human judgments of translation quality in different languages.	METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments Meteor is a tool for evaluating machine translations, and it has been shown to closely match human opinions on translation quality, doing better than the commonly used Bleu metric. It's one of several tools tested in this year's shared task at the ACL WMT-07 workshop. This paper reviews the technical details behind the tool and explains recent improvements. The latest version includes better settings and now can evaluate translations in Spanish, French, and German, as well as English. In evaluating machine translations, groups of similar meanings (sense clusters) have been added to METEOR, improving its agreement with human opinions on translation quality in various languages.	machine
932	The Third PASCAL Recognizing Textual Entailment Challenge This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems. In creating this year’s dataset, a number of longer texts were introduced to make the challenge more oriented to realistic scenarios. Additionally, a pool of resources was offered so that the participants could share common tools. A pilot task was also set up, aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions. 26 participants submitted 44 runs, using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges. The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al,2007). Textual Entailment (TE) has become a prominent paradigm for modeling semantic inference, capturing the needs of a broad range of text understanding applications.	The Third PASCAL Recognizing Textual Entailment Challenge This paper introduces the Third PASCAL Recognizing Textual Entailment Challenge (RTE-3), giving a summary of how the dataset was made and the systems that were submitted. For this year's dataset, longer texts were added to make the challenge more like real-life situations. Also, a set of resources was provided so participants could use the same tools. A trial task was set up to tell apart unknown entailments (where it's not clear if one sentence logically follows from another) from contradictions (where one sentence clearly disagrees with another) and to explain the system's overall decisions. 26 participants submitted 44 attempts, using different methods, usually introducing new ways to recognize entailment and achieving better results than in previous challenges. The task of recognizing textual entailment is to decide if the hypothesis sentence (a statement to check) can logically follow from the premise sentence (the given sentence) (Giampiccolo et al, 2007). Textual Entailment (TE) has become an important approach for understanding meaning, meeting the needs of many text understanding applications.	opinions
933	Detection of Grammatical Errors Involving Prepositions This paper presents ongoing work on the detection of preposition errors of non-native speakers of English. Since prepositions account for a substantial proportion of all grammatical errors by ESL (English as a Second Language) learners, developing an NLP application that can reliably detect these types of errors will provide an invaluable learning resource to ESL students. To address this problem, we use a maximum entropy classifier combined with rule-based filters to detect preposition errors in a corpus of student essays. Although our work is preliminary, we achieve a precision of 0.8 with a recall of 0.3. Chodorow et al (2007) employed a maximum entropy model to estimate the probability of 34 prepositions based on 25 local context features ranging from words to NP/VP chunks. A context is represented by 25 lexical features and 4 combination features: Lexical Token and POS n-grams in a 2 word window around the preposition, plus the head verb in the preceding verb phrase (PV), the head noun in the preceding noun phrase (PN) and the head noun in the following noun phrase (FN) when available.	Detection of Grammatical Errors Involving Prepositions This paper talks about ongoing work to find preposition mistakes made by people learning English who aren't native speakers. Since prepositions make up a large part of grammar mistakes by ESL (English as a Second Language) learners, creating a computer program (NLP application) that can spot these mistakes accurately will be a great help for ESL students. To solve this, we use a method called a maximum entropy classifier along with some rules to find preposition mistakes in a collection of student essays. Even though we are just starting, we successfully identify errors with 80% accuracy (precision) but only find 30% of all errors (recall). Chodorow and others (2007) used a similar method to predict the use of 34 prepositions by looking at 25 nearby context features, which include words and grammatical chunks. A context is described using 25 word features and 4 combination features, such as word patterns and parts of speech (POS) around the preposition, as well as the main verb before it (PV), the main noun before it (PN), and the main noun after it (FN), if available.	usually
934	SemEval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other supervised and knowledge-based systems. In total there were 6 participating systems. We reused the SemEval-2007 English lexical sample subtask of task 17, and set up both clustering-style unsupervised evaluation (using Onto Notes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping). We provide a comparison to the results of the systems participating in the lexical sample subtask of task 17. The object of the sense induction task of SENSEVAL-4 is to cluster 27,132 instances of 100 different words (35 nouns and 65 verbs) into senses or classes. Graph-based methods have been employed for word sense induction.	SemEval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems The goal of this task is to compare different systems that figure out the meaning of words and tell them apart, and also to compare these systems to other systems that are trained using examples or rely on existing knowledge. In total, there were 6 systems taking part. We used a part of the SemEval-2007 English test that deals with understanding word meanings, and set up both a test that groups things without prior training (using Onto Notes word meanings as the standard answer) and a test that uses training (using part of the data to match answers). We provide a comparison to the results of the systems that took part in the word meaning test of task 17. The goal of the word meaning task of SENSEVAL-4 is to group 27,132 examples of 100 different words (35 nouns and 65 verbs) into meanings or categories. Methods using graphs, which are like networks of connected points, have been used to figure out word meanings.	prepositions
935	SemEval-2007 Task 07: Coarse-Grained English All-Words Task This paper presents the coarse-grained English all-words task at SemEval-2007. We describe our experience in producing a coarse version of the WordNet sense inventory and preparing the sense-tagged corpus for the task. We present the results of participating systems and discuss future directions. We show that the performance of WSD systems clearly indicates that WSD is not easy unless one adopts a coarse-grained approach, and then systems tagging all words at best perform a few percentage points above the most frequent sense heuristic (Navigli et al, 2007).	SemEval-2007 Task 07: Coarse-Grained English All-Words Task This paper talks about the coarse-grained (simplified) English all-words task at SemEval-2007. We explain our experience in creating a simpler version of the WordNet sense inventory (a list of meanings) and getting the sense-tagged (labeled with meanings) text ready for the task. We show the results from different systems that took part and talk about future plans. We demonstrate that the performance of Word Sense Disambiguation (WSD, a task to find meanings of words in context) systems clearly shows that WSD is not easy unless one uses a coarse-grained (simplified) approach, and then systems tagging all words only perform slightly better than just guessing the most common meaning (Navigli et al, 2007).	SemEval
936	SemEval-2007 Task 10: English Lexical Substitution Task In this paper we describe the English Lexical Substitution task for SemEval. In the task, annotators and systems find an alternative substitute word or phrase for a target word in context. The task involves both finding the synonyms and disambiguating the context. Participating systems are free to use any lexical resource. There is a subtask which requires identifying cases where the word is functioning as part of a multiword in the sentence and detecting what that multiword is. In the lexical substitution task, a system attempts to generate a word (or a set of words) to replace a target word, such that the meaning of the sentence is preserved. We establish a benchmark for context-sensitive lexical similarity models.	SemEval-2007 Task 10: English Lexical Substitution Task In this paper we describe the English Lexical Substitution task for SemEval. In the task, annotators and systems find another word or phrase to replace a target word in a sentence. The task involves both finding similar words (synonyms) and understanding the sentence context. Participating systems can use any word-related resource. There is a subtask that requires finding cases where the word is part of a phrase and figuring out what that phrase is. In the lexical substitution task, a system tries to create a word (or group of words) to replace a target word, while keeping the sentence's meaning the same. We set a standard for models that compare word meanings based on context.	meaning
937	The SemEval-2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task This paper presents the task definition, resources, participation, and comparative results for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise. This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name. We consider the problem of disambiguating person names in a Web searching scenario. The goal of the Web People Search task is to assign Web pages to groups, where each group contains all (and only those) pages that refer to one unique entity. Our Web Persona Search (WePS) task has created a benchmark dataset.	The SemEval-2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task This paper introduces the task details, available resources, participant involvement, and results comparison for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise. This task involves grouping together a set of documents that mention a person’s name that can refer to multiple people, based on who is actually being referred to. We look at the challenge of figuring out which person a name refers to in a web search situation. The aim of the Web People Search task is to sort web pages into groups, where each group includes all and only the pages about one specific person. Our Web Persona Search (WePS) task has created a standard set of data for comparison.	cases where
938	SemEval-2007 Task 15: TempEval Temporal Relation Identification The TempEval task proposes a simple way to evaluate automatic extraction of temporal relations. It avoids the pitfalls of evaluating a graph of inter-related labels by defining three subtasks that allow pairwise evaluation of temporal relations. The task not only allows straightforward evaluation, it also avoids the complexities of full temporal parsing. Temporal information processing is a topic of natural language processing boosted by our evaluation campaign TempEval. TempEval07 (Verhagen et al, 2007) integrated 14 TLINK relations into three: before, after, and.	SemEval-2007 Task 15: TempEval Temporal Relation Identification The TempEval task suggests an easy method to test how well computers can find time-related connections in text. It avoids the difficulties of judging a network of related labels by having three smaller tasks that let us evaluate time-related connections one pair at a time. This task not only makes evaluation simple but also avoids the complicated process of fully understanding all time-related information in text. Understanding time-related information in text is an important part of language processing, and our project, TempEval, has helped improve it. TempEval07 (Verhagen et al, 2007) grouped 14 time link (TLINK) connections into three simple ones: before, after, and.	referred
939	SemEval-2007 Task-17: English Lexical Sample SRL and All Words This paper describes our experience in preparing the data and evaluating the results for three subtasks of SemEval-2007 Task-17 - Lexical Sample, Semantic Role Labeling (SRL) and All-Words respectively. We tabulate and analyze the results of participating systems. The use of coarse-grained sense groups (Palmer et al, 2007) has led to considerable advances in WSD performance, with accuracies of around 90%.	SemEval-2007 Task-17: English Lexical Sample SRL and All Words This paper describes our experience in preparing the data and evaluating the results for three parts of SemEval-2007 Task-17 - Lexical Sample, Semantic Role Labeling (SRL, which is identifying the roles words play in a sentence) and All-Words. We organize and examine the results of participating systems. Using broad sense groups (Palmer et al, 2007) has significantly improved Word Sense Disambiguation (WSD, which is figuring out the correct meaning of words) performance, with accuracies of about 90%.	related
940	SemEval-2007 Task 19: Frame Semantic Structure Extraction This task consists of recognizing words and phrases that evoke semantic frames as defined in the FrameNet project (http://framenet.icsi.berkeley.edu), and their semantic dependents, which are usually, but not always, their syntactic dependents (including subjects). The training data was FN annotated sentences. In testing, participants automatically annotated three previously unseen texts to match gold standard (human) annotation, including predicting previously unseen frames and roles. Precision and recall were measured both for matching of labels of frames and FEs and for matching of semantic dependency trees based on the annotation. Our shared tasks shows that frame-semantic SRL of running text is a hard problem, partly due to the fact that running text is bound to contain many frames for which no or little annotated training data are available.	SemEval-2007 Task 19: Frame Semantic Structure Extraction This task involves identifying words and phrases that trigger semantic frames, which are patterns of meaning, as defined by the FrameNet project (http://framenet.icsi.berkeley.edu). It also involves identifying their related words, which are often, but not always, their grammatical dependents (like subjects). The training data consisted of sentences that were already labeled with FrameNet information. During testing, participants automatically labeled three new texts to match a perfect standard set by humans, including predicting new patterns and roles that hadn't been seen before. Accuracy and completeness were measured both for matching the labels of frames and Frame Elements (FEs) and for matching the structures that show how words depend on each other based on the labels. Our shared tasks show that identifying frame-semantic structures in text is challenging, partly because texts often include many frames for which there is little or no labeled training data.	improved
941	On the Complexity of Non-Projective Data-Driven Dependency Parsing In this paper we investigate several non-projective parsing algorithms for dependency parsing, providing novel polynomial time solutions under the assumption that each dependency decision is independent of all the others, called here the edge-factored model. We also investigate algorithms for non-projective parsing that account for non-local information, and present several hardness results. This suggests that it is unlikely that exact non-projective dependency parsing is tractable for any model richer than the edge-factored model. We claim that the main obstacle is that non-projective parsing is NP-hard beyond arc-factored models.	On the Complexity of Non-Projective Data-Driven Dependency Parsing In this paper, we look at different algorithms for parsing sentences that don't follow a straight line structure (non-projective parsing). We offer new solutions that work efficiently (in polynomial time) by assuming that each decision about how words are related is made separately, which we call the edge-factored model. We also explore algorithms that consider more complex information and present some difficult problems related to these. This means it's probably not feasible to do exact non-projective parsing for models more complex than the edge-factored model. We argue that the main challenge is that non-projective parsing becomes very complicated (NP-hard) when moving beyond simple models like the arc-factored models.	related
942	Further Meta-Evaluation of Machine Translation This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish. We report the translation quality of over 30 diverse translation systems based on a large-scale manual evaluation involving hundreds of hours of effort. We use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strength of the correlation with human judgments at both the system-level and at the sentence-level. We validate our manual evaluation methodology by measuring intra- and inter-annotator agreement, and collecting timing information. Thus, the human an notation for the WMT 2008 dataset was collected in the form of binary pairwise preferences that are considerably easier to make. Traditionally, human ratings for MT quality have been collected in the form of absolute scores on a five or seven-point Likert scale, but low reliability numbers for this type of annotation have raised concerns.	Further Meta-Evaluation of Machine Translation This paper examines how well machine translation systems translate between 10 language pairs, including Czech, English, French, German, Hungarian, and Spanish. We present the translation quality of over 30 different translation systems based on a detailed manual evaluation that took hundreds of hours. We use human opinions about the systems to examine automatic methods for evaluating translation quality and report how closely these methods match human opinions at both the overall system level and for individual sentences. We confirm the reliability of our manual evaluation method by checking the consistency of ratings from the same person (intra-annotator agreement) and between different people (inter-annotator agreement), along with the time taken for evaluation. The human ratings for the WMT 2008 dataset were collected as simple yes-or-no comparisons, which are much easier to make. Usually, human ratings for machine translation quality are given as absolute scores on a scale of five or seven points, but concerns have been raised due to their low reliability.	different
943	Optimizing Chinese Word Segmentation for Machine Translation Performance Previous work has shown that Chinese word segmentation is useful for machine translation to English, yet the way different segmentation strategies affect MT is still poorly understood. In this paper, we demonstrate that optimizing segmentation for an existing segmentation standard does not always yield better MT performance. We find that other factors such as segmentation consistency and granularity of Chinese “words” can be more important for machine translation. Based on these findings, we implement methods inside a conditional random field segmenter that directly optimize segmentation granularity with respect to the MT task, providing an improvement of 0.73 BLEU. We also show that improving segmentation consistency using external lexicon and proper noun features yields a 0.32 BLEU increase. We develop the CRF-based Stanford Chinese segmenter that is trained on the segmentation of the Chinese Treebank for consistency. We enhance a CRF s segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence.	Optimizing Chinese Word Segmentation for Machine Translation Performance Previous studies have shown that dividing Chinese text into words is helpful for translating into English, but it's still unclear how different ways of dividing words affect translation. In this paper, we show that just following existing standards for word division doesn't always make translations better. We found that other things, like keeping word division consistent and deciding how detailed the division should be, can be more important for translation. Based on this, we use methods in a tool called a conditional random field segmenter to directly adjust how detailed the word division is to help with translation, which improves the score by 0.73 BLEU (a measure of translation quality). We also show that making word division more uniform by using external dictionaries and recognizing proper names improves the score by 0.32 BLEU. We created the CRF-based Stanford Chinese segmenter, which is trained to follow the Chinese Treebank's word division for uniformity. We improved a CRF's word division tool for translation tasks by adjusting the detail of word division and making it more consistent.	systems translate
944	Parallel Implementations of Word Alignment Tool Training word alignment models on large corpora is a very time-consuming processes. This paper describes two parallel implementations of GIZA++ that accelerate this word alignment process. One of the implementations runs on computer clusters, the other runs on multi-processor system using multi-threading technology. Results show a near-linear speed-up according to the number of CPUs used, and alignment quality is preserved. We use a multi-threaded version of the GIZA++ tool. This speeds up the process and corrects an error of GIZA++ that can appear with rare words.	Parallel Implementations of Word Alignment Tool Training word alignment models on large sets of text is a very time-consuming task. This paper describes two parallel versions of GIZA++ that make this word alignment process faster. One version runs on groups of computers working together (computer clusters), and the other runs on a single computer with several processors using multi-threading technology (a way for a computer to do multiple tasks at once). Results show that the process becomes almost proportionally faster with more CPUs (computer processing units) used, and the quality of alignment (matching words correctly) is maintained. We use a version of the GIZA++ tool that can do multiple tasks at once (multi-threaded version). This speeds up the process and fixes a mistake in GIZA++ that can happen with uncommon words.	Previous
945	The Stanford Typed Dependencies Representation This paper examines the Stanford typed dependencies representation, which was designed to provide a straightforward description of grammatical relations for any user who could benefit from automatic text understanding. For such purposes, we argue that dependency schemes must follow a simple design and provide semantically contentful information, as well as offer an automatic procedure to extract the relations. We consider the underlying design principles of the Stanford scheme from this perspective, and compare it to the GR and PARC representations. Finally, we address the question of the suitability of the Stanford scheme for parser evaluation. Stanford dependencies provide a simple description of relations between pairs of words in a sentence.	The Stanford Typed Dependencies Representation This paper looks at the Stanford typed dependencies representation, which was made to give an easy-to-understand description of how parts of a sentence relate to each other, helping anyone who needs to understand text automatically. For this, we believe that these systems should be simple and give meaningful information, as well as have a way to automatically find these relationships. We examine the basic design ideas of the Stanford system from this point of view and compare it to the GR and PARC systems. Lastly, we discuss whether the Stanford system is good for testing sentence analyzers (parsers). Stanford dependencies provide a simple description of relations between pairs of words in a sentence.	multiple tasks
946	TAG Dynamic Programming and the Perceptron for Efficient Feature-Rich Parsing We describe a parsing approach that makes use of the perceptron algorithm, in conjunction with dynamic programming methods, to recover full constituent-based parse trees. The formalism allows a rich set of parse-tree features, including PCFG-based features, bigram and trigram dependency features, and surface features. A severe challenge in applying such an approach to full syntactic parsing is the efficiency of the parsing algorithms involved. We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism. A lower-order dependency parsing model is used to restrict the search space of the full model, thereby making it efficient. Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy. Many edges can be ruled out beforehand based on the marginals computed from a simpler parsing model (Carreras et al2008).	TAG Dynamic Programming and the Perceptron for Efficient Feature-Rich Parsing We describe a parsing method that uses the perceptron algorithm (a simple type of machine learning) along with dynamic programming (a method to solve problems step by step) to create complete parse trees (structures showing how sentences are built). This method allows for detailed features of parse trees, like PCFG-based features (probability rules), bigram and trigram dependency features (relationships between two or three words), and surface features (basic sentence parts). A big challenge in using this method for full sentence parsing is making the parsing process fast enough. We show that fast training is possible using a Tree Adjoining Grammar (TAG) based parsing method. A simpler dependency parsing model is used to narrow down the options for the full model, making it faster. Tests on the Penn WSJ treebank (a collection of sentence examples) show that the model works very well, accurately understanding sentence structure and word relationships. Many connections can be ignored early on by using results from a simpler model (Carreras et al2008).	dependencies
947	The CoNLL 2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies The Conference on Computational Natural Language Learning is accompanied every year by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting. In 2008 the shared task was dedicated to the joint parsing of syntactic and semantic dependencies. This shared task not only unifies the shared tasks of the previous four years under a unique dependency-based formalism, but also extends them significantly: this year's syntactic dependencies include more information such as named-entity boundaries; the semantic dependencies model roles of both verbal and nominal predicates. In this paper, we define the shared task and describe how the data sets were created. Furthermore, we report and analyze the results and describe the approaches of the participating systems. We first introduce the predicate classification task, which can be regarded as the predicate sense disambiguation. The complete merging process and the conversion from the constituent representation to dependencies is detailed in this work.	The CoNLL 2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies The Conference on Computational Natural Language Learning has a yearly project aimed at improving tools that help computers understand human language and testing them in a consistent way. In 2008, the project focused on both syntactic and semantic dependencies, which means understanding sentence structure and meaning at the same time. This task brought together ideas from the past four years and added new features: this year, syntactic dependencies included more details like named-entity boundaries (identifying names of people, places, etc.), and semantic dependencies looked at roles of actions and things in sentences. In this paper, we explain what the task was about and how we prepared the data. We also discuss the results and how different systems approached the task. We start by explaining the task of classifying predicates, which is like figuring out the right meaning of a word based on context. We describe in detail how we combined information and changed sentence structures to understand them better.	possible
948	Dependency-based Syntactic&#x2013;Semantic Analysis with PropBank and NomBank This paper presents our contribution in the closed track of the 2008 CoNLL Shared Task (Surdeanu et al., 2008). To tackle the problem of joint syntactic-semantic analysis, the system relies on a syntactic and a semantic subcomponent. The syntactic model is a bottom-up projective parser using pseudo-projective transformations, and the semantic model uses global inference mechanisms on top of a pipeline of classifiers. The complete syntactic-semantic output is selected from a candidate pool generated by the subsystems. The system achieved the top score in the closed challenge: a labeled syntactic accuracy of 89.32%, a labeled semantic F1 of 81.65, and a labeled macro F1 of 85.49. Our system use two 30 different subsystems to handle verbal and nominal predicates, respectively. We present importance of capturing non-local dependencies of core arguments in predicate-argument structure analysis. In our work, the impact of different grammatical representations on the task of frame-based shallow semantic parsing is studied and the poor lexical generalization problem is outlined.	Dependency-based Syntactic–Semantic Analysis with PropBank and NomBank This paper presents what we contributed to the 2008 CoNLL Shared Task (Surdeanu et al., 2008). To solve the problem of analyzing sentence structure and meaning together, the system uses two parts: one for syntax (sentence structure) and one for semantics (meaning). The syntax part is a step-by-step parser that uses tricks to handle complex sentence structures, and the meaning part uses methods to make decisions based on a sequence of decision-making tools. The final result for both syntax and semantics is chosen from a group of options created by the subsystems. The system got the best score in the challenge: 89.32% accuracy for syntax, 81.65 for semantic F1 score (a measure of a test's accuracy), and 85.49 for macro F1 score. Our system uses 30 different subsystems to deal with action words (verbs) and naming words (nouns). We show how important it is to understand distant connections in sentence parts for analyzing how words relate to each other in a sentence. In our work, we looked at how different ways of representing grammar affect the task of basic semantic parsing (understanding the basic meaning of sentences) and pointed out the issue of having a limited ability to generalize word meanings.	systems
949	Findings of the 2009 Workshop on Statistical Machine Translation This paper presents the results of the WMT09 shared tasks, which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 87 machine translation systems and 22 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality, for more than 20 metrics. We present a new evaluation technique whereby system output is edited and judged for correctness. Our Fr-En 109 corpus aggregates huge numbers of parallel French English sentences from the web. We show that the performance of corpus-based statistical machine translation (SMT) has come up to the traditional rule-based method.	Findings of the 2009 Workshop on Statistical Machine Translation This paper shows the results from the WMT09 shared tasks, which included a translation task, a system combination task, and an evaluation task. We did a large manual review of 87 machine translation systems and 22 combined systems. We ranked these systems to see how closely automatic scoring methods match human opinions on translation quality, using more than 20 methods. We introduce a new evaluation method where system output is edited and checked for accuracy. Our Fr-En 109 collection gathers a large number of matching French-English sentences from the internet. We demonstrate that the performance of machine translation that uses statistical data from texts has reached the level of the traditional rule-based method.	sentence
965	Overview of Genia Event Task in BioNLP Shared Task 2011 The Genia event task, a bio-molecular event extraction task, is arranged as one of the main tasks of BioNLP Shared Task 2011. As its second time to be arranged for community-wide focused efforts, it aimed to measure the advance of the community since 2009, and to evaluate generalization of the technology to full text papers. After a 3-month system development period, 15 teams submitted their performance results on test cases. The results show the community has made a significant advancement in terms of both performance improvement and generalization.	Overview of Genia Event Task in BioNLP Shared Task 2011 The Genia event task, which is about finding and understanding events in biological texts, was one of the main challenges in the BioNLP Shared Task 2011. This was the second time this challenge was held to see how much progress had been made since 2009, and to check if the technology could work well with complete scientific papers. After working on their systems for 3 months, 15 teams sent in their test results. These results show that the community has greatly improved both in how well their systems perform and in how well these systems can handle different types of texts.	event focused
950	Joshua: An Open Source Toolkit for Parsing-Based Machine Translation We describe Joshua, an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for synchronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We demonstrate that the toolkit achieves state of the art translation performance on the WMT09 French-English translation task. We develop the syntax-based MT system Joshua, which implements dynamic programming algorithms for second-order expectation semi rings (Li and Eisner, 2009) to efficiently compute the gradients needed for optimization.	Joshua: An Open Source Toolkit for Parsing-Based Machine Translation We explain Joshua, a free toolkit for statistical machine translation. Joshua uses all the methods needed for special grammar rules: chart-parsing (breaking down sentences into parts), n-gram language model integration (using word patterns), beam- and cube-pruning (simplifying options), and k-best extraction (choosing the best outcomes). The toolkit can also create grammar rules from suffix-arrays (word endings) and uses minimum error rate training (improving accuracy). It uses shared and spread-out computing methods to handle large tasks. We show that the toolkit performs as well as the best in translating French to English in a specific 2009 test. We build the Joshua system, which uses clever methods to quickly calculate the changes needed for improving results.	systems
951	Domain Adaptation for Statistical Machine Translation with Monolingual Resources Domain adaptation has recently gained interest in statistical machine translation to cope with the performance drop observed when testing conditions deviate from training conditions. The basic idea is that in-domain training data can be exploited to adapt all components of an already developed system. Previous work showed small performance gains by adapting from limited in-domain bilingual data. Here, we aim instead at significant performance gains by exploiting large but cheap monolingual in-domain data, either in the source or in the target language. We propose to synthesize a bilingual corpus by translating the monolingual adaptation data into the counterpart language. Investigations were conducted on a state-of-the-art phrase-based system trained on the Spanish–English part of the UN corpus, and adapted on the corresponding Europarl data. Translation, re-ordering, and language models were estimated after translating in-domain texts with the baseline. By optimizing the interpolation of these models on a development set the BLEU score was improved from 22.60% to 28.10% on a test set. In order to use source-side monolingual data, we employ the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+indomain language model) and obtain 1-best translation for each source-side sentence. We adapt an SMT system with automatic translations and trained the translation and reordering models on the word alignment used by moses.	Domain Adaptation for Statistical Machine Translation with Monolingual Resources Domain adaptation is becoming popular in statistical machine translation (SMT) to handle the drop in performance when the conditions we test under are different from those we trained with. The main idea is to use data from the specific area of interest (in-domain data) to adjust all parts of an existing system. Earlier research showed small improvements by using limited bilingual data from the specific area. Here, we aim for bigger improvements by using a lot of cheap, single-language (monolingual) data from the area of interest, either from the original language or the language it’s being translated into. We suggest creating a bilingual dataset by translating this single-language data into the other language. Studies were done using a top-notch system based on phrases, trained on the Spanish–English part of the UN dataset, and adjusted with the matching Europarl data. Translation, rearranging of words, and language models were estimated after translating the in-domain texts with the original setup. By fine-tuning how we mix these models on a practice set, the BLEU score (a measure of translation quality) improved from 22.60% to 28.10% on a test set. To use single-language data from the original language, we use a method called transductive learning to first translate this data using the best setup (original setup plus in-domain word list and language model) and get the best translation for each sentence. We adjust an SMT system with these automatic translations and train the translation and reordering models on word alignments used by a tool called Moses.	Source Toolkit
952	Fluency Adequacy or HTER? Exploring Different Human Judgments with a Tunable MT Metric Automatic Machine Translation (MT) evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance. Different types of human judgments, such as Fluency, Adequacy, and HTER, measure varying aspects of MT performance that can be captured by automatic MT metrics. We explore these differences through the use of a new tunable MT metric: TER-Plus, which extends the Translation Edit Rate evaluation metric with tunable parameters and the incorporation of morphology, synonymy and paraphrases. TER-Plus was shown to be one of the top metrics in NIST's Metrics MATR 2008 Challenge, having the highest average rank in terms of Pearson and Spearman correlation. Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating significant differences between the types of human judgments. We extend the TER algorithm in a similar fashion to produce a new evaluation metric, TER plus (TERp), which allows tuning of the edit costs in order to maximize correlation with human judgment.	Fluency Adequacy or HTER? Exploring Different Human Judgments with a Tunable MT Metric Automatic Machine Translation (MT) evaluation metrics have traditionally been judged by how well their scores match with human opinions on translation quality. Different types of human opinions, like Fluency (how smooth the translation sounds), Adequacy (how much meaning is conveyed), and HTER (Human Translation Edit Rate), focus on different parts of translation quality that automatic MT metrics try to capture. We look at these differences using a new adjustable MT metric: TER-Plus, which builds on the Translation Edit Rate by adding adjustable settings and considering word forms, synonyms, and rephrased sentences. TER-Plus was found to be one of the best metrics in NIST's Metrics MATR 2008 Challenge, having the highest average score in terms of two statistical measures, Pearson and Spearman correlation, which show how well it matches human judgment. Adjusting TER-Plus to fit different types of human opinions leads to much better matches and important changes in the importance given to different types of changes, showing big differences between types of human judgments. We expand the TER method similarly to create a new evaluation metric, TER plus (TERp), which allows adjusting the cost of changes to get the best match with human opinions.	automatic
953	A Metalearning Approach to Processing the Scope of Negation Finding negation signals and their scope in text is an important subtask in information extraction. In this paper we present a machine learning system that finds the scope of negation in biomedical texts. The system combines several classifiers and works in two phases. To investigate the robustness of the approach, the system is tested on the three subcorpora of the BioScope corpus representing different text types. It achieves the best results to date for this task, with an error reduction of 32.07% compared to current state of the art results. we describe a method for improving resolution of the scope of negation by combining IGTREE, CRF, and Support Vector Machines (SVM) (Morante and Daelemans, 2009). we pioneered the research on negation scope finding by formulating it as a chunking problem, which classifies the words of a sentence as being inside or outside the scope of a negation signal.	A Metalearning Approach to Processing the Scope of Negation Finding negation signals and their scope in text is a key task in extracting information. In this paper, we introduce a computer system that uses machine learning to identify the scope of negation in medical texts. The system uses multiple computer programs called classifiers and operates in two steps. To see how strong the approach is, we tested the system on three parts of the BioScope collection, which include different types of texts. It achieves the best results so far for this task, with a 32.07% decrease in errors compared to the best methods available. We explain a way to better determine the scope of negation by combining three techniques: IGTREE, CRF (Conditional Random Fields), and Support Vector Machines (SVM) (Morante and Daelemans, 2009). We were the first to study how to find the scope of negation by treating it as a chunking problem, which means identifying whether words in a sentence are within or outside the range of a negation signal.	important
954	Design Challenges and Misconceptions in Named Entity Recognition We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset. We use the IOBES notation to represent NE mentions with label sequences, thereby NER is formalized as a multi class classification problem in which a given token is classified into IOBES labels. We investigate design challenges for named entity recognition, and showed that other design choices, such as the representation of output labels and using features built on external knowledge, are more important than the learning model itself. We have shown for the CoNLL-2003 shared task that Greedy decoding (i.e., beam search of width 1) is competitive to the widely used Viterbi algorithm while being over 100 times faster at the same time.	Design Challenges and Misconceptions in Named Entity Recognition We examine some basic challenges and misunderstandings in creating a strong and effective NER (Named Entity Recognition) system. Specifically, we look at problems like how to represent parts of text, how to combine small NER decisions into a bigger picture, where to get helpful background information, and how to use it in an NER system. While comparing different ways to solve these problems, we find some unexpected results and build an NER system that scores 90.8 F1 on the CoNLL-2003 NER task, the best result for this data. We use IOBES notation, which is a way to mark parts of text, treating NER as a task where each word is labeled in a specific way. We explore the challenges in designing NER and show that how we label outputs and use information from outside sources is more crucial than the learning model itself. In the CoNLL-2003 task, we demonstrate that a simple method called Greedy decoding, which is a fast way of making decisions, works nearly as well as the popular Viterbi algorithm but is over 100 times faster.	outside
1005	Language Independent Named Entity Recognition Combining Morphological And Contextual Evidence Identifying and classifying personal, geographic, institutional or other names in a text is an important task for numerous applications. This paper describes and evaluates a language-independent bootstrapping algorithm based on iterative learning and re-estimation of contextual and morphological patterns captured in hierarchically smoothed trie models. The algorithm learns from unannotated text and achieves competitive performance when trained on a very short labelled name list with no other required language-specific information, tokenizers or tools. We consider one sense per discourse phenomenon, the tendency of terms to have a fixed meaning within a single document. We take a character-level approach to named entity recognition (NER) using prefix and suffix tries. The bootstrapping stage uses the initial or current entity assignments to estimate the class conditional distributions for both entities and contexts along their trie paths, and then re-estimates the distributions of the contexts/entity-candidates to which they are linked, recursively, until all accessible nodes are reached.	Language Independent Named Entity Recognition Combining Morphological And Contextual Evidence Identifying and classifying names of people, places, organizations, or other things in a text is important for many uses. This paper explains and tests a method that works for any language. It uses a step-by-step learning process to find patterns in words and their surroundings. The method learns from text that hasn't been specially prepared and works well even with a very short list of labeled names and no need for specific language tools or information. We look at the idea that terms usually have the same meaning within one document. We examine named entity recognition (NER) at the level of individual characters, using the beginnings and endings of words. The learning stage uses the initial or current guesses about entities to estimate how likely different classes are for both the entities and their contexts along their paths, and then updates these guesses for the contexts or possible entities they are connected to, repeating this until all possible points are considered.	phrases
955	Learning the Scope of Hedge Cues in Biomedical Texts Identifying hedged information in biomedical literature is an important subtask in information extraction because it would be misleading to extract speculative information as factual information. In this paper we present a machine learning system that finds the scope of hedge cues in biomedical texts. The system is based on a similar system that finds the scope of negation cues. We show that the same scope finding approach can be applied to both negation and hedging. To investigate the robustness of the approach, the system is tested on the three subcorpora of the BioScope corpus that represent different text types. We develop a scope detector following a supervised sequence labeling approach. We present a meta-learning system that finds the scope of hedge cues in biomedical texts. We use shallow syntactic features.	Learning the Scope of Hedge Cues in Biomedical Texts Identifying hedged information in biomedical literature is an important task in gathering information because it can be misleading to treat uncertain information as if it were certain. In this paper, we introduce a computer learning system that identifies where uncertain phrases begin and end in biomedical texts. The system is based on a similar system that identifies where negation (saying "not") occurs. We show that this method of finding where phrases begin and end works for both negation and hedging (uncertain statements). To test how well the method works, the system is used on three parts of the BioScope collection that have different types of text. We create a tool to detect the beginning and end of phrases using a supervised (guided by examples) approach. We present a system that learns from itself to find where uncertain phrases begin and end in biomedical texts. We use simple sentence structure features.	called Greedy
956	Overview of BioNLP&rsquo;09 Shared Task on Event Extraction Even though considerable attention has been given to semantic orientation of words and the creation of large polarity lexicons, research in emotion analysis has had to rely on limited and small emotion lexicons. In this paper, we show how we create a high-quality, moderate-sized emotion lexicon using Mechanical Turk. In addition to questions about emotions evoked by terms, we show how the inclusion of a word choice question can discourage malicious data entry, help identify instances where the annotator may not be familiar with the target term (allowing us to reject such annotations), and help obtain annotations at sense level (rather than at word level). We perform an extensive analysis of the annotations to better understand the distribution of emotions evoked by terms of different parts of speech. We identify which emotions tend to be evoked simultaneously by the same term and show that certain emotions indeed go hand in hand. The BioNLP 09 Shared Task on Event Extraction, the first large scale evaluation of biomedical event extraction systems, drew the participation of 24 groups and established a standard event representation scheme and datasets. The BioNLP 09 Shared Task is the first shared task that provided a consistent data set and evaluation tools for extraction of such biological relations.	Overview of BioNLP&rsquo;09 Shared Task on Event Extraction Even though a lot of focus has been on understanding the meaning of words and making large lists that categorize words by positive or negative feelings, emotion analysis research has had to work with small and limited emotion lists. In this paper, we explain how we create a high-quality, medium-sized list of emotions using Mechanical Turk, an online platform for tasks. We also show how adding a question about word choice can prevent bad data input, identify when someone might not know the word well (so we can ignore such inputs), and help us get more accurate emotion tags at the specific meaning level of the word rather than just the word itself. We do a detailed study of these tags to understand which emotions are linked to words of different types of speech. We find out which emotions often occur together with the same word and show that some emotions are indeed closely related. The BioNLP 09 Shared Task on Event Extraction, the first large-scale test of systems for identifying biomedical events, attracted 24 groups and set up a standard method for representing events and provided datasets. The BioNLP 09 Shared Task is the first event that offered a consistent dataset and evaluation tools for finding such biological connections.	certain
957	Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon Even though considerable attention has been given to semantic orientation of words and the creation of large polarity lexicons, research in emotion analysis has had to rely on limited and small emotion lexicons. In this paper, we show how we create a high-quality, moderate-sized emotion lexicon using Mechanical Turk. In addition to questions about emotions evoked by terms, we show how the inclusion of a word choice question can discourage malicious data entry, help identify instances where the annotator may not be familiar with the target term (allowing us to reject such annotations), and help obtain annotations at sense level (rather than at word level). We perform an extensive analysis of the annotations to better understand the distribution of emotions evoked by terms of different parts of speech. We identify which emotions tend to be evoked simultaneously by the same term and show that certain emotions indeed go hand in hand. We focus on emotion evoked by common words and phrases. We explore the use of Mechanical Turk to build the lexicon based on human judgment. We create a crowd sourced term emotion association lexicon consisting of associations of over 10,000 word-sense pairs with eight emotions joy, sadness, anger, fear, trust, disgust, surprise, and anticipation argued to be the basic and prototypical emotions.	Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon Even though a lot of attention has been given to understanding the positive or negative meaning of words and creating large lists of such words, research in studying emotions has had to rely on small lists of emotions. In this paper, we show how we create a good-quality, medium-sized list of emotions using Mechanical Turk, a platform for gathering data from online workers. Besides asking questions about the feelings words bring up, we show how adding a word choice question can help prevent wrong data, identify when someone doesn't know the word (so we can ignore their input), and get more detailed emotion data. We do a detailed analysis of the data to better understand how different types of words cause different feelings. We find out which feelings often come together with the same word and show that some feelings indeed appear together. We focus on feelings caused by everyday words and phrases. We explore using Mechanical Turk to create this list based on people's opinions. We build a publicly gathered list of word-feeling connections with over 10,000 word-meaning pairs associated with eight basic emotions: joy, sadness, anger, fear, trust, disgust, surprise, and anticipation, which are considered the main typical emotions.	question about
958	Creating Speech and Language Data With Amazon&rsquo;s Mechanical Turk In this paper we give an introduction to using Amazon’s Mechanical Turk crowdsourcing platform for the purpose of collecting data for human language technologies. We survey the papers published in the NAACL2010 Workshop. 24 researchers participated in the workshop’s shared task to create data for speech and language applications with $100. We experiment with the use of Amazon Mechanical Turk (AMT) to create and evaluate human language data (Callison-Burch and Dredze, 2010). We provide an overview of various tasks for which MTurk has been used, and offer a set of best practices for ensuring high-quality data.	Creating Speech and Language Data With Amazon’s Mechanical Turk In this paper, we introduce how to use Amazon’s Mechanical Turk, an online platform where lots of people can help with tasks, to collect information for language technology. We look at papers from the NAACL2010 Workshop. In this workshop, 24 researchers joined a challenge to create speech and language data with just $100. We test using Amazon Mechanical Turk (AMT) to make and check human language information (Callison-Burch and Dredze, 2010). We give a summary of different tasks where MTurk has been used and suggest some best practices to make sure the data is of good quality.	Common Words
959	Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation This paper presents the results of the WMT10 and Metrics MATR10 shared tasks, which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 104 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 26 metrics. This year we also investigated increasing the number of human judgments by hiring non-expert annotators through Amazon's Mechanical Turk. We release the News test set in the 2010 Workshop on Statistical Machine Translation.	Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation This paper presents the results of the WMT10 and Metrics MATR10 shared tasks, which included a translation task (translating text from one language to another), a system combination task (combining different translation systems), and an evaluation task (checking how good the translations are). We conducted a large-scale manual evaluation of 104 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics (computer-based measurements) correlate with human judgments of translation quality for 26 metrics. This year we also investigated increasing the number of human judgments by hiring non-expert annotators (people who mark or comment on data) through Amazon's Mechanical Turk (an online platform for hiring people to do tasks). We release the News test set in the 2010 Workshop on Statistical Machine Translation.	online platform
960	A Regression Model of Adjective-Noun Compositionality in Distributional Semantics In this paper we explore the computational modelling of compositionality in distributional models of semantics. In particular, we model the semantic composition of pairs of adjacent English Adjectives and Nouns from the British National Corpus. We build a vector-based semantic space from a lemmatised version of the BNC, where the most frequent A-N lemma pairs are treated as single tokens. We then extrapolate three different models of compositionality: a simple additive model, a pointwise-multiplicative model and a Partial Least Squares Regression (PLSR) model. We propose two evaluation methods for the implemented models. Our study leads to the conclusion that regression-based models of compositionality generally out-perform additive and multiplicative approaches, and also show a number of advantages that make them very promising for future research. Our approach to the semantic composition of adjectives with nouns draws on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. Our main innovation is to use the co-occurrence vectors of corpus-observed ANs to train a supervised composition model. We look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically.	A Regression Model of Adjective-Noun Compositionality in Distributional Semantics In this paper, we explore how to use computers to understand how adjectives and nouns combine in language. We focus on how these words pair up in English, using examples from a large collection of British texts. We create a system that organizes words into a space where common adjective-noun pairs are grouped together. We then test three ways to understand how words combine: a simple addition method, a method that multiplies values, and a more complex method called Partial Least Squares Regression (PLSR). We suggest two ways to test these methods. Our study finds that the regression methods, which involve predicting outcomes based on patterns, work better than the simple add or multiply methods and have benefits that are promising for future research. Our method of combining adjectives with nouns is inspired by older theories where adjectives are seen as adding extra meaning to nouns, and we treat adjectives as sets of numbers that change noun meanings in a predictable way. Our main new idea is to use data from real texts to teach a computer model how these words work together. We analyze groups of words from the text to automatically understand how they combine.	Workshop
961	Driving Semantic Parsing from the World&rsquo;s Response Current approaches to semantic parsing, the task of converting text to a formal meaning representation, rely on annotated training data mapping sentences to logical forms. Providing this supervision is a major bottleneck in scaling semantic parsers. This paper presents a new learning paradigm aimed at alleviating the supervision burden. We develop two novel learning algorithms capable of predicting complex structures which only rely on a binary feedback signal based on the context of an external world. In addition we reformulate the semantic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers. We train systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers.	Driving Semantic Parsing from the World&rsquo;s Response Current methods for semantic parsing, which means turning text into a structured meaning, depend on training data that pairs sentences with their logical meanings. Getting this training data is a big problem when trying to improve semantic parsers. This paper introduces a new way of learning to reduce the need for this detailed training data. We created two new learning methods that can predict complicated structures using only simple yes/no feedback based on the surrounding environment. Additionally, we change the way we approach semantic parsing to make our model less dependent on sentence structures, which helps it improve with less training data. Surprisingly, our findings reveal that even without detailed examples, learning with simple feedback can create a parser that works almost as well as one trained with full examples. We teach systems using question and answer pairs by automatically figuring out the meanings of the questions that lead to the right answers.	methods
962	The CoNLL-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts. The motivation behind this task was that distinguishing factual and uncertain information in texts is of essential importance in information extraction. The CoNLL-2010 shared task, aimed at detecting uncertainty cues in texts, focused on these phrases in trying to determine whether sentences contain uncertain information. The goal of the CoNLL 2010 Shared Task is to develop linguistic scope detectors as well.	The CoNLL-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text The CoNLL-2010 Shared Task was about finding words or phrases that show uncertainty and understanding their context in regular language writing. The reason for this task was that telling apart facts from unsure information in texts is very important for gathering information. The CoNLL-2010 shared task, which looked for signs of uncertainty in texts, focused on these clues to see if sentences had unclear information. The goal of the CoNLL 2010 Shared Task is also to create tools that can identify the context of these uncertain phrases.	findings reveal
963	Sentiment Analysis of Twitter Data We examine sentiment analysis on Twitter data. The contributions of this paper are: (1) We introduce POS-specific prior polarity features. (2) We explore the use of a tree kernel to obviate the need for tedious feature engineering. The new features (in conjunction with previously proposed features) and the tree kernel perform approximately at the same level, both outperforming the state-of-the-art baseline. In our work a study was conducted on a reduced corpus of tweets labelled manually.	Sentiment Analysis of Twitter Data We look at how to analyze feelings or opinions in Twitter data. The main points of this paper are: (1) We present POS-specific prior polarity features, which are tools to help determine the emotion or attitude in a sentence based on parts of speech like nouns or verbs. (2) We investigate using a tree kernel, a method that makes it easier to analyze data without needing to do a lot of manual work to create features. The new features, along with previously suggested ones, and the tree kernel work about the same, both doing better than the current best method. In our work, we carried out a study on a smaller set of tweets that were labeled by hand.	create tools
964	Overview of BioNLP Shared Task 2011 The BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams. Five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully fully generalize in various aspects. The BioNLP 2011 Shared Task series generalized this defining a series of tasks involving more text types, domains and target event types.	Overview of BioNLP Shared Task 2011 The BioNLP Shared Task 2011, an event focused on pulling out specific information from texts, took place over 6 months, ending in March 2011. It had a lot of people taking part, with 46 final entries from 24 teams. There were five main tasks and three extra tasks. The outcomes showed progress in detailed information extraction in the biomedical field (related to biology and medicine) and proved that these methods work well in different areas. The 2011 series expanded by including more kinds of texts, fields, and target events.	sentence based
966	CoNLL-2011 Shared Task: Modeling Unrestricted Coreference in OntoNotes The CoNLL-2011 shared task involved predicting coreference using OntoNotes data. Resources in this field have tended to be limited to noun phrase coreference, often on a restricted set of entities, such as ACE entities. OntoNotes provides a large-scale corpus of general anaphoric coreference not restricted to noun phrases or to a specified set of entity types. OntoNotes also provides additional layers of integrated annotation, capturing additional shallow semantic structure. This paper briefly describes the OntoNotes annotation (coreference and other layers) and then describes the parameters of the shared task including the format, pre-processing information, and evaluation criteria, and presents and discusses the results achieved by the participating systems. Having a standard test set and evaluation parameters, all based on a new resource that provides multiple integrated annotation layers (parses, semantic roles, word senses, named entities and coreference) that could support joint models, should help to energize ongoing research in the task of entity and event coreference. An overview of all systems participating in the CONLL-2011 shared task and their results is provided here.	CoNLL-2011 Shared Task: Modeling Unrestricted Coreference in OntoNotes The CoNLL-2011 shared task involved predicting how things mentioned in a text are related using OntoNotes data. Resources in this field have tended to focus only on linking nouns to each other, often on a limited set of things, such as ACE entities. OntoNotes provides a large collection of data that covers general linking of words and phrases, not limited to nouns or specific types of things. OntoNotes also offers extra layers of connected information, capturing additional basic meaning structures. This paper briefly explains the OntoNotes data labeling (linking words and other layers) and then describes the shared task details including how it was set up, any preparation details, and how success was measured, and presents and discusses the results achieved by the participating systems. Having a standard test set and evaluation rules, all based on a new resource that provides multiple connected information layers (like sentence structures, roles words play in a sentence, meanings, named entities, and linking words) that could support combined models, should help to boost ongoing research in the task of linking people, places, or things and events in text. An overview of all systems participating in the CONLL-2011 shared task and their results is provided here.	greatly improved
967	Stanfordâ€™s Multi-Pass Sieve Coreference Resolution System at the CoNLL-2011 Shared Task This paper details the coreference resolution system submitted by Stanford at the CoNLL2011 shared task. Our system is a collection of deterministic coreference resolution models that incorporate lexical, syntactic, semantic, and discourse information. All these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster. We participated in both the open and closed tracks and submitted results using both predicted and gold mentions. Our system was ranked first in both tracks, with a score of 57.8 in the closed track and 58.3 in the open track. The Stanford coreference resolver, which won the CoNLL-2011 shared task on coreference resolution, adopts the once-popular rule-based approach, resolving pronouns simply with rules that encode the aforementioned traditional linguistic constraints on coreference, such as the Binding constraints and gender and number agreement. The infrequency of occurrences of difficult pronouns in these standard evaluation corpora by no means undermines their significance, however.	Stanford’s Multi-Pass Sieve Coreference Resolution System at the CoNLL-2011 Shared Task This paper explains the coreference resolution system Stanford submitted for the CoNLL-2011 shared task. Our system is a group of step-by-step models that solve coreference, which is identifying when different words refer to the same thing in a text, by using word choice, sentence structure, meaning, and how it fits in the larger conversation. All these models look at the whole document by sharing details like gender and number among the same groups of words or names. We competed in both the open and closed categories and provided results using both predicted and correct mentions. Our system achieved first place in both categories, scoring 57.8 in the closed track and 58.3 in the open track. The Stanford coreference resolver, which won the CoNLL-2011 shared task, uses a once-popular method based on rules to resolve pronouns, which are words like "he," "she," or "it," by applying traditional language rules like ensuring pronouns match in gender and number. Although these tricky pronouns don't occur often in standard test collections, they are still very important.	Unrestricted
968	Findings of the 2011 Workshop on Statistical Machine Translation This paper presents the results of the WMT11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics. We conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics. This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake. We also conducted a pilot 'tunable metrics' task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality. Despite avoiding language-specific resources and using only the training data provided by the workshop, an extensive manual evaluation determined that the outputs produced were of significantly higher quality than both statistical and rule-based systems that made use of language-specific resources.	Findings of the 2011 Workshop on Statistical Machine Translation This paper shows the results from the WMT11 shared tasks, which included a translation task, a system combination task (where different systems work together), and a task to evaluate machine translation quality. We did a large-scale manual review of 148 machine translation systems and 41 combined systems. We looked at the rankings of these systems to see how well automatic scoring methods matched human opinions of translation quality for 21 different scoring methods. This year, there was a task to translate SMS messages from Haitian Creole to English, which were sent to emergency services after the Haitian earthquake. We also tested a new 'tunable metrics' task to see if adjusting a single system to different scoring methods would lead to noticeably different translation results. Even without using special language tools and only using the training data provided by the workshop, a detailed manual review showed that the translations were much better than both statistical systems (using math-based methods) and rule-based systems (using language rules) that used special language tools.	important
969	Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems This paper describes Meteor 1.3, our submission to the 2011 EMNLP Workshop on Statistical Machine Translation automatic evaluation metric tasks. New metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words. We include Ranking and Adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced Tuning version shown to outperform BLEU in minimum error rate training for a phrase-based Urdu-English system.	Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems This paper talks about Meteor 1.3, which we submitted to the 2011 EMNLP Workshop where they focus on evaluating machine translation systems. The new features include better ways to standardize text, more accurate matching of similar phrases, and the ability to tell the difference between important words and less important ones. We also have different versions of the metric: one for ranking translations and one for checking how good they are, both closely matching human opinions on translation quality. There's also a balanced version for fine-tuning, which works better than BLEU, a common evaluation method, especially when training a phrase-based system for translating Urdu to English.	translation
984	Corpus Based PP Attachment Ambiguity Resolution With A Semantic Dictionary This paper deals with two important ambiguities of natural language: prepositional phrase attachment and word sense ambiguity. We propose a new supervised learning method for PP- attachment based on a semantically tagged corpus. Because any sufficiently big sense-tagged corpus does not exist, we also propose a new unsupervised context based word sense disambiguation algorithm which amends the training corpus for the PP attachment by word sense tags. We present the results of our approach and evaluate the achieved PP attachment accuracy in comparison with other methods. we developed a customized, explicit WSD algorithm as part of their decision tree system.	Corpus Based PP Attachment Ambiguity Resolution With A Semantic Dictionary This paper addresses two important issues in understanding language: where prepositional phrases (like "on the table") attach in a sentence, and the different meanings a word can have. We suggest a new supervised learning approach for deciding where prepositional phrases should attach, using a database of sentences where meanings are marked. Since there isn't a large enough database with marked meanings, we also suggest a new unsupervised method (which works on its own without guidance) to figure out word meanings based on the sentence context, which helps improve the training database for figuring out prepositional phrase attachment. We show the results of our method and check how well it figures out prepositional phrase attachment compared to other techniques. We created a specific, clear method to decide word meanings as part of their decision-making system.	different
970	KenLM: Faster and Smaller Language Model Queries We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and memory costs. The PROBING data structure uses linear probing hash tables and is designed for speed. Compared with the widely-used SRILM, our PROBING model is 2.4 times as fast while using 57% of the memory. The TRIE data structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed at lower memory consumption. TRIE simultaneously uses less memory than the smallest lossless baseline and less CPU than the fastest baseline. Our code is open-source, thread-safe, and integrated into the Moses, cdec, and Joshua translation systems. This paper describes the several performance techniques used and presents benchmarks against alternative implementations. We describe a language modeling library.	KenLM: Faster and Smaller Language Model Queries We present KenLM, a library that uses two smart ways to handle language model queries efficiently, making them faster and use less memory. The PROBING method uses a type of fast lookup table designed for speed. Compared to the popular SRILM, our PROBING model is 2.4 times faster and uses 57% less memory. The TRIE method is a structure that organizes data efficiently using less memory with special tricks like bit-level packing and other techniques to save space. TRIE uses less memory than the smallest similar option and less processing power than the fastest option. Our code is open-source (free for anyone to use and modify), safe for multiple users at the same time, and works with the Moses, cdec, and Joshua translation systems. This paper describes the different ways we boost performance and compares our methods to other options. We describe a library for language modeling.	difference
971	Findings of the 2012 Workshop on Statistical Machine Translation This paper presents the results of the WMT12 shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for run-time estimation of machine translation quality. We conducted a large-scale manual evaluation of 103 machine translation systems submitted by 34 teams. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics. We introduced a new quality estimation task this year, and evaluated submissions from 11 teams. We report for several automatic metrics on the whole WMT12 English-to-Czech dataset.	Findings of the 2012 Workshop on Statistical Machine Translation This paper shows the results of the WMT12 shared tasks, which included a translation task, a task to evaluate how well machine translation works, and a task to estimate translation quality quickly. We performed a large manual evaluation of 103 machine translation systems submitted by 34 teams. We used the ranking of these systems to see how well automatic measurements matched human opinions of translation quality for 12 evaluation metrics (ways to measure performance). We introduced a new task to estimate quality this year and looked at the submissions from 11 teams. We provide information for several automatic metrics on the entire WMT12 English-to-Czech dataset.	language modeling
972	Robust Bilingual Word Alignment For Machine Aided Translation We have developed a new program called word_align for aligning parallel text, text such as the Canadian Hansards that are available in two or more languages. The program takes the output of char_align (Church, 1993), a robust alternative to sentence-based alignment programs, and applies word-level constraints using a version of Brown el al.'s Model 2 (Brown et al., 1993), modified and extended to deal with robustness issues. Word_align was tested on a subset of Canadian Hansards supplied by Simard (Simard et al., 1992). The combination of word_align plus char_align reduces the variance (average square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology. We show that knowledge of target-text length is not crucial to the model's performance.	Robust Bilingual Word Alignment For Machine Aided Translation We have developed a new program called word_align for matching parallel text, like the Canadian Hansards, which are available in two or more languages. The program uses the result from char_align (Church, 1993), a strong alternative to programs that match sentences, and applies word-level limits using a version of Brown et al.'s Model 2 (Brown et al., 1993), which is changed and improved to handle consistency issues. Word_align was tested on a part of the Canadian Hansards provided by Simard (Simard et al., 1992). The combination of word_align and char_align decreases the variation (average square error) by 5 times compared to using char_align alone. More importantly, because word_align and char_align were made to work well on smaller and messier texts than the Hansards, they have been successfully used at AT&T Language Line Services, a business translation service, to help them with hard words. We show that knowing the length of the target text is not essential for the model's effectiveness.	quality
973	Has A Consensus NL Generation Architecture Appeared And Is It Psycholinguistically Plausible? I survey some recent applications-oriented NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations these modules perform, and the way the modules interact with each other. I also compare this 'consensus architecture' among applied NLG systems with psycholinguistic knowledge about how humans speak, and argue that at least some aspects of the consensus architecture seem to be in agreement with what is known about human language production, despite the fact that psycholinguistic plausibility was not in general a goal of the developers of the surveyed systems. Most generation systems pipeline pragmatic, semantic, lexical and syntactic decisions. We show that psycholinguistic and engineering approaches often result in systems, which are similar in crucial respects.	Has A Consensus NL Generation Architecture Appeared And Is It Psycholinguistically Plausible? I review some recent systems for generating natural language (NL) that focus on practical applications. I claim that even though these systems come from different theoretical backgrounds, they have a surprisingly similar structure. This structure involves dividing the process into parts (modules), what tasks these parts do, and how they work together. I also compare this shared structure among applied NL generation systems with knowledge about how humans naturally speak (psycholinguistics), and argue that some parts of this shared structure match what is known about how humans produce language, even though making it similar to human language wasn't a main goal for the developers of these systems. Most systems make decisions in stages about meaning (pragmatic and semantic), word choice (lexical), and sentence structure (syntactic). We show that both understanding human language and engineering solutions often create systems that are similar in important ways.	decreases
1003	Improved Alignment Models For Statistical Machine Translation In this paper, we describe improved alignment models for statistical machine translation. The statistical translation approach uses two types of information: a translation model and a language model. The language model used is a bigram or general m-gram model. The translation model is decomposed into a lexical and an alignment model. We describe two different approaches for statistical translation and present experimental results. The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words. We present results using the Verbmobil task (German-English, 6000-word vocabulary) which is a limited-domain spoken-language task. The experimental tests were performed on both the text transcription and the speech recognizer output. To obtain the best single alignment, we use a post-hoc algorithm to merge directional alignments. We propose a heuristic, where all the aligned phrase pairs (x?, a?, y?) satisfying the following criteria are extracted: (1) x? and y? consist of consecutive words of x and y, and both have length at most k, (2) a? is the alignment between words of x? and y? induced by a, (3) a? contains at least one link, and (4) there are no links in a that have just one end in x? or y?.	Improved Alignment Models For Statistical Machine Translation In this paper, we describe better methods for matching parts in statistical machine translation. This approach uses two types of information: a translation model and a language model. The language model is a bigram or general m-gram model, which means it looks at pairs or small groups of words. The translation model is broken down into a part that deals with words (lexical) and a part that deals with matching words and phrases (alignment). We explain two different methods for statistical translation and show test results. The first method looks at how single words depend on each other; the second method considers simple phrase structures by using two levels of alignment: one for phrases and one for individual words. We show results using the Verbmobil task, which is a specific spoken language task between German and English with a 6000-word vocabulary. The tests were done on both written text and spoken language recognized by a computer. To get the best single alignment, we use a method to combine alignments from different directions. We suggest a rule-based method where all matched phrase pairs (x?, a?, y?) that meet these criteria are chosen: (1) x? and y? are groups of consecutive words from x and y, and both are no longer than k words, (2) a? is the match between words of x? and y? created by a, (3) a? has at least one link, and (4) there are no links in a that have just one part in x? or y?.	WordNet
974	Unsupervised Learning Of Disambiguation Rules For Part Of Speech Tagging In this paper we describe an unsupervised learning algorithm for automatically training a rule-based part of speech tagger without using a manually tagged corpus. We compare this algorithm to the Baum-Welch algorithm, used for unsupervised training of stochastic taggers. Next, we show a method for combining unsupervised and supervised rule-based training algorithms to create a highly accurate tagger using only a small amount of manually tagged text. We present a rule-based part-of-speech tagger for unsupervised training corpus. We propose a method to acquire context-dependent POS disambiguation rules and created an accurate tagger, even from a very small annotated text by combining supervised and unsupervised learning.	Unsupervised Learning Of Disambiguation Rules For Part Of Speech Tagging In this paper we describe a method, called an unsupervised learning algorithm, that automatically teaches a computer program to identify parts of speech (like nouns, verbs, etc.) in sentences without needing examples that have already been labeled by humans. We compare this method to the Baum-Welch algorithm, which is another method used to teach computers how to identify parts of speech without pre-labeled examples. Then, we show a way to combine both unsupervised methods (without pre-labeled examples) and supervised methods (with some pre-labeled examples) to build a very accurate program using only a small amount of pre-labeled sentences. We introduce a method that creates rules for identifying parts of speech from scratch. We suggest a way to gather rules that depend on the surrounding words to figure out the correct part of speech, and we made an accurate program using even a tiny amount of labeled sentences by mixing both learning approaches.	meaning
975	Prepositional Phrase Attachment Through A Backed-Off Model Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity. Typically, ambiguous verb phrases of the form v rip1 p rip2 are resolved through a model which considers values of the four head words (v, nl, p and 77,2). This paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable. Results on Wall Street Journal data of 84.5% accuracy are obtained using this method. We use a Back-Off model, which enables them to take low frequency effects into account on the Ratnaparkhi dataset (with good results). We introduce modifications to the Ratnaparkhi et al (1994) dataset meant to combat data sparsity and used the modified version to train their backed-off model.	Prepositional Phrase Attachment Through A Backed-Off Model Recent studies have looked at using collections of text or statistical methods to solve the problem of unclear prepositional phrase attachment, which means deciding which part of a sentence a phrase belongs to. Usually, confusing verb phrases like v rip1 p rip2 are understood by a model that looks at the four main words involved (v, nl, p, and 77,2). This paper shows that this problem is similar to how n-gram language models work in speech recognition, and that a common method in language modeling, called the backed-off estimate, can be used here. Using this method on data from the Wall Street Journal achieved 84.5% accuracy. We use a Back-Off model, which allows them to consider rare occurrences effectively on the Ratnaparkhi dataset (with good results). We make changes to the Ratnaparkhi et al (1994) dataset to address the issue of not having enough data and use this updated version to train their backed-off model.	speech without
976	A Bayesian Hybrid Method For Context-Sensitive Spelling Correction Two classes of methods have been shown to be useful for resolving lexical ambiguity. The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word. These methods have complementary coverage: the former captures the lexical "atmosphere" (discourse topic, tense, etc.), while the latter captures local syntax. Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence. A new hybrid method, based on Bayesian classifiers, is presented for doing this, and its performance improvements are demonstrated. We build a classifier based on a rich set of context features.	A Bayesian Hybrid Method For Context-Sensitive Spelling Correction Two types of methods are useful for solving word confusion. The first looks at specific words near the confusing word; the second examines word patterns and grammar around the word. These methods cover different areas: the first captures the general topic or feel (like what the discussion is about or the tense), while the second focuses on sentence structure. Yarowsky combined these methods using decision lists, which are like a set of rules to decide the best solution, by using the strongest evidence from either method. This paper uses Yarowsky's idea to fix spelling mistakes that depend on context. Decision lists usually work better than using each method alone. However, better results are possible by considering all available evidence, not just the strongest one. A new combined method, using Bayesian classifiers (a type of statistical tool), is introduced to do this, showing improved results. We create a tool that uses a detailed set of context clues.	Through
977	Disambiguating Noun Groupings With Respect To Wordnet Senses Word groupings useful for language processing tasks are increasingly available, as thesauri appear on-line, and as distributional word clustering techniques improve. However, for many tasks, one is interested in relationships among word senses, not words. This paper presents a method for automatic sense disambiguafton of nouns appearing within sets of related nouns -- the kind of data one finds in on-line thesauri, or as the output of distributional clustering algorithms. Disambiguation is performed with respect to WordNet senses, which are fairly fine-gained; however, the method also permits the assignment of higher-level WordNet categories rather than sense labels. The method is illustrated primarily by example, though results of a more rigorous evaluation are also presented. In this work, the assessment of semantic similarity using a dictionary database as knowledge source is recognized as providing significant cues for word clustering. We define the semantic similarity between two words as the entropy value of the most informative concept subsuming the two words in a hierarchically structured thesaurus. We attempt to combine paradigmatic and syntagmatic similarity strategies.	Disambiguating Noun Groupings With Respect To Wordnet Senses Word groupings useful for language processing tasks are increasingly available, as thesauri (word lists that show words with similar meanings) appear online, and as methods for grouping words based on how they are used improve. However, for many tasks, people are interested in relationships among meanings of words, not just the words themselves. This paper presents a method for automatically figuring out which meaning of a noun is intended when it appears with related nouns, like in online thesauri or as a result of grouping words based on usage. This process is done using WordNet senses, which are detailed meanings; however, the method also allows using broader WordNet categories instead of specific meanings. The method is mainly shown using examples, but results from more detailed testing are also included. In this work, using a dictionary database to measure how similar words are in meaning is seen as providing important clues for grouping words. We define the meaning similarity between two words as the amount of shared information of the most informative idea that includes both words in a structured word list. We try to combine two strategies: one that looks at the role of words in language and one that looks at how words are used together.	mistakes
978	Text Chunking Using Transformation-Based Learning Eric Brill introduced transformation-based learning and showed that it can do part-of-speech tagging with fairly high accuracy. The same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive "baseNP" chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 92% for baseNP chunks and 88% for somewhat more complex chunks that partition the sentence. Some interesting adaptations to the transformation-based learning approach are also suggested by this application. We formalize chunking as a classification task, in which each word is classified as the (B)eginning, (I)nside or (O) outside of a chunk. We pioneer the machine learning techniques to chunking problem.	Text Chunking Using Transformation-Based Learning. Eric Brill introduced a learning method that can identify parts of speech with quite good accuracy. This method can also be used to find groups of words, called chunks, in sentences. These chunks include simple "baseNP" chunks, which don't have sub-chunks inside them. To do this, chunking is seen as a tagging task where each word in a sentence gets a new label that shows its chunk structure. In tests using data from a known source, this method accurately identified baseNP chunks 92% of the time and slightly more complex chunks 88% of the time. Some useful changes to this learning method are also suggested by this task. We explain chunking as a task where each word is marked as the beginning, inside, or outside of a chunk. We are among the first to use machine learning techniques for chunking.	combine
979	Automatic Evaluation And Uniform Filter Cascades For Inducing N-Best Translation Lexicons This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources. The knowledge sources are cast as filters, so that any subset of them can be cascaded in a uniform framework. A new objective evaluation measure is used to compare the quality of lexicons induced with different filter cascades. The best filter cascades improve lexicon quality by up to 137% over the plain vanilla statistical method, and approach human performance. Drastically reducing the size of the training corpus has a much smaller impact on lexicon quality when these knowledge sources are used. This makes it practical to train on small hand-built corpora for language pairs where large bilingual corpora are unavailable. Moreover, three of the four filters prove useful even when used with large training corpora. We use the Longest Common Subsequence Ratio (LCSR) to measure similarity.	Automatic Evaluation And Uniform Filter Cascades For Inducing N-Best Translation Lexicons This paper explains how to create a list of the best translations from a collection of bilingual texts by using statistics (math-based information) along with four extra sources of knowledge. These sources of knowledge are used like filters, so any combination of them can be applied in a consistent way. A new way to objectively measure and compare the quality of these translation lists is introduced, showing that using the best combination of filters can improve translation quality by up to 137% compared to just using basic statistical methods, and can even get close to how well humans translate. Cutting down the size of the text collection used for training has a much smaller negative effect on the translation quality when these knowledge sources are applied. This is helpful for training with small, manually created text collections when large ones are not available for certain language pairs. Additionally, three out of the four filters are still helpful even when using large text collections for training. We use a method called the Longest Common Subsequence Ratio (LCSR) to measure how similar things are.	chunk structure
980	MBT: A Memory-Based Part Of Speech Tagger-Generator We introduce a memory-based approach to part of speech tagging. Memory-based learning is a form of supervised learning based on similarity-based reasoning. The part of speech tag of a word in a particular context is extrapolated from the most similar cases held in memory. Supervised learning approaches are useful when a tagged corpus is available as an example of the desired output of the tagger. Based on such a corpus, the tagger-generator automatically builds a tagger which is able to tag new text the same way, diminishing development time for the construction of a tagger considerably. Memory-based tagging shares this advantage with other statistical or machine learning approaches. Additional advantages specific to a memory-based approach include (i) the relatively small tagged corpus size sufficient for training, (ii) incremental learning, (iii) explanation capabilities, (iv) flexible integration of information in case representations, (v) its non-parametric nature, (vi) reasonably good results on unknown words without morphological analysis, and (vii) fast learning and tagging. In this paper we show that a large-scale application of the memory-based approach is feasible: we obtain a tagging accuracy that is on a par with that of known statistical approaches, and with attractive space and time complexity properties when using IGTree, a tree-based formalism for indexing and searching huge case bases. The use of IGTree has as additional advantage that optimal context size for disambiguation is dynamically computed. Our tagger uses a very fine-grained tag set.	MBT: A Memory-Based Part Of Speech Tagger-Generator. We introduce a way to identify parts of speech (like nouns or verbs) using memory. This method is called memory-based learning, where the system learns by comparing similar past examples. To find the part of speech for a word, it looks at similar examples stored in memory. This method works well when we have a set of tagged examples to learn from. Using these examples, the system can automatically create a tool to tag new text, saving time. Memory-based tagging is similar to other smart learning methods but has extra benefits: (i) it needs only a small set of examples, (ii) it can learn bit by bit, (iii) it can explain its choices, (iv) it can easily use different kinds of information, (v) it doesn’t rely on fixed rules, (vi) it handles unknown words well, and (vii) it learns and tags quickly. We show that using this method on a large scale works well. It matches the accuracy of other known methods and is efficient in terms of space and time when using IGTree, a system for organizing and searching large amounts of information. IGTree also helps in deciding the best context size for understanding words. Our tool uses a very detailed set of tags.	Translation Lexicons
981	Comparative Experiments On Disambiguating Word Senses: An Illustration Of The Role Of Bias In Machine Learning This paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context. The algorithms tested include statistical, neural-network, decision-tree, rule-based, and case-based classification techniques. The specific problem tested involves disambiguating six senses of the word "line" using the words in the current and proceeding sentence as context. The statistical and neural-network methods perform the best on this particular problem and we discuss a potential reason for this observed difference. We also discuss the role of bias in machine learning and its importance in explaining performance differences observed on specific problems. We argue that Naive Bayes classification and perceptron classifiers are particularly fit for lexical sample word sense disambiguation problems, because they combine weighted evidence from all features rather than select a subset of features for early discrimination. Bag of words feature sets made up of unigrams have had a long history of success in text classification and word sense disambiguation (Mooney, 1996), and we believe that despite creating quite a bit of noise can provide useful information for discrimination.	Comparative Experiments On Disambiguating Word Senses: An Illustration Of The Role Of Bias In Machine Learning This paper compares seven different learning methods to figure out how to determine the meaning of a word based on its context. The tested methods include statistical approaches, neural networks (a type of computer system modeled after the human brain), decision trees (a decision support tool that uses a tree-like model), rule-based methods, and case-based classification techniques (which use past cases to solve new problems). The specific challenge was clarifying six meanings of the word "line" using words from the current and previous sentences for context. Statistical methods and neural networks performed the best on this task, and we explore why this might be the case. We also talk about how bias (a tendency to favor certain outcomes) in machine learning affects performance on specific tasks. We suggest that Naive Bayes classification and perceptron classifiers (types of algorithms used for sorting data into categories) are particularly good for problems involving word meanings because they consider all features rather than just a few, which helps in making decisions. Using sets of individual words (unigrams) has been successful in text classification and figuring out word meanings, and even though it can create a lot of unnecessary information (noise), it still provides helpful data for making distinctions.	methods
982	A Maximum Entropy Model For Part-Of-Speech Tagging This paper presents a statistical model which trains from a corpus annotated with Part-Of-Speech tags and assigns them to previously unseen text with state-of-the-art accuracy(96.6%). The model can be classified as a Maximum Entropy model and simultaneously uses many contextual "features" to predict the POS tag. Furthermore, this paper demonstrates the use of specialized features to model difficult tagging decisions, discusses the corpus consistency problems discovered during the implementation of these features, and proposes a training strategy that mitigates these problems. We assume that the tag of a word is independent of the tags of all preceding words given the tags of the previous two words. We release a publicly available maximum entropy tagger.	A Maximum Entropy Model For Part-Of-Speech Tagging This paper introduces a statistical model that learns from a text dataset marked with Part-Of-Speech tags and applies these tags to new text with very high accuracy (96.6%). The model, known as a Maximum Entropy model, uses various context clues (features) to predict the correct POS tag. Additionally, the paper shows how to use special features to handle tricky tagging choices, talks about problems with consistency in the dataset found while using these features, and suggests a training method to reduce these issues. We assume the tag of a word does not depend on the tags of all earlier words, just on the tags of the two words before it. We provide a publicly available maximum entropy tagger.	different
983	Efficient Algorithms For Parsing The DOP Model Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993c). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers. We give a polynomial time conversion of a DOP model into an equivalent PCFG whose size is linear in the size of the training set.	Efficient Algorithms For Parsing The DOP Model Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993c). Unfortunately, existing methods take a lot of computing power and are hard to carry out. Previous methods are costly because they need a huge number of rules and rely on a complex random process called Monte Carlo parsing. In this paper, we solve the first problem by simplifying the DOP model into a smaller, similar grammar that's easier to handle. We solve the second problem by using a straightforward method that aims to get the most correct parts, instead of focusing on the chance of getting the whole structure right. With these improvements, tests show a 97% rate of matching brackets and 88% rate with no mismatched brackets. This is very different from Bod's results and similar to another experiment by Pereira and Schabes (1992) on the same data. We show that Bod's results were partly due to lucky test choices and cleaner data than others used. We provide a way to change a DOP model into a similar structure in a reasonable time, where its size matches the size of the training data.	training method
985	Finding Terminology Translations From Non-Parallel Corpora We present a statistical word feature, the Word Relation Matrix, which can be used to find translated pairs of words and terms from non-parallel corpora, across language groups. Online dictionary entries are used as seed words to generate Word Relation Matrices for the unknown words according to correlation measures. Word Relation Matrices are then mapped across the corpora to find translation pairs. Translation accuracies are around 30% when only the top candidate is counted. Nevertheless, top 20 candidate output give a 50.9% average increase in accuracy on human translator performance. In our work, a translation model applied to a pair of unrelated languages (English/Japanese) with a random selection of test words, many of them multi-word terms, gives a precision around 30% when only the top candidate is proposed.	Finding Terminology Translations From Non-Parallel Corpora We introduce a statistical tool called the Word Relation Matrix, which helps find translated word pairs and terms from collections of texts that are not directly aligned, across different languages. Online dictionary entries are used as starting points to create Word Relation Matrices for unknown words based on how they relate to each other. These matrices are then compared across the texts to find translation pairs. The accuracy of translations is about 30% when only the top choice is considered. However, when considering the top 20 choices, the accuracy improves by an average of 50.9% compared to human translators. In our study, when using a translation model for two unrelated languages (English and Japanese) with randomly chosen test words, including many multi-word terms, the accuracy is around 30% when only the top choice is suggested.	paper addresses
986	Selectional Preference And Sense Disambiguation The absence of training data is a real problem for corpus-based approaches to sense disambiguation, one that is unlikely to be solved soon. Selectional preference is traditionally connected with sense ambiguity; this paper explores how a statistical model of selectional preference, requiring neither manual annotation of selection restrictions nor supervised training, can be used in sense disambiguation. We define selectional preference as the amount of information a verb provides about its semantic argument classes. We present a method to acquire a set of conceptual classes for word senses, employing selectional preferences, based on the idea that certain linguistic predicates constraint the semantic interpretation of underlying words into certain classes. In determining selectional preferences, we use uniformly distributing observed frequencies for a given word across all its senses.	Selectional Preference And Sense Disambiguation The lack of training data is a big issue for methods that rely on large text collections (corpus) to figure out word meanings, and it's a problem that won't be fixed soon. Selectional preference, which is usually linked with confusion in word meanings, is explored in this paper to show how a statistical model, which does not need manual labeling or guided training, can help in figuring out word meanings. We describe selectional preference as the information a verb gives about the types of meanings its related words can have. We introduce a way to gather groups of related meanings for words, using selectional preferences, based on the idea that some language structures limit how words can be understood into specific groups. To decide on selectional preferences, we evenly spread out the recorded usage counts of a word over all its possible meanings.	randomly chosen
987	A Linear Observed Time Statistical Parser Based On Maximum Entropy Models This paper presents a statistical parser for natural language that obtains a parsing accuracy--roughly 87% precision and 86% recall--which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall. We introduce the idea of oracle re-ranking: suppose there exists a perfect re-ranking scheme that magically picks the best parse that has the highest F-score among the top k parses for each sentence. We take a pipeline approach in that the classifiers are trained on individual decisions rather than on the overall quality of the parser, and chained to yield the global structure.	A Linear Observed Time Statistical Parser Based On Maximum Entropy Models This paper introduces a statistical parser for natural language, which is a tool that breaks down sentences to understand their structure. It achieves high accuracy, about 87% for precision (correct parts identified) and 86% for recall (total parts identified), better than previous results on the Wall Street Journal texts. The parser needs little human help because it uses simple, clear information to make decisions, and it combines this information automatically using a method called maximum entropy, which balances different factors to make the best prediction. The time it takes for the parser to analyze a sentence is directly related to the sentence's length, meaning it is efficient. Also, the parser gives several options for how a sentence can be structured, and this paper shows that choosing the best from the top 20 options can greatly improve accuracy to 93% for both precision and recall. The paper introduces a concept called oracle re-ranking, which imagines a perfect way to choose the best sentence structure with the highest quality score from the top choices. The method uses a step-by-step approach where decision-makers are trained on specific parts instead of the overall quality, and these parts are put together to form the complete structure.	Selectional
988	Global Thresholding And Multiple-Pass Parsing We present a variation on classic beam thresholding techniques that is up to an order of magnitude faster than the traditional method, at the same performance level. We also present a new thresholding technique, global thresholding, which, combined with the new beam thresholding, gives an additional factor of two improvement, and a novel technique, multiple pass parsing, that can be combined with the others to yield yet another 50% improvement. We use a new search algorithm to simultaneously optimize the thresholding parameters of the various algorithms. we describe a method for producing a simple but crude approximate grammar of a standard context-free grammar.	Global Thresholding And Multiple-Pass Parsing We introduce a new version of beam thresholding methods that is up to ten times faster than the old method, while still working just as well. We also introduce a new method called global thresholding, which when used with the new beam thresholding, makes things twice as fast. Additionally, we have a new method called multiple pass parsing that, when combined with the others, makes things 50% faster. We use a new search method to fine-tune the settings of these different techniques all at once. We explain a way to create a simple but rough version of a standard context-free grammar, which is a basic set of language rules.	overall
989	Automatic Discovery Of Non-Compositional Compounds In Parallel Data Automatic segmentation of text into minimal content-bearing units is an unsolved problem even for languages like English. Spaces between words offer an easy first approximation, but this approximation is not good enough for machine translation (MT), where many word sequences are not translated word-for-word. This paper presents an efficient automatic method for discovering sequences of words that are translated as a unit. The method proceeds by comparing pairs of statistical translation models induced from parallel texts in two languages. It can discover hundreds of non-compositional compounds on each iteration, and constructs longer compounds out of shorter ones. Objective evaluation on a simple machine translation task has shown the method's potential to improve the quality of MT output. The method makes few assumptions about the data, so it can be applied to parallel data other than parallel texts, such as word spellings and pronunciations. we propose a method for the recognition of multi word compounds in bi texts that is based on the predictive value of a translation model. we investigates techniques for identifying non-compositional compounds in English-French parallel corpora and emphasises that translation models that take non compositional compounds into account are more accurate.	Automatic Discovery Of Non-Compositional Compounds In Parallel Data Automatic splitting of text into the smallest meaningful parts is still a challenge, even for languages like English. Spaces between words give a simple starting point, but this is not enough for machine translation (MT), where many groups of words are not translated directly word-for-word. This paper shows an effective automatic way to find groups of words that are translated together as one unit. The method works by comparing pairs of statistical translation models created from texts that are in two different languages. It can find hundreds of non-compositional compounds (phrases that don't translate directly) in each round and builds longer phrases from shorter ones. Testing on a basic machine translation task has shown that this approach can help improve the quality of MT results. The method doesn't rely much on specific data types, so it can be used with other types of paired information, like word spellings and pronunciations. We suggest a way to identify multi-word compounds (phrases made of multiple words) in bilingual texts that relies on the predictive ability of a translation model. We explore techniques for finding non-compositional compounds in English-French text pairs and highlight that translation models considering these compounds are more precise.	standard
990	A Corpus-Based Approach For Building Semantic Lexicons Semantic knowledge can be a great asset to natural language processing systems, but it is usually hand-coded for each application. Although some semantic information is available in general-purpose knowledge bases such as WordNet and Cyc, many applications require domain-specific lexicons that represent words and categories for a particular topic. In this paper, we present a corpus-based method that can be used to build semantic lexicons for specific categories. The input to the system is a small set of seed words for a category and a representative text corpus. The output is a ranked list of words that are associated with the category. A user then reviews the top-ranked words and decides which ones should be entered in the semantic lexicon. In experiments with five categories, users typically found about 60 words per category in 10-15 minutes to build a core semantic lexicon. We find that nouns in conjunctions or appositives tend to be semantically related. We suggest using conjunction and appositive data to cluster nouns; we approximate this data by looking at the nearest NP on each side of a particular NP. We also give credit for words associated with but not belonging to a particular category.	A Corpus-Based Approach For Building Semantic Lexicons Semantic knowledge (understanding the meaning of words and sentences) can be very helpful for systems that process human language, but it is often manually created for each use. While some information about meaning is available in general databases like WordNet and Cyc, many systems need special dictionaries (lexicons) for specific topics. In this paper, we introduce a method that uses a collection of texts (corpus) to create dictionaries for specific topics. The system starts with a few example words (seed words) for a topic and a collection of texts. It produces a list of words ranked by how closely they relate to the topic. A person then checks the top words and decides which ones should be part of the dictionary. In tests with five topics, people usually found around 60 words for each topic in 10-15 minutes to create a basic dictionary. We notice that nouns (naming words) in pairs or groups often have related meanings. We suggest grouping nouns together by looking at these pairs or groups; we do this by checking the nearest noun phrases (NP) around a particular noun phrase. We also give importance to words linked with, but not part of, a specific category.	effective
991	Distinguishing Word Senses In Untagged Text This paper describes an experimental comparison of three unsupervised learning algorithms that distinguish the sense of an ambiguous word in untagged text. The methods described in this paper, McQuitty's similarity analysis, Ward's minimum-variance method, and the EM algorithm, assign each instance of an ambiguous word to a known sense definition based solely on the values of automatically identifiable features in text. These methods and feature sets are found to be more successful in disambiguating nouns rather than adjectives or verbs. Overall, the most accurate of these procedures is McQuitty's similarity analysis in combination with a high dimensional feature set. we propose a (dis) similarity based discrimination approach that computes (dis) similarity among each pair of instances of the target word.	Distinguishing Word Senses In Untagged Text This paper explains a test comparing three automatic learning methods that figure out the meaning of a confusing word in text without extra information. The methods explained are McQuitty's similarity analysis, Ward's minimum-variance method, and the EM algorithm. These methods use certain noticeable parts of the text to decide which meaning of a confusing word is correct. These methods work better for figuring out the meanings of nouns rather than adjectives or verbs. Overall, McQuitty's similarity analysis works the best when combined with a detailed set of features. We suggest a method that looks at how similar or different each pair of examples of the target word is.	suggest
992	Using Lexical Chains For Text Summarization We investigate one technique to produce a summary of an original text without requiring its full semantic interpretation, but instead relying on a model of the topic progression in the text derived from lexical chains. We present a new algorithm to compute lexical chains in a text, merging several robust knowledge sources: the WordNet thesaurus, a part-of-speech tagger and shallow parser for the identification of nominal groups, and a segmentation algorithm derived from (Hearst, 1994). Summarization proceeds in three steps: the original text is first segmented, lexical chains are constructed, strong chains are identified and significant sentences are extracted from the text. We present in this paper empirical results on the identification of strong chains and of significant sentences. We find that the use of a part-of-speech tagger could eliminate wrong inclusions of words such as read, which has both noun and verb entries in WordNet. In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary. Cohesion is achieved through the use in the text of semantically related terms, reference, ellipse and conjunctions.	Using Lexical Chains For Text Summarization We explore a method to create a summary of a text without needing to fully understand its meaning, instead using a model of how topics progress in the text based on lexical chains (links between related words). We introduce a new process to calculate these lexical chains by combining several reliable tools: the WordNet thesaurus (a database of words with similar meanings), a tool that identifies parts of speech (words like nouns or verbs), and a basic parser for identifying groups of nouns, along with a text-splitting method from Hearst, 1994. Summarization happens in three steps: first, the text is divided into sections, then lexical chains are created, strong chains are identified, and important sentences are picked out from the text. We show in this paper the practical results of finding strong chains and important sentences. We discover that using a part-of-speech tagger can prevent incorrect word inclusions, like the word "read," which can be a noun or a verb in WordNet. In automatic text summarization, similar words are used to recognize repeated information to avoid unnecessary repetition in a summary. Cohesion (connection) is achieved by using related terms, references, ellipses (omitting words), and conjunctions (connecting words) in the text.	McQuitty
993	From Discourse Structures To Text Summaries We describe experiments that show that the concepts of rhetorical analysts and nuclearity can be used effectively for determining the most important units in a text. We show how these concepts can be implemented and we discuss results that we obtained with a discourse-based summarization program.	From Discourse Structures To Text Summaries We describe tests that show how ideas of rhetorical analysis (how arguments are made) and nuclearity (identifying core parts) can be used to find the most important parts in a text. We explain how to apply these ideas and discuss the results we got with a program that summarizes text based on its structure.	connecting words
1004	Noun Phrase Coreference As Clustering This paper introduces a new, unsupervised algorithm for noun phrase coreference resolution. It differs from existing methods in that it views coreference resolution as a clustering task. In an evaluation on the MUC-6 coreference resolution corpus, the algorithm achieves an F-measure of 53.6%, placing it firmly between the worst (40%) and best (65%) systems in the MUC-6 evaluation. More importantly, the clustering approach outperforms the only MUC-6 system to treat coreference resolution as a learning problem. The clustering algorithm appears to provide a flexible mechanism for coordinating the application of context-independent and context-dependent constraints and preferences for accurate partitioning of noun phrases into coreference equivalence classes. We combine the use of WordNet with proper name gazetteers in order to obtain information on the compatibility of coreferential NPs in their clustering algorithm. Approaches to coreference resolution that rely only on clustering can easily enforce transitivity. We use pairwise NP distances to cluster document mentions. Our system uses the node distance in WordNet (with an upper limit of 4) as one component in the distance measure that guides their clustering algorithm. Coreference resolution is performed in two phases: a binary classification phase, in which the likelihood of coreference for each pair of noun phrases is assessed; and a partitioning phase, in which the clusters of mutually coreferring NPs are formed, maximizing some global criterion.	Noun Phrase Coreference As Clustering This paper introduces a new, unsupervised (not guided by human input) method for solving noun phrase coreference (identifying when different phrases refer to the same thing). Instead of using traditional methods, it treats this as a grouping task. In tests using the MUC-6 dataset, it gets a score of 53.6%, which is better than the worst system (40%) but not as good as the best one (65%). More importantly, this method does better than the only MUC-6 system that uses learning to solve coreference. The grouping method seems to offer a flexible way to apply rules that don't change with context and those that do, to correctly group noun phrases that refer to the same thing. We use WordNet (a database of words) along with lists of proper names to find out if noun phrases can refer to the same thing in the grouping method. Methods that only use grouping can easily make sure that all related phrases are connected. We use distances between noun phrases to group mentions in a document. Our system uses the distance between nodes in WordNet (up to 4 steps apart) as one part of the measure for guiding the grouping method. Solving coreference is done in two steps: first, checking how likely it is that each pair of noun phrases refers to the same thing; then, forming groups of phrases that refer to the same thing, aiming to make the best overall grouping.	results using
994	GermaNet - A Lexical-Semantic Net For German We present the lexical-semantic net for German "GermaNet" which integrates conceptual ontological information with lexical semantics, within and across word classes. It is compatible with the Princeton WordNet but integrates principle-based modifications on the constructional and organizational level as well as on the level of lexical and conceptual relations. GermaNet includes a new treatment of regular polysemy, artificial concepts and of particle verbs. It furthermore encodes cross-classification and basic syntactic information, constituting an interesting tool in exploring the interaction of syntax and semantics. The development of such a large scale resource is particularly important as German up to now lacks basic online tools for the semantic exploration of very large corpora. GermaNet is a large lexical database, where words are associated with POS in formation and semantic sorts, which are organized in a fine-grained hierarchy.	GermaNet - A Lexical-Semantic Net For German We introduce "GermaNet," a network that connects meanings and concepts for German words, combining knowledge about word meanings and categories. It's similar to the Princeton WordNet but includes specific changes to how it's built and organized, and how words and concepts relate to each other. GermaNet handles cases where words have multiple related meanings, made-up ideas, and verbs with added particles in a new way. It also includes information on how words are classified and basic sentence structure, making it a useful tool for studying how word order and meaning work together. Creating such a large resource is crucial because, until now, German hasn't had essential online tools to analyze huge collections of text for meanings. GermaNet is a big database of words, where words are linked with parts of speech (types of words like nouns or verbs) and types of meanings, arranged in a detailed order.	nuclearity
995	CogNIAC: High Precision Coreference With Limited Knowledge And Linguistic Resources This paper presents a high precision pronoun resolution system that is capable of greater than 90% precision with 60% and better recall for some pronouns. It is suggested that the system is resolving a sub-set of anaphors that do not require general world knowledge or sophisticated linguistic processing for successful resolution. The system does this by being very sensitive to ambiguity, and only resolving pronouns when very high confidence rules have been satisfied. The system is capable of 'noticing' ambiguity because it requires that there be a unique antecedent within a salience ranking, and the salience rankings are not total orders, i.e. two or more antecedents can be equally salient. Given the nature of the systems rules, it is very likely that they are largely domain independent and that they reflect processing strategies used by humans for general language comprehension. The system has been evaluated in two distinct experiments which support the overall validity of the approach. our method, CogNiac is a knowledge poor approach to anaphora resolution based on a set of high confidence rules which are successively applied over the pronoun under consideration.	CogNIAC: High Precision Coreference With Limited Knowledge And Linguistic Resources This paper introduces a system that can accurately figure out which noun a pronoun refers to, achieving over 90% accuracy and more than 60% effectiveness in finding pronouns. The system focuses on resolving certain pronouns that don't need broad knowledge of the world or complex language skills to understand. It does this by being very careful with uncertainty and only making decisions when it is very sure. The system can detect uncertainty by ensuring there is a clear previous noun (antecedent) in a list of important words, where multiple nouns can be equally important. The rules used by the system are likely suitable for many different topics and mimic how people generally understand language. The system has been tested in two separate studies that confirm its effectiveness. Our approach, CogNiac, uses a simple method that relies on a set of reliable rules applied one after another to the pronoun being analyzed.	meanings
996	Indexing With WordNet Synsets Can Improve Text Retrieval The classical, vector space model for text retrieval is shown to give better results (up to 29% better in our experiments) if WordNet synsets are chosen as the indexing space, instead of word forms. This result is obtained for a manually disambiguated test collection (of queries and documents) derived from the SEMCOR semantic concordance. The sensitivity of retrieval performance to (automatic) disambiguation errors when indexing documents is also measured. Finally, it is observed that if queries are not disambiguated, indexing by synsets performs (at best) only as good as standard word indexing. We point out some more weaknesses of WordNet for Information Retrieval purposes, in particular the lack of domain information and the fact that sense distinctions are excessively fine-grained for the task.	Indexing With WordNet Synsets Can Improve Text Retrieval The traditional method for finding text by using a vector space model works better (up to 29% better in our tests) when WordNet synsets (groups of related words) are used for organizing information instead of individual words. This result was achieved using a test set that was manually cleared of confusion (using SEMCOR, a set of text with meanings labeled). We also looked at how mistakes in automatically figuring out meanings affect performance when organizing documents. Lastly, if the search terms (queries) are not clarified, using synsets to organize information works only as well as the usual method of using words. We also mention some weaknesses of using WordNet for finding information, such as the lack of specific field information and the fact that WordNet separates word meanings too finely for this task.	separate
997	An Empirical Approach To Conceptual Case Frame Acquisition Conceptual natural language processing systems usually rely on case frame instantiation to recognize events and role objects in text. But generating a good set of case frames for a domain is time-consuming, tedious, and prone to errors of omission. We have developed a corpus-based algorithm for acquiring conceptual case frames empirically from unannotated text. Our algorithm builds on previous research on corpus-based methods for acquiring extraction patterns and semantic lexicons. Given extraction patterns and a semantic lexicon for a domain, our algorithm learns semantic preferences for each extraction pattern and merges the syntactically compatible patterns to produce multi-slot case frames with selectional restrictions. The case frames generate more cohesive output and produce fewer false hits than the original extraction patterns. Our system requires only preclassified training texts and a few hours of manual review to filter the dictionaries, demonstrating that conceptual case frames can be acquired from unannotated text without special training resources. our Conceptual Case Frame Acquisition project, extraction patterns, a domain semantic lexicon, and a list of conceptual roles and associated semantic categories for the domain are used to produce multiple-slot case frames with selectional restrictions.	An Empirical Approach To Conceptual Case Frame Acquisition Conceptual natural language processing systems usually depend on using case frames to understand events and the roles of objects in text. But creating a good set of these case frames for a specific topic is time-consuming, boring, and mistakes can easily happen. We have created a method using a large collection of texts to automatically develop conceptual case frames from text that hasn't been specially marked or annotated. Our method builds on earlier research that used large text collections to find patterns and create lists of word meanings for specific topics. With patterns and a list of word meanings for a topic, our method learns what types of words fit each pattern and combines patterns that fit together grammatically to create case frames with specific rules. These case frames provide more consistent results and make fewer mistakes than the original patterns. Our system only needs some pre-labeled texts and a few hours of manual checking to refine the word lists, showing that conceptual case frames can be made from unmarked text without special training materials. In our Conceptual Case Frame Acquisition project, patterns, a list of word meanings for the topic, and a list of roles and related word categories for the topic are used to create multi-part case frames with specific rules.	vector space
998	Edge-Based Best-First Chart Parsing Best-first probabilistic chart parsing attempts to parse efficiently by working on edges that are judged "best" by some probabilistic figure of merit (FOM). Recent work has used probabilistic context-free grammars (PCFGs) to assign probabilities to constituents, and to use these probabilities as the starting point for the FOM. This paper extends this approach to using a probabilistic FOM to judge edges (incomplete constituents), thereby giving a much finer-grained control over parsing effort. We show how this can be accomplished in a particularly simple way using the common idea of binarizing the PCFG. The results obtained are about a factor of twenty improvement over the best prior results - that is, our parser achieves equivalent results using one twentieth the number of edges. Furthermore we show that this improvement is obtained with parsing precision and recall levels superior to those achieved by exhaustive parsing. We introduce overparsing as a technique to improve parse accuracy by continuing parsing after the first complete parse tree is found.	Edge-Based Best-First Chart Parsing Best-first probabilistic chart parsing tries to work efficiently by focusing on the most promising parts of a sentence (called edges) based on a probability score. Recent methods use probabilistic context-free grammars (PCFGs), which assign probabilities to different parts of a sentence, using these probabilities to decide which parts to focus on first. This paper improves this method by using a probability score to evaluate parts of sentences that are not complete yet, allowing for more precise control over how parsing is done. We demonstrate a simple method to achieve this by using a common technique called binarizing the PCFG, which breaks down complex structures into simpler ones. The results are about twenty times better than previous methods, meaning our parser can achieve the same results with only one-twentieth of the work. Additionally, we show that this improvement comes with greater accuracy and recall compared to methods that check every possible interpretation. We also introduce a method called overparsing, which improves accuracy by continuing to analyze the sentence even after finding the first complete interpretation.	Frame Acquisition
999	Exploiting Diverse Knowledge Sources Via Maximum Entropy In Named Entity Recognition This paper describes a novel statistical named-entity (i.e. "proper name") recognition system built around a maximum entity framework. By working within the framework of maximum entropy theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions. These knowledge sources include capitalization features, lexical features, features indicating the current section of text (i.e. headline or main body), and dictionaries of single or multi-word terms. The purely statistical system contains no hand-generated patterns and achieves a result comparable with the best statistical systems. However, when combined with other handcoded systems, the system achieves scores that exceed the highest comparable scores thus-far published.	Exploiting Diverse Knowledge Sources Via Maximum Entropy In Named Entity Recognition This paper explains a new type of system for identifying named entities (like "proper names") using a method called maximum entropy. This system uses a flexible design to take in lots of different types of information when deciding how to label words. These types of information include whether a word is capitalized, the type of words used, where the text appears (like in a headline or the main part), and lists of words or phrases. The system relies entirely on statistical methods without using any manually created rules, and it works as well as the best systems that also use statistics. But when this system is used together with systems that do have manually created rules, it performs even better than any other systems published so far.	different
1000	A Statistical Approach To Anaphora Resolution This paper presents an algorithm for identifying pronominal anaphora and two experiments based upon this algorithm. We incorporate multiple anaphora resolution factors into a statistical framework -- specifically the distance between the pronoun and the proposed antecedent, gender/number/animaticity of the proposed antecedent, governing head information and noun phrase repetition. We combine them into a single probability that enables us to identify the referent. Our first experiment shows the relative contribution of each source of information and demonstrates a success rate of 82.9% for all sources combined. The second experiment investigates a method for unsupervised learning of gender/number/animaticity information. We present some experiments illustrating the accuracy of the method and note that with this information added, our pronoun resolution method achieves 84.2% accuracy. We add annotation of the antecedents of definite pronouns to Treebank. We implement a Hobbs distance feature, which encodes the rank assigned to a candidate antecedent for a pronoun by Hobbs's (1978) seminal syntax-based pronoun resolution algorithm. We count the number of times a discourse entities has been mentioned in the discourse already. Our probabilistic approach combines three factors (aside from the agreement filter): the result of the Hobbs algorithm, Mention Count dependent on the position of the sentence in the article, and the probability of the antecedent occurring in the local context of the pronoun. We describe a supervised probabilistic pronoun resolution algorithm which is based on complete syntactic information.	A Statistical Approach To Anaphora Resolution This paper introduces a method for identifying pronouns and their references, and includes two experiments using this method. We include several factors for resolving pronouns into a statistical system—such as the distance between the pronoun and its possible reference, whether the reference matches in gender/number/is a living thing, key word information, and repetition of noun phrases. We merge these into one probability to help identify what the pronoun refers to. Our first test shows how much each piece of information helps and reports an 82.9% success rate when using all sources together. The second test explores a method for learning gender/number/living thing information without human guidance. We show some tests demonstrating the method's accuracy and note that with this additional information, our pronoun identification method reaches 84.2% accuracy. We add labels to the references of specific pronouns in a database. We implement a feature called Hobbs distance, which assigns a rank to a possible pronoun reference based on Hobbs's important rule-based method from 1978. We count how often a reference has been mentioned already in the conversation. Our probability-based method combines three factors (besides the agreement check): the result from the Hobbs method, how often something is mentioned depending on where the sentence is in the article, and the likelihood of the reference appearing near the pronoun. We explain a pronoun identification method using complete sentence structure information.	different types
1001	Experiments Using Stochastic Search For Text Planning Marcu has characterised an important and difficult problem in text planning: given a set of facts to convey and a set of rhetorical relations that can be used to link them together, how can one arrange this material so as to yield the best possible text? We describe experiments with a number of heuristic Search methods for this task. We investigate the problem of determining a discourse tree for a set of elementary speech acts which are partially constrained by rhetorical relations. We advocate genetic algorithms as an alternative to exhaustively searching for the optimal ordering of descriptions of museum artefacts.	Experiments Using Stochastic Search For Text Planning Marcu highlighted a tough problem in arranging information in writing: how to best organize facts and use linking ideas to create the most effective text. We explain tests with different shortcut search methods to tackle this issue. We explore how to create a conversation structure from basic speech parts that are somewhat limited by linking ideas. We suggest using genetic algorithms, which mimic natural selection, instead of trying every possible way to order descriptions of museum objects.	information
1002	WordNet 2 - A Morphologically And Semantically Enhanced Resource This paper presents an on-going project intended to enhance WordNet morphologically and semantically. The motivation for this work steams from the current limitations of WordNet when used as a linguistic knowledge base. We envision a software tool that automatically parses the conceptual defining glosses, attributing part-of-speech tags and phrasal brackets. The nouns, verbs, adjectives and adverbs from every definition are then disambiguated and linked to the corresponding synsets. This increases the connectivity between synsets allowing the retrieval of topically related concepts. Furthermore, the tool transforms the glosses, first into logical forms and then into semantic forms. Using derivational morphology new links are added between the synsets. We propose a scheme for attaching sense tags to predicates within the framework of transforming WordNet glosses into a logical form. The eXtended WordNet is a publicly available version of WordNet in which (among other things) each term occurring in a WordNet gloss (except those in example phrases) is lemmatized and mapped to the synset in which it belongs.	WordNet 2 - A Morphologically And Semantically Enhanced Resource This paper describes an ongoing project to improve WordNet by focusing on word structure (morphology) and meaning (semantics). The reason for this work comes from the current weaknesses of WordNet when used as a language knowledge base. We imagine a software tool that automatically analyzes the explanations of concepts, assigning grammatical categories and grouping phrases. The nouns, verbs, adjectives, and adverbs from each definition are then clarified and connected to their corresponding groups of synonyms (synsets). This boosts the links between synsets, allowing the retrieval of related concepts. Additionally, the tool changes the explanations, first into logical forms and then into meaning-based forms. By using word formation rules, new connections are added between the synsets. We suggest a method for adding meaning tags to key parts of sentences while changing WordNet explanations into a logical form. The eXtended WordNet is a publicly available version of WordNet where each term in a WordNet explanation (except for those in example phrases) is simplified to its base form and linked to the synset it belongs to.	natural selection
1006	Unsupervised Models For Named Entity Classification This paper discusses the use of unlabeled examples for the problem of named entity classification. A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classifier. However, we show that the use of unlabeled data can reduce the requirements for supervision to just 7 simple "seed" rules. The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type. We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98). We extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree.	Unsupervised Models For Named Entity Classification This paper talks about using examples that aren't labeled to figure out what type of named entities (like names of people, places, or organizations) are. Usually, you need lots of rules and labeled examples to teach a computer program to do this. But we show that using examples without labels can make it easier and you only need 7 basic "seed" rules to start with. This method works well because, in many cases, both the way a name is spelled and the words around it can tell you what type of thing it is. We introduce two methods. The first one is similar to a method from a 1995 study by Yarowsky, with some changes inspired by a 1998 study by Blum and Mitchell. The second method builds on ideas from boosting algorithms, which are usually used for tasks where you know the answers (supervised learning), and adapts them to the approach suggested by Blum and Mitchell in 1998. We improve the use of programs that help each other by adding rules to a boosting method called AdaBoost, making them agree with each other.	current
1007	Exploiting Diversity In Natural Language Processing: Combining Parsers Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy. Two general approaches are presented and two combination techniques are described for each approach. Both parametric and non-parametric models are explored. The resulting parsers surpass the best previously published performance results for the Penn Treebank. We improve the best parser's F-measure of 89.7 to 91.3, using naive Bayes voting on the Penn TreeBank constituent structures (16% error reduction). Regarding the system combination study, we propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees. We perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined.	Exploiting Diversity In Natural Language Processing: Combining Parsers Three advanced computer programs that analyze sentence structure (parsers) are combined to create more precise sentence structures and define new limits on accuracy. Two main methods are introduced, and each method has two ways of combining parsers. Both models that use strict rules (parametric) and those that don't (non-parametric) are examined. The new parsers achieve better results than the best previous results for analyzing sentences in the Penn Treebank, which is a large language database. We improve the best parser's accuracy score, known as F-measure, from 89.7 to 91.3 by using a simple voting method called naive Bayes on sentence parts in the Penn TreeBank (a 16% reduction in mistakes). In our study on combining systems, we suggest two ways to combine parsers: one method chooses an entire tree from one parser, and the other method creates a new tree by picking parts from the original trees. We choose the best sentence structure by trying to get the highest possible accuracy from the combined sentence structures.	learning
1008	Detecting Text Similarity Over Short Passages: Exploring Linguistic Feature Combinations Via Machine Learning We present a new composite similarity metric that combines information from multiple linguistic indicators to measure semantic distance between pairs of small textual units. Several potential features are investigated and an optimal combination is selected via machine learning. We discuss a more restrictive definition of similarity than traditional, document-level and information retrieval-oriented, notions of similarity, and motivate it by showing its relevance to the multi-document text summarization problem. Results from our system are evaluated against standard information retrieval techniques, establishing that the new method is more effective in identifying closely related textual units. At the level of short passages or sentences, we go beyond N-gram, taking advantage of WordNet synonyms, as well as ordering and distance between shared words.	Detecting Text Similarity Over Short Passages: Exploring Linguistic Feature Combinations Via Machine Learning We introduce a new way to measure how similar pieces of text are by combining information from different language features to see how much meaning differs between small text parts. We look at several possible features and use machine learning to find the best mix. We talk about a stricter definition of similarity than the usual methods that focus on larger text or finding information and explain why this is important for summarizing information from multiple documents. Our system's results are compared to standard methods, showing that our new approach is better at finding closely related text pieces. For short texts or sentences, we use more than just N-grams (sequences of words), incorporating WordNet synonyms (words with similar meanings), and considering the order and spacing between shared words.	combining parsers
1009	Cascaded Grammatical Relation Assignment In this paper we discuss cascaded Memory-Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder. We achieve 71.2 F-score for grammatical relation assignment on automatically tagged and chunked text after training on about 40,000 Wall Street Journal sentences.	Cascaded Grammatical Relation Assignment In this paper, we talk about a step-by-step method using memory-based techniques to assign grammatical roles in sentences. In the first steps, we identify groups of words (called chunks) like noun phrases (NP), verb phrases (VP), adjective phrases (ADJP), adverb phrases (ADVP), and prepositional phrases (PP), and give them a role like indicating time or place. In the final step, we connect these word groups to show their grammatical relationships. We examined the impact of adding more steps to this method and found that even less accurate word group detectors improved the process of finding relationships. We achieved a 71.2 F-score, which is a measure of accuracy, for identifying grammatical relationships in text that has been automatically labeled and grouped, after training on about 40,000 sentences from the Wall Street Journal.	Combinations
1010	NaN	NaN	phrases
\.


--
-- Data for Name: responses; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.responses (user_id, passage_id, understanding, naturalness, simplicity, understanding_comment, naturalness_comment, simplicity_comment, id, complex_a, complex_b) FROM stdin;
351	501	Passage 2	Passage 2	Passage 2	Much less special lingo in 2	Same reasoning as above	Shorter sentences, easily understandable	105	A TAG-Based Noisy-Channel Model Of Speech Repairs This paper describes a noisy channel model of speech repairs, which can identify and correct repairs in speech transcripts.:::\n A [syntactic parser] is used as the [source model], and a novel type of [TAG-based transducer] is the [channel model].:::\n The use of TAG is motivated by the intuition that the [reparandum] is a "rough copy" of the repair.:::\n The model is trained and tested on the [Switchboard disfluency-annotated corpus].:::\n [Noisy channel models] do well on the [disfluency detection task].:::\n Although the standard noisy channel model performs well, a [log linear re-ranker] can be used to increase performance.:::\n Our TAG system achieves a high [EDIT-F score], largely as a result of its explicit tracking of overlapping words between [reparanda and alterations].	N/A
354	511	Passage 2	Passage 2	No Difference	better grammar and use of synonyms makes it easier to read	Passage 1 reads like a collection of sentences, Passage 2 like a text	Passage 1 has shorter sentences, Passage 2 ist better understandable	106	Discovering Relations Among Named Entities From Large Corpora Discovering the significant relations embedded in documents would be very useful not only for information retrieval but also for question answering and summarization.	Discovering Relations Among Named Entities From [Large Corpora Discovering] the significant relations embedded in documents would be very useful not only for information retrieval but also for question answering and summarization
354	512	Passage 1	No Difference	No Difference	the added clarifications in brackets	the differ mostly in a few synonyms	the differ mostly in a few synonyms	107	N/A	We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a [bag-of-words kernel]. 
354	513	Passage 2	Passage 2	No Difference	simpler word choices	Passage 1 is overloaded with scientific terms	the passages differ mostly in word choice	108	We present AImed, a corpus for the evaluation of [PPI] extraction systems	N/A
358	521	Passage 2	Passage 2	Passage 2	Passage 2 uses simpler words.	N/A	Passage 1 has more conjuctions while Passage 2 has shorter simple sentances.	109	By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve performance on the target problem.	N/A
358	522	Passage 1	Passage 2	Passage 2	Passage 1 explains the technical words better.	No excess explaination.	Sentances are very simple.	110	In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of [86.6% (F1, sentences <= 40 words)], which is comparable to that of an unlexicalized [PCFG parse]r created using extensive manual feature selection.	N/A
360	531	Passage 2	Passage 2	Passage 2	The words are easier, especially for non native speakers and those not used to talking about computer learning usw. Passage 2 also seems to explain things better, even though it  conveys the same information.	Closer to spoken language.	While the sentences are about the same lenght, the ones in Passage 2 feel more fluent and connected through the usage of conjunctions.	111	The model is formally a [synchronous] context-free grammar but is learned from a bitext without any syntactic information.:::\n We note that whenever we combine two dynamic programming items, we need to score the [fluency of their concatentation] by incorporating the score of any language model features which cross the [target side boundaries of the two concatenated items].:::\n We use the [k-best parsing algorithm in a CFG-based log-linear translation model] in order to learn feature weights which maximize BLEU.	The model is technically a [synchronous] context-free grammar, but it is learned from a pair of aligned texts (bitext) without using any grammar rules.:::\n We use the [k-best parsing method] in a grammar-based model to find the best feature weights that improve BLEU scores.
360	532	Passage 1	Passage 1	Passage 1	easier words	sentences are structured less complex	sentences are linked better	112	N/A	This method requires a source language dependency [parser], target language word segmentation and an unsupervised word alignment component.:::\n We align a [parallel corpus], project the [source dependency parse] onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model.:::\n We describe an efficient decoder and show that using these tree-based models in combination with [conventional SMT models] provides a promising approach that incorporates the power of phrasal SMT with the linguistic [generality] available in a [parser].
364	541	Passage 2	Passage 2	Passage 2	Explains context for readers without a technical background	Passage 2 uses more everyday phrasing (an essential part of being good at a language)vs. Passage 1's academic expressions (a fundamental component of language competency.)	Passage 2 breaks down sentences and makes the content more digestible	114	We develop a SVM categoriser combining a classifier based on trigram language models (one for each level of difficulty), some parsing features such as average tree height, and variables traditionally used in readability.:::\n Reading Level Assessment Using Support Vector Machines And Statistical Language Models Reading proficiency is a fundamental component of language competency.	n/a
364	542	Passage 1	Passage 1	Passage 1	phrases like “break down the sentence” and “rearranging the words" in Passage 1 are clearer than technical phrases like “parse the source language string” or “surface string" from Passage 2	Passage 1 is less formal	Passage 1 breaks ideas into short, direct sentences. Passage 2 describes complex ideas with longer, technical sentences	115	Clause Restructuring For Statistical Machine Translation We describe a method for incorporating syntactic information in statistical machine translation systems.:::\n The first step of the method is to parse the source language string that is being translated.:::\n The second step is to apply a series of transformations to the parse tree, effectively reordering the surface string on the source language side of the translation system.:::\n The reordering approach is applied as a pre-processing step in both the training and decoding phases of a phrase-based statistical MT system.	We point out it's unsure if the conditions needed for another method, called bootstrap resampling, apply to Bleu scores, so we suggest using the sign test.
364	548	Passage 1	Passage 1	Passage 2	explaining definitions	more approachable	less clear but passage 2 looks cleaner without parenthesis/added context	124	Randomized Algorithms And NLP: Using Locality Sensitive Hash Functions For High Speed Noun Clustering In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data.	We reduce the running time from quadratic (which grows quickly as data increases) to practically linear (grows slowly with more data) in the number of elements to be computed.:::\n We show that by using the LSH (Locality Sensitive Hashing, a method to find similar items) nearest neighbors calculation can be done in O(nd) time (a way to describe how fast an algorithm runs).
364	543	Passage 2	Passage 2	Passage 2	Passage 2 explains unfamiliar technical terms 	Passage 2 uses clearer everyday language	Passage 1 feels packed with long and formal technical sentences	116	In this paper, we present a syntax-based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar.:::\n Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees.:::\n We first introduce our approach to inducing such a grammar from parallel corpora.:::\n Second, we describe the graphical model for the machine translation task, which can also be viewed as a stochastic tree-to-tree transducer.:::\n We introduce a polynomial time decoding algorithm for the model.:::\n We present a translation model based on Synchronous Dependency Insertion Grammar (SDIG), which handles some of the non-isomorphism but requires both source and target dependency structures.	Next, we describe our method for translation using a visual model that acts like a tree-to-tree converter, changing one language structure into another using probability.:::\n In this paper, we introduce a system that uses these statistical methods with a specific type of grammar called probabilistic synchronous dependency insertion grammar.
364	544	Passage 1	Passage 1	Passage 1	Simpler words	More direct	Too many jargons in Passage 2	117	n/a	Arabic Tokenization Part-Of-Speech Tagging And Morphological Disambiguation In One Fell Swoop We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including part-of-speech tagging) Arabic words in one process.:::\n We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer.
10	31	Passage 2	No Difference	Passage 1	test test test test a	test test test test a	test test test test a	118	A Comparison Of Alignment Models For Statistical Machine Translation In this paper, we present and compare various alignnment models for statistical machine translation.	To improve word sequence models in the Hidden Markov Model (HMM) based matching, we make these models depend on word categories.
11	31	Passage 1	No Difference	Passage 2	testestestesttestestestesttestestestest	testestestesttestestestesttestestestest	testestestesttestestestesttestestestest	119	In order to improve transition models in the HMM based alignment, we extend the transition models to be word-class dependent.	We also look at how different matching methods affect the quality of translations in a statistical machine translation system.
364	545	Passage 2	Passage 2	Passage 2	explains definition of unfamiliar terms	less academic	passage 1 sentences are dense and nested	120	Semantic Role Labeling Using Different Syntactic Views Semantic role labeling is the process of annotating the predicate-argument structure in text with semantic labels.:::\n In this paper we present a state-of-the-art baseline semantic role labeling system based on Support Vector Machine classifiers.:::\n In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses.:::\n We use the Constituent, Predicate, and Predicate-Constituent related features for the kernel, resulting in the best performance.:::\n We combine the outputs of multiple parsers to extract reliable syntactic information, which is translated into features for a machine learning experiment in assigning semantic roles.	To fix this, we combined different sentence structure analyses from Minipar and a simplified method with our original system based on Charniak's analysis.
364	546	Passage 1	Passage 1	Passage 1	easier words, definition provided. (it's still a bit long though)	more conversational	sentences too complex in passage 2	121	Joint Learning Improves Semantic Role Labeling Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding.:::\n This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between arguments.:::\n We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative log-linear models.:::\n This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a state-of-the art independent classifier for gold-standard parse trees on PropBank.:::\n We introduce a joint approach for SRL and demonstrate that a model that scores the full predicate argument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation.:::\n We employ decomposition for efficiency in training: that is, the decomposition allows us to train the classification models on a subset of training examples consisting only of those phrases that have a case marker.	Joint Learning Improves Semantic Role Labeling Despite much recent progress in accurately identifying and labeling the roles words play in sentences (semantic role labeling), previous methods mostly used separate tools that worked independently, sometimes combining these with other models that sequence labels using a method called Viterbi decoding.
364	547	Passage 2	Passage 2	Passage 2	easier words	more natural	more direct sentences but still long	122	We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account.:::\n We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments.:::\n We define a paraphrasing probability between two phrases based on their translation probability through all possible pivot phrases.:::\n Paraphrasing With Bilingual Parallel Corpora Previous work has used monolingual parallel corpora to extract and generate paraphrases.	Paraphrasing With Bilingual Parallel Corpora Previous work has used collections of similar texts in the same language to find and create paraphrases, which are different ways of saying the same thing.:::\n We define a way to measure how likely something is a paraphrase, which helps organize paraphrases found in two-language text collections using how likely they are to translate, and show how to improve this by considering surrounding words.:::\n By using methods from computer programs that translate languages, we show how you can find paraphrases in one language by using a phrase in another language as a middle step.
12	31	Passage 1	Passage 2	Passage 2	testestsetestsesetestes	testestsetestsesetestes	testestsetestsesetestes	123	A Comparison Of Alignment Models For Statistical Machine Translation In this paper, we present and compare various alignnment models for statistical machine translation.	To improve word sequence models in the Hidden Markov Model (HMM) based matching, we make these models depend on word categories.
364	549	Passage 2	Passage 2	Passage 2	easier words	more natural expressions	some words didn't necessarily need added explainers in passage 2, but still easier to read	125	This paper demonstrates that match with respect to domain and time is also important, and presents preliminary experiments with training data labeled with emoticons, which has the potential of being independent of domain, topic and time.:::\n Traditional machine learning techniques have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the training and test data with respect to topic.	n/a
13	41	Passage 2	Passage 1	No Difference	testestestestsetetsete	testestestestsetetsete	testestestestsetetsete	126	Efficient Parsing Of Highly Ambiguous Context-Free Grammars With Bit Vectors An efficient bit-vector-based CKY-style parser for context-free parsing is presented.	We use the Viterbi algorithm, which is good at handling situations with many possible grammar interpretations.
13	42	Passage 2	Passage 2	No Difference	We suggest a way to combine the supertagger with the parser: at first, only a few categories are given to each word, and more are added if the parser can't find a complete analysis.	We suggest a way to combine the supertagger with the parser: at first, only a few categories are given to each word, and more are added if the parser can't find a complete analysis.	We suggest a way to combine the supertagger with the parser: at first, only a few categories are given to each word, and more are added if the parser can't find a complete analysis.	127	We suggest a way to combine the supertagger with the parser: at first, only a few categories are given to each word, and more are added if the parser can't find a complete analysis.	The Importance Of Supertagging For Wide-Coverage CCG Parsing This paper explains the role of supertagging, which is like a label assigning process, in a CCG parser that uses a mathematical method to choose the best analysis.:::\n We suggest a way to combine the supertagger with the parser: at first, only a few categories are given to each word, and more are added if the parser can't find a complete analysis.
13	43	Passage 2	Passage 1	No Difference	We suggest a way to combine the supertagger with the parser: at first, only a few categories are given to each word, and more are added if the parser can't find a complete analysis.	We suggest a way to combine the supertagger with the parser: at first, only a few categories are given to each word, and more are added if the parser can't find a complete analysis.	We suggest a way to combine the supertagger with the parser: at first, only a few categories are given to each word, and more are added if the parser can't find a complete analysis.	128	We introduce a sentence level QE system where an arbitrary threshold is used to classify the MT output as good or bad.	Various methods for determining whether MT (Machine Translation) output is correct are investigated, for both whole sentences and words.
14	41	Passage 2	Passage 2	No Difference	We suggest a way to combine the supertagger with the parser: at first, only a few categories are given to each word, and more are added if the parser can't find a complete analysis.	We suggest a way to combine the supertagger with the parser: at first, only a few categories are given to each word, and more are added if the parser can't find a complete analysis.	We suggest a way to combine the supertagger with the parser: at first, only a few categories are given to each word, and more are added if the parser can't find a complete analysis.	129	Efficient Parsing Of Highly Ambiguous Context-Free Grammars With Bit Vectors An efficient bit-vector-based CKY-style parser for context-free parsing is presented.	This parser is especially useful when every possible analysis is needed, not just the most likely one.
364	550	Passage 1	Passage 1	Passage 1	easier words	more natural expressions	less technical	130	n/a	Multi-Engine Machine Translation Guided By Explicit Word Matching We describe a new approach for synthetically combining the output of several different Machine Translation (MT) engines operating on the same input.:::\n The goal is to produce a synthetic combination that surpasses all of the original systems in translation quality.:::\n Our approach uses the individual MT engines as "black boxes" and does not require any explicit cooperation from the original MT systems.:::\n A decoding algorithm uses explicit word matches, in conjunction with confidence estimates for the various engines and a trigram language model in order to score and rank a collection of sentence hypotheses that are synthetic combinations of words from the various original engines.
365	551	Passage 2	No Difference	Passage 2	passage 2 uses easier language	Both passages are grammatically correct	simple sentence structures	131	We create a [corpus] of course lectures segmented by four annotators, noting that the annotators operated at different levels of [granularity].:::\n Our results demonstrate that global analysis improves the [segmentation accuracy] and is robust in the presence of speech recognition errors.	We focus on finding the best way to cut based on how similar the sentences are to each other, using a method similar to [cosine similarity] (a way to measure how alike two things are).
365	552	Passage 1	Passage 2	Passage 1	Simple sentence structure	Sentences are longer but make sense more.	Simple sentence structure	132	N/A	Highly [coreferent paths] also allow mining of precise probabilistic gender/number information.:::\n Given an automatically parsed corpus, we extract from each parse tree a dependency path, which is represented as a sequence of nodes and dependency labels connecting a pronoun and a candidate [antecedent], and collect statistical information from these paths to determine the likelihood that a pronoun and a candidate antecedent connected by a given path are coreferent.:::\n We build a statistical model from paths that include [the lemma of the intermediate tokens], but replace the end nodes with noun, pronoun, or pronoun-self for nouns, pronouns, and reflexive pronouns, respectively.
365	553	Passage 2	No Difference	Passage 2	Easier vocabs	Both sound natural in English	simple sentence structure	133	Moreover, the CRF has efficient training and decoding processes which both find globally [optimal] solutions.:::\n We show how a large number of highly predictive features can be easily incorporated into the CRF, and [demonstrate] that even with only a few hundred word-aligned training sentences, our model improves over the current [state-of-the-art] with [alignment] error rates of 5.29 and 25.8 for the two tasks respectively.	N/A
365	554	Passage 1	No Difference	Passage 1	easier terms	Both speaks good english	simpler sentence structure	134	We introduce two different methods for changing names: one uses sound-based [transliteration] and the other looks at how often the name pairs appear together over time.	We present two [distinct] methods for transliteration, one approach using [phonetic transliteration], and the second using the temporal distribution of candidate pairs.:::\n We then propose [a novel score propagation method] that utilizes the co-occurrence of transliteration pairs within document pairs.:::\n This [propagation] method achieves further improvement over the best results from the previous step.
365	555	Passage 2	No Difference	Passage 2	less technical terms are used	Both passages sound natural in english	passage 2 sounds simpler	135	This method enables us to extract useful machine translation training data even from very [non-parallel corpora], which contain no parallel sentence pairs.:::\n We first use the GI ZA++ [(with grow-diag-final-and heuristic)] to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words.:::\n We first extract the candidate parallel sentences from the comparable corpora and further extract the accurate [sub-sentential] bilingual fragments from the candidate parallel sentences using the in-domain probabilistic [bilingual lexicon.]:::\n We perform phrase extraction by combining clean [alignment lexica] for initial signals with heuristics to smooth alignments for final fragment extraction.	N/A
366	551	Passage 2	Passage 2	Passage 1	states the goal explicitly 	less repetitive, no notable grammatical errors when Passage 1 has one (a normalized minimum-cut criteria-> "a normalized minimum-cut criterion" OR "normalized minimum-cut criteria")	shorter sentences	136	Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of [speech recognition errors].	Our method looks at the big picture instead of just small parts and considers how well everything sticks together over long stretches.:::\n We focus on finding the best way to cut based on how similar the sentences are to each other, using a method similar to cosine similarity (a way to measure how alike two things are).:::\n Our findings show that this broader view helps us cut the lecture more accurately, even when there are mistakes in the speech-to-text process.
15	41	Passage 2	Passage 2	Passage 2	Passage 2 is easier to grasp because it replaces dense terms (“novel deep-learning architecture”) with simpler ones (“new deep-learning model”), breaks long sentences into shorter, clearer statements, and avoids parenthetical clauses such as “while also outlining,” so the paper’s objective emerges with less ambiguity.	Passage 2 sounds more natural.\nIts sentences are shorter and more direct, it uses everyday verbs such as “present” and “combines” instead of heavier phrases like “introduce a novel architecture,” and it avoids stacked modifiers (e.g., “robust performance in handling noisy data”) that make Passage 1 feel denser. Both passages are grammatically correct, but Passage 2’s simpler word choices and cleaner sentence structure give it a smoother, more fluent flow.	Passage 2 again comes out ahead.\nEvery sentence is short enough to fit on one-to-two text lines, avoids subordinate clauses (“although,” “while,” “whereas”), and sticks to a clear subject-verb-object pattern, so a reader can skim a single sentence and immediately grasp a key point (e.g., “Tests on standard datasets show that this model improves accuracy by 15%…”). Passage 1, by contrast, strings together multiple ideas in longer sentences that often spill beyond two rows, making it harder to spot the main content at a glance.	137	Recent advancements in [machine learning] have led to significant improvements in [natural language processing (NLP)].\nIn this study, researchers introduce a novel [deep learning architecture] that integrates [convolutional neural networks] with [recurrent neural networks] to enhance text understanding.\nExperimental results on [benchmark datasets] show that the proposed model achieves a 15 % improvement in accuracy over traditional methods.\nAdditionally, the model demonstrates robust performance in handling [noisy data] and [long-range dependencies] within text.\nThe authors discuss potential applications in [sentiment analysis], [machine translation], and [information extraction], while also outlining future work to further optimize the model’s efficiency and scalability.	In this study, researchers present a new [deep learning model] that combines [convolutional and recurrent neural networks] to better understand text.\n\nThe authors highlight potential uses in [sentiment analysis], [machine translation], and [information extraction], and they plan to continue improving the model’s efficiency and scalability.
15	42	Passage 1	Passage 2	Passage 1	Passage 1 is easier to grasp.\nIt replaces technical terms such as “log-linear model,” “derivation space,” and “normal-form derivation model” with everyday phrases like “mathematical method” and “label-assigning process,” breaks ideas into shorter, plainly worded sentences, and relies on clear cause-and-effect statements (“The supertagger makes the process more efficient by reducing the number of possible options…”). Passage 2, by contrast, packs dense jargon and longer noun phrases into single sentences, so the paper’s objective is harder to spot without prior knowledge of parsing theory.	Passage 2 sounds more natural.\nIts sentences follow standard academic‐English patterns (“reduces the derivation space over which model estimation is performed”), keep subjects and verbs close together, and avoid informal or awkward phrases such as “label assigning process,” “grammar created by a machine,” or “you can achieve much faster speeds,” which make Passage 1 feel less polished. Despite the heavier jargon, Passage 2’s syntax, word choice, and punctuation read smoothly and contain no grammatical slips, giving it a more fluent, professional tone.\n\n\n\n\n\n\n\n\n\n	Passage 1.\nMost of its points are broken into shorter, plain-language sentences without stacked clauses, so readers can process each idea quickly. Passage 2 often compresses multiple technical phrases into longer sentences, making its structure comparatively more complex.	138	This paper explains the role of [supertagging] in a [CCG parser] that uses a [mathematical method] to choose the best analysis.\n\nThe [supertagger] makes the process more efficient by reducing the number of possible options the model needs to evaluate, which also makes training faster.\n\nCCG parsing has two steps: first, the [supertagger] assigns likely [categories] to each word, and then a few rules—along with [type-changing] and [punctuation rules]—help build a [structured representation] using a method called the [CKY algorithm].\n\nWe suggest a way to combine the [supertagger] with the [parser]: at first, only a few categories are given to each word, and more are added if the parser can’t find a complete analysis.	This paper describes the role of [supertagging] in a [wide-coverage CCG parser] which uses a [log-linear model] to select an analysis.\n\nWe show that large increases in speed can be obtained by tightly integrating the [supertagger] with the [CCG grammar] and parser.\n\nThis is the first work we are aware of to successfully integrate a [supertagger] with a [full parser] which uses an [automatically extracted grammar].\n\nThe result is an accurate [wide-coverage CCG parser] which is an [order of magnitude] faster than comparable systems for other [linguistically motivated formalisms].\n\nThe [CCG parsing] consists of two phases: first the [supertagger] assigns the most probable [categories] to each word, and then the small number of [combinatory rules], plus the [type-changing] and [punctuation rules], are used with the [CKY algorithm] to build a [packed chart].\n\nWe propose a method for integrating the [supertagger] with the parser: initially a small number of [categories] is assigned to each word, and more categories are requested if the parser can not find a [spanning analysis].
15	43	Passage 2	Passage 1	Passage 1	Passage 2 is easier to follow.\nIt replaces less common terms like “notion” and “arbitrary threshold” with plainer phrases such as “idea” and “chosen limit,” and it explicitly spells out abbreviations—adding “(Machine Translation)” after “MT” and “(Quality Estimation)” after “QE.” Those small tweaks reduce jargon and ambiguity, so the paper’s objective—evaluating ways to judge MT quality at sentence and word levels—comes across more clearly.\n\n\n\n\n\n\n\n\n\n	Passage 1 sounds a little more natural overall.\nIts word choices (“arbitrary threshold,” “notion of correctness,” “sentence-level QE system”) match standard academic English and read smoothly, whereas some substitutions in Passage 2—such as “idea of correctness is not easy to understand” and “chosen limit”—feel slightly informal or awkward. Both passages are grammatically correct, but Passage 1’s vocabulary and phrasing align better with conventional, fluent scholarly style.\n\n\n\n\n\n\n\n\n\n	Passage 1—its sentences are a little shorter and have fewer parenthetical insertions or stacked phrases, so each idea is delivered in a cleaner, more easily digestible structure.	139	Various methods for determining whether [MT] output is correct are investigated, for both [whole sentences] and [words].\n\nSince the [notion of correctness] is not intuitively clear in this context, different ways of defining it are proposed.\n\nWe introduce a [sentence-level QE system] where an [arbitrary threshold] is used to classify the [MT output] as good or bad.	Since the [idea of correctness] is not easy to understand in this context, different ways of defining it are proposed.\nWe introduce a [sentence-level QE (Quality Estimation) system] where a chosen limit is used to classify the [MT output] as good or bad.
365	560	Passage 1	No Difference	Passage 1	better explanation for technical terms	both sound natural in English	shorter and simpler sentence structure	151	N/A	Much of this improvement, however, is based upon an [ever-increasing] number of features to be trained on (typically) the WSJ treebank data.:::\n This paper should allay these fears.:::\n We successfully applied self-training to parsing by [exploiting] available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation.
15	44	Passage 1	Passage 2	Passage 1	Passage 1 is easier to grasp.\nIt replaces dense technical phrases (“monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles”) with plainer language (“sentences with the same meaning but different words from news articles grouped by time and topic”), breaks long ideas into shorter statements, and offers quick glosses for jargon (e.g., explaining edit distance as “counting changes needed to make them the same”). Those choices reduce ambiguity and let the paper’s objective—building a large paraphrase corpus using two unsupervised methods—stand out clearly.	Passage 2 reads more naturally in standard academic English.\nIts sentences follow familiar scholarly patterns (“We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases …”), avoid contractions such as “don’t,” maintain consistent terminology, and keep subjects and verbs close together, giving the prose a smooth, formal flow. Passage 1, by contrast, mixes conversational wording (“look into methods that don’t need human guidance,” “rule of thumb”) with some awkward constructions (“a scoring method from translating languages”), which makes it feel less polished even though both passages are largely grammatical.\n\n\n\n\n\n\n\n\n\n	Passage 1 uses simpler sentence structure.\nMost ideas are broken into shorter, standalone sentences with basic subject-verb-object patterns and few embedded clauses, making each point easy to process quickly. Passage 2, although well-written, frequently packs multiple technical phrases into longer sentences and uses more subordinate clauses (e.g., “collected from thousands of web-based news sources”), which increases structural complexity.	140	We look into methods that don't need human guidance for finding [paraphrases] from a [collection of news articles] grouped by time and topic from many online news sources.\n\nWe use two methods: (1) [simple string edit distance], and (2) a [heuristic strategy] that pairs initial sentences from different news stories in the same group.\n\nResults show that the [edit-distance data] is cleaner and easier to match, with an [alignment error rate (AER)] of 11.58 % on a similar test set.\n\nWe introduce the [Microsoft Research Paraphrase Corpus (MSRPC)], which leverages [web-aggregated news stories] to learn both sentence- and word-level alignments.	We investigate [unsupervised techniques] for acquiring [monolingual sentence-level paraphrases] from a corpus of [temporally and topically clustered news articles] collected from thousands of [web-based news sources].\n\nTwo techniques are employed: (1) [simple string edit distance], and (2) a [heuristic strategy] that pairs initial (presumably summary) sentences from different news stories in the same cluster.\n\nResults show that edit-distance data is cleaner and more easily aligned than the heuristic data, with an overall [alignment error rate (AER)] of 11.58 % on a similarly extracted test set.\n\nOn test data extracted by the [heuristic strategy], however, performance of the two training sets is similar, with [AERs] of 13.2 % and 14.7 % respectively.\n\nAnalysis of 100 pairs of sentences from each set reveals that the edit-distance data lacks many of the complex [lexical and syntactic alternations] that characterize [monolingual paraphrase].\n\nThe summary sentences, while less readily alignable, retain more of the [non-trivial alternations] that are of greatest interest when learning [paraphrase relationships].
15	45	Passage 2	Passage 1	Passage 2	Passage 2 is easier to grasp.\nIt swaps specialized terms like “unsupervised language-model adaptation techniques” and “interpolated with a general background model” for plainer phrases (“adjust language models … without human guidance,” “combined with a general model”), breaks dense ideas into shorter sentences, and repeats the core workflow in straightforward language (“turn results into search requests,” “find similar sentences,” “create specific language models”). Those choices eliminate jargon and reduce sentence complexity, so the paper’s main objective—using MT output as queries to build better language models that improve translation—emerges with the least ambiguity.\n\n\n\n\n\n\n\n\n\n	Passage 1 reads more naturally in polished academic English.\nIts sentences employ standard technical terms (“unsupervised language-model adaptation,” “interpolated with a general background model”) and vary rhythm without awkward repetition. Passage 2, while perfectly grammatical, relies on informal or slightly clumsy substitutes (“search requests,” “big improvements,” “adjust language models … without human guidance”) and repeats the same ideas almost verbatim in successive sentences, which makes the prose feel less smooth and professional.\n\n\n\n\n\n\n\n\n\n	Passage 2.\nIts points are divided into shorter, straightforward sentences with few embedded clauses, making each step of the method easy to follow, whereas Passage 1 often compresses several technical details into longer, multi-clause statements.	141	The hypotheses from the [machine translation output] are converted into [queries at different levels of representation power] and used to extract similar sentences from a [very large monolingual text collection].\n\nWe apply a slightly different [sentence-level strategy] to language-model adaptation, first generating an [nbest list] with a [baseline system], then finding similar sentences in a [monolingual target-language corpus].\n\nWe construct [specific language models] by using [machine translation output] as queries to extract similar sentences from [large monolingual corpora].	The results from the translation process are turned into [search requests] at [different levels of detail] and used to find similar sentences from a [huge collection of text] in one language.\n\nWe use a slightly different method by first creating a list of possible translations, then finding similar sentences in a [text collection] of the [target language].\n\nWe make specific language models by using [machine translation results] as [search requests] to find similar sentences in [large text collections].\n\nWe change initial [machine translation guesses] into [search requests] and find similar sentences from a [large collection of text] in one language.
15	46	Passage 1	Passage 2	Passage 1	Passage 1 is easier to grasp.\nIt swaps technical phrases like “corpus level,” “product‐moment,” and “higher-ordered n-grams” for ordinary words (“large set of texts,” “two-word matches”), breaks ideas into brisk, declarative sentences, and consistently explains jargon in plain language (“accuracy and smoothness,” “human opinions can be unreliable and costly”). That straightforward wording and sentence structure make the paper’s aim—offering ORANGE as a way to test MT-evaluation metrics without extra human effort—immediately clear and unambiguous.	Passage 2 sounds more natural and polished.\nIts sentences follow standard academic phrasing (“are usually conducted on corpus level,” “we introduce a new evaluation method”), it uses precise connectors (“however,” “such comparisons rely on”), and it avoids conversational fillers like “looking at things like” found in Passage 1. The terminology is consistent and domain-appropriate, and punctuation is handled cleanly, giving the prose a smoother, more professional flow with no noticeable grammatical slips.\n\n\n\n\n\n\n\n\n\n	Passage 1.\nMost of its ideas are delivered in short, stand-alone sentences with basic subject-verb-object patterns and few embedded clauses, so the structure is quick to parse. Passage 2 often stacks multiple technical phrases in one sentence (e.g., the long opening sentence describing correlation statistics), which makes its syntax denser and harder to scan at a glance.	142	Comparisons of [automatic evaluation measures] for [machine translation] are usually done on a large set of texts using [statistical methods] like [Pearson’s correlation coefficient] or [Spearman’s rank correlation coefficient].\n\nHowever, these comparisons depend on [human opinions] about how good the translation is, looking at things like [how accurate and smooth] it is.\n\nIn this paper, we introduce a new method called [ORANGE], which allows us to evaluate [automatic translation evaluation tools] without needing [extra human input], except for using a set of [reference translations].\n\nBLEU, a common evaluation tool, is adjusted (Lin and Och, 2004b) and it only looks at [two-word matches] because this aligns better with [human opinions] than using [longer word sequences].\n\n\n\n\n\n\n\n\n\n	Comparisons of [automatic evaluation metrics] for [machine translation] are usually conducted on [corpus level] using [correlation statistics] such as [Pearson’s product-moment correlation coefficient] or [Spearman’s rank-order correlation coefficient] between [human scores] and [automatic scores].\n\nIn this paper, we introduce a new evaluation method, [ORANGE], for evaluating [automatic machine-translation evaluation metrics] automatically without [extra human involvement] other than using a set of [reference translations].\n\n[BLEU] is [smoothed] (Lin and Och, 2004b), and it considers only matching up to [bigrams] because this has higher correlations with [human judgments] than when [higher-ordered n-grams] are included.\n\n\n\n\n\n\n\n\n\n
15	47	Passage 2	Passage 1	Passage 2	Passage 2 is easier to understand.\nIt substitutes technical terms like “clump-based,” “linguistic phrases,” and “broad-coverage rule-based parsers” with everyday wording (“group-based,” “language phrases,” “tools that analyze sentence structure”), explains jargon in parentheses (e.g., defining parsers and the BLEU metric), and breaks dense ideas into shorter, direct sentences—so the paper’s aim of learning rewrite patterns to fix word-order problems in machine translation is immediately clear and unambiguous.\n\n\n\n\n\n\n\n\n\n	Passage 1 reads more naturally in polished academic English.\nIts terminology (“clump-based statistical MT systems,” “broad-coverage rule-based parsers”) and sentence rhythm match the conventions of research writing, it avoids contractions, and it stays concise without parenthetical asides that break flow. Passage 2, though grammatical, introduces informal touches (“don’t,” “tools that analyze sentence structure”), repeats ideas almost verbatim, and uses slightly awkward substitutions (“group-based,” “while running”), so the prose feels less smooth and professional.	Passage 2.\nIts ideas are broken into shorter, plain sentences—often one clause each—and it adds clarifying parentheses instead of embedding long, technical phrases inside single statements, so readers can process each point quickly. Passage 1 bundles more information into longer, multi-clause sentences and uses denser technical wording, making its structure comparatively more complex.	143	Current [clump-based statistical MT systems] have two limitations with respect to word ordering: first, they lack a [mechanism for expressing and using generalization] that accounts for reorderings of [linguistic phrases].\n\nTo address these limitations, we propose to use [automatically learned rewrite patterns] to preprocess the source sentences so that they have a word order similar to that of the [target language].\n\nThe basic model is statistical, but we use [broad-coverage rule-based parsers] in two ways—during [training] for learning rewrite patterns and at [runtime] for reordering the source sentences.\n\nOur [reordering rules] are automatically learned from aligning [parse trees] for both the source and target sentences.	Current [group-based statistical machine translation (MT) systems] have two problems with word order: first, they don’t have a way to handle changes in word order that cover different language phrases.\n\nTo fix these problems, we suggest using [automatically learned rewrite patterns] to rearrange the words in the original sentences so they match the order of the target language.\n\nThe main model is statistical, but we also use [rule-based parsers] in two ways—one during [training] to learn rewrite patterns and the other while [running] to rearrange the original sentences.
365	556	Passage 1	No Difference	Passage 1	easier terms, less technical vocabs	both sounds natural in english	shorter sentences with explanations	144	N/A	In this paper, we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense [hierarchies], namely the Oxford Dictionary of English.:::\n In our [coarse-grained task], the sense inventory was first clustered semi-automatically with each cluster representing an equivalence class over senses.
15	48	Passage 1	Passage 2	Passage 1	Passage 1 is easier to grasp.\nIt replaces dense academic phrases such as “lexicon,” “bi-tag HMMs,” and “non-training intensive framework” with plainer words like “dictionary,” “simpler method,” and “doesn’t require extensive training,” and it briefly defines technical ideas (e.g., “a tool that labels words in a sentence”). Sentences are shorter, connectors are straightforward, and there are fewer embedded clauses, so the paper’s main objective—building and evaluating a context-aware HMM tagger whose accuracy hinges on dictionary quality—emerges clearly and without ambiguity.\n\n\n\n\n\n\n\n\n\n	Passage 2 sounds more natural and polished.\nIts sentences follow standard academic style (“we present,” “evaluate it in both the unsupervised and supervised case”), employ precise vocabulary (“lexicon,” “state-of-the-art,” “framework”), and avoid informal wording or minor slips such as “with help (supervised)” found in Passage 1. Passage 2 also keeps grammatical structures consistent and uses smoother connectors (“Finally,” “Observing that”), giving the prose a fluent, professional flow without awkward pauses or redundancies.	Passage 1.\nIts ideas are mostly delivered in short, single-clause sentences (often under 20 words) with straightforward subject-verb-object patterns and minimal parenthetical insertions, so each point can be digested quickly. Passage 2, by contrast, often packs several technical phrases into longer, multi-clause sentences—especially in the opening and closing lines—which makes its structure denser and slightly harder to scan at a glance.	145	We also provide the first detailed comparison of [unsupervised methods] for labeling [parts of speech], pointing out that previous results have not been consistent across different sets of texts or dictionaries.\n\nWe find that the dictionary's quality greatly affects how accurate the labeling is, so we offer a way to train [HMMs] that makes the labeling more accurate when the dictionary is unstable.\n\nWhile repeating previous tests, we find that success relies on cleaning up the [tag dictionaries] with helpful data from the text.\n\nWe show that the [expectation maximization algorithm], a method for improving accuracy in tagging, works well with [HMMs] when we have a dictionary and certain favorable conditions.\n\nWe notice that past high results in [unsupervised HMM-EM] tests were due to using improved dictionaries, where only common word analyses were included.\n\n\n\n\n\n\n\n\n\n	We present a new [HMM tagger] that exploits context on both sides of a word to be tagged, and evaluate it in both the [unsupervised] and [supervised] case.\n\nAlong the way, we present the first comprehensive comparison of [unsupervised methods] for [part-of-speech tagging], noting that published results to date have not been comparable across [corpora] or [lexicons].\n\nObserving that the quality of the [lexicon] greatly impacts the accuracy that can be achieved by the algorithms, we present a method of [HMM training] that improves accuracy when training of [lexical probabilities] is unstable.\n\nWhile replicating earlier experiments, we discover that performance was highly dependent on cleaning [tag dictionaries] using statistics gleaned from the tokens.\n\nWe show that the [expectation maximization algorithm] for [bi-tag HMMs] is efficient and quite effective for acquiring accurate POS taggers given only a [lexicon] ([tag dictionary]) and certain favorable conditions.\n\nWe observe that earlier [unsupervised HMM-EM] results were artificially high due to use of [Optimized Lexicons], in which only frequent-enough analyses of each word were kept.
15	49	Passage 2	Passage 2	Passage 2	Passage 2 is easier to follow.\nIt replaces technical jargon like “linear-chain conditional random fields (CRFs) to perform robust and accurate Chinese word segmentation by providing a principled framework” with plain, explanatory phrases (“a method called linear-chain conditional random fields … can effectively handle Chinese word segmentation”), defines the task in everyday terms (“breaking down sentences into words”), and explains CRFs as “a statistical tool for analyzing sequences.” Sentences are shorter, definitions are embedded where needed, and the workflow—segmenting characters and detecting new words—is described in simple yes-or-no decisions, so the paper’s objective comes across with minimal ambiguity.	Passage 2 sounds more natural overall.\nIts sentences flow smoothly, avoid duplication, and contain no obvious grammatical slips, whereas Passage 1 repeats ideas almost verbatim, contains awkward phrasing (“was also demonstrated in word segmentation)”), and has mismatched punctuation. Passage 2 keeps terminology consistent, explains concepts clearly, and maintains a steady, readable rhythm, giving it the more fluent, polished feel.	Passage 2.\nMost ideas are delivered in short, single-clause sentences (e.g., “We also introduce a method for finding new words, which helps improve accuracy.”), and technical concepts are broken into step-by-step descriptions—so readers can grasp each point quickly. Passage 1, by contrast, packs multiple technical phrases into longer, multi-clause sentences and repeats similar statements, making its structure denser and harder to scan at a glance.	146	This paper demonstrates the ability of [linear-chain conditional random fields (CRFs)] to perform robust and accurate Chinese word segmentation by providing a [principled framework] that easily supports the integration of domain knowledge via [multiple lexicons of characters and words].\n\nCRF is a [statistical sequence-modeling framework] introduced by [Lafferty et al. (2001)], and we use it for the Chinese word-segmentation task by treating word segmentation as a [binary decision task].\n\nWe first use this framework for Chinese word segmentation by treating it as a [binary decision task], such that each character is labeled either as the [beginning of a word] or the [continuation of one].\n\nWe define the [word-segmentation problem] as labeling each character according to whether the [previous character boundary] of the current character is a [word boundary].\n\n\n\n\n\n\n\n\n\n	This paper shows how a method called [linear-chain conditional random fields (CRFs)] can effectively handle Chinese word segmentation by providing a [structured approach] that easily incorporates [specialized knowledge] using [various lists of characters and words].\n\nCRF is a [statistical tool for analyzing sequences], introduced by Lafferty et al. (2001), and we use it for Chinese word segmentation by [treating it as a task] where you make [yes-or-no decisions].\n\nWe apply this method by [labeling each character as either the start of a new word or a continuation of the current word].\n\nWe define the word-segmentation problem by [deciding if each character is the start of a new word or continues from the previous character].
365	557	Passage 1	No Difference	Passage 1	easier terms and shorter sentences	both sounds natural in English	shorter sentences	147	We present an empirical comparison of Espresso with various state of the art systems, on different size and [genre corpora], on extracting various general and specific relations.:::\n Experimental results show that our [exploitation] of generic patterns substantially increases system recall with small effect on overall [precision.]:::\n In the pattern [induction step], our system computes a [reliability score] for each candidate pattern based on the weighted pointwise mutual information, PMI, of the pattern with all instances extracted so far.	Our program, which doesn't need much [supervision], starts with a single set that mixes different types of examples, like leader-panel and oxygen-water, which relate to member-of and part-of relationships as described by Keet and Artale in 2008.
15	50	Passage 1	Passage 2	Passage 1	Passage 1 is easier to understand.\nIt translates the technical workflow into everyday language—e.g., “identifying the main parts of sentences and the meaning behind them” rather than “identifying predicate-argument structures,” and “make informed guesses” instead of “perform structured probabilistic inference.” Jargon such as “semantic representations,” “domain and scenario model,” or “scalable and expressive representation” is either simplified or omitted. Sentences are generally shorter, with clear step-by-step explanations, so the paper’s goal—using semantic structures plus probabilistic reasoning to answer tough natural-language questions—emerges with little ambiguity.\n\n\n\n\n\n\n\n\n\n	Passage 2 reads more naturally in polished academic English.\nIts sentences follow standard scholarly phrasing (“complex questions posed in Natural Language,” “performing structured probabilistic inference”), use precise connectors (“In this paper we describe…,” “The results indicate…”), and avoid colloquial expressions like “make informed guesses” that appear in Passage 1. Terminology is used consistently, clauses are balanced, and punctuation is smooth, giving Passage 2 a fluent, professional cadence with no noticeable grammatical slips.	Passage 1.\nMost ideas are broken into shorter, single-clause sentences with everyday wording (“make informed guesses,” “using the relationships we extract”) and only occasional parenthetical phrases, so each step is quick to parse. Passage 2 often combines several technical phrases in longer, multi-clause sentences—especially when describing semantic representations and inference—making its structure denser and harder to scan at a glance.	148	The ability to answer hard questions written in everyday language depends on (1) how detailed the [semantic information] is and (2) the [reasoning methods] that this information supports.\n\nIn this paper, we describe a [question-answering (QA) system] where questions are examined and possible answers are generated by 1) identifying the [main parts of sentences] and the [meaning] behind them from the input, and 2) using these [extracted relationships] to make [informed guesses] within a [specific topic or situation].\n\nA new feature of our system is a flexible and clear way of showing actions and events using [Coordinated Probabilistic Relational Models (CPRM)], which is a method to predict outcomes based on relationships.\n\nOur [QA system] uses [PropBank/FrameNet labels] as input, which help identify what actions are being described and with what details, and then it gives an answer by using [models that predict actions based on chance and reasoning].	The ability to answer complex questions posed in [Natural Language] depends on (1) the depth of the available [semantic representations] and (2) the [inferential mechanisms] they support.\n\nIn this paper we describe a [QA architecture] where questions are analyzed and candidate answers generated by 1) identifying [predicate argument structures] and [semantic frames] from the input and 2) performing [structured probabilistic inference] using the extracted relations in the context of a [domain and scenario model].\n\nA novel aspect of our system is a scalable and expressive representation of actions and events based on [Coordinated Probabilistic Relational Models (CPRM)].\n\nIn this paper we report on the ability of the implemented system to perform several forms of [probabilistic and temporal inferences] to extract answers to complex questions.\n\nOur [question answering system] takes [PropBank/FrameNet annotations] as input, uses the [PropBank targets] to indicate which actions are being described with which arguments, and produces an answer using [probabilistic models] of actions as the tools of inference.
365	558	Passage 1	No Difference	Passage 1	easier terms	both sound natural	simpler and shorter length	149	We show that using the SMT approach can find mistakes that common [proofreading tools] for native speakers often miss.	Our system was able to correct 61.81% of mistakes in a set of naturally-occurring examples of mass noun errors found on the World Wide Web, suggesting that efforts to collect [alignable corpora] of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the [complex syntactic and lexical problems] found in the writing of ESL learners.:::\n We utilize phrasal Statistical Machine Translation (SMT) techniques to correct ESL writing errors and demonstrate that this [data-intensive SMT approach] is very promising, but we also point out SMT approach relies on the availability of large amount of training data.
365	559	Passage 2	No Difference	Passage 2	easier terms and less technical vocabs	both sound natural	shorter and simpler sentences	150	We [utilize meta-patterns] of high-frequency words and content words in order to discover pattern candidates.:::\n Symmetric patterns are then identified using graph-based measures, and word categories are created based on [graph clique sets].	N/A
16	51	Passage 2	Passage 2	Passage 2	For people who has no idea what the topic is, passage 2 make it easier for readers to understand 	Simply because it is easier to understand 	Compared to passage 1 it is much easier to understand 	152	We propose, in the scenario of extracting is-a relations, one pattern-based approach and compared it with a baseline syntactic distributional similarity method (called syntactic co-occurrence in their paper).:::\n We propose a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performance and efficiency.	N/A
16	52	Passage 1	Passage 1	Passage 1	Concise and use simple words	Complexity in passage 2	Simpler vocabulary 	153	N/A	Abstracting from results for concrete test sets, we try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends.
16	53	Passage 1	Passage 1	Passage 1	Clearer phrases0	Natural and easy to understand 	Easily understandable 	154	N/A	Unlike the basic connection structures (dependency structures) the parser gives, these can be used directly to understand meaning (semantic interpretation).
16	54	Passage 2	Passage 2	Passage 2	Easier words 	Natural	Easily understandable 	155	The system is tested on data from a specific competition in 2004 about identifying word roles and performs very well.	N/A
\.


--
-- Data for Name: survey_set_passages; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.survey_set_passages (id, survey_set_id, passage_id) FROM stdin;
2	1	1
3	1	2
4	1	3
5	1	4
6	1	5
7	1	6
8	1	7
9	1	8
10	1	9
11	1	10
12	2	11
13	2	12
14	2	13
15	2	14
16	2	15
17	2	16
18	2	17
19	2	18
20	2	19
21	2	20
22	3	21
23	3	22
24	3	23
25	3	24
26	3	25
27	3	26
28	3	27
29	3	28
30	3	29
31	3	30
32	4	31
33	4	32
34	4	33
35	4	34
36	4	35
37	4	36
38	4	37
39	4	38
40	4	39
41	4	40
42	5	41
43	5	42
44	5	43
45	5	44
46	5	45
47	5	46
48	5	47
49	5	48
50	5	49
51	5	50
52	6	51
53	6	52
54	6	53
55	6	54
56	6	55
57	6	56
58	6	57
59	6	58
60	6	59
61	6	60
62	7	61
63	7	62
64	7	63
65	7	64
66	7	65
67	7	66
68	7	67
69	7	68
70	7	69
71	7	70
72	8	71
73	8	72
74	8	73
75	8	74
76	8	75
77	8	76
78	8	77
79	8	78
80	8	79
81	8	80
82	9	81
83	9	82
84	9	83
85	9	84
86	9	85
87	9	86
88	9	87
89	9	88
90	9	89
91	9	90
92	10	91
93	10	92
94	10	93
95	10	94
96	10	95
97	10	96
98	10	97
99	10	98
100	10	99
101	10	100
102	11	101
103	11	102
104	11	103
105	11	104
106	11	105
107	11	106
108	11	107
109	11	108
110	11	109
111	11	110
112	12	111
113	12	112
114	12	113
115	12	114
116	12	115
117	12	116
118	12	117
119	12	118
120	12	119
121	12	120
122	13	121
123	13	122
124	13	123
125	13	124
126	13	125
127	13	126
128	13	127
129	13	128
130	13	129
131	13	130
132	14	131
133	14	132
134	14	133
135	14	134
136	14	135
137	14	136
138	14	137
139	14	138
140	14	139
141	14	140
142	15	141
143	15	142
144	15	143
145	15	144
146	15	145
147	15	146
148	15	147
149	15	148
150	15	149
151	15	150
152	16	151
153	16	152
154	16	153
155	16	154
156	16	155
157	16	156
158	16	157
159	16	158
160	16	159
161	16	160
162	17	161
163	17	162
164	17	163
165	17	164
166	17	165
167	17	166
168	17	167
169	17	168
170	17	169
171	17	170
172	18	171
173	18	172
174	18	173
175	18	174
176	18	175
177	18	176
178	18	177
179	18	178
180	18	179
181	18	180
182	19	181
183	19	182
184	19	183
185	19	184
186	19	185
187	19	186
188	19	187
189	19	188
190	19	189
191	19	190
192	20	191
193	20	192
194	20	193
195	20	194
196	20	195
197	20	196
198	20	197
199	20	198
200	20	199
201	20	200
202	21	201
203	21	202
204	21	203
205	21	204
206	21	205
207	21	206
208	21	207
209	21	208
210	21	209
211	21	210
212	22	211
213	22	212
214	22	213
215	22	214
216	22	215
217	22	216
218	22	217
219	22	218
220	22	219
221	22	220
222	23	221
223	23	222
224	23	223
225	23	224
226	23	225
227	23	226
228	23	227
229	23	228
230	23	229
231	23	230
232	24	231
233	24	232
234	24	233
235	24	234
236	24	235
237	24	236
238	24	237
239	24	238
240	24	239
241	24	240
242	25	241
243	25	242
244	25	243
245	25	244
246	25	245
247	25	246
248	25	247
249	25	248
250	25	249
251	25	250
252	26	251
253	26	252
254	26	253
255	26	254
256	26	255
257	26	256
258	26	257
259	26	258
260	26	259
261	26	260
262	27	261
263	27	262
264	27	263
265	27	264
266	27	265
267	27	266
268	27	267
269	27	268
270	27	269
271	27	270
272	28	271
273	28	272
274	28	273
275	28	274
276	28	275
277	28	276
278	28	277
279	28	278
280	28	279
281	28	280
282	29	281
283	29	282
284	29	283
285	29	284
286	29	285
287	29	286
288	29	287
289	29	288
290	29	289
291	29	290
292	30	291
293	30	292
294	30	293
295	30	294
296	30	295
297	30	296
298	30	297
299	30	298
300	30	299
301	30	300
302	31	301
303	31	302
304	31	303
305	31	304
306	31	305
307	31	306
308	31	307
309	31	308
310	31	309
311	31	310
312	32	311
313	32	312
314	32	313
315	32	314
316	32	315
317	32	316
318	32	317
319	32	318
320	32	319
321	32	320
322	33	321
323	33	322
324	33	323
325	33	324
326	33	325
327	33	326
328	33	327
329	33	328
330	33	329
331	33	330
332	34	331
333	34	332
334	34	333
335	34	334
336	34	335
337	34	336
338	34	337
339	34	338
340	34	339
341	34	340
342	35	341
343	35	342
344	35	343
345	35	344
346	35	345
347	35	346
348	35	347
349	35	348
350	35	349
351	35	350
352	36	351
353	36	352
354	36	353
355	36	354
356	36	355
357	36	356
358	36	357
359	36	358
360	36	359
361	36	360
362	37	361
363	37	362
364	37	363
365	37	364
366	37	365
367	37	366
368	37	367
369	37	368
370	37	369
371	37	370
372	38	371
373	38	372
374	38	373
375	38	374
376	38	375
377	38	376
378	38	377
379	38	378
380	38	379
381	38	380
382	39	381
383	39	382
384	39	383
385	39	384
386	39	385
387	39	386
388	39	387
389	39	388
390	39	389
391	39	390
392	40	391
393	40	392
394	40	393
395	40	394
396	40	395
397	40	396
398	40	397
399	40	398
400	40	399
401	40	400
402	41	401
403	41	402
404	41	403
405	41	404
406	41	405
407	41	406
408	41	407
409	41	408
410	41	409
411	41	410
412	42	411
413	42	412
414	42	413
415	42	414
416	42	415
417	42	416
418	42	417
419	42	418
420	42	419
421	42	420
422	43	421
423	43	422
424	43	423
425	43	424
426	43	425
427	43	426
428	43	427
429	43	428
430	43	429
431	43	430
432	44	431
433	44	432
434	44	433
435	44	434
436	44	435
437	44	436
438	44	437
439	44	438
440	44	439
441	44	440
442	45	441
443	45	442
444	45	443
445	45	444
446	45	445
447	45	446
448	45	447
449	45	448
450	45	449
451	45	450
452	46	451
453	46	452
454	46	453
455	46	454
456	46	455
457	46	456
458	46	457
459	46	458
460	46	459
461	46	460
462	47	461
463	47	462
464	47	463
465	47	464
466	47	465
467	47	466
468	47	467
469	47	468
470	47	469
471	47	470
472	48	471
473	48	472
474	48	473
475	48	474
476	48	475
477	48	476
478	48	477
479	48	478
480	48	479
481	48	480
482	49	481
483	49	482
484	49	483
485	49	484
486	49	485
487	49	486
488	49	487
489	49	488
490	49	489
491	49	490
492	50	491
493	50	492
494	50	493
495	50	494
496	50	495
497	50	496
498	50	497
499	50	498
500	50	499
501	50	500
502	51	501
503	51	502
504	51	503
505	51	504
506	51	505
507	51	506
508	51	507
509	51	508
510	51	509
511	51	510
512	52	511
513	52	512
514	52	513
515	52	514
516	52	515
517	52	516
518	52	517
519	52	518
520	52	519
521	52	520
522	53	521
523	53	522
524	53	523
525	53	524
526	53	525
527	53	526
528	53	527
529	53	528
530	53	529
531	53	530
532	54	531
533	54	532
534	54	533
535	54	534
536	54	535
537	54	536
538	54	537
539	54	538
540	54	539
541	54	540
542	55	541
543	55	542
544	55	543
545	55	544
546	55	545
547	55	546
548	55	547
549	55	548
550	55	549
551	55	550
552	56	551
553	56	552
554	56	553
555	56	554
556	56	555
557	56	556
558	56	557
559	56	558
560	56	559
561	56	560
562	57	561
563	57	562
564	57	563
565	57	564
566	57	565
567	57	566
568	57	567
569	57	568
570	57	569
571	57	570
572	58	571
573	58	572
574	58	573
575	58	574
576	58	575
577	58	576
578	58	577
579	58	578
580	58	579
581	58	580
582	59	581
583	59	582
584	59	583
585	59	584
586	59	585
587	59	586
588	59	587
589	59	588
590	59	589
591	59	590
592	60	591
593	60	592
594	60	593
595	60	594
596	60	595
597	60	596
598	60	597
599	60	598
600	60	599
601	60	600
602	61	601
603	61	602
604	61	603
605	61	604
606	61	605
607	61	606
608	61	607
609	61	608
610	61	609
611	61	610
612	62	611
613	62	612
614	62	613
615	62	614
616	62	615
617	62	616
618	62	617
619	62	618
620	62	619
621	62	620
622	63	621
623	63	622
624	63	623
625	63	624
626	63	625
627	63	626
628	63	627
629	63	628
630	63	629
631	63	630
632	64	631
633	64	632
634	64	633
635	64	634
636	64	635
637	64	636
638	64	637
639	64	638
640	64	639
641	64	640
642	65	641
643	65	642
644	65	643
645	65	644
646	65	645
647	65	646
648	65	647
649	65	648
650	65	649
651	65	650
652	66	651
653	66	652
654	66	653
655	66	654
656	66	655
657	66	656
658	66	657
659	66	658
660	66	659
661	66	660
662	67	661
663	67	662
664	67	663
665	67	664
666	67	665
667	67	666
668	67	667
669	67	668
670	67	669
671	67	670
672	68	671
673	68	672
674	68	673
675	68	674
676	68	675
677	68	676
678	68	677
679	68	678
680	68	679
681	68	680
682	69	681
683	69	682
684	69	683
685	69	684
686	69	685
687	69	686
688	69	687
689	69	688
690	69	689
691	69	690
692	70	691
693	70	692
694	70	693
695	70	694
696	70	695
697	70	696
698	70	697
699	70	698
700	70	699
701	70	700
702	71	701
703	71	702
704	71	703
705	71	704
706	71	705
707	71	706
708	71	707
709	71	708
710	71	709
711	71	710
712	72	711
713	72	712
714	72	713
715	72	714
716	72	715
717	72	716
718	72	717
719	72	718
720	72	719
721	72	720
722	73	721
723	73	722
724	73	723
725	73	724
726	73	725
727	73	726
728	73	727
729	73	728
730	73	729
731	73	730
732	74	731
733	74	732
734	74	733
735	74	734
736	74	735
737	74	736
738	74	737
739	74	738
740	74	739
741	74	740
742	75	741
743	75	742
744	75	743
745	75	744
746	75	745
747	75	746
748	75	747
749	75	748
750	75	749
751	75	750
752	76	751
753	76	752
754	76	753
755	76	754
756	76	755
757	76	756
758	76	757
759	76	758
760	76	759
761	76	760
762	77	761
763	77	762
764	77	763
765	77	764
766	77	765
767	77	766
768	77	767
769	77	768
770	77	769
771	77	770
772	78	771
773	78	772
774	78	773
775	78	774
776	78	775
777	78	776
778	78	777
779	78	778
780	78	779
781	78	780
782	79	781
783	79	782
784	79	783
785	79	784
786	79	785
787	79	786
788	79	787
789	79	788
790	79	789
791	79	790
792	80	791
793	80	792
794	80	793
795	80	794
796	80	795
797	80	796
798	80	797
799	80	798
800	80	799
801	80	800
802	81	801
803	81	802
804	81	803
805	81	804
806	81	805
807	81	806
808	81	807
809	81	808
810	81	809
811	81	810
812	82	811
813	82	812
814	82	813
815	82	814
816	82	815
817	82	816
818	82	817
819	82	818
820	82	819
821	82	820
822	83	821
823	83	822
824	83	823
825	83	824
826	83	825
827	83	826
828	83	827
829	83	828
830	83	829
831	83	830
832	84	831
833	84	832
834	84	833
835	84	834
836	84	835
837	84	836
838	84	837
839	84	838
840	84	839
841	84	840
842	85	841
843	85	842
844	85	843
845	85	844
846	85	845
847	85	846
848	85	847
849	85	848
850	85	849
851	85	850
852	86	851
853	86	852
854	86	853
855	86	854
856	86	855
857	86	856
858	86	857
859	86	858
860	86	859
861	86	860
862	87	861
863	87	862
864	87	863
865	87	864
866	87	865
867	87	866
868	87	867
869	87	868
870	87	869
871	87	870
872	88	871
873	88	872
874	88	873
875	88	874
876	88	875
877	88	876
878	88	877
879	88	878
880	88	879
881	88	880
882	89	881
883	89	882
884	89	883
885	89	884
886	89	885
887	89	886
888	89	887
889	89	888
890	89	889
891	89	890
892	90	891
893	90	892
894	90	893
895	90	894
896	90	895
897	90	896
898	90	897
899	90	898
900	90	899
901	90	900
902	91	901
903	91	902
904	91	903
905	91	904
906	91	905
907	91	906
908	91	907
909	91	908
910	91	909
911	91	910
912	92	911
913	92	912
914	92	913
915	92	914
916	92	915
917	92	916
918	92	917
919	92	918
920	92	919
921	92	920
922	93	921
923	93	922
924	93	923
925	93	924
926	93	925
927	93	926
928	93	927
929	93	928
930	93	929
931	93	930
932	94	931
933	94	932
934	94	933
935	94	934
936	94	935
937	94	936
938	94	937
939	94	938
940	94	939
941	94	940
942	95	941
943	95	942
944	95	943
945	95	944
946	95	945
947	95	946
948	95	947
949	95	948
950	95	949
951	95	950
952	96	951
953	96	952
954	96	953
955	96	954
956	96	955
957	96	956
958	96	957
959	96	958
960	96	959
961	96	960
962	97	961
963	97	962
964	97	963
965	97	964
966	97	965
967	97	966
968	97	967
969	97	968
970	97	969
971	97	970
972	98	971
973	98	972
974	98	973
975	98	974
976	98	975
977	98	976
978	98	977
979	98	978
980	98	979
981	98	980
982	99	981
983	99	982
984	99	983
985	99	984
986	99	985
987	99	986
988	99	987
989	99	988
990	99	989
991	99	990
992	100	991
993	100	992
994	100	993
995	100	994
996	100	995
997	100	996
998	100	997
999	100	998
1000	100	999
1001	100	1000
1002	101	1001
1003	101	1002
1004	101	1003
1005	101	1004
1006	101	1005
1007	101	1006
1008	101	1007
1009	101	1008
1010	101	1009
1011	101	1010
\.


--
-- Data for Name: survey_sets; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.survey_sets (id, name) FROM stdin;
1	survey set 1
2	survey set 2
3	survey set 3
4	survey set 4
5	survey set 5
6	survey set 6
7	survey set 7
8	survey set 8
9	survey set 9
10	survey set 10
11	survey set 11
12	survey set 12
13	survey set 13
14	survey set 14
15	survey set 15
16	survey set 16
17	survey set 17
18	survey set 18
19	survey set 19
20	survey set 20
21	survey set 21
22	survey set 22
23	survey set 23
24	survey set 24
25	survey set 25
26	survey set 26
27	survey set 27
28	survey set 28
29	survey set 29
30	survey set 30
31	survey set 31
32	survey set 32
33	survey set 33
34	survey set 34
35	survey set 35
36	survey set 36
37	survey set 37
38	survey set 38
39	survey set 39
40	survey set 40
41	survey set 41
42	survey set 42
43	survey set 43
44	survey set 44
45	survey set 45
46	survey set 46
47	survey set 47
48	survey set 48
49	survey set 49
50	survey set 50
51	survey set 51
52	survey set 52
53	survey set 53
54	survey set 54
55	survey set 55
56	survey set 56
57	survey set 57
58	survey set 58
59	survey set 59
60	survey set 60
61	survey set 61
62	survey set 62
63	survey set 63
64	survey set 64
65	survey set 65
66	survey set 66
67	survey set 67
68	survey set 68
69	survey set 69
70	survey set 70
71	survey set 71
72	survey set 72
73	survey set 73
74	survey set 74
75	survey set 75
76	survey set 76
77	survey set 77
78	survey set 78
79	survey set 79
80	survey set 80
81	survey set 81
82	survey set 82
83	survey set 83
84	survey set 84
85	survey set 85
86	survey set 86
87	survey set 87
88	survey set 88
89	survey set 89
90	survey set 90
91	survey set 91
92	survey set 92
93	survey set 93
94	survey set 94
95	survey set 95
96	survey set 96
97	survey set 97
98	survey set 98
99	survey set 99
100	survey set 100
101	survey set 101
\.


--
-- Data for Name: updated_passages; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.updated_passages (id, passage_a, passage_b) FROM stdin;
501	A TAG-Based Noisy-Channel Model Of Speech Repairs This paper describes a noisy channel model of speech repairs, which can identify and correct repairs in speech transcripts.:::\n A [syntactic parser] is used as the [source model], and a novel type of [TAG-based transducer] is the [channel model].:::\n The use of TAG is motivated by the intuition that the [reparandum] is a "rough copy" of the repair.:::\n The model is trained and tested on the [Switchboard disfluency-annotated corpus].:::\n [Noisy channel models] do well on the [disfluency detection task].:::\n Although the standard noisy channel model performs well, a [log linear re-ranker] can be used to increase performance.:::\n Our TAG system achieves a high [EDIT-F score], largely as a result of its explicit tracking of overlapping words between [reparanda and alterations]. A syntactic parser is used as the source model, and a novel type of TAG-based transducer is the channel model. The use of TAG is motivated by the intuition that the reparandum is a "rough copy" of the repair. The model is trained and tested on the Switchboard disfluency-annotated corpus. Noisy channel models do well on the disfluency detection task. Although the standard noisy channel model performs well, a log linear re-ranker can be used to increase performance. Our TAG system achieves a high EDIT-F score, largely as a result of its explicit tracking of overlapping words between reparanda and alterations.	A TAG-Based Noisy-Channel Model Of Speech Repairs This paper talks about a system that can find and fix mistakes in spoken words written down. It uses a tool that checks sentence structure as the main model and a new kind of machine (TAG-based transducer) as the second model. This method uses the idea that the mistake is a "rough copy" of the correction. The system is tested with a special set of conversations that have mistakes marked in them. N/A Even though the basic model works well, using a special tool called a log linear re-ranker can make it even better. Our TAG system is very good at fixing errors because it keeps track of words that overlap between mistakes and corrections.
511	Discovering Relations Among Named Entities From [Large Corpora Discovering] the significant relations embedded in documents would be very useful not only for information retrieval but also for question answering and summarization Prior methods for relation discovery, however, needed large annotated corpora which cost a great deal of time and effort. We propose an unsupervised method for relation discovery from large corpora. The key idea is clustering pairs of named entities according to the similarity of context words intervening between the named entities. Our experiments using one year of newspapers reveals not only that the relations among named entities could be detected with high recall and precision, but also that appropriate labels could be automatically provided for the relations. We introduce a fully unsupervised Open IE systems, based on clustering of entity pair contexts to produce clusters of entity pairs that share the same relations. We use large corpora and an Extended Named Entity tagger to find novel relations and their participants.	Discovering Relations Among Named Entities From Large Corpora Discovering important connections hidden in documents would be very helpful not just for finding information but also for answering questions and creating summaries. Previous methods for finding these connections needed large collections of labeled documents, which took a lot of time and effort. We suggest a method that doesn't require labeled data (unsupervised method) to find connections in large collections of text. The main idea is to group pairs of named entities (like people, places, organizations) based on the similarity of the words found between them. Our tests using a year's worth of newspaper articles show that not only can we find connections between named entities accurately, but we can also automatically assign suitable labels to these connections. We introduce a fully unsupervised system called Open IE, which clusters pairs of entities that share the same connections. We use large text collections and a tool called an Extended Named Entity tagger to discover new types of connections and their participants.
512	Dependency Tree Kernels For Relation Extraction We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences. Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles. We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a [bag-of-words kernel]. To compare relations in two instance sentences, we propose to compare the subtrees induced by the relation arguments i.e. computing the node kernel between the two lowest common ancestors (lca) in the dependecy tree. We also use a dependency tree kernel to detect the Named Entity classes in natural language texts.	Dependency Tree Kernels For Relation Extraction We build on earlier work to measure how similar the sentence structures (dependency trees) are. Using this method with a machine learning tool called Support Vector Machine, we identify and categorize connections between pieces of information (entities) in a set of news stories (ACE corpus). We test different features like Wordnet hypernyms (a tool to find word meanings), parts of speech (like nouns, verbs), and types of entities, and discover that our method improves accuracy by 20% compared to just counting word occurrences. To compare connections in two sentences, we suggest looking at the smaller tree structures formed by the relation parts, which means calculating similarities between the shared points (lowest common ancestors) in the sentence structure. N/A
513	Collective Information Extraction With Relational Markov Networks Most information extraction (IE) systems treat separate potential extractions as independent. However, in many cases, considering influences between different potential extractions could improve overall accuracy. Statistical methods based on undirected graphical models, such as conditional random fields (CRFs), have been shown to be an effective approach to learning accurate IE systems. We present a new IE method that employs Relational Markov Networks (a generalization of CRFs), which can represent arbitrary dependencies between extractions. This allows for "collective information extraction" that exploits the mutual influence between possible extractions. Experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach. We present AImed, a corpus for the evaluation of PPI extraction systems.	Collective Information Extraction With Relational Markov Networks Most information extraction (IE) systems treat each piece of information they find as separate and not connected to others. However, in many cases, thinking about how different pieces of information affect each other could make the system more accurate. Statistical methods using undirected graphical models, like conditional random fields (CRFs), have been shown to work well for creating accurate IE systems. We introduce a new IE method that uses Relational Markov Networks (an advanced form of CRFs), which can show any kind of connection between pieces of information. This allows for "collective information extraction" that takes advantage of the way possible pieces of information can influence each other. Tests on learning to find protein names from scientific text show the benefits of this method. N/A
50	Question Answering Based On Semantic Structures The ability to answer complex questions posed in Natural Language depends on (1) the depth of the available semantic representations and (2) the inferential mechanisms they support. The ability to answer complex questions posed in [Natural Language] depends on (1) the depth of the available [semantic representations] and (2) the [inferential mechanisms] they support.\n\nIn this paper we describe a [QA architecture] where questions are analyzed and candidate answers generated by 1) identifying [predicate argument structures] and [semantic frames] from the input and 2) performing [structured probabilistic inference] using the extracted relations in the context of a [domain and scenario model].\n\nA novel aspect of our system is a scalable and expressive representation of actions and events based on [Coordinated Probabilistic Relational Models (CPRM)].\n\nIn this paper we report on the ability of the implemented system to perform several forms of [probabilistic and temporal inferences] to extract answers to complex questions.\n\nOur [question answering system] takes [PropBank/FrameNet annotations] as input, uses the [PropBank targets] to indicate which actions are being described with which arguments, and produces an answer using [probabilistic models] of actions as the tools of inference. A novel aspect of our system is a scalable and expressive representation of actions and events based on Coordinated Probabilistic Relational Models (CPRM). In this paper we report on the ability of the implemented system to perform several forms of probabilistic and temporal inferences to extract answers to complex questions. The results indicate enhanced accuracy over current state-of-the-art Q/A systems. We explore the role of semantic structures in question answering. We demonstrate that question answering can stand to benefit from broad coverage semantic processing. Our question answering system takes PropBank/FrameNet annotations as input, uses the PropBank targets to indicate which actions are being described with which arguments and produces an answer using probabilistic models of actions as the tools of inference.	The ability to answer hard questions written in everyday language depends on (1) how detailed the [semantic information] is and (2) the [reasoning methods] that this information supports.\n\nIn this paper, we describe a [question-answering (QA) system] where questions are examined and possible answers are generated by 1) identifying the [main parts of sentences] and the [meaning] behind them from the input, and 2) using these [extracted relationships] to make [informed guesses] within a [specific topic or situation].\n\nA new feature of our system is a flexible and clear way of showing actions and events using [Coordinated Probabilistic Relational Models (CPRM)], which is a method to predict outcomes based on relationships.\n\nOur [QA system] uses [PropBank/FrameNet labels] as input, which help identify what actions are being described and with what details, and then it gives an answer by using [models that predict actions based on chance and reasoning]. In this paper, we describe a question-answering (QA) system where questions are examined and possible answers are generated by 1) identifying the main parts of sentences and the meaning behind them from the input, and 2) using these extracted relationships to make informed guesses within a specific topic or situation. A new feature of our system is a flexible and clear way of showing actions and events using Coordinated Probabilistic Relational Models (CPRM), which is a method to predict outcomes based on relationships. In this paper, we share how well the system can make different types of informed guesses based on probability and time to find answers to difficult questions. The results show better accuracy compared to the best current QA systems. We look into how understanding the meaning behind words helps in answering questions. We show that answering questions can benefit from wide-ranging understanding of meanings. Our QA system uses PropBank/FrameNet labels as input, which help identify what actions are being described and with what details, and then it gives an answer by using models that predict actions based on chance and reasoning.
521	A High-Performance Semi-Supervised Learning Method For Text Chunking In machine learning, whether one can build a more accurate classifier by using unlabeled data (semi-supervised learning) is an important issue. Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear. This paper presents a novel semi-supervised method that employs a learning paradigm which we call structural learning. The idea is to find “what good classifiers are like” by learning from thousands of automatically generated auxiliary classification problems on unlabeled data. By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve performance on the target problem. The method produces performance higher than the previous best results on CoNLL’00 syntactic chunking and CoNLL’03 named entity chunking (English and German). We utilize a multi task learner within our semi-supervised algorithm to learn feature representations which are useful across a large number of related tasks. Our structural learning method uses alternating structural optimization (ASO). For both computational and statistical reasons, we follow compute a low-dimensional linear approximation to the pivot predictor space.	A High-Performance Semi-Supervised Learning Method For Text Chunking In machine learning, a key question is whether we can make a more accurate system for classifying data by using data that isn't labeled (this is called semi-supervised learning). Although many semi-supervised methods have been suggested, how well they work for natural language processing (NLP) tasks isn't always certain. This paper introduces a new semi-supervised method using a learning approach we call structural learning. The idea is to understand "what makes a good classifier" by studying thousands of automatically made extra classification problems using unlabeled data. By doing this, the common patterns that help predict outcomes in these problems can be identified and used to perform better on the main problem we want to solve. This method achieves better results than the previous best methods on CoNLL’00 syntactic chunking and CoNLL’03 named entity chunking (in English and German). We use a system that learns from multiple tasks within our semi-supervised algorithm to find features that are helpful for many related tasks. N/A For reasons related to both computing speed and statistical analysis, we calculate a simplified version of the space where important predictors are found.
522	Probabilistic CFG With Latent Annotations This paper defines a generative probabilistic model of parse trees, which we call PCFG-LA. This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables. Fine-grained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm. Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared. In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of [86.6% (F1, sentences <= 40 words)], which is comparable to that of an unlexicalized [PCFG parse]r created using extensive manual feature selection. We use a markovized grammar to get a better unannotated parse forest during decoding, but we do not markovize the training data. We right-binarize the tree bank data to construct grammars with only unary and binary productions.	Probabilistic CFG With Latent Annotations This paper describes a method for creating parse trees, which are structures used in understanding sentences, called PCFG-LA. N/A Detailed rules for constructing these trees are automatically learned from a collection of example sentences by training the PCFG-LA model using a technique called the EM-algorithm. Since finding an exact solution with PCFG-LA is very complex and difficult (NP-hard), the paper discusses several simplified methods and compares them through experiments. In tests with a well-known set of sentences (Penn WSJ corpus), our model, which was trained automatically, performed quite well with an accuracy of 86.6% for sentences up to 40 words long, similar to another model that required a lot of manual adjustment. We use a special method (markovized grammar) to get better results when interpreting sentence structures but do not apply this method to the training examples. We also change the structure of example sentence data to make it simpler, using only two types of constructions (unary and binary).
531	A Hierarchical Phrase-Based Model For Statistical Machine Translation We present a statistical phrase-based translation model that uses hierarchical phrases - phrases that contain subphrases. The model is formally a [synchronous] context-free grammar but is learned from a bitext without any syntactic information.:::\n We note that whenever we combine two dynamic programming items, we need to score the [fluency of their concatentation] by incorporating the score of any language model features which cross the [target side boundaries of the two concatenated items].:::\n We use the [k-best parsing algorithm in a CFG-based log-linear translation model] in order to learn feature weights which maximize BLEU. Thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system. We use the k-best parsing algorithm in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU. We note that whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items. To better leverage syntactic constraint yet still allow non-syntactic translations, we introduce a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. Our hierarchical phrase models for machine translation is an evolution from the traditional word (Brown et al, 1993) and phrase based (Koehn et al, 2003a) models.	A Hierarchical Phrase-Based Model For Statistical Machine Translation We present a statistical phrase-based translation model that uses hierarchical phrases - phrases that contain smaller phrases within them. The model is technically a [synchronous] context-free grammar, but it is learned from a pair of aligned texts (bitext) without using any grammar rules.:::\n We use the [k-best parsing method] in a grammar-based model to find the best feature weights that improve BLEU scores. This means it shifts to using formal grammar rules like syntax-based translation systems but without relying on language rules. In our tests, using a scoring method called BLEU, the hierarchical phrase-based model shows a 7.5% improvement compared to Pharaoh, which is a top-notch phrase-based system. We use the k-best parsing method in a grammar-based model to find the best feature weights that improve BLEU scores. We note that when combining two parts of a translation, we need to check how smooth they sound together by including the score of any language model features that affect the overall flow of the sentence. To better use grammar rules but still allow for translations that don't follow those rules, we keep a count for each proposed translation and add it whenever it fits perfectly with the grammar rules of the original text. Our hierarchical phrase models for machine translation build on from the traditional word-based models (Brown et al, 1993) and phrase-based models (Koehn et al, 2003a).
532	Dependency Treelet Translation: Syntactically Informed Phrasal SMT We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation. This method requires a source language dependency parser, target language word segmentation and an unsupervised word alignment component. We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model. This method requires a source language dependency [parser], target language word segmentation and an unsupervised word alignment component.:::\n We align a [parallel corpus], project the [source dependency parse] onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model.:::\n We describe an efficient decoder and show that using these tree-based models in combination with [conventional SMT models] provides a promising approach that incorporates the power of phrasal SMT with the linguistic [generality] available in a [parser]. Our treelet-based SMT system is trained on about 4.6M parallel sentence pairs from diverse sources including bilingual books, dictionaries and web publications. We extend paths to treelets, arbitrary connected subgraphs of dependency structures, and propose a model based on treelet pairs. We demonstrate the success of using fragments of a target language's grammar, treelets, to improve performance in phrasal translation.	Dependency Treelet Translation: Syntactically Informed Phrasal SMT We explain a new way to use computers to translate languages by mixing grammar details from the original language with modern methods of translating phrases. This technique needs a tool that breaks down the sentence structure of the original language, splits words in the target language, and finds word connections without human help. We match sentences that say the same thing in two languages, apply the original language's grammar structure to the target language sentence, pull out small parts of sentences that match, and teach a model to arrange these parts correctly. We talk about a fast program that uses these small sentence parts together with usual translation models to create a strong method that mixes phrase translation with the broad understanding of language from grammar tools. Our system is trained using about 4.6 million pairs of sentences from different places like books in two languages, dictionaries, and online articles. We expand from paths to treelets, which are any connected parts of sentence structures, and suggest a model based on these pairs. N/A
541	Reading Level Assessment Using Support Vector Machines And Statistical Language Models Reading proficiency is a fundamental component of language competency. However, finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers. This task can be addressed with natural language processing technology to assess reading level. Existing measures of reading level are not well suited to this task, but previous work and our own pilot experiments have shown the benefit of using statistical language models. In this paper, we also use support vector machines to combine features from traditional reading level measures, statistical language models, and other language processing tools to produce a better method of assessing reading level. We develop a SVM categoriser combining a classifier based on trigram language models (one for each level of difficulty), some parsing features such as average tree height, and variables traditionally used in readability.:::\n Reading Level Assessment Using Support Vector Machines And Statistical Language Models Reading proficiency is a fundamental component of language competency. We use syntactic features, such as parse tree height or the number of passive sentences, to predict reading grade levels.	Reading Level Assessment Using Support Vector Machines And Statistical Language Models Reading skill is an essential part of being good at a language. n/a This problem can be solved using technology that helps understand language to check the reading level. Current ways to measure reading levels are not very effective for this job, but past research and our own small tests show that using statistical language models (ways to predict text patterns) can help. In this paper, we also use support vector machines (a type of computer algorithm) to combine different features from traditional reading level methods, statistical language models, and other language tools to create a better way to evaluate reading levels. We create a system using support vector machines that combines a text classifier based on trigram language models (which looks at three-word combinations for each difficulty level), some features like average tree height from parsing (breaking down sentences into parts), and variables usually used to determine readability. We use sentence structure features, such as the height of parse trees (a visual breakdown of sentence structure) or the number of passive sentences, to estimate reading grade levels.
558	Correcting ESL Errors Using Phrasal SMT Techniques This paper presents a pilot study of the use of phrasal Statistical Machine Translation (SMT) techniques to identify and correct writing errors made by learners of English as a Second Language (ESL). Using examples of mass noun errors found in the Chinese Learner Error Corpus (CLEC) to guide creation of an engineered training set, we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers. Our system was able to correct 61.81% of mistakes in a set of naturally-occurring examples of mass noun errors found on the World Wide Web, suggesting that efforts to collect [alignable corpora] of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the [complex syntactic and lexical problems] found in the writing of ESL learners.:::\n We utilize phrasal Statistical Machine Translation (SMT) techniques to correct ESL writing errors and demonstrate that this [data-intensive SMT approach] is very promising, but we also point out SMT approach relies on the availability of large amount of training data. We utilize phrasal Statistical Machine Translation (SMT) techniques to correct ESL writing errors and demonstrate that this data-intensive SMT approach is very promising, but we also point out SMT approach relies on the availability of large amount of training data.	Correcting ESL Errors Using Phrasal SMT Techniques This paper presents an initial study on using phrasal Statistical Machine Translation (SMT) methods to find and fix writing mistakes made by people learning English as a Second Language (ESL). By using examples of errors with mass nouns (words like "information" that don't usually have a plural form) found in the Chinese Learner Error Corpus (CLEC), we created a special training set. We show that using the SMT approach can find mistakes that common [proofreading tools] for native speakers often miss. Our system was able to fix 61.81% of errors in real-life examples of mass noun mistakes found online, indicating that collecting examples of before and after edits of ESL writings can help develop SMT-based writing tools. These tools can fix many complicated grammar and word choice problems in ESL writing. We use phrasal SMT methods to correct ESL writing errors and show that this approach, which needs a lot of data, is very promising. However, we also note that the SMT approach depends on having a large amount of training data.
542	Clause Restructuring For Statistical Machine Translation We describe a method for incorporating syntactic information in statistical machine translation systems. The first step of the method is to parse the source language string that is being translated. Clause Restructuring For Statistical Machine Translation We describe a method for incorporating syntactic information in statistical machine translation systems.:::\n The first step of the method is to parse the source language string that is being translated.:::\n The second step is to apply a series of transformations to the parse tree, effectively reordering the surface string on the source language side of the translation system.:::\n The reordering approach is applied as a pre-processing step in both the training and decoding phases of a phrase-based statistical MT system. The goal of this step is to recover an underlying word order that is closer to the target language word-order than the original string. The reordering approach is applied as a pre-processing step in both the training and decoding phases of a phrase-based statistical MT system. We describe experiments on translation from German to English, showing an improvement from 25.2% Bleu score for a baseline system to 26.8% Bleu score for the system with reordering, a statistically significant improvement. We present sign test to measure the siginificance of score improvement in BLUE. We note that it is not clear whether the conditions required by bootstrap resampling are met in the case of BLUE, and recommend the sign test instead. We use six hand-crafted reordering rules targeting the placement of verbs, subjects, particles and negation.	Clause Restructuring For Statistical Machine Translation We explain a way to include grammar information in systems that automatically translate languages. The first step is to break down the sentence from the language being translated. The second step is to make changes to the sentence structure, rearranging the words to match the word order of the target language better than the original. This rearranging is done before both learning and translating phases in a system that translates language phrases. We did tests translating German to English, showing an improvement in the translation quality from a 25.2% Bleu score (a quality measure) for a basic system to 26.8% with rearranging, a noticeable improvement. We use a sign test to check if the score improvement is meaningful. We point out it's unsure if the conditions needed for another method, called bootstrap resampling, apply to Bleu scores, so we suggest using the sign test. We use six specific rules to rearrange verbs, subjects, small words, and negative words.
548	Randomized Algorithms And NLP: Using Locality Sensitive Hash Functions For High Speed Noun Clustering In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data. We apply these algorithms to generate noun similarity lists from 70 million pages. We reduce the running time from quadratic to practically linear in the number of elements to be computed. We show that by using the LSH nearest neighbors calculation can be done in O(nd) time. Our method can produce over 70% accuracy in extracting synonyms.	Randomized Algorithms And NLP: Using Locality Sensitive Hash Functions For High Speed Noun Clustering In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data. We apply these algorithms to generate noun similarity lists from 70 million pages. We reduce the running time from quadratic (which grows quickly as data increases) to practically linear (grows slowly with more data) in the number of elements to be computed. We reduce the running time from quadratic (which grows quickly as data increases) to practically linear (grows slowly with more data) in the number of elements to be computed.:::\n We show that by using the LSH (Locality Sensitive Hashing, a method to find similar items) nearest neighbors calculation can be done in O(nd) time (a way to describe how fast an algorithm runs). Our method can produce over 70% accuracy in extracting synonyms (words with similar meanings).
543	Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data. In this paper, we present a syntax-based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar. Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees. We first introduce our approach to inducing such a grammar from parallel corpora. Second, we describe the graphical model for the machine translation task, which can also be viewed as a stochastic tree-to-tree transducer. We introduce a polynomial time decoding algorithm for the model. We evaluate the outputs of our MT system using the NIST and Bleu automatic MT evaluation software. The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality. Our approach requires some assumptions on the level of isomorphism (lexical and/or structural) between two languages. In this paper, we present a syntax-based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar.:::\n Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees.:::\n We first introduce our approach to inducing such a grammar from parallel corpora.:::\n Second, we describe the graphical model for the machine translation task, which can also be viewed as a stochastic tree-to-tree transducer.:::\n We introduce a polynomial time decoding algorithm for the model.:::\n We present a translation model based on Synchronous Dependency Insertion Grammar (SDIG), which handles some of the non-isomorphism but requires both source and target dependency structures.	Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars Syntax-based statistical machine translation (MT) uses statistical models to understand and translate structured language data. In this paper, we introduce a system that uses these statistical methods with a specific type of grammar called probabilistic synchronous dependency insertion grammar. This grammar is a version that works with dependency trees, which show how words in a sentence relate to each other. We explain how we create this grammar using collections of translated texts known as parallel corpora. Next, we describe our method for translation using a visual model that acts like a tree-to-tree converter, changing one language structure into another using probability.:::\n In this paper, we introduce a system that uses these statistical methods with a specific type of grammar called probabilistic synchronous dependency insertion grammar. We also developed a fast method to decode, or interpret, this model. We tested our translation system with software that automatically checks translation quality, called NIST and Bleu. The results showed our system translates faster and better than a basic system using older methods from IBM. Our method needs some assumptions about how similar the two languages are, either in vocabulary or structure. We propose a translation model based on Synchronous Dependency Insertion Grammar (SDIG), which can handle some differences between languages but still needs the structure of both the original and translated sentences.
544	Arabic Tokenization Part-Of-Speech Tagging And Morphological Disambiguation In One Fell Swoop We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including part-of-speech tagging) Arabic words in one process.:::\n We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer. We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer. We obtain accuracy rates on all tasks in the high nineties. For choosing the best Buckwalter morphological analyzer (BAMA) results, we simply count the number of predicted values for the set of linguistic features in each candidate analysis.	Arabic Tokenization Part-Of-Speech Tagging And Morphological Disambiguation In One Fell Swoop We present a method that uses a tool to break down and label Arabic words with their grammatical roles, all in one step. We train computer programs to recognize specific word features and decide which analysis to choose from the tool's results. n/a To select the best results from the Buckwalter morphological analyzer (BAMA), we count how many times each predicted feature appears in the possible analyses.
545	Semantic Role Labeling Using Different Syntactic Views Semantic role labeling is the process of annotating the predicate-argument structure in text with semantic labels. In this paper we present a state-of-the-art baseline semantic role labeling system based on Support Vector Machine classifiers. We show improvements on this system by: i) adding new features including features extracted from dependency parses, ii) performing feature selection and calibration and iii) combining parses obtained from semantic parsers trained using different syntactic views. Error analysis of the baseline system showed that approximately half of the argument identification errors resulted from parse errors in which there was no syntactic constituent that aligned with the correct argument. In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements. We combine systems that are based on phrase-structure parsing, dependency parsing, and shallow parsing. We use the Constituent, Predicate, and Predicate-Constituent related features for the kernel, resulting in the best performance. Semantic Role Labeling Using Different Syntactic Views Semantic role labeling is the process of annotating the predicate-argument structure in text with semantic labels.:::\n In this paper we present a state-of-the-art baseline semantic role labeling system based on Support Vector Machine classifiers.:::\n In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses.:::\n We use the Constituent, Predicate, and Predicate-Constituent related features for the kernel, resulting in the best performance.:::\n We combine the outputs of multiple parsers to extract reliable syntactic information, which is translated into features for a machine learning experiment in assigning semantic roles.	Semantic Role Labeling Using Different Syntactic Views Semantic role labeling is the task of marking parts of a sentence with labels that show their meaning and role. In this paper, we introduce a top-level system for semantic role labeling using Support Vector Machine classifiers. We enhance this system by: i) adding new details, including information from dependency parses (which show how words in a sentence are related), ii) choosing and adjusting the details carefully, and iii) merging results from different methods of analyzing sentence structure. An error check of the basic system showed that about half of the mistakes in identifying roles came from errors in sentence structure analysis. To fix this, we combined different sentence structure analyses from Minipar and a simplified method with our original system based on Charniak's analysis. All these methods led to better results. We mix systems based on different ways to analyze sentence structure, like phrase-structure, dependency, and shallow parsing. We use features related to parts of the sentence and their roles to improve performance. We combine results from multiple analyzers to gather accurate sentence structure information, which is used in a machine learning test to assign meaning roles.
546	Joint Learning Improves Semantic Role Labeling Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding.:::\n This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between arguments.:::\n We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative log-linear models.:::\n This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a state-of-the art independent classifier for gold-standard parse trees on PropBank.:::\n We introduce a joint approach for SRL and demonstrate that a model that scores the full predicate argument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation.:::\n We employ decomposition for efficiency in training: that is, the decomposition allows us to train the classification models on a subset of training examples consisting only of those phrases that have a case marker. This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between arguments. We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative log-linear models. This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a state-of-the art independent classifier for gold-standard parse trees on PropBank. We introduce a joint approach for SRL and demonstrate that a model that scores the full predicate argument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. We employ decomposition for efficiency in training: that is, the decomposition allows us to train the classification models on a subset of training examples consisting only of those phrases that have a case marker.	Joint Learning Improves Semantic Role Labeling Despite much recent progress in accurately identifying and labeling the roles words play in sentences (semantic role labeling), previous methods mostly used separate tools that worked independently, sometimes combining these with other models that sequence labels using a method called Viterbi decoding. This approach is very different from the linguistic idea that the main parts of a sentence (core argument frame) are connected and depend on each other. We demonstrate how to create a combined model that takes into account these connections, using new features that capture these interactions in advanced log-linear models (a type of mathematical model). This system reduces errors by 22% for all arguments and 32% for core arguments compared to a top independent classifier when using perfect sentence structures from PropBank (a resource for linguistic analysis). We present a combined approach for semantic role labeling (SRL) and show that a model assessing the complete structure of a sentence can significantly reduce errors compared to using separate classifiers for each part of the sentence. To make training efficient, we use a method called decomposition, which means we train the models using only a part of the training examples, specifically those that have a case marker (a word indicating the grammatical function of a noun).
547	Paraphrasing With Bilingual Parallel Corpora Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrase-based statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account.:::\n We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments.:::\n We define a paraphrasing probability between two phrases based on their translation probability through all possible pivot phrases.:::\n Paraphrasing With Bilingual Parallel Corpora Previous work has used monolingual parallel corpora to extract and generate paraphrases. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments. We define a paraphrasing probability between two phrases based on their translation probability through all possible pivot phrases.	Paraphrasing With Bilingual Parallel Corpora Previous work has used collections of similar texts in the same language to find and create paraphrases, which are different ways of saying the same thing.:::\n We define a way to measure how likely something is a paraphrase, which helps organize paraphrases found in two-language text collections using how likely they are to translate, and show how to improve this by considering surrounding words.:::\n By using methods from computer programs that translate languages, we show how you can find paraphrases in one language by using a phrase in another language as a middle step. We show that this can be done using collections of texts in two different languages, a more common resource. By using methods from computer programs that translate languages, we show how you can find paraphrases in one language by using a phrase in another language as a middle step. We define a way to measure how likely something is a paraphrase, which helps organize paraphrases found in two-language text collections using how likely they are to translate, and show how to improve this by considering surrounding words. We test our methods for finding and organizing paraphrases using a set of carefully matched words, and compare the quality with paraphrases found using automatic matching. We define how likely two phrases are paraphrases based on their chance of being translated through all possible middle phrases.
549	Using Emoticons To Reduce Dependency In Machine Learning Techniques For Sentiment Classification Sentiment Classification seeks to identify a piece of text according to its author's general feeling toward their subject, be it positive or negative. Traditional machine learning techniques have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the training and test data with respect to topic. This paper demonstrates that match with respect to domain and time is also important, and presents preliminary experiments with training data labeled with emoticons, which has the potential of being independent of domain, topic and time.:::\n Traditional machine learning techniques have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the training and test data with respect to topic. In sentiment analysis research, we use emoticons in newsgroup articles to extract instances relevant for training polarity classifiers. We find that when authors of an electronic communication use an emotion, they are effectively marking up their own text with an emotional state.	Using Emoticons To Reduce Dependency In Machine Learning Techniques For Sentiment Classification Sentiment Classification tries to figure out if a text shows a positive or negative feeling from the writer towards a topic. Traditional machine learning methods have been used for this and have worked fairly well, but they only work well if the training data (examples used to teach the computer) and test data (new examples to check the computer) are similar in topic. n/a It shares early experiments using training data with emoticons (like smiley faces or frowns), which might not depend on subject, topic, or time. In sentiment analysis research, we use emoticons in online group posts to find examples useful for training emotion detectors. We notice that when people use emoticons in messages, they are actually adding emotional signals to their text.
550	Multi-Engine Machine Translation Guided By Explicit Word Matching We describe a new approach for synthetically combining the output of several different Machine Translation (MT) engines operating on the same input. The goal is to produce a synthetic combination that surpasses all of the original systems in translation quality. Our approach uses the individual MT engines as "black boxes" and does not require any explicit cooperation from the original MT systems. Multi-Engine Machine Translation Guided By Explicit Word Matching We describe a new approach for synthetically combining the output of several different Machine Translation (MT) engines operating on the same input.:::\n The goal is to produce a synthetic combination that surpasses all of the original systems in translation quality.:::\n Our approach uses the individual MT engines as "black boxes" and does not require any explicit cooperation from the original MT systems.:::\n A decoding algorithm uses explicit word matches, in conjunction with confidence estimates for the various engines and a trigram language model in order to score and rank a collection of sentence hypotheses that are synthetic combinations of words from the various original engines. The highest scoring sentence hypothesis is selected as the final output of our system. Experiments, using several Arabic-to-English systems of similar quality, show a substantial improvement in the quality of the translation output. We propose a heuristic-based matching algorithm which allows non monotonic alignments to align the words between the hypotheses.	Multi-Engine Machine Translation Guided By Explicit Word Matching We describe a new way to combine the outputs of different Machine Translation (MT) systems working on the same text. The aim is to create a combined translation that is better than all the original translations. Our method treats each MT system as a separate unit and doesn't need them to work together. A decoding process uses specific word matches, along with confidence levels for each system and a three-word language model, to evaluate and rank different sentence options that mix words from the original systems. n/a Tests with several Arabic-to-English systems of similar quality show a big improvement in translation quality. We suggest a method based on rules that allows flexible word matching between the different sentence options.
552	Bootstrapping Path-Based Pronoun Resolution We present an approach to pronoun resolution based on syntactic paths. Through a simple bootstrapping procedure, we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entities. This path information enables us to handle previously challenging resolution instances, and also robustly addresses traditional syntactic coreference constraints. N/A We combine statistical knowledge with well known features in a Support Vector Machine pronoun resolution classifier. Significant gains in performance are observed on several datasets. Highly [coreferent paths] also allow mining of precise probabilistic gender/number information.:::\n Given an automatically parsed corpus, we extract from each parse tree a dependency path, which is represented as a sequence of nodes and dependency labels connecting a pronoun and a candidate [antecedent], and collect statistical information from these paths to determine the likelihood that a pronoun and a candidate antecedent connected by a given path are coreferent.:::\n We build a statistical model from paths that include [the lemma of the intermediate tokens], but replace the end nodes with noun, pronoun, or pronoun-self for nouns, pronouns, and reflexive pronouns, respectively. We show that learned gender is the most important feature in their pronoun resolution systems. We achieve achieve state-of-the-art noun gender classification performance, and we make the database of the obtained noun genders available online. We build a statistical model from paths that include the lemma of the intermediate tokens, but replace the end nodes with noun, pronoun, or pronoun-self for nouns, pronouns, and reflexive pronouns, respectively.	Bootstrapping Path-Based Pronoun Resolution We present a method for figuring out what pronouns refer to using sentence structure paths. Through a simple learning process, we determine how likely it is that a pronoun refers to a specific noun based on the path between them in a sentence structure diagram. This path information helps us solve difficult pronoun references and also effectively deals with traditional sentence structure rules for pronouns. Highly connected paths also allow us to gather detailed information about gender and number. We mix statistical knowledge with well-known features in a machine learning tool for resolving pronouns. We see major improvements in performance across several data sets. Using an automatically processed text collection, we take from each sentence structure a path showing relationships, represented as a series of points and labels linking a pronoun and a possible noun it refers to, and gather statistical data from these paths to determine how likely they are linked. We show that learning about gender is the most important part of their pronoun resolution systems. We reach top-level performance in identifying noun gender, and we share the database of the noun genders we found online. We create a statistical model from paths that include the base form of middle words, but replace the end points with noun, pronoun, or a special pronoun type for nouns, pronouns, and reflexive pronouns, respectively.
553	Discriminative Word Alignment With Conditional Random Fields In this paper we present a novel approach for inducing word alignments from sentence aligned data. We use a Conditional Random Field (CRF), a discriminative model, which is estimated on a small supervised training set. The CRF is conditioned on both the source and target texts, and thus allows for the use of arbitrary and overlapping features over these data. Moreover, the CRF has efficient training and decoding processes which both find globally [optimal] solutions.:::\n We show how a large number of highly predictive features can be easily incorporated into the CRF, and [demonstrate] that even with only a few hundred word-aligned training sentences, our model improves over the current [state-of-the-art] with [alignment] error rates of 5.29 and 25.8 for the two tasks respectively. We apply this alignment model to both French-English and Romanian-English language pairs. We show how a large number of highly predictive features can be easily incorporated into the CRF, and demonstrate that even with only a few hundred word-aligned training sentences, our model improves over the current state-of-the-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively.	Discriminative Word Alignment With Conditional Random Fields In this paper, we introduce a new way to match words from sentences in two different languages. We use a Conditional Random Field (CRF), which is a special method trained with a small set of examples. The CRF looks at both the original and translated texts, allowing it to use different and overlapping characteristics from these texts. N/A We use this model to match words in both French-English and Romanian-English language pairs. We show that many useful characteristics can easily be added to the CRF, and that even with just a few hundred training sentences, our model is better than the current best methods, with error rates of 5.29 and 25.8 for the two tasks, respectively.
554	Named Entity Transliteration With Comparable Corpora In this paper we investigate Chinese-English name transliteration using comparable corpora, corpora where texts in the two languages deal in some of the same topics - and therefore share references to named entities - but are not translations of each other. We present two [distinct] methods for transliteration, one approach using [phonetic transliteration], and the second using the temporal distribution of candidate pairs.:::\n We then propose [a novel score propagation method] that utilizes the co-occurrence of transliteration pairs within document pairs.:::\n This [propagation] method achieves further improvement over the best results from the previous step. Each of these approaches works quite well, but by combining the approaches one can achieve even better results. We then propose a novel score propagation method that utilizes the co-occurrence of transliteration pairs within document pairs. This propagation method achieves further improvement over the best results from the previous step. We compare names from comparable and contemporaneous English and Chinese texts, scoring matches by training a learning algorithm to compare the phonemic representations of the names in the pair, in addition to taking into account the frequency distribution of the pair over time.	Named Entity Transliteration With Comparable Corpora In this paper, we explore how to change Chinese names into English using collections of texts in both languages that are on similar topics but are not direct translations of each other. We introduce two different methods for changing names: one uses sound-based [transliteration] and the other looks at how often the name pairs appear together over time. Both methods work well, but combining them gives even better results. We then suggest a new way to improve these results further by using a method that spreads scores based on how often name pairs appear together in related documents. This new method improves the results even more. We compare names from similar and current English and Chinese texts by using a learning program that checks how the sounds of the names match and also considers how often the names appear over time.
555	Extracting Parallel Sub-Sentential Fragments From Non-Parallel Corpora We present a novel method for extracting parallel sub-sentential fragments from comparable, non-parallel bilingual corpora. By analyzing potentially similar sentence pairs using a signal processing-inspired approach, we detect which segments of the source sentence are translated into segments in the target sentence, and which are not. This method enables us to extract useful machine translation training data even from very non-parallel corpora, which contain no parallel sentence pairs. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. This method enables us to extract useful machine translation training data even from very [non-parallel corpora], which contain no parallel sentence pairs.:::\n We first use the GI ZA++ [(with grow-diag-final-and heuristic)] to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words.:::\n We first extract the candidate parallel sentences from the comparable corpora and further extract the accurate [sub-sentential] bilingual fragments from the candidate parallel sentences using the in-domain probabilistic [bilingual lexicon.]:::\n We perform phrase extraction by combining clean [alignment lexica] for initial signals with heuristics to smooth alignments for final fragment extraction. We first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. We use standard information retrieval together with simple word-based translation for cross-lingual information retrieval (CLIR), and extract phrases from the retrieval results using a clean bilingual lexicon and an averaging filter. We perform phrase extraction by combining clean alignment lexica for initial signals with heuristics to smooth alignments for final fragment extraction.	Extracting Parallel Sub-Sentential Fragments From Non-Parallel Corpora We introduce a new way to find matching parts of sentences in texts that are similar but not exactly the same in two different languages. By looking at sentence pairs that might be similar using a method inspired by signal processing, we figure out which parts of the original sentence match parts in the translated sentence and which do not. This allows us to gather useful data for training translation machines, even from texts that don't have any matching sentences. N/A We start by using a tool called GIZA++ (with a specific method called grow-diag-final-and) to match words between the original and translated texts, then measure how strongly these words are connected. We first find possible matching sentences from the similar texts and then accurately pick out matching parts of sentences using a special bilingual dictionary. We use standard methods to search for information along with simple word-based translation to find information across languages (CLIR), and we identify phrases from the search results using a clear bilingual dictionary and a method to smooth out the data. We identify phrases by combining clean word matching dictionaries for initial signals with strategies to refine the matches for final extraction of sentence parts.
551	Minimum Cut Model For Spoken Lecture Segmentation We consider the task of unsupervised lecture segmentation. We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion. Our approach moves beyond localized comparisons and takes into account long-range cohesion dependencies. Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of [speech recognition errors]. We optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences. Our problem is to find topical boundaries in transcripts of course lectures. We create a corpus of course lectures segmented by four annotators, noting that the annotators operated at different levels of granularity.	Minimum Cut Model For Spoken Lecture Segmentation We are working on dividing lectures into sections without any prior training. We treat this task like solving a puzzle by cutting a "graph" into parts in the best way possible. Our method looks at the big picture instead of just small parts and considers how well everything sticks together over long stretches. Our findings show that this broader view helps us cut the lecture more accurately, even when there are mistakes in the speech-to-text process. Our method looks at the big picture instead of just small parts and considers how well everything sticks together over long stretches.:::\n We focus on finding the best way to cut based on how similar the sentences are to each other, using a method similar to cosine similarity (a way to measure how alike two things are).:::\n Our findings show that this broader view helps us cut the lecture more accurately, even when there are mistakes in the speech-to-text process. Our goal is to identify where new topics start in the written version of the lectures. We collected a set of lectures that were divided into sections by four different people, each doing it in their own detailed way.
41	Efficient Parsing Of Highly Ambiguous Context-Free Grammars With Bit Vectors An efficient bit-vector-based CKY-style parser for context-free parsing is presented. The parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences. The parser uses bit-vector operations to parallelise the basic parsing operations. The parser is particularly useful when all analyses are needed rather than just the most probable one. We apply the Viterbi algorithm, exploiting its ability to deal with highly-ambiguous grammars.	Efficient Parsing Of Highly Ambiguous Context-Free Grammars With Bit Vectors An efficient bit-vector-based CKY-style parser for context-free parsing is presented. Recent advancements in [machine learning] have led to significant improvements in [natural language processing (NLP)]. In this study, researchers introduce a novel [deep learning architecture] that integrates [convolutional neural networks] with [recurrent neural networks] to enhance text understanding. Experimental results on [benchmark datasets] show that the proposed model achieves a 15 % improvement in accuracy over traditional methods. Additionally, the model demonstrates robust performance in handling [noisy data] and [long-range dependencies] within text. In this study, researchers present a new [deep learning model] that combines [convolutional and recurrent neural networks] to better understand text.\n\nThe authors highlight potential uses in [sentiment analysis], [machine translation], and [information extraction], and they plan to continue improving the model’s efficiency and scalability. The parser uses bit-vector operations, which are a type of computer operation, to do multiple parsing tasks at the same time. This parser is especially useful when every possible analysis is needed, not just the most likely one. We use the Viterbi algorithm, which is good at handling situations with many possible grammar interpretations.
42	The Importance Of Supertagging For Wide-Coverage CCG Parsing This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis. The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training. It also dramatically increases the speed of the parser. We show that large increases in speed can be obtained by tightly integrating the supertagger with the CCG grammar and parser. This paper describes the role of [supertagging] in a [wide-coverage CCG parser] which uses a [log-linear model] to select an analysis.\n\nWe show that large increases in speed can be obtained by tightly integrating the [supertagger] with the [CCG grammar] and parser.\n\nThis is the first work we are aware of to successfully integrate a [supertagger] with a [full parser] which uses an [automatically extracted grammar].\n\nThe result is an accurate [wide-coverage CCG parser] which is an [order of magnitude] faster than comparable systems for other [linguistically motivated formalisms].\n\nThe [CCG parsing] consists of two phases: first the [supertagger] assigns the most probable [categories] to each word, and then the small number of [combinatory rules], plus the [type-changing] and [punctuation rules], are used with the [CKY algorithm] to build a [packed chart].\n\nWe propose a method for integrating the [supertagger] with the parser: initially a small number of [categories] is assigned to each word, and more categories are requested if the parser can not find a [spanning analysis]. We also further reduce the derivation space using constraints on category combination. The result is an accurate wide-coverage CCG parser which is an order of magnitude faster than comparable systems for other linguistically motivated formalisms. Our scores give an indication of how supertagging accuracy corresponds to overall dependency recovery. We describe two log-linear parsing models for CCG: a normal-form derivation model and a dependency model. The CCG parsing consists of two phases: first the supertagger assigns the most probable categories toeach word, and then the small number of combinatory rules, plus the type-changing and punctuation rules, are used with the CKY algorithm to build a packed chart. We propose a method for integrating the supertagger with the parser: initially a small number of categories is assigned to each word, and more categories are requested if the parser can not find a spanning analysis.	The Importance Of Supertagging For Wide-Coverage CCG Parsing This paper explains the role of supertagging, which is like a label assigning process, in a CCG parser that uses a mathematical method to choose the best analysis. The supertagger makes the process more efficient by reducing the number of possible options the model needs to evaluate, which also makes training faster. It also significantly speeds up the parser. We demonstrate that you can achieve much faster speeds by closely linking the supertagger with the CCG grammar and parser. This is the first known successful attempt to combine a supertagger with a complete parser that uses a grammar created by a machine. We also cut down on options by using rules that limit how categories can be combined. The result is a very fast and accurate CCG parser compared to other systems that use similar complex rules. Our results show how the accuracy of supertagging is related to the overall accuracy of understanding sentence structure. We describe two mathematical models for CCG parsing: one for how sentences are built and another for understanding relationships between words. CCG parsing has two steps: first, the supertagger assigns likely categories to each word, and then a few rules, along with type-changing and punctuation rules, help build a structured representation using a method called the CKY algorithm. This paper explains the role of [supertagging] in a [CCG parser] that uses a [mathematical method] to choose the best analysis.\n\nThe [supertagger] makes the process more efficient by reducing the number of possible options the model needs to evaluate, which also makes training faster.\n\nCCG parsing has two steps: first, the [supertagger] assigns likely [categories] to each word, and then a few rules—along with [type-changing] and [punctuation rules]—help build a [structured representation] using a method called the [CKY algorithm].\n\nWe suggest a way to combine the [supertagger] with the [parser]: at first, only a few categories are given to each word, and more are added if the parser can’t find a complete analysis.
43	Confidence Estimation For Machine Translation We present a detailed study of confidence estimation for machine translation. Various methods for determining whether [MT] output is correct are investigated, for both [whole sentences] and [words].\n\nSince the [notion of correctness] is not intuitively clear in this context, different ways of defining it are proposed.\n\nWe introduce a [sentence-level QE system] where an [arbitrary threshold] is used to classify the [MT output] as good or bad. Since the notion of correctness is not intuitively clear in this context, different ways of defining it are proposed. We present results on data from the NIST 2003 Chinese-to-English MT evaluation. We introduce a sentence level QE system where an arbitrary threshold is used to classify the MT output as good or bad. We study sentence and word level features for translation error prediction.	Confidence Estimation For Machine Translation We present a detailed study of confidence estimation for machine translation. Various methods for determining whether MT (Machine Translation) output is correct are investigated, for both whole sentences and words. Since the [idea of correctness] is not easy to understand in this context, different ways of defining it are proposed.\nWe introduce a [sentence-level QE (Quality Estimation) system] where a chosen limit is used to classify the [MT output] as good or bad. We present results on data from the NIST 2003 Chinese-to-English MT evaluation. We introduce a sentence level QE (Quality Estimation) system where a chosen limit is used to classify the MT output as good or bad. We study sentence and word level features for translation error prediction.
560	Reranking And Self-Training For Parser Adaptation Statistical parsers trained and tested on the Penn Wall Street Journal (WSJ) treebank have shown vast improvements over the last 10 years. Much of this improvement, however, is based upon an ever-increasing number of features to be trained on (typically) the WSJ treebank data. This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres. Such worries have merit. The standard "Charniak parser" checks in at a labeled precision-recall f-measure of 89.7% on the Penn WSJ test set, but only 82.9% on the test set from the Brown treebank corpus. This paper should allay these fears. In particular, we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%. Furthermore, use of the self-training techniques described in (McClosky et al., 2006) raise this to 87.8% (an error reduction of 28%) again without any use of labeled Brown data. Much of this improvement, however, is based upon an [ever-increasing] number of features to be trained on (typically) the WSJ treebank data.:::\n This paper should allay these fears.:::\n We successfully applied self-training to parsing by [exploiting] available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation.	Reranking And Self-Training For Parser Adaptation Statistical parsers, which are computer programs that analyze sentence structure, have gotten much better over the last 10 years when they are trained and tested using the Penn Wall Street Journal (WSJ) language data set. This improvement is mostly because they are learning from more and more details in the WSJ data. However, there is a worry that these programs might work too well only on this specific data and not as well on other types of writing. N/A The commonly used "Charniak parser" scores 89.7% accuracy in understanding sentences in the WSJ test but drops to 82.9% in a different set of sentences from the Brown data set. This paper aims to ease these worries. It shows that by using a technique called reranking, which was explained by Charniak and Johnson in 2005, the parser's performance on the Brown data improves to 85.2%. Additionally, using another method called self-training, mentioned in a 2006 study by McClosky and others, raises the accuracy to 87.8%, which means 28% fewer mistakes, and this is done without using labeled Brown data, which are pre-identified examples. We used self-training, a method that learns from unlabeled or raw data, to improve sentence understanding and saw impressive results when adapting this technique to new types of writing.
44	Unsupervised Construction Of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources. Two techniques are employed: (1) simple string edit distance, and (2) a heuristic strategy that pairs initial (presumably summary) sentences from different news stories in the same cluster. We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation. We investigate [unsupervised techniques] for acquiring [monolingual sentence-level paraphrases] from a corpus of [temporally and topically clustered news articles] collected from thousands of [web-based news sources].\n\nTwo techniques are employed: (1) [simple string edit distance], and (2) a [heuristic strategy] that pairs initial (presumably summary) sentences from different news stories in the same cluster.\n\nResults show that edit-distance data is cleaner and more easily aligned than the heuristic data, with an overall [alignment error rate (AER)] of 11.58 % on a similarly extracted test set.\n\nOn test data extracted by the [heuristic strategy], however, performance of the two training sets is similar, with [AERs] of 13.2 % and 14.7 % respectively.\n\nAnalysis of 100 pairs of sentences from each set reveals that the edit-distance data lacks many of the complex [lexical and syntactic alternations] that characterize [monolingual paraphrase].\n\nThe summary sentences, while less readily alignable, retain more of the [non-trivial alternations] that are of greatest interest when learning [paraphrase relationships]. On test data extracted by the heuristic strategy, however, performance of the two training sets is similar, with AERs of 13.2% and 14.7% respectively. Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase. The summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships. we introduce Microsoft Research Paraphrase Corpus (MSRPC). We use Web-aggregated news stories to learn both sentence-level and word-level alignments.	Unsupervised Construction Of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources We look into methods that don't need human guidance for finding paraphrases, which are sentences with the same meaning but different words, from a collection of news articles grouped by time and topic from many online news sources. We use two methods: (1) simple string edit distance, which measures how different two sentences are by counting changes needed to make them the same, and (2) a heuristic strategy, which is a rule of thumb that pairs starting sentences, likely summaries, from different news stories in the same group. We assess both data sets using a tool that aligns words and a scoring method from translating languages. We look into methods that don't need human guidance for finding [paraphrases] from a [collection of news articles] grouped by time and topic from many online news sources.\n\nWe use two methods: (1) [simple string edit distance], and (2) a [heuristic strategy] that pairs initial sentences from different news stories in the same group.\n\nResults show that the [edit-distance data] is cleaner and easier to match, with an [alignment error rate (AER)] of 11.58 % on a similar test set.\n\nWe introduce the [Microsoft Research Paraphrase Corpus (MSRPC)], which leverages [web-aggregated news stories] to learn both sentence- and word-level alignments. For data from the heuristic strategy, both training sets perform similarly, with AERs of 13.2% and 14.7%. Looking at 100 sentence pairs from each set shows that the edit distance data misses some of the complex word and sentence changes typical in paraphrases. The summary sentences, though harder to match, keep more of the interesting changes useful for learning paraphrases. We introduce the Microsoft Research Paraphrase Corpus (MSRPC), which uses combined news stories from the web to learn both sentence and word alignments.
45	Language Model Adaptation For Statistical Machine Translation Via Structured Query Models We explore unsupervised language model adaptation techniques for Statistical Machine Translation. The hypotheses from the [machine translation output] are converted into [queries at different levels of representation power] and used to extract similar sentences from a [very large monolingual text collection].\n\nWe apply a slightly different [sentence-level strategy] to language-model adaptation, first generating an [nbest list] with a [baseline system], then finding similar sentences in a [monolingual target-language corpus].\n\nWe construct [specific language models] by using [machine translation output] as queries to extract similar sentences from [large monolingual corpora]. Specific language models are then build from the retrieved data and interpolated with a general background model. Experiments show significant improvements when translating with these adapted language models. We apply a slightly different sentence-level strategy to language model adaptation, first generating an nbest list with a baseline system, then finding similar sentences in a monolingual target language corpus. We construct specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. We convert initial SMT hypotheses to queries and retrieved similar sentences from a large monolingual collection.	Language Model Adaptation For Statistical Machine Translation Via Structured Query Models We explore methods to adjust language models for machine translation without human guidance. The results from the translation process are turned into search requests at different levels of detail and used to find similar sentences from a huge collection of text in one language. Specific language models are then created from this gathered data and combined with a general model. Tests show big improvements when translating with these adjusted language models. The results from the translation process are turned into [search requests] at [different levels of detail] and used to find similar sentences from a [huge collection of text] in one language.\n\nWe use a slightly different method by first creating a list of possible translations, then finding similar sentences in a [text collection] of the [target language].\n\nWe make specific language models by using [machine translation results] as [search requests] to find similar sentences in [large text collections].\n\nWe change initial [machine translation guesses] into [search requests] and find similar sentences from a [large collection of text] in one language. We make specific language models by using machine translation results as search requests to find similar sentences in large text collections. We change initial machine translation guesses into search requests and find similar sentences from a large collection of text in one language.
46	ORANGE: A Method For Evaluating Automatic Evaluation Metrics For Machine Translation Comparisons of automatic evaluation metrics for machine translation are usually conducted on corpus level using correlation statistics such as Pearson’s product moment correlation coefficient or  Spearman’s rank order correlation coefficient between human scores and automatic scores. However, such comparisons rely on human judgments of translation qualities such as adequacy and fluency. Unfortunately, these judgments are often inconsistent and very expensive to acquire. In this paper, we introduce a new evaluation method, ORANGE, for evaluating automatic machine translation evaluation metrics automatically without extra human involvement other than using a set of reference translations. We also show the results of comparing several existing automatic metrics and three new automatic metrics using ORANGE. Comparisons of [automatic evaluation metrics] for [machine translation] are usually conducted on [corpus level] using [correlation statistics] such as [Pearson’s product-moment correlation coefficient] or [Spearman’s rank-order correlation coefficient] between [human scores] and [automatic scores].\n\nIn this paper, we introduce a new evaluation method, [ORANGE], for evaluating [automatic machine-translation evaluation metrics] automatically without [extra human involvement] other than using a set of [reference translations].\n\n[BLEU] is [smoothed] (Lin and Och, 2004b), and it considers only matching up to [bigrams] because this has higher correlations with [human judgments] than when [higher-ordered n-grams] are included. Smoothed per-sentence BLEU was used as a similarity metric.	:ORANGE: A Method For Evaluating Automatic Evaluation Metrics For Machine Translation Comparisons of automatic evaluation measures for machine translation are usually done on a large set of texts using statistical methods like Pearson’s correlation coefficient or Spearman’s rank correlation coefficient. These methods compare the scores given by humans and the automatic scores. However, these comparisons depend on human opinions about how good the translation is, looking at things like how accurate and smooth it is. Unfortunately, these human opinions can be unreliable and costly to get. In this paper, we introduce a new method called ORANGE, which allows us to evaluate automatic translation evaluation tools without needing extra human input, except for using a set of reference translations. We also present the results of comparing some existing automatic tools and three new ones using ORANGE. Comparisons of [automatic evaluation measures] for [machine translation] are usually done on a large set of texts using [statistical methods] like [Pearson’s correlation coefficient] or [Spearman’s rank correlation coefficient].\n\nHowever, these comparisons depend on [human opinions] about how good the translation is, looking at things like [how accurate and smooth] it is.\n\nIn this paper, we introduce a new method called [ORANGE], which allows us to evaluate [automatic translation evaluation tools] without needing [extra human input], except for using a set of [reference translations].\n\nBLEU, a common evaluation tool, is adjusted (Lin and Och, 2004b) and it only looks at [two-word matches] because this aligns better with [human opinions] than using [longer word sequences]. Adjusted per-sentence BLEU was used as a way to measure similarity.
47	Improving A Statistical MT System With Automatically Learned Rewrite Patterns Current clump-based statistical MT systems have two limitations with respect to word ordering: First, they lack a mechanism for expressing and using generalization that accounts for reorderings of linguistic phrases. Second, the ordering of target words in such systems does not respect linguistic phrase boundaries. Current [clump-based statistical MT systems] have two limitations with respect to word ordering: first, they lack a [mechanism for expressing and using generalization] that accounts for reorderings of [linguistic phrases].\n\nTo address these limitations, we propose to use [automatically learned rewrite patterns] to preprocess the source sentences so that they have a word order similar to that of the [target language].\n\nThe basic model is statistical, but we use [broad-coverage rule-based parsers] in two ways—during [training] for learning rewrite patterns and at [runtime] for reordering the source sentences.\n\nOur [reordering rules] are automatically learned from aligning [parse trees] for both the source and target sentences. Our system is a hybrid one. The basic model is statistical, but we use broad-coverage rule-based parsers in two ways - during training for learning rewrite patterns, and at runtime for reordering the source sentences. Our experiments show 10% relative improvement in Bleu measure. We describe an approach for translation from French to English, where reordering rules are acquired automatically. Our re-ordering rules are automatically learned from aligning parse trees for both the source and target sentences.	Improving A Statistical MT System With Automatically Learned Rewrite Patterns Current group-based statistical machine translation (MT) systems have two problems with word order: First, they don't have a way to handle changes in word order that cover different language phrases. Second, the order of words in the output doesn't follow natural language phrase breaks. Current [group-based statistical machine translation (MT) systems] have two problems with word order: first, they don’t have a way to handle changes in word order that cover different language phrases.\n\nTo fix these problems, we suggest using [automatically learned rewrite patterns] to rearrange the words in the original sentences so they match the order of the target language.\n\nThe main model is statistical, but we also use [rule-based parsers] in two ways—one during [training] to learn rewrite patterns and the other while [running] to rearrange the original sentences. Our system combines different methods. The main model is statistical, but we also use rule-based parsers (tools that analyze sentence structure) in two ways - one during training to learn rewrite patterns, and the other while running to rearrange the original sentences. Our tests show a 10% improvement in the Bleu measure (a way to check translation quality). We explain a method for translating from French to English, where reordering rules are learned automatically. Our reordering rules are learned automatically by matching sentence structures in both the original and target sentences.
556	Meaningful Clustering Of Senses Helps Boost Word Sense Disambiguation Performance Fine-grained sense distinctions are one of the major obstacles to successful Word Sense Disambiguation. In this paper, we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense [hierarchies], namely the Oxford Dictionary of English.:::\n In our [coarse-grained task], the sense inventory was first clustered semi-automatically with each cluster representing an equivalence class over senses. We assess the quality of the mapping and the induced clustering, and evaluate the performance of coarse WSD systems in the Senseval-3 English all-words task. In our coarse-grained task, the sense inventory was first clustered semi-automatically with each cluster representing an equivalence class over senses. We present an automatic approach for mapping between sense inventories; here similarities in gloss definition and structured relations between the two sense inventories are exploited in order to map between WordNet senses and distinctions made within the coarser-grained Oxford English Dictionary. We argue that automatically creating new alignments is difficult because of word ambiguities, different granularities of senses, or language specific conceptualizations.	Meaningful Clustering Of Senses Helps Boost Word Sense Disambiguation Performance Fine-grained sense distinctions, which means very detailed differences between word meanings, are one of the main challenges in successfully telling apart different meanings of a word. In this paper, we present a method for making the WordNet sense inventory, a collection of word meanings, less detailed by connecting it to a manually created dictionary that organizes meanings, specifically the Oxford Dictionary of English. We check how good this connection is and how well the groupings work, and we test how well systems that handle less detailed word meanings perform in the Senseval-3 English all-words task. N/A We present an automatic way to connect different collections of word meanings; here, we use similarities in word explanations and the organized relationships between the two collections of meanings to connect WordNet meanings with the broader categories in the Oxford English Dictionary. We believe that automatically creating new connections is hard because of word ambiguities, which means words can have multiple meanings, different levels of detail in meanings, or ideas that are specific to a language.
48	Part-Of-Speech Tagging In Context We present a new HMM tagger that exploits context on both sides of a word to be tagged, and evaluate it in both the unsupervised and supervised case. Along the way, we present the first comprehensive comparison of unsupervised methods for part-of-speech tagging, noting that published results to date have not been comparable across corpora or lexicons. Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms, we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable. Finally, we show how this new tagger achieves state-of-the-art results in a supervised, non-training intensive framework. We present a new [HMM tagger] that exploits context on both sides of a word to be tagged, and evaluate it in both the [unsupervised] and [supervised] case.\n\nAlong the way, we present the first comprehensive comparison of [unsupervised methods] for [part-of-speech tagging], noting that published results to date have not been comparable across [corpora] or [lexicons].\n\nObserving that the quality of the [lexicon] greatly impacts the accuracy that can be achieved by the algorithms, we present a method of [HMM training] that improves accuracy when training of [lexical probabilities] is unstable.\n\nWhile replicating earlier experiments, we discover that performance was highly dependent on cleaning [tag dictionaries] using statistics gleaned from the tokens.\n\nWe show that the [expectation maximization algorithm] for [bi-tag HMMs] is efficient and quite effective for acquiring accurate POS taggers given only a [lexicon] ([tag dictionary]) and certain favorable conditions.\n\nWe observe that earlier [unsupervised HMM-EM] results were artificially high due to use of [Optimized Lexicons], in which only frequent-enough analyses of each word were kept. We show that he expectation maximization algorithm for bi tag HMMs is efficient and quite effective for acquiring accurate POS taggers given only a lexicon (tag dictionary) and certain favorable conditions. We observe that earlier unsupervised HMM-EM results were artificially high due to use of Optimized Lexicons, in which only frequent-enough analyses of each word were kept.	Part-Of-Speech Tagging In Context We introduce a new HMM tagger, a tool that labels words in a sentence, using the words around each word to make decisions. We test it in two different ways: without help (unsupervised) and with help (supervised). We also provide the first detailed comparison of unsupervised methods for labeling parts of speech, pointing out that previous results have not been consistent across different sets of texts or dictionaries. We also provide the first detailed comparison of [unsupervised methods] for labeling [parts of speech], pointing out that previous results have not been consistent across different sets of texts or dictionaries.\n\nWe find that the dictionary's quality greatly affects how accurate the labeling is, so we offer a way to train [HMMs] that makes the labeling more accurate when the dictionary is unstable.\n\nWhile repeating previous tests, we find that success relies on cleaning up the [tag dictionaries] with helpful data from the text.\n\nWe show that the [expectation maximization algorithm], a method for improving accuracy in tagging, works well with [HMMs] when we have a dictionary and certain favorable conditions.\n\nWe notice that past high results in [unsupervised HMM-EM] tests were due to using improved dictionaries, where only common word analyses were included. Lastly, we demonstrate that this new tagger achieves top results using a simpler method that doesn't require extensive training. While repeating previous tests, we find that success relies on cleaning up the tag dictionaries with helpful data from the text. We show that the expectation maximization algorithm, a method for improving accuracy in tagging, works well with HMMs when we have a dictionary and certain favorable conditions. We notice that past high results in unsupervised HMM-EM tests were due to using improved dictionaries, where only common word analyses were included.
49	Chinese Segmentation And New Word Detection Using Conditional Random Fields Chinese word segmentation is a difficult, important and widely-studied sequence modeling problem. This paper demonstrates the ability of [linear-chain conditional random fields (CRFs)] to perform robust and accurate Chinese word segmentation by providing a [principled framework] that easily supports the integration of domain knowledge via [multiple lexicons of characters and words].\n\nCRF is a [statistical sequence-modeling framework] introduced by [Lafferty et al. (2001)], and we use it for the Chinese word-segmentation task by treating word segmentation as a [binary decision task].\n\nWe first use this framework for Chinese word segmentation by treating it as a [binary decision task], such that each character is labeled either as the [beginning of a word] or the [continuation of one].\n\nWe define the [word-segmentation problem] as labeling each character according to whether the [previous character boundary] of the current character is a [word boundary]. We also present a probabilistic new word detection method, which further improves performance. Our system is evaluated on four datasets used in a recent comprehensive Chinese word segmentation competition. State-of-the-art performance is obtained. The superiority of CRFs on Chinese information processing was also demonstrated in word segmentation). CRF is a statistical sequence modeling framework introduced by Lafferty et al (2001), and we use it for the Chinese word segmentation task by treating word segmentation as a binary decision task. We first use this framework for Chinese word segmentation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one. We define the word segmentation problem as labeling each character as whether or not the previous character boundary of the current character is a word boundary.	Chinese Segmentation And New Word Detection Using Conditional Random Fields Chinese word segmentation is a challenging and important problem that involves breaking down sentences into words. This paper shows how a method called linear-chain conditional random fields (CRFs) can effectively handle Chinese word segmentation by providing a structured approach that easily incorporates specialized knowledge using various lists of characters and words. We also introduce a method for finding new words, which helps improve accuracy. Our system is tested on four sets of data from a recent Chinese word segmentation competition, achieving top-level results. The effectiveness of CRFs for processing Chinese information is also shown in word segmentation. This paper shows how a method called [linear-chain conditional random fields (CRFs)] can effectively handle Chinese word segmentation by providing a [structured approach] that easily incorporates [specialized knowledge] using [various lists of characters and words].\n\nCRF is a [statistical tool for analyzing sequences], introduced by Lafferty et al. (2001), and we use it for Chinese word segmentation by [treating it as a task] where you make [yes-or-no decisions].\n\nWe apply this method by [labeling each character as either the start of a new word or a continuation of the current word].\n\nWe define the word-segmentation problem by [deciding if each character is the start of a new word or continues from the previous character]. We apply this method by labeling each character as either the start of a new word or a continuation of the current word. We define the word segmentation problem by deciding if each character is the start of a new word or continues from the previous character.
557	Espresso: Leveraging Generic Patterns For Automatically Harvesting Semantic Relations In this paper, we present Espresso, a weakly-supervised, general-purpose, and accurate algorithm for harvesting semantic relations. The main contributions are: i) a method for exploiting generic patterns by filtering incorrect instances using the Web; and ii) a principled measure of pattern and instance reliability enabling the filtering algorithm. We present an empirical comparison of Espresso with various state of the art systems, on different size and [genre corpora], on extracting various general and specific relations.:::\n Experimental results show that our [exploitation] of generic patterns substantially increases system recall with small effect on overall [precision.]:::\n In the pattern [induction step], our system computes a [reliability score] for each candidate pattern based on the weighted pointwise mutual information, PMI, of the pattern with all instances extracted so far. Experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision. In the pattern induction step, our system computes a reliability score for each candidate pattern based on the weighted pointwise mutual information, PMI, of the pattern with all instances extracted so far. We induce specific reliable patterns in a bootstrapping manner for entity relation extraction. Our minimally-supervised Espresso algorithm is initialized with a single set that mixes seeds of heterogeneous types, such as leader-panel and oxygen-water, which respectively correspond to the member-of and sub-quantity-of relations in the taxonomy of Keet and Artale (2008).	Espresso: Leveraging Generic Patterns For Automatically Harvesting Semantic Relations In this paper, we introduce Espresso, a computer program that is not heavily supervised, is versatile, and accurately finds connections between meanings. Our main contributions are: i) a way to use broad patterns by removing wrong examples with help from the Internet; and ii) a careful method to judge how trustworthy patterns and examples are, which helps in filtering. We compare Espresso with other top systems on various texts of different sizes and types, to find different general and specific connections. Tests show that using broad patterns greatly improves the system's ability to find connections without much loss in accuracy. In the pattern creation step, our system calculates a trust score for each possible pattern using a method called weighted pointwise mutual information, PMI, which looks at how often the pattern appears with all examples found so far. We create specific trustworthy patterns step-by-step for finding connections between entities. Our program, which doesn't need much [supervision], starts with a single set that mixes different types of examples, like leader-panel and oxygen-water, which relate to member-of and part-of relationships as described by Keet and Artale in 2008.
559	Efficient Unsupervised Discovery Of Word Categories Using Symmetric Patterns And High Frequency Words We present a novel approach for discovering word categories, sets of words sharing a significant aspect of their meaning. We utilize meta-patterns of high-frequency words and content words in order to discover pattern candidates. We [utilize meta-patterns] of high-frequency words and content words in order to discover pattern candidates.:::\n Symmetric patterns are then identified using graph-based measures, and word categories are created based on [graph clique sets]. Our method is the first pattern-based method that requires no corpus annotation or manually provided seed patterns or words. We evaluate our algorithm on very large corpora in two languages, using both human judgments and WordNet-based evaluation. Our fully unsupervised results are superior to previous work that used a POS tagged corpus, and computation time for huge corpora are orders of magnitude faster than previously reported. We show that pairs of words that often appear together in symmetric patterns tend to belong to the same class (that is, they share some notable aspect of their semantics).	Efficient Unsupervised Discovery Of Word Categories Using Symmetric Patterns And High Frequency Words We introduce a new way to find groups of words that share similar meanings. We use common patterns of frequently used words and important words to find potential patterns. Then, we identify balanced patterns using methods based on graphs, and word groups are formed based on connected sets in these graphs. N/A We test our method on large text collections in two languages, using both human feedback and checks against WordNet, a language database. Our method, which doesn't need any pre-labeled text, works better than past methods that used labeled texts and is much faster for large text collections. We demonstrate that words that often appear together in balanced patterns usually belong to the same group, meaning they have similar meanings.
51	Towards Terascale Semantic Acquisition Although vast amounts of textual data are freely available, many NLP algorithms exploit only a minute percentage of it. In this paper, we study the challenges of working at the terascale. We present an algorithm, designed for the terascale, for mining is-a relations that achieves similar performance to a state-of-the-art linguistically-rich method. We focus on the accuracy of these two systems as a function of processing time and corpus size. We propose a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performance and efficiency. We extend is-a relation acquisition towards terascale, and automatically identified hypernym patterns by minimal edit distance. We propose, in the scenario of extracting is-a relations, one pattern-based approach and compared it with a baseline syntactic distributional similarity method (called syntactic co-occurrence in their paper).:::\n We propose a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performance and efficiency.	Towards Terascale Semantic Acquisition Although there is a lot of text data available for free, many natural language processing (NLP) methods use only a small part of it. N/A We introduce a method designed for handling terascale data that identifies "is-a" relationships (like "a dog is a mammal") and performs similarly to a very advanced language-based method. We examine how accurate these two systems are, based on how long they take to process and the size of the data set. We suggest a similar, very expandable method using an edit-distance technique (a way to measure how different two pieces of text are) to learn patterns of words and parts of speech (POS), demonstrating both good results and efficiency. We improve the process of finding "is-a" relationships on a large scale and automatically find broader category patterns using minimal text differences. We suggest a pattern-based method for finding "is-a" relationships and compare it to a basic method that uses how often words appear together in sentences (referred to as syntactic co-occurrence in their paper).
52	Characterising Measures Of Lexical Distributional Similarity This work investigates the variation in a word's distributionally nearest neighbours with respect to the similarity measure used. We identify one type of variation as being the relative frequency of the neighbour words with respect to the frequency of the target word. We then demonstrate a three-way connection between relative frequency of similar words, a concept of distributional gnerality and the semantic relation of hyponymy. Finally, we consider the impact that this has on one application of distributional similarity methods (judging the compositionality of collocations). Abstracting from results for concrete test sets, we try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends. We also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently. We analyzed the variation in a word's distribution ally nearest neighbours with respect to a variety of similarity measures. N/A	Characterising Measures Of Lexical Distributional Similarity This study looks at how the closest related words to a given word change depending on how we measure similarity. We find that one type of change is how often these related words appear compared to the main word. We then show a three-way link between how often similar words appear, a broad idea of word usage, and a specific word meaning relationship called hyponymy (where one word is a more specific version of another). Finally, we examine how this affects a method used to judge how words work together in phrases. By looking beyond specific examples, we try to find patterns and language features that affect how well similarity measures work. We also discovered that how often words appear was important in deciding which word was more general, with the more general word often appearing more. We studied how the closest related words to a word change with different ways of measuring similarity. We aimed to improve the goal of similarity to predict if one word is a broader or narrower version of another.
\.


--
-- Data for Name: user_assignments; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.user_assignments (user_id, survey_set_id) FROM stdin;
1	1
2	1
3	1
4	2
5	2
6	2
7	3
8	3
9	3
10	4
11	4
12	4
13	5
14	5
15	5
16	6
17	6
18	6
19	7
20	7
21	7
22	8
23	8
24	8
25	9
26	9
27	9
28	10
29	10
30	10
31	11
32	11
33	11
34	12
35	12
36	12
37	13
38	13
39	13
40	14
41	14
42	14
43	15
44	15
45	15
46	16
47	16
48	16
49	17
50	17
51	17
52	18
53	18
54	18
55	19
56	19
57	19
58	20
59	20
60	20
61	21
62	21
63	21
64	22
65	22
66	22
67	23
68	23
69	23
70	24
71	24
72	24
73	25
74	25
75	25
76	26
77	26
78	26
79	27
80	27
81	27
82	28
83	28
84	28
85	29
86	29
87	29
88	30
89	30
90	30
91	31
92	31
93	31
94	32
95	32
96	32
97	33
98	33
99	33
100	34
101	34
102	34
103	35
104	35
105	35
106	36
107	36
108	36
109	37
110	37
111	37
112	38
113	38
114	38
115	39
116	39
117	39
118	40
119	40
120	40
121	41
122	41
123	41
124	42
125	42
126	42
127	43
128	43
129	43
130	44
131	44
132	44
133	45
134	45
135	45
136	46
137	46
138	46
139	47
140	47
141	47
142	48
143	48
144	48
145	49
146	49
147	49
148	50
149	50
150	50
151	51
152	51
153	51
154	52
155	52
156	52
157	53
158	53
159	53
160	54
161	54
162	54
163	55
164	55
165	55
166	56
167	56
168	56
169	57
170	57
171	57
172	58
173	58
174	58
175	59
176	59
177	59
178	60
179	60
180	60
181	61
182	61
183	61
184	62
185	62
186	62
187	63
188	63
189	63
190	64
191	64
192	64
193	65
194	65
195	65
196	66
197	66
198	66
199	67
200	67
201	67
202	68
203	68
204	68
205	69
206	69
207	69
208	70
209	70
210	70
211	71
212	71
213	71
214	72
215	72
216	72
217	73
218	73
219	73
220	74
221	74
222	74
223	75
224	75
225	75
226	76
227	76
228	76
229	77
230	77
231	77
232	78
233	78
234	78
235	79
236	79
237	79
238	80
239	80
240	80
241	81
242	81
243	81
244	82
245	82
246	82
247	83
248	83
249	83
250	84
251	84
252	84
253	85
254	85
255	85
256	86
257	86
258	86
259	87
260	87
261	87
262	88
263	88
264	88
265	89
266	89
267	89
268	90
269	90
270	90
271	91
272	91
273	91
274	92
275	92
276	92
277	93
278	93
279	93
280	94
281	94
282	94
283	95
284	95
285	95
286	96
287	96
288	96
289	97
290	97
291	97
292	98
293	98
294	98
295	99
296	99
297	99
298	100
299	100
300	100
301	101
302	101
303	101
304	1
305	1
306	1
307	2
308	2
309	2
350	51
351	51
352	51
353	52
354	52
355	52
356	53
357	53
358	53
359	54
360	54
361	54
362	55
363	55
364	55
365	56
366	56
367	56
368	57
369	57
370	57
371	58
372	58
373	58
374	59
375	59
376	59
377	60
378	60
379	60
380	61
381	61
382	61
383	62
384	62
385	62
386	63
387	63
388	63
389	64
390	64
391	64
392	65
393	65
394	65
395	66
396	66
397	66
398	67
399	67
400	67
401	68
402	68
403	68
404	69
405	69
406	69
407	70
408	70
409	70
410	71
411	71
412	71
413	72
414	72
415	72
416	73
417	73
418	73
419	74
420	74
421	74
422	75
423	75
424	75
425	76
426	76
427	76
428	77
429	77
430	77
431	78
432	78
433	78
434	79
435	79
436	79
437	80
438	80
439	80
440	81
441	81
442	81
443	82
444	82
445	82
446	83
447	83
448	83
449	84
450	84
451	84
452	85
453	85
454	85
455	86
456	86
457	86
458	87
459	87
460	87
461	88
462	88
463	88
464	89
465	89
466	89
467	90
468	90
469	90
470	91
471	91
472	91
473	92
474	92
475	92
476	93
477	93
478	93
479	94
480	94
481	94
482	95
483	95
484	95
485	96
486	96
487	96
488	97
489	97
490	97
491	98
492	98
493	98
494	99
495	99
496	99
497	100
498	100
499	100
500	101
\.


--
-- Data for Name: users; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.users (user_id, survey_token, assigned) FROM stdin;
470	\N	f
471	\N	f
472	\N	f
473	\N	f
474	\N	f
475	\N	f
476	\N	f
477	\N	f
478	\N	f
479	\N	f
480	\N	f
481	\N	f
482	\N	f
483	\N	f
484	\N	f
485	\N	f
486	\N	f
487	\N	f
166	\N	f
167	\N	f
168	\N	f
169	\N	f
170	\N	f
171	\N	f
172	\N	f
173	\N	f
174	\N	f
175	\N	f
176	\N	f
177	\N	f
178	\N	f
179	\N	f
180	\N	f
181	\N	f
182	\N	f
183	\N	f
184	\N	f
185	\N	f
186	\N	f
187	\N	f
188	\N	f
189	\N	f
190	\N	f
191	\N	f
192	\N	f
193	\N	f
194	\N	f
195	\N	f
196	\N	f
197	\N	f
198	\N	f
199	\N	f
200	\N	f
201	\N	f
202	\N	f
203	\N	f
204	\N	f
205	\N	f
206	\N	f
207	\N	f
208	\N	f
209	\N	f
210	\N	f
211	\N	f
212	\N	f
213	\N	f
214	\N	f
215	\N	f
216	\N	f
217	\N	f
218	\N	f
219	\N	f
220	\N	f
221	\N	f
222	\N	f
223	\N	f
224	\N	f
225	\N	f
226	\N	f
227	\N	f
228	\N	f
229	\N	f
230	\N	f
231	\N	f
232	\N	f
233	\N	f
234	\N	f
235	\N	f
236	\N	f
237	\N	f
238	\N	f
239	\N	f
240	\N	f
241	\N	f
242	\N	f
243	\N	f
244	\N	f
245	\N	f
246	\N	f
488	\N	f
489	\N	f
490	\N	f
491	\N	f
492	\N	f
102	\N	f
103	\N	f
104	\N	f
105	\N	f
106	\N	f
107	\N	f
108	\N	f
109	\N	f
110	\N	f
111	\N	f
112	\N	f
113	\N	f
114	\N	f
115	\N	f
116	\N	f
117	\N	f
118	\N	f
119	\N	f
120	\N	f
121	\N	f
122	\N	f
123	\N	f
124	\N	f
125	\N	f
126	\N	f
127	\N	f
128	\N	f
129	\N	f
130	\N	f
131	\N	f
132	\N	f
133	\N	f
134	\N	f
135	\N	f
136	\N	f
137	\N	f
138	\N	f
139	\N	f
140	\N	f
141	\N	f
142	\N	f
143	\N	f
144	\N	f
145	\N	f
146	\N	f
147	\N	f
148	\N	f
149	\N	f
150	\N	f
151	\N	f
152	\N	f
153	\N	f
154	\N	f
155	\N	f
156	\N	f
157	\N	f
158	\N	f
159	\N	f
160	\N	f
161	\N	f
162	\N	f
163	\N	f
164	\N	f
165	\N	f
247	\N	f
1	\N	t
248	\N	f
249	\N	f
250	\N	f
251	\N	f
252	\N	f
253	\N	f
254	\N	f
255	\N	f
256	\N	f
257	\N	f
258	\N	f
259	\N	f
260	\N	f
261	\N	f
262	\N	f
263	\N	f
264	\N	f
265	\N	f
266	\N	f
267	\N	f
268	\N	f
269	\N	f
270	\N	f
271	\N	f
272	\N	f
273	\N	f
274	\N	f
275	\N	f
276	\N	f
277	\N	f
278	\N	f
279	\N	f
280	\N	f
281	\N	f
282	\N	f
283	\N	f
284	\N	f
285	\N	f
286	\N	f
17	\N	f
18	\N	f
19	\N	f
20	\N	f
21	\N	f
22	\N	f
23	\N	f
24	\N	f
25	\N	f
26	\N	f
27	\N	f
28	\N	f
29	\N	f
30	\N	f
31	\N	f
32	\N	f
33	\N	f
34	\N	f
35	\N	f
36	\N	f
37	\N	f
38	\N	f
39	\N	f
40	\N	f
41	\N	f
42	\N	f
43	\N	f
44	\N	f
45	\N	f
46	\N	f
47	\N	f
48	\N	f
49	\N	f
50	\N	f
51	\N	f
52	\N	f
53	\N	f
54	\N	f
55	\N	f
56	\N	f
57	\N	f
58	\N	f
59	\N	f
60	\N	f
61	\N	f
62	\N	f
63	\N	f
64	\N	f
65	\N	f
66	\N	f
67	\N	f
68	\N	f
69	\N	f
70	\N	f
2	\N	t
3	\N	t
4	\N	t
5	\N	t
6	\N	t
7	\N	t
8	\N	t
9	\N	t
10	\N	t
11	\N	t
12	\N	t
13	\N	t
14	\N	t
15	\N	t
16	\N	t
71	\N	f
72	\N	f
73	\N	f
74	\N	f
75	\N	f
76	\N	f
77	\N	f
78	\N	f
79	\N	f
80	\N	f
81	\N	f
82	\N	f
83	\N	f
84	\N	f
85	\N	f
86	\N	f
87	\N	f
88	\N	f
89	\N	f
90	\N	f
91	\N	f
92	\N	f
93	\N	f
94	\N	f
95	\N	f
96	\N	f
97	\N	f
98	\N	f
99	\N	f
100	\N	f
101	\N	f
416	\N	f
417	\N	f
418	\N	f
419	\N	f
420	\N	f
421	\N	f
422	\N	f
423	\N	f
424	\N	f
425	\N	f
426	\N	f
427	\N	f
428	\N	f
429	\N	f
430	\N	f
431	\N	f
432	\N	f
433	\N	f
434	\N	f
435	\N	f
436	\N	f
437	\N	f
438	\N	f
439	\N	f
440	\N	f
441	\N	f
442	\N	f
443	\N	f
444	\N	f
445	\N	f
446	\N	f
447	\N	f
448	\N	f
449	\N	f
450	\N	f
451	\N	f
452	\N	f
453	\N	f
454	\N	f
455	\N	f
456	\N	f
457	\N	f
458	\N	f
459	\N	f
460	\N	f
461	\N	f
462	\N	f
463	\N	f
464	\N	f
465	\N	f
466	\N	f
467	\N	f
468	\N	f
469	\N	f
350	\N	t
351	\N	t
352	\N	t
353	\N	t
287	\N	f
288	\N	f
289	\N	f
290	\N	f
291	\N	f
292	\N	f
293	\N	f
294	\N	f
295	\N	f
296	\N	f
297	\N	f
298	\N	f
299	\N	f
300	\N	f
301	\N	f
302	\N	f
303	\N	f
371	\N	f
372	\N	f
373	\N	f
374	\N	f
375	\N	f
376	\N	f
377	\N	f
378	\N	f
379	\N	f
380	\N	f
381	\N	f
382	\N	f
383	\N	f
384	\N	f
385	\N	f
386	\N	f
387	\N	f
388	\N	f
389	\N	f
390	\N	f
391	\N	f
392	\N	f
393	\N	f
394	\N	f
395	\N	f
396	\N	f
397	\N	f
398	\N	f
399	\N	f
400	\N	f
401	\N	f
402	\N	f
403	\N	f
404	\N	f
405	\N	f
406	\N	f
407	\N	f
408	\N	f
409	\N	f
410	\N	f
411	\N	f
412	\N	f
413	\N	f
414	\N	f
415	\N	f
493	\N	f
494	\N	f
495	\N	f
496	\N	f
497	\N	f
498	\N	f
499	\N	f
500	\N	f
354	\N	t
355	\N	t
356	\N	t
357	\N	t
358	\N	t
359	\N	t
360	\N	t
361	\N	t
362	\N	t
363	\N	t
364	\N	t
365	\N	t
366	\N	t
367	\N	t
368	\N	t
369	\N	t
370	\N	t
\.


--
-- Name: passages_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.passages_id_seq', 1010, true);


--
-- Name: responses_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.responses_id_seq', 155, true);


--
-- Name: survey_set_passages_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.survey_set_passages_id_seq', 1011, true);


--
-- Name: survey_sets_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.survey_sets_id_seq', 101, true);


--
-- Name: users_user_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.users_user_id_seq', 1, false);


--
-- Name: passages passages_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.passages
    ADD CONSTRAINT passages_pkey PRIMARY KEY (id);


--
-- Name: responses responses_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.responses
    ADD CONSTRAINT responses_pkey PRIMARY KEY (id);


--
-- Name: survey_set_passages survey_set_passages_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.survey_set_passages
    ADD CONSTRAINT survey_set_passages_pkey PRIMARY KEY (id);


--
-- Name: survey_sets survey_sets_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.survey_sets
    ADD CONSTRAINT survey_sets_pkey PRIMARY KEY (id);


--
-- Name: updated_passages updated_passages_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.updated_passages
    ADD CONSTRAINT updated_passages_pkey PRIMARY KEY (id);


--
-- Name: user_assignments user_assignments_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.user_assignments
    ADD CONSTRAINT user_assignments_pkey PRIMARY KEY (user_id);


--
-- Name: users users_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.users
    ADD CONSTRAINT users_pkey PRIMARY KEY (user_id);


--
-- Name: users users_survey_token_key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.users
    ADD CONSTRAINT users_survey_token_key UNIQUE (survey_token);


--
-- Name: survey_set_passages survey_set_passages_passage_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.survey_set_passages
    ADD CONSTRAINT survey_set_passages_passage_id_fkey FOREIGN KEY (passage_id) REFERENCES public.passages(id) ON DELETE CASCADE;


--
-- Name: survey_set_passages survey_set_passages_survey_set_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.survey_set_passages
    ADD CONSTRAINT survey_set_passages_survey_set_id_fkey FOREIGN KEY (survey_set_id) REFERENCES public.survey_sets(id) ON DELETE CASCADE;


--
-- PostgreSQL database dump complete
--

